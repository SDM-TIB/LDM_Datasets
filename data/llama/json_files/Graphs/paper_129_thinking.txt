

<think>
To extract datasets from the research paper titled "SPATIO-TEMPORAL CROP CLASSIFICATION ON VOLUMETRIC DATAMuhammad Usman Qadeer, Salar Saeed, Murtaza Taj and Abubakr Muhammad1202raM81]VC.sc[1v05001.3012:viXraABSTRACTLarge-area crop classiication using multi-spectral imageryis a widely studied problem for several decades and is gen-erally addressed using classical Random Forest classiier.Recently, deep convolutional neural networks (DCNN) havebeen proposed. However, these methods only achieved resultscomparable with Random Forest.In this work, we presenta novel CNN based architecture for large-area crop classi-ication. Our methodology combines both spatio-temporalanalysis via 3D CNN as well as temporal analysis via 1DCNN. We evaluated the eficacy of our approach on Yolo andImperial county benchmark datasets. Our combined strategyoutperforms both classical as well as recent DCNN basedmethods in terms of classiication accuracy by 2% whilemaintaining a minimum number of parameters and the lowestinference time.Index Termsa Satellite data, CNN, Crop Classiication1. INTRODUCTIONThe freely available multispectral satellite imagery and ad-vancement in modern machine learning have paved the wayfor a wide variety of applications. These include disaster as-sessment [1], crop classiication [2, 3], urbanization [4] andenvironment monitoring [5]. The increasing quality and reso-lution of available remote sensing imagery have made it pos-sible to perform robust crop monitoring and yield estimationover large areas [6]. For instance, the Sentinel-2 satellite im-agery with a spatial resolution of 10m and Landsat-8 imagerywith a spatial resolution of 30m is available every ive daysand 16 days, respectively. Thus, enabling us to perform de-tailed spatio-temporal analysis [7].Supervised learning is considered to be the state-of-the-artapproach to produce crop maps. Some traditional machinelearning algorithms like Support Vector Machines (SVM)[8], K-nearest neighbors (KNN) [9], Cart [10], and RandomForest [11] have been widely applied for crop classiication.Among these methods, those using a series of images at dif-ferent time stamps [12] have shown better results than the oneusing a single image [8].Due to the popularity of deep learning, recently,theproblem of crop classiication has also been attempted usingdifferent deep learning architectures. In earlier work, a 1Dconvolutional neural network model has been proposed [13]which stacked features of different time stamps as in the caseof random forest. To use the temporal information more ef-iciently,[14] has proposed a new 1D CNN model. Thismethod feeds multivariate time series to 1D CNN insteadof stacked values of features. Similarly, a new 1D CNNarchitecture based on the idea of the Inception network hasbeen introduced [15]. It has shown improved performance ontraditional machine learning algorithms, including RandomForest and XGboost [16].The 2D convolutional neural networks show outstandingimage classiication performance, and many architectures,including Alexnet [17], Inception, Resnets [18], etc., havebeen introduced. A new dataset comprising Sentinel-2 im-ages of different landcover types from around the world hasbeen developed [19] and different 2D CNN architectures, in-cluding Resnet50 and GoogleNet [20], have been comparedto classify the multispectral images. Recently, an ensem-ble of 2D CNN has been used for crop classiication usingmulti-temporal Sentinel-1 and Landsat-8 imagery [21]. Thisstudy shows the signiicance of using spatial information inaddition to temporal one.The recurrent neural networks are very popular for ap-plications where time-series data is used. The idea of us-ing LSTM [22] has also been applied to landcover classiica-tion using multi-temporal SAR imagery [15]. This work hasclaimed that LSTM performs worse than Random Forest clas-siier for crop classiication using multispectral imagery. Therecurrent convolutional neural network (R-CNN) [23] usesLSTM followed by 2D CNN, but unlike [21], this architectureuses the information of only one pixel. The idea of combin-ing CNN and LSTM has also been introduced for landcoverclassiication [24][25].The 3D convolutional neural networks are widely consid-ered for applications related to video, medical imaging, andIn [27], a 3D CNN architecture hasremote sensing [26].been presented showing better performance than 2D CNNas it uses the temporal information better than the 2D CNNwhere timestamps of all features are stacked on one axis.Existing literature suggests various ways to exploit avail-able spatio-temporal data [26, 27] however, these methodsare either good at utilizing spatial information or tempo-ral information but not both. We instead propose a novelDCNN based architecture that combines both spatial as wellas temporal analysis. In the irst stage of our architecture, wepropose to use 3D convolutions that perform spatio-temporalanalysis without collapsing the temporal dimension. Once Fig. 1. Proposed model architectures showing combination of spatio-temporal and temporal convolution blocks. Spatio-temporal block via 3D CNN are shown in blue whereas the temporal only block via 1D convolution are shown in green.the spatio-temporal features are extracted via multiple 3DCNN layers, we introduce temporal only analysis to furtherextract important information from the temporal dimensiononly. This spatio-temporal followed by temporal only analy-sis helps eliminate noise, usually present in the temporal onlyanalysis. Thus the proposed approach outperforms classicalas well as state-of-art-methods on benchmark datasets.2. METHODOLOGY2.1. Data Pre-processingWe have divided the pre-processing of data into ive steps.The irst step is to select all the images of a multispectralsatellite for the region of interest in a suitable period, cov-ering every cropas cropping season under consideration. Forthis purpose, the NDVI time series for different crops havebeen examined. The second step is to select the least cloudyimages in that period.In our case, the images with cloudcover less than 10% have been selected. The third step is thecloud masking of the chosen images. The fourth step is totake the median of images chosen for each month. This stepis for larger areas, as in our case, because the region canat becovered in a single image of a satellite. In the ifth and inalstep, the missing pixels due to the cloud or other reasons havebeen illed, which can be done using simple interpolation orlinear regression. All these pre-processing steps have beenperformed on the Google Earth Engine Platform.2.2. Feature SelectionFor each image in our dataset, 13 features have been se-lected. The selected features have six multi-spectral satelliteimagery bands, including Blue, Green, Red, Near Infrared(NIR), and two Short Wave Infrared (SWIR) bands. Table 1shows the selected Sentinel-2 and Landsat-8 bands. The otherseven features are indices derived from these bands, includingNormalized Difference Vegetation Index (NDVI), EnhancedVegetation Index, Green Normalized Difference VegetationIndex (GNDVI), Soil Adjusted Vegetation Index (SAVI),Bare Soil Index (BSI), Normalized Difference Water Index(NDWI) and Normalized Difference Buildup Index (NDBI).These indices [28, 29] are calculated as follows:NDVI = N IRaREDN IR+RED, GNDVI = N IRaGREENN IR+GREENEVI = 2.5 AN IRaREDN IR+6AREDa7.5ABLU E+1SAVI = 1.5 A N IRaREDN IR+RED+0.5BSI = (SW IR+RED)a(N IR+BLU E)(SW IR+RED)+(N IR+BLU E)NDBI = SW IR1aN IRSW IR1+N IR, NDWI = GREEN aN IRGREEN +N IR2.3. Model ArchitectureThe model architecture is shown in Fig. 1. The model consistsof three parts concatenated to each other. The irst part shownin blue performs spatio-temporal convolution using 3D CNN.The second part shown in green performs the temporal con-volution. The output of 3D CNN is squeezed before feedingto 1D CNN. These two parts extract the features from input.The third part consists of a fully connected neural network,which predicts the label from multi-temporal input images.The model input shape is (r, r, t, c) where r is the spa-tial dimension of input, t is the number of timestamps, and cis the number of channels. In our implementation, spatial di-mension r is 7, timestamps t are 9 and channels c are 13. Eachconvolutional block has a batch normalization layer before ac-tivation. It has been observed that with batch-normalization,the model converges faster as well as shows better accuracy.Table 1. Selected Bands of Landsat-8 and Sentinel-2BandsBlue Green Red NIR SWIR1 SWIR2Sentinel-2Landsat-8B2B2B3B3B4B4B8B5B11B6B12B7Table 2. Summary of survey data from Imperial and Yolo counties provided by California Department of Water Re-sources (CDWR)Imperial CountyYolo CountyClassAlfalfaPasturesLettuceWheatOnionsTruck CropsCornField CropsSubtropicalOthersParcels %ageCountareaClassParcels %ageCountarea2156115061931522910262321922711066CornField cropsAlfalfa31.3% Rice15.4% Safiower8.2%4.3%3.0%11.6% Pastures3.5%2.9%1.6%18.2% AlmondsCucurbitsTomatoesTruck CropsDeciduousSubtropicalVineyardOthers350161150407575440168496289799867176688187411.2%2.4%2.1%6.6%8.7%4.3%1.8%9.8%1.3%8.9%5.3%1.1%5.6%30.9%tional Neural Network, the model proposed in [27] using theconcept of VGG has been used, but the number of ilters hasbeen increased to 64 and 128 in both layers. Our proposedmethod has the same architecture as explained in Sec. 2.All the deep learning networks have been trained for about50 epochs using Adam optimizer with a default learning rateof 0.001. The batch size of 128 has been used, and each net-workas best results have been recorded. The accuracy andweighted F1-score for each classiier are mentioned in theTables 3. The table shows the pixel-wise accuracy and F1-score as well as the parcel-wise results.It is evident fromboth tables that our proposed model has outperformed all theother classiiers in both experiments. Also, other deep learn-ing models donat show much better accuracy than the randomforest, and parcel-wise accuracy for the Imperial county ofrandom forest is greater than other deep learning models ex-cept for our proposed one. On the other hand, our 3Da1DCNN model improves the accuracy and F1-score by 2%. Theparcel-wise results have been calculated by assigning eachparcel a label based on the majority voting of pixels in it.Contrary to our expectation, the parcel-wise accuracy is4 a 5% less than pixel-wise accuracy for each classiier in thecase of Yolo County. The Yolo County survey data analysisrevealed that 3.5% parcels in test data were tagged as multi-use, i.e., more than one crop was sown in those ields.Incontrast, Imperial County data have no ield labeled as multi-use. Therefore, accuracy increased in the case of ImperialCounty while decreased in the case of Yolo County. Anotherreason is the presence of larger polygons in the test set of YoloCounty.The Qualitative Analysis is shown in Fig. 3 which showsthe original labels from CDWR data and results produced by3D CNN [27] and our proposed 3Da1D CNN model. In theFig. 2. The regions of interest showing training (red) and test(blue) data distribution for Yolo (left) and Imperial Counties(right) distribution.3. RESULTS AND EVALUATION3.1. Experimental SetupThe experiments have been performed on two different Cali-fornia counties, including Imperial County and Yolo County.For both regions, the crop survey data provided by the Cali-fornia Department of Water Resources (CDWR) [30] for theyear 2016 have been used as ground truth information.For Imperial County, 10 major classes have been consid-ered, while 14 classes have been taken into account for YoloCounty. The aOthersa class contains either minor crop types,fallow land, or build up. There are about 7256 and 7340 to-tal parcels for Imperial County and Yolo County, respectively,as mentioned in Table 2. These parcels have been split into60% training data and 40% test data. The ields have beendivided into training and test sets before sampling to ensurethat no sample from the training region is included in the testset. The Fig. 2 shows the distribution of training and testingparcels. The imagery of Sentinel-2 and Landsat-8 has beenused for Imperial County and Yolo County, respectively. Inboth cases, nine scenes from March to November 2016 havebeen used. The same dataset has been used to train all theclassiiers, including proposed and state-of-the-art methods,in both pixel and parcel-based analysis.3.2. Comparative AnalysisWe compared our proposed hybrid model with four state-of-the-art methods, namely Random Forest, 1D CNN [15], 2DCNN[21] and 3D CNN[27]For random forest, the number of trees has been set to100, the minimum number of samples required to split a nodeis two, and the maximum number of features is set to thesquare root of the number of features. For the 1D Convolu-tional Neural Network, the architecture proposed in [15] hasbeen used except that more features have been used insteadof just EVI. The idea given by [14] has been applied to con-sider more features in 1D CNN. For 2D Convolutional NeuralNetwork, ive ensembles of 2D CNN have been used as pro-posed in [21]. The number of ilters in the network ensemblesis 160, 170, 180, 190, and 200. Due to the larger number ofinput features, more ilters have to be used. For 3D Convolu-Table 3. Table showing comparison of our proposed 3Da1D CNN with state-of-the-art methods on Imperial and Yolo counties.The comparison is performed using pixel-wise and parcel-wise strategies (provides single classiication within parcel boundary)ClassiierPixel-wiseParcel-wisePixel-wiseParcel-wiseImperial CountyYolo CountyAccuracyF1-Score AccuracyF1-Score AccuracyF1-Score AccuracyF1-ScoreRandom Forest1D CNN [15]2D CNN [21]3D CNN [27]3Da1D CNN (Ours)78.96%79.46%79.71%79.95%81.23%78.60%79.05%79.48%79.70%81.16%80.61%80.33%80.54%80.80%81.76%80.39%80.24%80.40%80.72%81.76%86.68%87.94%88.50%88.01%90.23%86.25%87.80%88.36%87.82%90.08%82.23%84.45%84.55%83.17%85.87%81.21%83.90%84.18%82.74%85.51%(a) Ground Truth(b) 3D CNN[27](c) 3Da1D CNN (Ours)Fig. 3. Ground truth and maps produced for Imperial County (Top) and Yolo County (Bottom) by 3D CNN and our network.igure, there are some blank spaces where ground truth wasnot available, and it can be observed that those blank spaceshave been predicted as aOthersa represented by black color.The zoomed portion of the results in Fig. 3 further illustratesthat the maps produced by our hybrid model have lesser noisecompared to the ones produced by vanilla 3D CNN.Table 4. Performance Comparison of State of the art modelsParametersInference time (ms)Classiier1D CNN [15]2D CNN [21]3D CNN [27]3Da1D CNN (Ours)1,083,9704,278,282394,050361,4066.397.114.724.583.3. Computational CostTable 4 shows the comparison of the number of parametersin different deep learning models and their inference time.Our model has a lesser number of parameters as well as thefastest inference time. This is because, instead of iatteningthe output of 3D CNN, we fed it to 1D CNN, which reducedthe number of parameters and improved the performance.4. CONCLUSIONIn this paper, a new architecture and a method for large-areacrop classiication have been presented. Our hybrid modelcombines the spatio-temporal representation via 3D CNNwith temporal only representation via 1D CNN. Differentneural network architectures proposed in the past for cropmapping have been implemented and compared with ourdesigned model. Experiments have been performed usingmultispectral imagery of different satellites to show the gen-erality of our method. The experimentation has revealed thatthe proposed hybrid network performs better on benchmarkCDWR datasets.In the future, we aim to extend our approach to ields thathave been marked as multi-use (having more than one crop ina small parcel) as multi-use is a more commonly observedpattern in agriculture-based economies, particularly amongdeveloping countries.5. REFERENCES[1] P. Dao and Y. Liou, aObject-based iood mapping and affectedrice ield estimation with Landsat 8 oli and modis data,a Re-mote Sensing, vol. 7, no. 5, pp. 5077a5097, 2015.[2] O. Csillik et al., aObject-based time-constrained dynamic timewarping classiication of crops using Sentinel-2,a Remote sens-ing, vol. 11, no. 10, pp. 1257, 2019.[3] N. Kobayashi, H. Tani, X. Wang, and R. Sonobe, aCrop clas-siication using spectral indices derived from Sentinel-2a im-agery,a Journal of Information and Telecommunication, vol. 4,no. 1, pp. 67a90, 2020.[4] W. Sultani A. Shakeel and M. Ali, aDeep built-structure count-ing in satellite imagery using attention based re-weighting,a inISPRS Journal of Photogrammetry and Remote Sensing, 2019.[5] C. Donlon et al., aThe global monitoring for environment andsecurity (gmes) Sentinel-3 mission,a Remote Sensing of Envi-ronment, vol. 120, pp. 37a57, 2012.[6] M. Battude et al., aEstimating maize biomass and yield overlarge areas using high spatial and temporal resolution Sentinel-2 like remote sensing data,a Remote Sensing of Environment,vol. 184, pp. 668a681, 2016.[7] F. Vuolo et al., aHow much does multi-temporal Sentinel-2data improve crop type classiication?,a International journalof applied earth observation and geoinformation, vol. 72, pp.122a130, 2018.[8] R Saini and SK Ghosh, aCrop classiication on single dateSentinel-2 imagery using random forest and suppor vector ma-chine.,a International Archives of the Photogrammetry, RemoteSensing & Spatial Information Sciences, 2018.[9] P. T. Noi and M. Kappas, aComparison of random forest, k-nearest neighbor, and support vector machine classiiers forland cover classiication using Sentinel-2 imagery,a Sensors,vol. 18, no. 1, pp. 18, 2018.[10] R. Sonobe et al., aAn experimental comparison between kelmand cart for crop classiication using Landsat-8 oli data,a Geo-carto international, vol. 32, no. 2, pp. 128a138, 2017.[11] R. Galiano et al., aAn assessment of the effectiveness of arandom forest classiier for land-cover classiication,a ISPRSJournal of Photogrammetry and Remote Sensing, vol. 67, pp.93a104, 2012.[12] C. GA omez et al., aOptical remotely sensed time series data forland cover classiication: A review,a ISPRS Journal of Pho-togrammetry and Remote Sensing, vol. 116, pp. 55a72, 2016.[13] Z. Zhou, S. Li, and Y. Shao,aCrops classiication fromSentinel-2a multi-spectral remote sensing images based onconvolutional neural networks,a in IEEE Int. Geoscience andRemote Sensing Symposium, 2018, pp. 5300a5303.[14] C. Pelletier, G. I. Webb, and F. Petitjean, aTemporal convo-lutional neural network for the classiication of satellite imagetime series,a Remote Sensing, vol. 11, no. 5, pp. 523, 2019.[15] L. Zhong, L. Hu, and H.a Zhou, aDeep learning based multi-temporal crop classiicationa,a Remote sensing of environment,vol. 221, pp. 430a443, 2019.[16] A. M. Abdi, aLand cover and land use classiication perfor-mance of machine learning algorithms in a boreal landscapeusing Sentinel-2 data,a GIScience & Remote Sensing, vol. 57,no. 1, pp. 1a20, 2020.[17] A. Krizhevsky et al., aImagenet classiication with deep con-volutional neural networks,a Advances in Neural InformationProcessing Systems, vol. 25, pp. 1097a1105, 2012.[18] K. He, X. Zhang, S. Ren, and J. Sun, aDeep residual learningfor image recognition,a in Proceedings of the IEEE Int. Conf.on Computer Vision and Pattern Recognition, 2016, pp. 770a778.[19] P Helber et al., aEurosat: A novel dataset and deep learningbenchmark for land use and land cover classiication,a IEEEJournal of Selected Topics in Applied Earth Observations andRemote Sensing, vol. 12, no. 7, pp. 2217a2226, 2019.[20] C. Szegedy et al., aGoing deeper with convolutions,a in Pro-ceedings of the IEEE Int. Conf. on Computer Vision and Pat-tern Recognition, 2015, pp. 1a9.[21] K. Nataliia et al., aDeep learning classiication of land coverand crop types using remote sensing data,a IEEE Geoscienceand Remote Sensing Letters, vol. 14, no. 5, pp. 778a782, 2017.[22] Y. Zhou et al., aLong-short-term-memory-based crop classii-cation using high-resolution optical images and multi-temporalsar data,a GIScience & Remote Sensing, vol. 56, no. 8, pp.1170a1191, 2019.[23] V. Mazzia, A. Khaliq, and M. Chiaberge, aImprovementiland cover and crop classiication based on temporal featureslearning from Sentinel-2 data using recurrent-convolutionalneural network (r-cnn),a Applied Sciences, vol. 10, no. 1, pp.238, 2020.[24] G. Kwak et al., aCombining 2d cnn and bidirectional lstm toconsider spatio-temporal features in crop classiication,a Ko-rean Journal of Remote Sensing, vol. 35, no. 5 1, pp. 681a692,2019.[25] M. RuAwurm and M. KA orner, aMulti-temporal land cover clas-siication with sequential recurrent encoders,a ISPRS Interna-tional Journal of Geo-Information, vol. 7, no. 4, pp. 129, 2018.[26] M. A. Bhimra, U. Nazir, and M. Taj, aUsing 3d residual net-work for spatio-temporal analysis of remote sensing data,a inIEEE Int. Conf. on Acoustics, Speech and Signal Processing,2019, pp. 1403a1407.[27] S. Ji, C. Zhang, A. Xu, Y. Shi, and Y. Duan, a3d convolutionalneural networks for crop classiication with multi-temporal re-mote sensing images,a Remote Sensing, vol. 10, no. 1, pp. 75,2018.[28] M. Pal and K. Antil, aComparison of Landsat 8 and Sentinel2 data for accurate mapping of built-up area and bare soil,a inAsian Conf. on Remote Sensing: Space Applications: TouchingHuman Lives Paper, 2017, pp. 137a140.[29] E. Vermote et al., aPreliminary analysis of the performanceof the Landsat 8/oli land surface reiectance product,a RemoteSensing of Environment, vol. 185, pp. 46a56, 2016.[30] California Department of Water Resources (CDWR), a2016statewide crop mapping gis shapeiles,a Accessed Jan. 07, 2020[online], Available: https://data.cnra.ca.gov/dataset/statewide-crop-mapping
</think>