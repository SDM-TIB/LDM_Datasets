>

<think>
To extract datasets from the research paper titled "Aligning Cross-Lingual Entities with Multi-Aspect Information" by Hsiu-Wei Yang et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the use of two benchmark datasets: DBP15K and DBP100K.

In the **introduction**, the authors discuss the task of cross-lingual entity alignment and the importance of multilingual knowledge graphs.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention two benchmark datasets:

1. **DBP15K**: A dataset containing 15,000 ILLs, divided into three subsets: Chinese-English (ZH-EN), Japanese-English (JA-EN), and French-English (FR-EN).
2. **DBP100K**: A dataset containing 100,000 ILLs, also divided into three subsets: Chinese-English (ZH-EN), Japanese-English (JA-EN), and French-English (FR-EN).

In the **results section**, the authors confirm that these datasets were used for extrinsic evaluation of their models.

Now, I will check the **references section** to find the full citations for these datasets:

- For **DBP15K**, the citation is:
  > Sun et al. (2017) Cross-lingual entity alignment via joint attribute embedding.

- For **DBP100K**, the citation is:
  > Sun et al. (2017) Cross-lingual entity alignment via joint attribute embedding.

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>