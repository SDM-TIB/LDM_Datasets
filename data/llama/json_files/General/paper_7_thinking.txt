>

<think>
To extract datasets from the research paper titled "Efficient Contextual Representation Learning Without Softmax Layer" by Liunian Harold Lia, Patrick H. Chena, Cho-Jui Hsieha, Kai-Wei Changaa Peking UniversityaUniversity of California, Los Angelesliliunian@pku.edu.cn, patrickchen@g.ucla.edu{chohsieh, kwchang}@cs.ucla.eduAbstractrepresentation models haveContextualachieved great success in improving var-ious downstream tasks. However,theselanguage-model-based encoders are difi-cult to train due to the large parameter sizesand high computational complexity. Bycarefully examining the training procedure,we ind that the softmax layer (the outputineficiency duelayer) causes signiicantto the large vocabulary size. Therefore,we redesign the learning objective andpropose an eficient framework for trainingcontextual representation models. Specif-ically,the proposed approach bypassesthe softmax layer by performing languagemodeling with dimension reduction, andallows the models to leverage pre-trainedword embeddings. Our framework reducesthe time spent on the output layer to anegligible level, eliminates almost all thetrainable parameters of the softmax layerand performs language modeling withouttruncating the vocabulary. When appliedto ELMo, our method achieves a 4 timesspeedup and eliminates 80% trainableparameters while achieving competitiveperformance on downstream tasks.
</think>