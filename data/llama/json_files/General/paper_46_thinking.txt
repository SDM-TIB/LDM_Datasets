>

<think>
To extract datasets from the research paper titled "Towards a better understanding of VectorQuantized AutoencodersAnonymous authorsPaper under double-blind reviewAbstractDeep neural networks with discrete latent variables oier the promise ofbetter symbolic reasoning, and learning abstractions that are more usefulto new tasks. There has been a surge in interest in discrete latent variablemodels, however, despite several recent improvements, the training of discretelatent variable models has remained challenging and their performance hasmostly failed to match their continuous counterparts. Recent work on vectorquantized autoencoders (VQ-VAE) has made substantial progress in thisdirection, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training techniquefor VQ-VAE, inspired by its connection to the Expectation Maximization(EM) algorithm. Training the discrete autoencoder with EM and combiningit with sequence level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matchesa strong greedy autoregressive baseline Transformer, while being 3.3 timesfaster at inference.1IntroductionUnsupervised learning of meaningful representations is a fundamental problem in machinelearning since obtaining labeled data can often be very expensive. Continuous representationshave largely been the workhorse of unsupervised deep learning models of images (Goodfellowet al., 2014; van den Oord et al., 2016; Kingma et al., 2016; Salimans et al., 2017; Parmaret al., 2018), audio (Van Den Oord et al., 2016; Reed et al., 2017), and video (Kalchbrenneret al., 2016). However, it is often the case that datasets are more naturally modeled as asequence of discrete symbols rather than continuous ones. For example, language and speechare inherently discrete in nature and images are often concisely described by language, seee.g., Vinyals et al. (2015). Improved discrete latent variable models could also prove usefulfor learning novel data compression algorithms (Theis et al., 2017), while having far moreinterpretable representations of the data.We build on Vector Quantized Variational Autoencoder (VQ-VAE) (van den Oord et al.,2017), a recently proposed training technique for learning discrete latent variables. Themethod uses a learned code-book combined with nearest neighbor search to train the discretelatent variable model. The nearest neighbor search is performed between the encoder outputand the embedding of the latent code using the 2 distance metric. VQ-VAE adopts thestandard latent variable model generative process, irst sampling latent codes from a prior,P (z), which are then consumed by the decoder to generate data from P (x | z). In van denOord et al. (2017), the authors use both uniform and autoregressive priors for P (z). Theresulting discrete autoencoder obtains impressive results on unconditional image, speech, andvideo generation. In particular, on image generation, VQ-VAE was shown to perform almoston par with continuous VAEs on datasets such as CIFAR-10 (van den Oord et al., 2017).An extension of this method to conditional supervised generation, out-performs continuousautoencoders on WMT English-German translation task (Kaiser et al., 2018).The work of Kaiser et al. (2018) introduced the Latent Transformer, which set a new state-of-the-art in non-autoregressive Neural Machine Translation. However, additional trainingheuristics, namely, exponential moving averages (EMA) of cluster assignment counts, and1Under review as a conference paper at ICLR 2019product quantization (Norouzi & Fleet, 2013) were essential to achieve competitive resultswith VQ-VAE. In this work, we show that tuning for the code-book size can signiicantlyoutperform the results presented in Kaiser et al. (2018). We also exploit VQ-VAEas connec-tion with the expectation maximization (EM) algorithm (Dempster et al., 1977), yieldingadditional improvements. With both improvements, we achieve a BLEU score of 22.4 onEnglish to German translation, outperforming Kaiser et al. (2018) by 2.6 BLEU. Knowledgedistillation (Hinton et al., 2015; Kim & Rush, 2016) provides signiicant gains with our bestmodels and EM, achieving 26.7 BLEU, which almost matches the autoregressive transformermodel with no beam search at 27.0 BLEU, while being 3.3A faster.Our contributions can be summarized as follows:1. We show that VQ-VAE from van den Oord et al. (2017) can outperform previousstate-of-the-art without product quantization.2. Inspired by the EM algorithm, we introduce a new training algorithm for trainingdiscrete variational autoencoders, that outperforms the previous best result withdiscrete latent autoencoders for neural machine translation.3. Using EM training, and combining it sequence level knowledge distillation (Hintonet al., 2015; Kim & Rush, 2016), allows us to develop a non-autoregressive machinetranslation model whose accuracy almost matches a strong greedy autoregressivebaseline Transformer, while being 3.3 times faster at inference.4. On the larger English-French dataset, we show that denoising discrete autoencodersgives us a signiicant improvement (1.0 BLEU) on top of our non-autoregressivebaseline (see Section D).2 VQ-VAE and the Hard EM AlgorithmThe connection between K-means, and hard EM, or the Viterbi EM algorithm is wellknown (Bottou & Bengio, 1995), where the former can be seen a special case of hard-EMstyle algorithm with a mixture-of-Gaussians model with identity covariance and uniformprior over cluster probabilities. In the following sections we brieiy explain the VQ-VAEdiscrete autoencoder for completeness and itas connection to classical EM.2.1 VQ-VAE discretization algorithmVQ-VAE models the joint distribution PI(x, z) where I are the model parameters, x is thedata point and z is the sequence of discrete latent variables or codes. Each position in theencoded sequence has its own set of latent codes. Given a data point, the discrete latentcode in each position is selected independently using the encoder output. For simplicity,we describe the procedure for selecting the discrete latent code (zi) in one position giventhe data point (xi). The encoder output ze(xi) a RD is passed through a discretizationbottleneck using a nearest-neighbor lookup on embedding vectors e a RKAD. Here K isthenumber of latent codes (in a particular position of the discrete latent sequence) in the model.More speciically, the discrete latent variable assignment is given by,zi = arg minja[K]ze(xi) a ej2(1)The selected latent variableas embedding is passed as input to the decoder,zq(xi) = eziThe model is trained to minimize:L = lr + I2 ze(xi) a sg (zq(xi))2,where lr is the reconstruction loss of the decoder given zq(x) (e.g., the cross entropy loss),and, sg (.) is the stop gradient operator deined as follows:(2)sg (x) =x forward pass0 backward pass2(4)(5),Under review as a conference paper at ICLR 2019To train the embedding vectors e a RKAD, van den Oord et al. (2017) proposed using agradient based loss functionsg (ze(xi)) a zq(xi)2,and also suggested an alternate technique of training the embeddings: by maintaining anexponential moving average (EMA) of all the encoder hidden states that get assigned toit. It was observed in Kaiser et al. (2018) that the EMA update for training the code-bookembedding, results in more stable training than using gradient-based methods. We analyzethis in more detail in Section 5.1.(3)Speciically, an exponential moving average is maintained over the following two quantities:1) the embeddings ej for every j a [1,..., K] and, 2) the count cj measuring the number ofencoder hidden states that have ej as itas nearest neighbor. The counts are updated in amini-batch of targets as:cj a Icj + (1 a I)1 [zq(xi) = ej],iwith the embedding ej being subsequently updated as:ej a Iej + (1 a I)1 [zq(xi) = ej] ze(xi)cjiwhere 1[.] is the indicator function and I is a decay parameter which we set to 0.999 inour experiments. This amounts to doing stochastic gradient in the space of both code-bookembeddings and cluster assignments. These techniques have also been successfully used inminibatch K-means (Sculley, 2010) and online EM (Liang & Klein, 2009; Sato & Ishii, 2000).The generative process for our latent variable NMT model, P (y, z | x), begins by autoregres-sively sampling a sequence of discrete latent codes from a model conditioned on the inputx,P (z | x) =|z|i=1P zi | z1,...,(ia1), x,(6)which we refer to as the Latent Predictor model (Kaiser et al., 2018). The decoder thenconsumes this sequence of discrete latent variables to generate the target y all at once, whereP (y | z, x) =|y|j=1P (yj | z, x).(7)The autoregressive learned prior prior is it on the discrete latent variables produced bythe autoencoder. Our goal is to learn a sequence of latents, that is much shorter than thetargets, |z| |y|, thereby speeding up decoding signiicantly with no loss in accuracy. Thearchitecture of the encoder, the decoder, and the latent predictor model are described infurther detail in Section 5.2.2 Hard EM and the K-means algorithmIn this section we brieiy recall the hard Expectation maximization (EM) algorithm (Dempsteret al., 1977). Given a set of data points (x1,..., xN ), the hard EM algorithm approximatelysolves the following optimization problem:Ia = arg maxImaxz1,...,zNPI(x1,..., xN, z1,..., zN ),(8)Hard EM performs coordinate descent over the following two coordinates: the modelparameters I, and the hidden variables z1,..., zN. In other words, hard EM consists ofrepeating the following two steps until convergence:1. E step: (z1,..., zN ) a arg maxz1,...,zN PI(x1,..., xN, z1,..., zN ),3Under review as a conference paper at ICLR 2019
</think>