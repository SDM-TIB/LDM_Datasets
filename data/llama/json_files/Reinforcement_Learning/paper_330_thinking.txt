>

<think>
To extract datasets from the research paper titled "3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for SemanticParsing of Large-scale 3D Point Clouds" by Fangyu Liua 1, Shuaipeng Lia 1, Liqiang Zhanga 1, Chenghu Zhoua 2,Rongtian Ye 1, Yuebin Wang 1, Jiwen Lu 31Faculty of Geographical Science, Beijing Normal University, Beijing, China2Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences3Department of Automation, Tsinghua University, Beijing, ChinaAbstractSemantic parsing of large-scale 3D point clouds is an im-portant research topic in computer vision and remote sens-ing ields. Most existing approaches utilize hand-craftedfeatures for each modality independently and combine themin a heuristic manner. They often fail to consider the consis-tency and complementary information among features ade-quately, which makes them dificult to capture high-level se-mantic structures. The features learned by most of the cur-rent deep learning methods can obtain high-quality imageclassiication results. However, these methods are hard tobe applied to recognize 3D point clouds due to unorganizeddistribution and various point density of data. In this paper,we propose a 3DCNN-DQN-RNN method which fuses the3D convolutional neural network (CNN), Deep Q-Network(DQN) and Residual recurrent neural network (RNN) for aneficient semantic parsing of large-scale 3D point clouds.In our method, an eye window under control of the 3D CNNand DQN can localize and segment the points of the objectasclass eficiently. The 3D CNN and Residual RNN furtherextract robust and discriminative features of the points inthe eye window, and thus greatly enhance the parsing accu-racy of large-scale point clouds. Our method provides anautomatic process that maps the raw data to the classiica-tion results. It also integrates object localization, segmen-tation and classiication into one framework. Experimentalresults demonstrate that the proposed method outperformsthe state-of-the-art point cloud classiication methods.
</think>