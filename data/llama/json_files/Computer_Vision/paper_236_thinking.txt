>

<think>
To extract datasets from the research paper titled "VL-BERT: Pre-training of Generic Visual-Linguistic Representations" by Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the Conceptual Captions dataset and the BooksCorpus and English Wikipedia datasets.

In the **introduction**, the authors discuss the use of the Conceptual Captions dataset as the visual-linguistic corpus and the BooksCorpus and English Wikipedia datasets as the text-only corpus.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for pre-training and fine-tuning. Here, the authors mention the following datasets:

1. **Conceptual Captions dataset**: This dataset is used for pre-training the visual-linguistic representation.
2. **BooksCorpus**: This dataset is used for pre-training the text-only representation.
3. **English Wikipedia**: This dataset is used for pre-training the text-only representation.
4. **COCO Caption**: This dataset is used for pre-training the text-only representation.
5. **VG Caption**: This dataset is used for pre-training the text-only representation.
6. **VG QA**: This dataset is used for pre-training the text-only representation.
7. **VQA**: This dataset is used for pre-training the text-only representation.
8. **GQA**: This dataset is used for pre-training the text-only representation.
9. **RefCOCO+**: This dataset is used for fine-tuning the visual-linguistic representation.

In the **results section**, the authors confirm that these datasets were used for pre-training and fine-tuning the VL-BERT model.

Now, I will check the **references section** to find the full citations for these datasets:

- For **Conceptual Captions**, the citation is:
  > P. Sharma, N. Ding, S. Goodman, and R. Soricut. *Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning*. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556–2565, 2018.

- For **BooksCorpus**, the citation is:
  > Z. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. *Aligning books and movies: Towards story-like visual explanations by watching and reading books*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 19–27, 2016.

- For **English Wikipedia**, the citation is:
  > None (not mentioned in the paper)

- For **COCO Caption**, the citation is:
  > C. Chen, Y. Zhu, X. Zhang, Z. Chen, and H. Li. *Microsoft coco captions: Data collection and evaluation server*. arXiv preprint arXiv:1504.00325, 2015.

- For **VG Caption**, the citation is:
  > R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Li, D. A. Shamma, et al. *Visual genome: Connecting language and vision using crowdsourced dense image annotations*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 19–27, 2017.

- For **VG QA**, the citation is:
  > Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. *Aligning books and movies: Towards story-like visual explanations by watching and reading books*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 19–27, 2016.

- For **VQA**, the citation is:
  > J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Zitnick, and R. Girshick. *Clevr: A diagnostic dataset for compositional language and elementary visual reasoning*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901–2910, 2017.

- For **GQA**, the citation is:
  > J. Hudson and C. Manning. *Gqa: A new dataset for real-world visual reasoning*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6700–6709, 2019.

- For **RefCOCO+**, the citation is:
  > S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. *Referitgame: Referring to objects in photographs of natural scenes*. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014.
</think>