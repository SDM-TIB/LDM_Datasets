>

<think>
To extract datasets from the research paper titled "Kernel Transformer Networks for Compact Spherical Convolution" by Yu-Chuan Su, Kristen Grauman, and Facebook AI Research, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the 360a camera and the amount of 360a data, which suggests that datasets are likely involved.

In the **introduction**, the authors discuss the limitations of existing methods for learning CNNs on 360a images and the need for a new approach.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention three datasets:

1. **Spherical MNIST**: A dataset constructed from the MNIST dataset by backprojecting the digits into equirectangular projection.
2. **APano2Vid**: A real-world 360a video dataset.
3. **APascal VOC 2007**: A perspective image dataset with object annotations.

In the **results section**, the authors confirm that these datasets were used for extrinsic evaluation of their models, which reinforces their relevance.

Now, I will check the **references section** to find the full citations for these datasets:

- For **Spherical MNIST**, the citation is:
  > Yu-Chuan Su and Kristen Grauman. Making 360a videowatchable in 2d: Learning videography for click free view-ing. In CVPR, 2017. 1, 2, 6

- For **APano2Vid**, the citation is:
  > Yu-Chuan Su, Dinesh Jayaraman, and Kristen Grauman. Pano2vid: Automatic cinematography for watching 360avideos. In ACCV, 2016. 1, 2, 6, 9

- For **APascal VOC 2007**, the citation is:
  > Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with regionproposal networks. In NIPS, 2015. 6, 10

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>