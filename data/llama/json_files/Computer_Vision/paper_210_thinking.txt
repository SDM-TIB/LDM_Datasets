>

<think>
To extract datasets from the research paper titled "Recurrent Neural Networks have long been the dominating choice for sequencemodeling. However, it severely suffers from two issues: impotent in capturingvery long-term dependencies and unable to parallelize the sequential computationprocedure. Therefore, many non-recurrent sequence models that are built on con-volution and attention operations have been proposed recently. Notably, modelswith multi-head attention such as Transformer have demonstrated extreme effec-tiveness in capturing long-term dependencies in a variety of sequence modelingtasks. Despite their success, however, these models lack necessary components tomodel local structures in sequences and heavily rely on position embeddings thathave limited effects and require a considerable amount of design efforts. In thispaper, we propose the R-Transformer which enjoys the advantages of both RNNsand the multi-head attention mechanism while avoids their respective drawbacks.The proposed model can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings. We eval-uate R-Transformer through extensive experiments with data from a wide range ofdomains and the empirical results show that R-Transformer outperforms the state-of-the-art methods by a large margin in most of the tasks. We have made the codepublicly available at https://github.com/DSE-MSU/R-transformer.1INTRODUCTIONRecurrent Neural Networks (RNNs) especially its variants such as Long Short-Term Memory(LSTM) and Gated Recurrent Unit (GRU) have achieved great success in a wide range of sequencelearning tasks including language modeling, speech recognition, recommendation, etc (Mikolovet al., 2010; Sundermeyer et al., 2012; Graves & Jaitly, 2014; Hinton et al., 2012; Hidasi et al.,2015). Despite their success, however, the recurrent structure is often troubled by two notoriousissues. First, it easily suffers from gradient vanishing and exploding problems, which largely limitstheir ability to learn very long-term dependencies (Pascanu et al., 2013). Second, the sequentialnature of both forward and backward passes makes it extremely dificult, if not impossible, to par-allelize the computation, which dramatically increases the time complexity in both training andtesting procedure. Therefore, many recently developed sequence learning models have completelyjettisoned the recurrent structure and only rely on convolution operation or attention mechanism thatare easy to parallelize and allow the information iow at an arbitrary length. Two representative mod-els that have drawn great attention are Temporal Convolution Networks(TCN) (Bai et al., 2018) andTransformer (Vaswani et al., 2017). In a variety of sequence learning tasks, they have demonstratedcomparable or even better performance than that of RNNs (Gehring et al., 2017; Bai et al., 2018;Devlin et al., 2018).1 Figure 1: The illustration of one layer of R-Transformer. There are three different networks that arearranged hierarchically. In particular, the lower-level is localRNNs that process positions in a localwindow sequentially (This igure shows an example of local window of size 3); The middle-levelis multi-head attention networks which capture the global long-term dependencies; The upper-levelis Position-wise feedforward networks that conduct non-linear feature transformation. These threenetworks are connected by a residual and layer normalization operation. The circles with dash lineare the paddings of the input sequenceThe remarkable performance achieved by such models largely comes from their ability to capturelong-term dependencies in sequences. In particular, the multi-head attention mechanism in Trans-former allows every position to be directly connected to any other positions in a sequence. Thus,the information can iow across positions without any intermediate loss. Nevertheless, there aretwo issues that can harm the effectiveness of multi-head attention mechanism for sequence learn-ing. The irst comes from the loss of sequential information of positions as it treats every positionidentically. To mitigate this problem, Transformer introduces position embeddings, whose effects,however, have been shown to be limited (Dehghani et al., 2018; Al-Rfou et al., 2018). In addition,it requires considerable amount of efforts to design more effective position embeddings or differentways to incorporate them in the learning process (Dai et al., 2019). Second, while multi-head atten-tion mechanism is able to learn the global dependencies, we argue that it ignores the local structuresthat are inherently important in sequences such as natural languages. Even with the help of positionembeddings, the signals at local positions can still be very weak as the number of other positions issigniicantly more.To address the aforementioned limitations of the standard Transformer, in this paper, we proposea novel sequence learning model, termed as R-Transformer. It is a multi-layer architecture builton RNNs and the standard Transformer, and enjoys the advantages of both worlds while naturallyavoids their respective drawbacks. More speciically, before computing global dependencies of po-sitions with the multi-head attention mechanism, we irstly reine the representation of each positionsuch that the sequential and local information within its neighborhood can be compressed in therepresentation. To do this, we introduce a local recurrent neural network, referred to as LocalRNN,to process signals within a local window ending at a given position. In addition, the LocalRNNoperates on local windows of all the positions identically and independently and produces a latentrepresentation for each of them. In this way, the locality in the sequence is explicitly captured. Inaddition, as the local window is sliding along the sequence one position by one position, the globalsequential information is also incorporated. More importantly, because the localRNN is only ap-plied to local windows, the aforementioned two drawbacks of RNNs can be naturally mitigated. Weevaluate the effectiveness of R-Transformer with a various of sequence learning tasks from differentdomains and the empirical results demonstrate that R-Transformer achieves much stronger perfor-mance than both TCN and standard Transformer as well as other state-of-the-art sequence models.The rest of the paper is organized as follows: Section 2 discusses the sequence modeling problemwe aim to solve; The proposed R-Transformer model is presented in Section 3. In Section 4, we2FeedForward:FMulti-head Attention: MFMFMFMFMAdd & NormAdd & NormLocal RNN: RRRRRRAdd & Normdescribe the experimental details and discuss the results. The related work is brieiy reviewed inSection 5. Section 6 concludes this work.2 SEQUENCE MODELING PROBLEMBefore introducing the proposed R-Transformer model, we formally describe the sequence modelingproblem. Given a sequence of length N : x1, x2, A A A, xN, we aim to learn a function that maps theinput sequence into a label space Y: (f : X N a Y). Formally,y = f (x1, x2, A A A, xN )(1)where y a Y is the label of the input sequence. Depending on the deinition of label y, manytasks can be formatted as the sequence modeling problem deined above. For example, in languagemodeling task, xt is the character/word in a textual sentence and y is the character/word at nextposition (Mikolov et al., 2010); in session-based recommendation, xt is the user-item interactionin a session and y is the future item that users will interact with (Hidasi et al.,2015). When xt is anucleotide in a DNA sequence and y is its function, this problem becomes a DNA function predictiontask (Quang & Xie, 2016). Note that, in this paper, we do not consider the sequence-to-sequencelearning problems. However, the proposed model can be easily extended to solve these problemsand we will leave it as one future work.3 THE R-TRANSFORMER MODELThe proposed R-Transformer consists of a stack of identical layers. Each layer has 3 componentsthat are organized hierarchically and the architecture of the layer structure is shown in Figure 1.As shown in the igure, the lower level is the local recurrent neural networks that are designed tomodel local structures in a sequence; the middle level is a multi-head attention that is able to captureglobal long-term dependencies; and the upper level is a position-wise feedforward networks whichconducts a non-linear feature transformation. Next, we describe each level in detail.3.1 LOCALRNN: MODELING LOCAL STRUCTURESSequential data such as natural language inherently exhibits strong local structures. Thus, it is desir-able and necessary to design components to model such locality. In this subsection, we propose totake the advantage of RNNs to achieve this. Unlike previous works where RNNs are often appliedto the whole sequence, we instead reorganize the original long sequence into many short sequenceswhich only contain local information and are processed by a shared RNN independently and identi-cally. In particular, we construct a local window of size M for each target position such that the localwindow includes M consecutive positions and ends at the target position. Thus, positions in eachlocal window form a local short sequence, from which the shared RNN will learn a latent represen-tation. In this way, the local structure information of each local region of the sequence is explicitlyincorporated in the learned latent representations. We refer to the shared RNN as LocalRNN. Com-paring to original RNN operation, LocalRNN only focuses on local short-term dependencies withoutconsidering any long-term dependencies. Figure 2 shows the different between original RNN andLocalRNN operations. Concretely, given the positions xtaM a1, xtaM a2, A A A, xt of a local shortsequence of length M, the LocalRNN processes them sequentially and outputs M hidden states, thelast of which is used as the representation of the local short sequences:ht = LocalRNN(xtaM a1, xtaM a2, A A A, xt)(2)where RNN denotes any RNN cell such as Vanilla RNN cell, LSTM, GRU, etc. To enable the modelto process the sequence in an auto-regressive manner and take care that no future information isavailable when processing one position, we pad the input sequence by (M a 1) positions before thestart of a sequence. Thus, from sequence perspective, the LocalRNN takes an input sequence andoutputs a sequence of hidden representations that incorporate information of local regions:h1, h2, A A A, hN = LocalRN N (x1, x2, A A A, xN )(3)The localRNN is analogous to 1-D Convolution Neural Networks where each local window is pro-cessed by convolution operations. However, the convolution operation completely ignores the se-quential information of positions within the local window. Although the position embeddings have3(a) Original RNN.(b) Local RNN.Figure 2: An illustration of the original and local RNN. In contrast to orignal RNN which maintainsa hidden state at each position summarizing all the information seen so far, LocalRNN only operateson positions within a local window. At each position, LocalRNN will produce a hidden state thatrepresents the information in the local window ending at that position.been proposed to mitigate this problem, a major deiciency of this approach is that the effective-ness of the position embedding could be limited; thus it requires considerable amount of extra ef-forts (Gehring et al., 2017). On the other hand, the LocalRNN is able to fully capture the sequentialinformation within each window. In addition, the one-by-one sliding operation also naturally incor-porates the global sequential information.Discussion: RNNs have long been a dominating choice for sequence modeling but it severely suffersfrom two problems a The irst one is its limited ability to capture the long-term dependencies and thesecond one is the time complexity, which is linear to the sequence length. However, in LocalRNN,these problems are naturally mitigated. Because the LocalRNN is applied to a short sequence withina local window of ixed size, where no long-term dependency is needed to capture. In addition, thecomputation procedures for processing the short sequences are independent of each other. Therefore,it is very straightforward for the parallel implementation (e.g., using GPUs), which can greatlyimprove the computation eficiency.3.2 CAPTURING THE GLOBAL LONG-TERM DEPENDENCIES WITH MULTI-HEADATTENTIONThe RNNs at the lower level introduced in the previous subsection will reine representation of eachpositions such that it incorporates its local information. In this subsection, we build a sub-layeron top of the LocalRNN to capture the global long-term dependencies. We term it as pooling sub-layer because it functions similarly to the pooling operation in CNNs. Recent works have shownthat the multi-head attention mechanism is extremely effective to learn the long-term dependencies,as it allows a direct connection between every pair of positions. More speciically, in the multi-head attention mechanism, each position will attend to all the positions in the past and obtainsa set of attention scores that are used to reine its representation. Mathematically, given currentrepresentations h1, h2, A A A, ht, the reined new representations ut are calculated as:ut = M ultiHeadAttention(h1, h2, A A A, ht)(4)= Concatenation(head1(ht), head2(ht), A A A, headk(ht))W owhere headk(ht) is the result of kth attention pooling and W o is a linear projection matrix. Con-sidering both eficiency and effectiveness, the scaled dot product is used as the attention func-tion (Vaswani et al., 2017). Speciically, headi(ht) is the weighted sum of all value vectors and4RRRRthe weights are calculated by applying attention function to all the query, key pairs:{I1, I2, A A A In } = Sof tmax({< q, k1 >(dk),< q, k2 >(dk), A A A,< q, kn >(dk)})(5)headi(ht) =nj=1Ijvjwhere q, ki, and vi are the query, key, and value vectors and dk is the dimension of ki. More-over, q, ki, and vi are obtained by projecting the input vectors into query, key and value spaces,respectively (Vaswani et al., 2017). They are formally deined as:q, ki, vi = W qht, W khi, W vhi(6)where W q, W k and W v are the projection matrices and each attention pooling headi has its ownprojection matrices. As shown in Eq. (5), each headi is obtained by letting ht attending to all theapasta positions, thus any long-term dependencies between ht and hi can be captured. In addition,different heads will focus on dependencies in different aspects. After obtaining the reined rep-resentation of each position by the multi-head attention mechanism, we add a position-wise fullyconnected feed-forward network sub-layer, which is applied to each position independently andidentically. This feedforward network transforms the features non-linearly and is deined as follows:F eedF orward(mt) = max(0, utW1 + b1)W2 + b2(7)Following (Vaswani et al., 2017), We add a residual (He et al., 2016) and layernorm (Ba et al., 2016)connection between all the sub-layers.3.3 OVERALL ARCHITECTURE OF R-TRANSFORMERWith all the aforementioned model components, we can now give a formal description of the overallarchitecture of an N -layer R-Transformer. For the ith layer (i a {1, 2, A A A N }):1, hihiEhi1, Ehiui1, ui1, EuiEuimi1, mi, xi+121, xi1 + xi2, A A A, xi1, hi2, A A A, hi2, A A A, Ehi2, A A A, ui2, A A A, Eui2, A A A, mi, A A A, xi+1T = LocalRN N (xiT )2, A A A, hiT = LayerN orm(hi2 + xi2, A A A, EhiT = M ultiHeadAttention(Ehv1, Ehi2 + Ehi2, A A A, uiT = LayerN orm(ui1, ui2, A A A, EuiT = F eedF orward(EuiT )2 + Eui1, miT = LayerNorm(mi1 + Ehi1, Eui1 + Euixi+11T + xiT )T + EhiT )T )2, A A A, miT + EuiT )(8)where T is the length of the input sequence and xit is the input position of the layer i at time step t.
</think>