>
<think>
To extract datasets from the research paper titled "Serial-section Electron MicroscopyImage Registration using a Spatial TransformerNetwork with Learned FeaturesInwan Yoo1, David G. C. Hildebrand2, Willie F. Tobin3,Wei-Chung Allen Lee3 and Won-Ki Jeong1Ulsan National Institute of Science and Technology1The Rockefeller University2Harvard Medical School3E-mail: iwyoo@unist.ac.krAbstract. The alignment of serial-section electron microscopy (ssEM)images is critical for eiorts in neuroscience that seek to reconstruct neu-ronal circuits. However, each ssEM plane contains densely packed struc-tures that vary from one section to the next, which makes matching fea-tures across images a challenge. Advances in deep learning has resultedin unprecedented performance in similar computer vision problems, butto our knowledge, they have not been successfully applied to ssEM imageco-registration. In this paper, we introduce a novel deep network modelthat combines a spatial transformer for image deformation and a convo-lutional autoencoder for unsupervised feature learning for robust ssEMimage alignment. This results in improved accuracy and robustness whilerequiring substantially less user intervention than conventional methods.We evaluate our method by comparing registration quality across severaldatasets.1IntroductionAmbitious eiorts in neuroscienceareferred to as aconnectomicsaaseek togenerate comprehensive brain connectivity maps. This ield utilizes the high res-olution of electron microscopy (EM) to resolve neuronal structures such as den-dritic spine necks and synapses, which are only tens of nanometers in size [5].A standard procedure for obtaining such datasets is cutting brain tissue into30a50 nm-thick sections (e.g. ATUM [4]), acquiring images with 2a5 nm lat-eral resolution for each section, and aligning two-dimensional (2D) images intothree-dimensional (3D) volumes. Though the tissue is chemically ixed and em-bedded in epoxy resin to preserve ultrastructure, several deformations occurin this serial-section EM (ssEM) process. These include tissue shrinkage, com-pression or expansion during sectioning, and warping from sample heating orcharging due to the electron beam. Overcoming such non-linear distortions arenecessary to reproduce a 3D image volume in a state as close as possible to theoriginal biological specimen. Therefore, excellent image alignment is an impor-tant prerequisite for subsequent analysis.7102ceD5]VC.sc[2v33870.7071:viXra 2Authors Suppressed Due to Excessive LengthSigniicant research eiorts in image registration have been made to addressmedical imaging needs. However, ssEM image registration remains challengingdue to its image characteristics: large and irregular tissue deformations withartifacts such as dusts and folds, drifting for long image sequences alignment,and diiculty in inding the optimal alignment parameters. Several open-sourcessEM image registration tools are available, such as bUnwarpJ [1] and Elasticalignment [9] (available via TrakEM2 [2]). They partially address the aboveissues, but some of them still remain, such as lack of global regularization andcomplicated parameter tuning.Our work is motivated by recent advances in deep neural networks. Convo-lutional neural networks (CNNs) and their variants have shown unprecedentedpotential by largely outperforming conventional computer vision algorithms us-ing hand-crafted feature descriptors, but their application to ssEM image regis-tration has not been explored. Wu et al. [10] used a 3D autoencoder to extractfeatures from MRI volumes, which are then combined with a conventional sparse,feature-driven registration method. Recent work by Jaderberg et al. [6] on thespatial transformer network (STN) uses a diierentiable network module insidea CNN to overcome the drawbacks of CNNs (i.e., lack of scale- and rotation-invariance). Another interesting application of deep neural networks is energyoptimization using backpropagation, as shown in the neural artistic style transferproposed by Gatys et al. [3].Inspired by these studies, we propose a novel deep network model that isspeciically designed for ssEM image registration. The proposed model is a novelcombination of an STN and a convolutional autoencoder that generates a de-formation map (i.e., vector map) for the entire image alignment via backprop-agation of the network. We propose a feature-based image similarity measure,which is learned from the training images in an unsupervised fashion by theautoencoder. Unlike other conventional hand-crafted features, such as SIFT andblock-matching, the learned features used in our method signiicantly reducethe required user parameters and make the method easy to use and less error-prone. To the best of our knowledge, this is the irst data-driven ssEM imageregistration method based on deep learning, which can easily extend to variousapplications by employing diierent feature encoding networks.2 Method2.1 Feature Generation using a Convolutional AutoencoderTo compute similarities between adjacent EM sections, we generate data-driven features via a convolutional autoencoder, which consists of 1) a convolu-tional encoder comprised of convolutional layers with ReLU activations and 2)a deconvolutional decoder comprised of deconvolutional layers with ReLU ac-tivations that were symmetrical to the encoder without fully connected layers.Therefore, our method is applicable to any sized dataset (i.e., the network size isnot constrained to the input data size). Our autoencoder can be formally deinedssEM Image Registration using a STN with Learned Features3as follows:h = fI (x)y = gI(h)LI,I =Ni=1||xi a yi||22 + I(||I k||22 +k||Ik||22)k(1)(2)(3)where fI and gI are the encoder and the decoder and I and I are their param-eters, respectively. The loss function (Eq. 3) consists of the reconstruction termminimizing the diierence between the input and output images and the regu-larization terms minimizing the 2-norm of the weights of the network to avoidoveritting. Fig. 1 shows that our autoencoder feature-based registration gener-ates more accurate results compared to the conventional pixel intensity-basedregistration, i.e., (d) shows the smaller normalized cross correlation (NCC) errorbetween aligned images than (c).Fig. 1. Comparison between the pixel intensity-based and the autoencoder feature-based registration with backpropagation. (a) the ixed image, (b) the moving image,(c) the heat map of NCC of the pixel intensity-based registration result (NCC : 0.1670),and (d) the heat map of NCC of the autoencoder feature-based registration (NCC :0.28) in red box region.2.2 Deformable Image Registration using a Spatial TransformerNetworkUpon completion of autoencoder training, a spatial transformer (ST) mod-ule T is attached to the front half (i.e., encoder) to form the proposed spatialtransformer network (see Fig. 2, refer to [6] for the details of the ST module).This design is intended to ind the proper deformation of the input image viaan ST by minimizing the registration error measured by the pre-trained autoen-coder. The objective function for registration errors between the reference andthe moving images is formulated as Eq. 4. The reference image I1 is ixed, andthe moving image I0 is deformed by the ST with the corresponding vector mapv. Notably, the resolution of vector map v is usually coarser than that of theinput image. Therefore, we need smooth interpolation of a coarse vector map toobtain a per-pixel moving vector for actual deformation of the moving image. Athin plate spline (TPS) was used in the original STN for a smooth deformable(a)(b)(c)(d)4Authors Suppressed Due to Excessive LengthFig. 2. The overview of our method. The upper right dashed box represents the pre-trained convolution autoencoder (CAE). The alignment is processed by backpropaga-tion with loss of autoencoder features.transform, but other interpolation schemes, such as bilinear, bicubic, B-spline,etc., can be used as well. In our experiment, bilinear interpolation producedbetter results with iner deformation compared to the TPS.Lv(I0, I1) = ||fI (I1) a fI (Tv(I0))||22 + I(||I k||22 +k||Ik||22)k(1)(2)(3)The irst term of Eq. 4 measures how two images are contextually diierent viaa trained autoencoder. We assumed that if the encoded features of two imagesare similar, then the images themselves are also similar and well-aligned. The restof the terms in Eq. 4 reiect the regularization of vector map v, which penalizeslarge deformation while promoting smooth variation of the vector map, and I, I2,I3 are their corresponding weights. Because every layer is diierentiable, includingan ST, we directly optimize v by backpropagation with a chain rule, in whichonly v is updated and the weights in the autoencoder are ixed. We used theADAM optimizer [7] for all our experiments.The objective function using only adjacent image pairs could be vulnerable toimaging artifacts, which may result in drifting due to error accumulation whenmany sections are aligned. To increase the robustness of alignment, we extend theobjective function (Eq. 4) to leverage multiple neighbor sections. Let the movingimage be I0, its neighbor reference n images be I1 to In, and their correspondingweights be wi. The proposed objective function (Eq. 5) combines the registrationerrors across neighbor images, which can lessen strong registration errors fromimages with artifacts and avoid large deformation.To accumulate the registration error only within the image after deformation,we applied the empty space mask that represents the empty area outside theimage. After image deformation, we collect the pixels outside the valid imageFeedforwardBackpropagationvSTI0IiCAEI a = I ()a I (140)(Shared weights)a I (14)ssEM Image Registration using a STN with Learned Features5region and make a binary mask image. We resize this mask image to match thesize of the autoencoder feature map using a bilinear interpolation (shown asM (T ) in Eq. 5). Based on this objective function, the alignment of many EMsections is possible in an out-of-core fashion using a sliding-window method.Lv(I0,..., In) =ni=1wiM (Tv)||fI (Ii)afI (Tv(I0))||22+I||v||22+I2||avx||22+I3||avy||22(5)We also developed a technique for handling images with dusts and folds.Because the feature errors are high in the corrupted regions, we selectively ignoresuch regions during the optimization, which we call loss drop. This is similar toapplying an empty space mask except that pixel selection is based on featureerror. In our implementation, we irst dropped the top 50% of high error features,and then reduced the dropping rate by half per every iteration. By doing this, weeiectively prevented local minimums and obtained smoother registration results.3 ResultsWe implemented our method using TensorFlow, and used a GPU workstationequipped with an NVIDIA Titan X GPU. We used three EM datasets: trans-mission EM (TEM) images of Drosophila brain, human-labeled TEM images ofanother Drosophila brain provided by CREMI challenge1 (those two Drosophilaimages are collected independently on separate imaging systems), and mousebrain scanning EM (SEM) images with fold artifacts. We used two convolu-tional autoencoders: one is a deeper network (as shown in Fig. 2) with 3 A 3ilters used for the Drosophila TEM datasets, and the other is a shallower net-work with a larger ilter size (i.e., 6 layers with 7 A 7 ilters) used for the mouseSEM dataset. In bUnwarpJ and elastic alignment experiments, we performedvarious experiments to ind the optimal parameters and selected the parametersthat gave the best results.Drosophila TEM data The original volumetric dataset comprises the ante-rior portion of an adult female Drosophila melanogaster brain cut in the frontalplane. Each section was acquired at 4 A 4 A 40 nm3vxa1, amounting to 4 mil-lion camera images and 50 TB of raw data. The original large-scale datasetwas aligned with AlignTK (http://mmbios.org/aligntk-home) requiring exten-sive human eiort and supercomputing resources. Although the alignment wassuicient for manual tracing of neurons, it must be improved for accurate andeicient automated segmentation approaches. Small volumes (512 A 512 A 47)were exported for re-alignment centered around synapses of identiied connec-tions between olfactory receptor neurons and second order projection neurons inthe antennal lobe. Fig. 3 shows the result of our registration method. Fig. 3 leftis the oblique (i.e., not axis-aligned) cross-sectional view of the original stack.Due to inaccurate pre-alignment, some discontinuous membranes are shown (seethe red circle areas), which are corrected in the aligned result using our method(Fig. 3 right).1 https://cremi.org/
</think>