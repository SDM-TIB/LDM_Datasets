>

<think>
To extract datasets from the research paper titled "Identifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study" by Navedanjum Ansari and Rajesh Sharma, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the optimization and comparison of representations for the duplicate question pair task, which suggests that datasets are likely involved.

In the **introduction**, the authors discuss the importance of identifying semantically duplicate questions on Quora and the need for high-quality content. They hint at the use of existing datasets for this task.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention the following named datasets:

1. **First Quora Dataset release**: This dataset is used for the duplicate question pair task.
2. **SNLI dataset**: This dataset is used for the entailment task.
3. **Quora Corpus**: This dataset is used for word embeddings.

In the **results section**, the authors confirm that these datasets were used for extrinsic evaluation of their models, which reinforces their relevance.

Now, I will check the **references section** to find the full citations for these datasets:

- For **First Quora Dataset release**, the citation is:
  > https://qim.fs.quoracdn.net/quora_duplicate_questions.tsv

- For **SNLI dataset**, the citation is:
  > https://nlp.stanford.edu/projects/glove/

- For **Quora Corpus**, the citation is:
  > https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>