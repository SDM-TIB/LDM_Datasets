>
<think>
To extract datasets from the research paper titled "JUSTDeep at NLP4IF 2019 Shared Task: Propaganda Detection using Ensemble Deep Learning Models" by Hani Al-Omari, Jordan University of Science, Malak Abdullah, Jordan University of Science, Ola Al-Titi, Jordan University of Science, and Samira Shaikh, University of North Carolina at Charlotte, Computer Science Department, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the NLP4IF 2019 Shared Task, which suggests that datasets are likely involved.

In the **introduction**, the authors discuss the challenge of propaganda detection and mention the dataset provided by the challenge NLP4IF 2019, Task 1 Sentence Level Classification (SLC).

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention the following dataset:

1. **NLP4IF 2019 Task 1 Sentence Level Classification (SLC) dataset**: This dataset consists of 411 articles, with 16975 sentences, where 12244 are non-propaganda and 4721 are propaganda.

In the **results section**, the authors confirm that this dataset was used for evaluation.

Now, I will check the **references section** to find the full citation for this dataset:

- For the **NLP4IF 2019 Task 1 Sentence Level Classification (SLC) dataset**, the citation is:
  > Da San Martino et al., 2019. Fine-grained analysis of propaganda in news articles. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP2019, Hong Kong, China.

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the dataset and its citation from the paper.
</think>