

<think>
To extract datasets from the Proceedings of NAACL-HLT 2018, pages 206a211New Orleans, Louisiana, June 1 - 6, 2018. c2018 Association for Computational Linguistics206Lexical Substitution for EvaluatingCompositional Distributional ModelsMaja BuljanaSebastian PadA oaa Institut fA ur Maschinelle Sprachverarbeitung, University of StuttgartJan ESnajdera maja.buljan,pado}{@ims.uni-stuttgart.dea TakeLab, Faculty of Electrical Engineering and Computing, University of Zagrebjan.snajder@fer.hrAbstractCompositional Distributional Semantic Mod-els (CDSMs) model the meaning of phrasesand sentences in vector space. They havebeen predominantly evaluated on limited, ar-tiicial tasks such as semantic sentence simi-larity on hand-constructed datasets. This pa-per argues for lexical substitution as a meansto evaluate CDSMs. Lexical substitution isa more natural task, enables us to evaluatemeaning composition at the level of individ-ual words, and provides a common ground tocompare CDSMs with dedicated lexical substi-tution models. We create a lexical substitutiondataset for CDSM evaluation from an English-language corpus with manual aall-wordsa lexi-cal substitution annotation. Our experimentsindicate that the Practical Lexical FunctionCDSM outperforms simple component-wiseCDSMs and performs on par with the con-text2vec lexical substitution model using thesame context.1IntroductionCompositional Distributional Semantics Models(CDSMs) compute phrase meaning in semanticspace as a function of the meanings of the phraseconstituents (Baroni et al., 2014). The most basicCDSMs represent words as vectors and composephrase vectors by component-wise operations ofthe constituent vectors (Mitchell and Lapata, 2008).More complex models represent predicates withmatrices and tensors (Baroni and Zamparelli, 2010;Grefenstette, 2013; Paperno et al., 2014).Given the large number of different CDSMs pro-posed in the literature (Erk, 2012), meaningful eval-uation becomes crucial. The dominant evaluationmethod, adopted by the majority of CDSM studies,is pairwise phrase similarity (Mitchell and Lap-ata, 2008; Guevara, 2010; Grefenstette et al., 2012;Grefenstette, 2013; Paperno et al., 2014). Only ahandful of studies pursued other evaluation tasks,such as textual entailment (Marelli et al., 2014a,b)or sentiment analysis (Socher et al., 2013).Arguably, phrase similarity evaluation has threemajor problems. First, the task is affected by thegeneral limitations of rating scales, such as incon-sistencies in annotations, scale region bias, andixed granularity (Schuman and Presser, 1996).Phrase similarity datasets used for CDSM eval-uation demonstrate slight to fair inter-annotatoragreement, as well as overlap between groups ofitems rated as low and high in similarity (Mitchelland Lapata, 2008).Secondly, phrase similarity is a task that is ratherdificult to put down precisely, especially for longphrases. Generally, phrases can be (dis)similar inany number of ways. Annotators commonly agreethat some sentence pairs are semantically highlysimilar (private company iles annual account andprivate company registers annual account, Picker-ing and Frisson 2001), and others are semanticallyunrelated (man waves hand vs. employee leavescompany). In contrast, their assessments becomeless conident for cases like delegate buys land andagent sells property (Kartsaklis et al., 2013), wherethere is a semantic relation other than synonymy.Similarity is also arguably not a useful measurewhen sentences are semantically deviant, as it is of-ten the case in the datasets: how similar are privatecompany iles annual account and private companysmooths annual account?The third problem is that the most widely usedphrase similarity datasets are constructed in a bal-anced fashion along psycholinguistic principles.For instance, the adjective-noun-verb-adjective-noun (aANVANa) dataset (Pickering and Frisson,2001; Kartsaklis et al., 2013), from which the exam-ples above are drawn, was constructed from a set ofparticularly ambiguous verbs paired with stronglydisambiguating contexts. Such setups often do not207correlate well with usefulness on more natural data.In a previous study on lexical substitution, Kremeret al. (2014) found that the advantage of machinelearning-based models over simple baselines wasmuch harder to show on a real-world corpus thanon the previously used manually constructed bench-mark dataset (McCarthy and Navigli, 2009).In this paper, we pursue the idea that lexical sub-stitution (McCarthy and Navigli, 2009) is a moresuitable evaluation task for CDSMs. Lexical sub-stitution is the task of inding meaning-preservingsubstitutes for a target word in context: e.g., theword submits is a legitimate substitute for iles inprivate company iles annual account, but not inofice clerk iles old papers. Lexical substitutionprovides a frame for comparing synonyms in con-text, and disambiguating the context-appropriatesense of polysemous words. Since this is a prob-lem CDSMs can account for, lexical substitutionseems a suitable task for testing and comparing dif-ferent CDSMs. Additionally, lexical substitution isa more natural task than similarity ratings, it makesit possible to evaluate meaning composition at thelevel of individual words, and provides a commonground to compare CDSMs with dedicated lexicalsubstitution models.2 The ANVAN-LS DatasetTo perform more realistic evaluations for CDSMs,we would like to test them on a sample of actualhuman utterances rather than hand-selected exam-ples. That being said, for the evaluation to be use-ful, the structures on which we evaluate shouldbe relatively uniform and limited, at least initiallywhile moving from artiicial to natural datasets. Wethus construct a dataset1 for English that strikes abalance between these factors: we maintain theadjective-noun-verb-adjective-noun (ANVAN) for-mat, but our phrases are based on corpus sentences,and some or all words in the ANVAN can form thetargets of lexical substitution ranking tasks.Sampling ANVAN Queries. The starting pointof our corpus construction is the English CoInCocorpus (Kremer et al., 2014), which consists ofroughly 2,500 sentences taken from the news andiction section of the MASC corpus and providesmanually annotated lexical substitutes for all con-tent words (nouns, adjectives, verbs, and adverbs).1The corpusisfreely available at http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/ANVAN-LS.htmlWe extracted all clauses from CoInCo that metthe following requirements: (1) the dependencystructure of the clause includes an ANVAN struc-ture; (2) at least one constituent word of the AN-VAN sub-clause has at least two human-providedsingle-word substitutes that exceeded a minimalfrequency threshold (more than 5 occurrences ina large corpus, cf. Section 3); (3) the POS tagswere correct. This resulted in 165 ANVAN phrases,which contained an average of 4.4 (out of 5) tar-get words with substitutes, for a total of 732 targetwords for lexical substitution.An issue that required additional considerationwas the adjective positions of the ANVANs. In theCoInCo, we found a substantial number of noun-noun compounds whose modiiers were tagged asadjectives. Conversely, many adjective modiiersin the corpus were substituted with nouns by hu-man annotators. To account for this variability, weextended the ANVAN schema conservatively byallowing nouns to ill the A position if it was ob-served in the large corpus robustly as a modiier(i.e., as part of at least 100 N-N bigram types, eachof which occurred at least 300 times).Building Lexical Substitution Tasks. For eachof the 732 target words, we constructed a lexicalsubstitution query by pairing the target with twocorrect substitutes and two confounders (see Ta-ble 1). Since substitutes provided by more annota-tors in CoInCo tend to be more reliable, we pickedthe two most frequently given substitutes for eachtarget. In the case of ties, we chose the lemma withthe highest corpus frequency.To acquire challenging confounders, we re-trieved the 20 most similar lemmas with the samepart of speech for each target (according to the un-igram space; cf. Section 3) and then eliminatedall annotator-provided substitutes for this target.From the remainder, we chose the two most closelymatched by corpus frequency to the frequenciesof the two chosen annotator-provided substitutes.Given the relatively high number of human substi-tutes in CoInCo, this results in highly similar, butcontextually inappropriate confounders. Finally,the acquired confounders were manually checkedto make sure that the automatic selection processdid not yield a context-appropriate substitute or asemantically unrelated term. In such cases, the nextbest candidate (by lemma similarity and frequency)was chosen.208targetconstruction arm build large airieldconstruction branch build large airieldsubstitute1substitute2construction partbuild large airieldconfounder1 construction backbuild large airieldconfounder2 construction handbuild large airieldTable 1: ANVAN-LS lexical substitution example3 Experimental SetupTask and Evaluation. We evaluate models onANVAN-LS in the form of a ranking task for eachquery: models are supposed to rank the correct sub-stitutes for each target higher than its confounders.Our evaluation measure is the mean average preci-sion (MAP) of all queries. In our case,MAP =1NN4Pli(k)arli(k)Xi=1Xk=1where k is the rank in the 4-item list l of substi-tution candidates, Pl(k) is the precision at cut-offpoint k in list l, the arl(k) is the change in recallfrom items k1 to k in l, and N is the total numberof ANVAN queries. We calculate MAP both over-all, and by target positions in ANVAN, to obtainmore detailed insights into performance dependingon part of speech and word position.aCorpus and Semantic Space. Following Baroniand Zamparelli (2010) and Paperno et al. (2014),we use a concatenation of the ukWaC, BNC, andEnglish Wikipedia corpora (around 2.8G words).We build a square co-occurrence matrix, usingthe complete vocabulary of our ANVAN sentencedataset, and a set of the most frequent contentwords in our corpus, for a total of 30K words. Inaddition, we built two co-occurrence matrices ofbigrams, composed of all predicates (adjectives andverbs) in our vocabulary and their most frequentnoun co-occurrents, as observed in the corpus, witha threshold of 300. The bigrams were observed inco-occurrence with the aforementioned 30K vocab-ulary and most frequent corpus terms, resulting ina 650K-by-30K matrix for A-N and N-N bigramsand a 620K-by-30K matrix for V-N bigrams.For all three matrices, the 3-word-window co-occurrence counts were transformed with PPMIand reduced to 300 dimensions with SVD. All mod-els were built with DISSECT (Dinu et al., 2013).two component-wiseCDSMs. We considerCDSMs:the simple additive and multiplicativemodels (Mitchell and Lapata, 2008), deined as2Scatchaaear+A2OcatchAaaaasound + aaaacatch2Scatch{Aaaear + aaaacatch,2Ocatch}aaaasoundaaearaaaacatch,{2Scatch,2Ocatch}Figure 1: Example for Practical Lexical Functionmodel composition for ear catch soundaav, wherep = uis either component-wise ad-dition or multiplication. We also work with twovariants of the Practical Lexical Function model(PLF), which are derived from the Lexical Func-tion Model (LF, Baroni and Zamparelli (2010)),which represents predicates (verbs and adjectivesin our case) as matrices to be multiplied with vectorrepresentations of their nominal arguments. Morespeciically, when applied to an ANVAN sentence(such as pointed ear catch sharp sound), the PLFmodel incorporates vector representations for eachof the ive constituent words, along with an ad-jective matrix for pointed and sharp, as well asverbsubject and verbobject matrices for the verb andits subject and object arguments. An example ofcomposition for a verb with its subject and objectarguments is given in Figure 1.Formally, the standard PLF (PLFPaperno) deinesthe composition for ANVAN-style sentences as:aaS + V 2V 2aaO + aaV, where aaS and aaO are theS AO Acomposed A-N bigrams, A2aaN. The requiredmatrices are learned with ridge regression fromunigram and bigram vectors.AGupta et al. (2015) pointed out that the standardPLF aovercountsa the predicate by adding it explic-itly, and proposed a rectiied variant which simplyleaves out the function word vector aaV. We alsoexperiment with this model, PLFGupta.All CDSMs rank the four candidates for eachtarget (cf. Table 1) by comparing the vector for theoriginal sentence against four sentences in whichthe target is replaced by the two correct and twoincorrect substitutes. We use the raw dot productas similarity measure, following Roller and Erk(2016), to boost frequent candidates.Lexical Substitution Model. As competitor, weconsider a dedicated lexical substitution model,namely context2vec (Melamud et al., 2016). Sinceit has demonstrated state-of-the-art performance onlexical substitution and word sense disambiguation210tasks, it is a suitable competitor model for CDSMson a similar problem. Context2vec uses word em-beddings to compute a set of viable substitutesgiven a context, using a bidirectional LSTM recur-rent neural network to build a sentential contextrepresentation. We work with two instantiations:irst, using only ANVAN as context (C2VANVAN),and second, using the full CoInCo sentence fromwhich the ANVAN was extracted (C2VSent). Theirst model is directly comparable to the CDSMs inthat it uses the same context information, while thesecond one enables us to gauge to what extent themodels can beneit from a richer context. We letcontext2vec generate 1000 substitutions for eachtarget word. If any substitution candidates werenot included in this list, the missing items weredeined to be tied at the last position, and the APwas deined as the MAP of all permutations of themissing items with respect to their ranking.Baselines. Two baselines are deined to compareour models against. The irst is the random base-line (Random): the MAP of all possible rankingsof two relevant and two irrelevant items, equal to0.680. The second baseline (LemmaSim) ranks thelemma-level similarities of the target word and itssubstitutes candidates without context.4 ResultsTable 2 lists the performance of our experimentson ANVAN-LS, irst evaluated for each target wordposition (top rows) and then averaged across alltarget positions (bottom row). The rightmost col-umn shows the average performance of all testedmodels. Even though there is some variance inthe numbers between subject-position targets andobject-position targets, we see consistent patternsby part of speech: adjectives are easiest to sub-stitute, followed by nouns, while verbs are sub-stantially more dificult. The dificulty with verbscorresponds to the indings of MediA c et al. (2017)for Croatian, who proposed a correlation betweenthe syntactic valence of words and their dificulty(but see below).The worst model by far is LemmaSim. This issurprising, given that Kremer et al. (2014) foundlemma-level similarity to be very competitive. Fur-ther analysis showed that its bad performance isdue to our choice of confounders as highly similarlemmas (cf. Section 2). An example where highsimilarity indicates syntagmatic rather than paradig-matic relatedness is ohio democrat embark over-land trip/*itinerary/*sightseeing/journey/travel,2where the two confounders can form noun com-pounds with the target, like sightseeing trip. Thesimple Add and Mult models also perform worsethan random, in contrast to many other studies, un-derscoring the dificulty of performing well on ourdataset. They do relatively well for adjectives, butworse on nouns, and Add struggles with verbs.The PLFPaperno model performs at baseline leveloverall, but shows particularly clear differencesamong parts of speech. It does very well for alladjectives and nouns, but very poorly on verbs,where it is in fact the worst model overall, bothmeasured absolutely, and relatively to the overallperformance of the model. An analysis showedthat this is indeed, as Gupta et al. (2015) claim,due to the role of valence, arguing that the functional roleof the verbs, and the disambiguation potential of itsargument positions, is better captured by the PLF.In contrast, the variance in performance betweenword positions that we ind for the different modelson the ANVAN-LS dataset indicates that the dif-iculty of substituting verbs might not be due tothe intrinsic factor of valence, but due to remain-ing shortcomings in all CDSM models to properlymodel predicate-argument combination.ANVAN-style sentences to more complex and var-ied sentence structures. It remains to be researchedhow capable CDSMs are to model meaning modu-lation that extends beyond the immediate predicate-argument structure (Kremer et al., 2014).Acknowledgments. We acknowledge partialfunding by Deutsche Forschungsgemeinschaftthrough SFB 732 (project D10) and by the CroatianScience Foundation under the project UIP-2014-09-7312. We would also like to thank DomagojAlagiA c for his support.5 ConclusionThis paper presented the case for using lexical sub-stitution as a better evaluation setup for compo-sitional distributional semantic models (CDSMs).We created a new corpus, ANVAN-LS, on the basisof a corpus with manually annotated lexical substi-tution, and evaluated a battery of models. Our eval-uation on this corpus (1) uses a corpus-based, ratherthan manually constructed, dataset, and should bemore indicative for the performance of models onareal-world dataa than previous ANVAN-based eval-uations; (2) is challenging, with a high baseline,which simple CDSMs like component-wise addi-tion and multiplication were indeed not able to beat;(3) enables a detailed evaluation at a per-positionlevel; (4) makes it possible, to our knowledge forthe irst time, to compare CDSMs with dedicatedlexical substitution models on par, and shows thatthe two model families perform comparably whenusing the same context, but differ in their perfor-mance by position.The last result in particular opens up a new lineof research, namely an investigation of similari-ties and differences between the two model fam-ilies. The improvement we observe for C2VSentover C2VANVAN in particular calls for a move fromReferencesMarco Baroni, Raffaela Bernardi, and Roberto Zampar-elli. 2014. Frege in space: A program of composi-tional distributional semantics. LiLT (Linguistic Is-sues in Language Technology) 9.Marco Baroni and Roberto Zamparelli. 2010. Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space. InProceedings of EMNLP. Cambridge, MA, pages1183a1193.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013. DISSECT a distributional semantics composi-tion toolkit. In Proceedings of ACL. Soia, Bulgaria,pages 31a36.Katrin Erk. 2012. Vector Space Models of Word Mean-ing and Phrase Meaning: A Survey. Language andLinguistics Compass 6(10):635a653.Edward Grefenstette. 2013.Category-theoreticquantitative compositional distributional modelsarXiv preprintof naturalarXiv:1311.1539.language semantics.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2012. Multi-step regression learning for composi-In Proceedings oftional distributional semantics.IWCS 2012. Potsdam, Germany.211Stephen Roller and Katrin Erk. 2016. Pic a differentword: A simple model for lexical substitution in con-In Proceedings of NAACL/HLT. San Diego,text.CA, pages 1121a1126.Howard Schuman and Stanley Presser. 1996. Ques-tions and answers in attitude surveys: Experimentson question form, wording, and context. Sage.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep modelsfor semantic compositionality over a sentiment tree-In Proceedings of EMNLP. Seattle, WA,bank.pages 1631a1642.Emiliano Guevara. 2010. A regression model ofadjective-noun compositionality in distributional se-mantics. In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Seman-tics. Uppsala, Sweden, pages 33a37.Abhijeet Gupta, Jason Utt, and Sebastian PadA o. 2015.Dissecting the practical lexical function model forcompositional distributional semantics. In Proceed-ings of STARSEM. Denver, Colorado, pages 153a158.Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and StephenPulman. 2013.Separating disambiguation fromcomposition in distributional semantics. In Proceed-ings of CoNLL. Soia, Bulgaria, pages 114a123.Gerhard Kremer, Katrin Erk, Sebastian PadA o, and Ste-fan Thater. 2014. What substitutes tell us-analysisof an aall-wordsa lexical substitution corpus. In Pro-ceedings of EACL. Gothenburg, Sweden, pages 540a549.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli. 2014a. SemEval-2014 Task 1: Evaluationof compositional distributional semantic models onfull sentences through semantic relatedness and tex-tual entailment. In Proceedings of SemEval. Dublin,Ireland, pages 1a8.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zampar-elli. 2014b. A SICK cure for the evaluation of com-In Pro-positional distributional semantic models.ceedings of LREC. ReykjavA Ak, Iceland, pages 216a223.Diana McCarthy and Roberto Navigli. 2009. The En-glish lexical substitution task. Language Resourcesand Evaluation 43(2):139a159.Zoran MediA c, Jan ESnajder, and Sebastian PadA o. 2017.Does free word order hurt? Assessing the PracticalIn Proceed-Lexical Function model for Croatian.ings of STARSEM. Vancouver, BC, pages 115a120.Oren Melamud, Jacob Goldberger, and Ido Dagan.2016. context2vec: Learning generic context em-In Proceedingsbedding with bidirectional LSTM.of CONLL. Berlin, Germany, pages 51a61.Jeff Mitchell and Mirella Lapata. 2008. Vector-basedmodels of semantic composition. In Proceedings ofACL. Columbus, OH, pages 236a244.Denis Paperno, Nghia The Pham, and Marco Baroni.2014. A practical and linguistically-motivated ap-proach to compositional distributional semantics. InProceedings of ACL. Baltimore, MD, pages 90a99.Martin Pickering and Steven Frisson. 2001. Process-ing ambiguous verbs: Evidence from eye move-ments. Journal of Experimental Psychology: Learn-ing, Memory, and Cognition 27(2).
</think>