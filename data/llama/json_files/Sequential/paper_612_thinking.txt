>

<think>
To extract datasets from the research paper titled "On Sentence Representations for Propaganda Detection: From Handcrafted Features to Word Embeddings" by AndrA e Ferreira Cruz and Gil Rocha and Henrique Lopes Cardoso, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the optimization and comparison of representations for the propaganda detection task, which suggests that datasets are likely involved.

In the **introduction**, the authors discuss the importance of propaganda detection and the need for high-quality datasets. They mention the use of a provided train dataset consisting of 350 articles, with a total of 16,965 sentences.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention the following datasets:

1. **Propaganda detection dataset**: This dataset consists of 350 articles, with a total of 16,965 sentences, where 4,720 sentences are labeled as propaganda and the remaining 12,245 are labeled as non-propaganda.

Now, I will check the **references section** to find the full citation for this dataset:

- For the **Propaganda detection dataset**, the citation is:
  > AndrA e Ferreira Cruz, Gil Rocha, and Henrique Lopes Cardoso. *On Sentence Representations for Propaganda Detection: From Handcrafted Features to Word Embeddings*. Proceedings of the 2nd Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda, pages 107a112, 2019.

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>