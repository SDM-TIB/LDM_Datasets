>

<think>
To extract datasets from the research paper titled "Modality and Component Aware Feature Fusion for RGB-D Scene Classification" by Anran Wang, Jianfei Cai, Jiwen Lu, and Tat-Jen Cham, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the optimization and comparison of representations for scene classification, which suggests that datasets are likely involved.

In the **introduction**, the authors discuss the limitations of standard full-image CNN features for scene classification and the importance of incorporating local CNN features from RGB-D data.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention two named datasets:

1. **SUNRGBD Dataset**: This dataset has 19 scene categories and consists of 10,335 RGB-D scene images.
2. **NYU Depth Dataset V2**: This dataset consists of 1,449 images and has 27 scene categories.

In the **results section**, the authors confirm that these datasets were used for extrinsic evaluation of their models, which reinforces their relevance.

Now, I will check the **references section** to find the full citations for these datasets:

- For **SUNRGBD Dataset**, the citation is:
  > Song et al. *Sun RGB-D: A RGB-D scene understanding benchmark suite*. In CVPR, pages 567a576, 2015.

- For **NYU Depth Dataset V2**, the citation is:
  > Silberman et al. *Indoor scene segmentation using a depth camera*. In ECCV, pages 746a760, 2012.

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>