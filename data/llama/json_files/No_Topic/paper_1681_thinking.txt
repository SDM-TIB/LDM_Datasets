>

<think>
To extract datasets from the research paper titled "Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 85a89,Tianjin, China, 20-21 DEC. 201285Micro blogs Oriented Word Segmentation SystemYijia Liua, Meishan Zhanga, Wanxiang Chea, Ting Liua, Yihe Dengaa Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China{yjliu,mszhang,car,tliu}@ir.hit.edu.cna School attached to Huazhong University of Science and TechnologyWuhan, 430074brooklet60@gmail.comAbstractWe present a Chinese word segmentationsystem submitted to the irst task on CLP2012 back-offs. Our segmenter is built us-ing a conditional random ield sequencemodel. We set the combination of a fewannotated micro blogs and People Dailycorpus as the training data. We encodespecial words detected by rules and infor-mation extracted from unlabeled data intofeatures. These features are used to im-prove our modelas performance. We alsoderive a micro blog speciied lexicon fromauto-analyzed data and use lexicon relatedfeatures to assist the model. When test-ing on the sample data of this task, thesefeatures result in 1.8% improvement overthe baseline model. Finally, our modelachieves F-score of 94.07% on the bake-offas test set.1 IntroductionChinese word segmentation is the initial stepof many NLP tasks,includes information re-trieve, dependency parsing and semantic role la-beling. Previous studies focus on word segmen-tation problem on standard data set, of which thetraining and testing data are drawn from same do-main. However itas not always true when it comesto micro blogs. As a new source of information,micro blogs produce rich vocabulary ranging overmany topics and changing with the times. Word-s like acaa never appear in traditional data set,but occur frequently in micro blogs. At the sametime, owing to the informal nature of micro blog,new type of words, such as URL, smiley and eventhe misspelled words, also make it very differentfrom traditional task.According to empirical analysis, one challengeof word segmentation on micro blogs is the sparsi-ty issue resulting from lack of micro blog speciieddata. Current systems trained on standard data setperform poorly on micro blogs, because of domainmismatch. However, building a micro blogs spe-ciic word segmenter in standard supervised man-ner requires a lot of annotated data. Mannual-ly creating them is a tedious and time-consumingwork. Semi-supervised approaches, which makeuse of large scale unlabeled data is a promising so-lution to this issue. It enhances the segmenter withmicro blog information and thus reduces sparsi-ty in labeled training data. Recent studies haveadopted semi-supervised approaches in word seg-mentation system(Wang et al., 2011; Sun and Xu,2011), and improvement over the traditional su-pervised approach is observed.Another challenge is the special wordas detec-tion. Due to the character of micro blogs, thereare plentiful special words, such as hash tag, user-name, URL. Here is an example of micro blogentry: a[e3 a1] # a a# @MCHOTDOGcc a a a a a c ahttp://t.cn/h0VJQ i14 a aoea@a34 a e3 a1 c i14/ [music] #Iam listening#@MCHOTDOC Mr. Ordinary http://t.cn/h0VJQ(share from @weibomusicbox)a. Words surround-ed by a#a are hash tag, usually indicating the top-ics of the micro blog. a@MCHOTDOCcca rep-resent user names, and ahttp://t.cn/h0VJQa is ashortened URL link.Itas usually dificult for aword segmentation model to learn these change-able words from the training data. However, somecertain type of special word can be detected bysome rules easily and unambiguously.In thispaper, we introduce some regular expressions tomatch special words in micro blog. The matchingresults, along with information extracted from un-86labeled data, are integrated into a CRF sequencemodel to learn a robust and high performancesegmenter. We also derive a lexicon from auto-analyzed micro blog data and enhance our modelwith the lexicon information.The reminder of this paper is organized as fol-lows. Section 2 describes the details of our system.Section 3 presents experimental results and empir-ical analysis. Section 4 concludes this paper.2 System ArchitectureIn this section, we describe the details of our sys-tem. We use some regular expressions to detectspecial words in micro blog. The detected wordboundary of URL, English word and special punc-tuation, along with other information from unla-beled data, are integrated into a CRF sequencemodel as features. We build our irst segmenterwith information mentioned above and use thissegmenter to parse large scale unlabeled data. Af-ter that, we extract a lexicon from auto-analyzeddata and retrained the CRF model with informa-tion provided by the lexicon. The architecture ofour system is illustrated in Figure 1.2.1 Model and Basic FeaturesWe employ a character-based sequence labelingmodel for word segmentation, which assign labelsto the characters indicating whether a character isthe beginning(B), inside(M), end of a word(E) or aunit-length word(S). A linear chain CRFs is usedto learn model from annotated data. When con-sidering the candidate character token ci, the basictypes of features of our model are listed below.a character unigram: cs (i a 2 a s a i + 2)a character bigram: cscs+1 (i a 2 a s a i + 1),cscs+2 (i a 2 a s a i)a character trigram: csa1cscs+1 (s = i)a repetition of characters: is cs equals cs+1 (ia1 a s a i), is cs equals cs+2 (i a 2 a s a i)a character type: is ci an alphabet, digit, punc-tuation or others2.2 Rule Detection FeaturesWe introduce regular expressions to detect threekinds of special words in micro blog, URL, En-glish word and Irregular suspension. These threetype of words are demonstrate as below.a URL: a c a c cU36e a e a i14http://t.cn/aBPi3D / Come and seethe reviews of newly released ASUS U36!http://t.cn/aBPi3Daa English word: aaaoColbie Caillat c2/Share Colbie Caillatas songaa Irregular suspension: aea ca34....... / Iamexpecting.......aWe encode word boundary detected by the reg-ular expressions into a new type of preprocessingfeatures. If the candidate character token ci, thefollowing features about URL is extracted.a beginning of a URL: U RL(ci) = Ba inside of a URL: U RL(ci) = Ma end of a URL: U RL(ci) = EFeatures of English word and irregular suspensioncan be represented in same manner.We expect that CRF model learns from thesematching results and this information assists theCRF model to detect special words and words sur-rounding them.2.3 Semi-supervised FeaturesInformation of unlabeled data can be easily com-puted and beneit the word segmentation mod-el. When integrated into machine learning frame-work, it will help reduce sparsity issue caused bythe out of vocabulary words.2.3.1 Mutual InformationIn probability theory, mutual information mea-sures the mutual dependency of two random vari-ables. Empirical study shows that observation ofhigh mutual information between two charactersmay indicates real association of these two charac-ters in a word, while low mutual information usu-ally means they belongs to different words.In this paper, we follow Sun and Xu (2011)asdeinition of mutual information. For a characterbigram cici+1, their mutual information is com-puted as follow:M I(cici+1) = logp(cici+1)p(ci)p(ci+1)ci, M I(cici+1)Foreach characterandM I(cia1ci) are computed and rounded downto integer. We incorporate these values into ourmodel as a type of features.87Auto-analyzeddataExtractinglexiconBoundaryofspecicalwordsMutualinforma-tion andaccessoryvarity..Labeled...............dataUnlabeledDataAuto-analyzeExtractinginformationMutualinforma-tion andaccessoryvarityRule detectBoundaryofspecicalwordsTrainingModelLexiconRetrainingNewModelFigure 1: System architecture2.3.2 Accessory VarietyAnother empirical study of word segmentationboundary is that if some n-gram appears in manydifferent environments, itas more likely that thisn-gram be a real word. Sun and Xu (2011) in-troduce a criterion Accessory Variety to evaluatehow independently a n-gram is used. In this pa-per, we follow this study and incorporate the fol-AV (c[i:i+la1]), Lllowing features LlAV (c[i+1:i+l]),RlAV (c[ial+1:i]), RlAV (c[ial:ia1]) (l = 2, 3, 4) in-to our model. Here LlAV (c[s:e])means accessor variety of strings with length l,c[s:e] means the character sequence starts from csand ends with ce.AV (c[s:e]) and Rl2.4 Extracting LexiconStudy has shown that CRF model can beneit fromlexicon features(Zhang et al., 2010). Micro blogspeciied lexicon provides a clue for detectingwords in unfamiliar context.In this paper, wetry to extract a micro blog speciied lexicon fromauto-analyzed data to improve our modelas perfor-mance.Firstly, we train a CRF model with features de-scribed in 2.2 and 2.3. We use this model toparse large scale unlabeled data, and a list of wordis obtained.Intuitively, high frequency word inthe auto-analyzed results is more likely to be realword. Therefore, we collect words that never oc-cur in the training data and rank them in order offrequency. A lexicon of words whose frequencyis higher than a threshold is extracted. In this pa-per, top 80% most frequent words is extracted. Wedrop the tokens with more than 5 characters, andthen build the lexicon.After the lexicon D is built, we encode the in-formation of lexicon into a type of features. Wefollow Zhang et al. (2010)as work on utilization oflexicon When considering ci, the lexicon featurewe extract is shown below:a match pref ix(ci, D) the length of longestword in lexicon D which starts with cia match mid(ci, D) the length oflongestword in lexicon D which contains with cia match suf f ix(ci, D) the length of longestword in lexicon D which ends with ci3 Experiments3.1 Data Preparation and SettingWe crawl some micro blog from September 1st,2011 to September 5nd, 2011, and drop the entrieswhich not contains simpliied Chinese characters.We got 1 million entries and use them as unlabeleddata. From these micro blog entries, we random-ly sampled 1,442 entries and manually annotatedtheir segmentation. This set of corpus is use as onepart of the labeled data. There are 23.3 words eachentry in the annotated micro blogs on average. Atthe same time, 183,630 lines of sentences fromPeople daily is also used as labeled data. All ofthe character in training and testing data is convertfrom single-byte character to double-byte charac-ter.We use a toolkit - CRFSuite(Okazaki, 2007) tolearning the sequence labeling model for segmen-tation. L-BFGS algorithm is set to solve the opti-mization problem.88We conclude our experiments result on the sam-ple data of the bake-off task. There are 503 en-tries in the test data set, with 38.9 words each en-try. Recall(R), precision(P ) and F1 is used as e-valuation metrics of system performance. We al-so report the recall of out of vocabulary(OOV)words(Roov).3.2 Effect of Annotated Micro blogIn this set experiments, we test performance of s-tandard supervised learning on different trainingdata. As mentioned above, we have a large setof annotated corpus on newswire and a small setof micro blogs. We expected that a combinationof these two corpus will help promote the perfor-mance.We extract basic features from this two data andtrained two CRF model BLpd and BLmb. Then wecombine two data and trained another CRF mod-el BLcomb. Performance of these three models isshown is Table 1.ModelBLpdBLmbBLcombP0.88200.89030.9161R0.86940.89250.9098F0.87570.89140.9130Table 1: Effect of different annotated corpusIn previous study, the state-of-the-art word seg-mentation system can achieve F-score of about97%(Che et al., 2010) when tested in-domain data.However, Table 1 shows that when applied to mi-cro blogs, traditional word segmentation systemasperformance drops severely.Experiment result also shows that, a small setof annotated micro blog corpus can achieve bet-ter performance than the traditional newswire cor-pus. And the model trained with combination oftwo corpus out performance the others. In the fol-lowing section, all of our models are built on thecombination of these two corpus.3.3 Effect of Rule Detection FeaturesTable 2 compares the baseline model with modelthat integrates rule detection features.ModelBL+PREP0.91610.9216R0.90980.9178F0.91300.9197Roov0.57630.6715Table 2: Effect of preprocessingthe modelas performance, especially the recall ofOOV. To give a farther analysis of rule detectionfeaturesa effect, we categorized words in test setinto four sort: URL, English word, Punctuation,Others and evaluate the recall of certain type ofword. Table 3 shows the experiment result.Model RU RL0.8940BL0.9536+PRERP unc0.98570.9862REng0.60180.9227ROthers0.89970.9040Table 3: Recall of preprocessing on four sort ofwordsThe experiment result shows that rule detectionfeatures improves the recall of special word type,especially the English words occur in micro blog.With more accurate detection of sepecial words,accuracy on ordinary words is also improved.3.4 Effect of Semi-supervised FeaturesTable 4 summarizes the experiment result on dif-ferent combination of semi-supervised features.PModel0.9216BL+PRE0.9282+MI0.9309+AV+MI+AV 0.9304R0.91780.92200.92310.9231F0.91970.92510.92700.9268Roov0.67150.70460.72500.7123Table 4: Effect of semi-supervised featuresIt can be seen that two types of semi-supervisedfeatures both result in improvement on perfor-mance. However, when two types of feature com-bined, the performance drops slightly. Empirical-ly, we consider that the effect of these two typefeatures overlaps due to they share some commonproperty.3.5 Effect of LexiconWe also compare our model integrating lexiconfeatures and without lexicon features. The resultsare shown in Table 5.ModelBL+PRE+MI+AV 0.93040.9352+LexiconPR0.92310.9275F0.92680.9314Roov0.71230.7337Table 5: Effect of lexicon featuresAs expected, lexicon features result in improve-ment over performance.3.6 Final SystemWe can see that rule detection features improveOur inal system is set as the coniguration ofaBL+PRE+MI+AV+Lexicona. Our experimental89results show that our inal system achieves an F-score of 93.14% and an improvement of 1.8%comparing to our baseline model. On the evalua-tion data of the bake-off, the F-score of our systemis 94.07%.
</think>