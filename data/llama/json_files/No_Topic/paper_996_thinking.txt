>
<think>
To extract datasets from the research paper titled "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 425a429, Atlanta, Georgia, June 14-15, 2013. c2013 Association for Computational Linguistics425SwatCS: Combining simple classiiers with estimated accuracySam Clark and Richard WicentowskiDepartment of Computer ScienceSwarthmore CollegeSwarthmore, PA 19081 USAsclark2@sccs.swarthmore.edu and richardw@cs.swarthmore.eduAbstractThis paper is an overview of the SwatCSsystem submitted to SemEval-2013 Task 2A:Contextual Polarity Disambiguation. The sen-timent of individual phrases within a tweetare labeled using a combination of classiierstrained on a range of lexical features. Theclassiiers are combined by estimating the ac-curacy of the classiiers on each tweet. Perfor-mance is measured when using only the pro-vided training data, and separately when in-cluding external data.1IntroductionSpurred on by the wide-spread use of the social net-works to communicate with friends, fans and cus-tomers around the globe, Twitter has been adoptedby celebrities, athletes, politicians, and major com-panies as a platform that mitigates the interaction be-tween individuals.Analysis of this Twitter data can provide insightsinto how users express themselves. For example,many new forms of expression and language fea-tures have emerged on Twitter, including expres-sions containing mentions, hashtags, emoticons, andabbreviations. This research leverages the lexicalfeatures in tweets to predict whether a phrase withina tweet conveys a positive or negative sentiment.2 Related WorkA common goal of past research has been to discoverand extract features from tweets that accurately in-dicate sentiment (Liu, 2010). The importance offeature selection and machine learning in sentimentanalysis has been explored prior to the rise of so-cial networks. For example, Pang and Lee (2004)apply machine learning techniques to extracted fea-tures from movie reviews.More recent feature-based systems include alexicon-based approach (Taboada et al., 2011), anda more focused study on the importance of both ad-verbs and adjectives in determining sentiment (Be-namara et al., 2007). Other examples include us-ing looser descriptions of sentiment rather than rigidpositive/negative labelings (Whitelaw et al., 2005)and investigating how connections between userscan be used to predict sentiment (Tan et al., 2011).This task differs from past work in sentiment anal-ysis of tweets because we aim to build a model capa-ble of predicting the sentiment of sub-phrases withinthe tweet rather than considering the entire tweet.Speciically, agiven a message containing a markedinstance of a word or a phrase, determine whetherthat instance is positive, negative or neutral in thatcontexta (Wilson et al., 2013). Research on context-oriented polarity predates the emergence of socialnetworks: (Nasukawa and Yi, 2003) predict senti-ment of subsections in a larger document.N-gram features, part of speech features andamicro-blogging featuresa have been used as accu-rate indicators of polarity (Kouloumpis et al., 2011).The amicro-blogging featuresa are of particular in-terest as they provide insight into how users haveadapted Twitter tokens to natural language to por-tray sentiment. These features include hashtags andemoticons (Kouloumpis et al., 2011).4263 DataThe task organizers provided a manually-labeled setof tweets. For parts of this study, their data was sup-plemented with external data (Go et al., 2009).As part of pre-processing, alltweets werepart-of-speech tagged using the ARK TweetNLPtools (Owoputi et al., 2013). All punctuation wasstripped, except for #hashtags, @mentions,emoticons :), and exclamation marks. All hyper-links were replaced with a common string, aURLa.3.1 Common DataThe provided training data was a collection of ap-proximately 15K tweets, manually labeled for senti-ment (positive, negative, neutral, or objective) (Wil-son et al., 2013). These sentiment labels appliedto a speciic phrase within the tweet and did notnecessarily match the sentiment of the entire tweet.Each tweet had at least one labeled phrase, thoughsome tweets had multiple phrases labeled individu-ally. Overall, 37% of tweets had one labeled phrase,with an average of 2.58 labeled phrases per tweet.Each of our classiiers were binary classiiers, la-beling phrases as either positive or negative. Assuch, approximately 10.5K phrases labeled as objec-tive or neutral were pruned from the training data,resulting in a inal training set containing 5362 la-beled phrases, 3445 positive and 1917 negative.The test data consisted of tweets and SMS mes-sages, although the training data contained onlytweets. The test set for the phrase-level task (Task A)contained 4435 tweets and 2334 SMS messages.3.2 Outside DataTask organizers allowed two submissions, a con-strained submission using only the provided trainingdata, and an unconstrained submission allowing theuse of external data. For the unconstrained submis-sion, we used a data set built by Go et al. (2009). Thedata set was automatically labeled using emoticonsto predict sentiment. We used a 50K tweet subsetcontaining 25K positive and 25K negative tweets.3.3 Phrase IsolationFor tweets containing a single labeled phrase, we usethe entire tweet as the context for the phrase. Fortweets containing two labeled phrases, we use theunigram labelhappy posgood posgreat poslove posbest posbigram labelnot going neglooking forward poshappy birthday poslast episode negiam mad negTable 1: The 5 most iniuential unigram and bigramsranked by information gain.context from the start of the tweet to the end of theirst phrase as the context for the irst phrase, and thecontext from the start of the second phrase to the endof the tweet for the second phrase. If more than twophrases are present, the context for any phrase in themiddle of the tweet is limited to only the words inthe labeled phrase.4 ClassiiersThe system uses a combination of naive Bayes clas-siiers to label the input. Each classiier is trained ona single feature extracted from the tweet. The classi-iers are combined using a conidence-weighted vot-ing scheme. The system applies a simple negationscheme to all of the language features used by theclassiiers. Any word following a negation term inthe phrase has the substring aNOTa preixed to it.This negation scheme was applied to n-gram fea-tures and lexicon features.4.1 N-gram FeaturesRather than use all of the n-grams as features, weranked each n-gram (w/POS tags) by calculating itschi-square-based information gain. The top 2000n-grams (1000 positive, 1000 negative) are used asfeatures in the n-gram classiier. Both a unigram andbigram classiier use these ranked (word/POS) fea-tures. Table 1 shows the highest ranked unigramsand bigrams using this method.4.2 Sentiment Lexicon FeaturesA second classiier uses the MPQA subjectivity lex-icon (Wiebe et al., 2005). We extract both the po-larity and the polarity strength for each word/POSin the lexicon matching a word/POS in the phraseascontext. We refer to this classiier as the lexiconclassiier.427unigramsunigramsrank classiier data polarity(C) positive(U) positivelexicon (C) negativelexicon (U) negative(C) positive(C) positive(U)(U)acc0.890.880.830.810.780.75novote <0.65novote <0.65tagcountbigramstagcountbigrams12345678Figure 1: Classiier accuracy increases as the differencebetween the probabilities of the labelings increases.4.3 Part of Speech and Special Token FeaturesThree additional classiiers were built using featuresextracted from the tweets. Our third classiier usesonly the raw counts of speciic part of speech tags:adjectives, adverbs,interjections, and emoticons.The fourth classiier uses the emoticons as a fea-ture. To reduce the noise in the emoticon feature set,many (over 25) different emoticons are mapped tothe basic a:)a and a:(a expressions. Some emoticonssuch as axDa did not map to these basic expressions.A ifth classiier gives added weight to words withextraneous repeated letters. Words containing twoor more repeated letters (that are not in a dictionary,e.g. aheyyyyya, asweeeeta) are mapped to their pre-sumed correct spelling (e.g. aheya, asweeta).5 Conidence-Based ClassiicationTo combine all of the classiiers, the system esti-mates the conidence of each classiier and only ac-cepts the classiication output if the conidence ishigher than a speciied baseline. To establish a clas-siieras conidence, we take the absolute value ofthe difference between a classiieras positive outputprobability and negative output probability, whichwe call alpha. Alpha values close to 1 indicate highconidence in the predicted label; values close to 0indicate low conidence in the predicted label.Table 2: An example of the polarity and correspondingaccuracy output for each classiier for a single tweet. Thelabels (C) and (U) indicate whether the classiier wastrained on constrained training data or on unconstraineddata (Go et al., 2009).ier at alpha values between 0 and 1. The result isa trained classiier with an approximation of overallclassiication accuracy at a given alpha value. Fig-ure 1 shows the relationship between alpha valueand overall classiier accuracy. As expected, classi-ication accuracy increases as conidence increases.Table 2 shows the breakdown of classiier accu-racy for a single tweet using both provided and ex-ternal data. The accuracy listed is the classiier-speciic accuracy determined by the alpha value forthat phrase in the tweet. Using a dev set, we ex-perimentally established the most effective baselineIn the voting system described below,to be 0.65.only classiiers with conidence above the baseline(per marked phrase) are used. Therefore, the spe-ciic combination of classiiers used for each phrasemay be different.An unlabeled phrase is assigned a polarity andconidence value from each classiier. These proba-bilities are combined using a voting system to deter-mine a single output. This voting system calculatesthe inal labeling by computing the average proba-bility for each label only for those classiiers withestimated accuracies above the baseline. The labelwith the highest overall probability is selected.6 Results5.1 Classiier VotingThe predicted accuracy of each classiier is deter-mined after the trained classiiers are evaluated us-ing a development set with known labels. Using thedev set, we calculate the accuracy of each classi-
</think>