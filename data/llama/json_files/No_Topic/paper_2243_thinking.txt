>

<think>
To extract datasets from the research paper titled "Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge TransferAnonymous ACL submission001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042AbstractMachines that can represent and describe environmental soundscapes have practical potential, e.g., for audio tagging and captioning. Pre-vailing learning paradigms of audio-text connections have been relying on parallel audio-text data, which is, however, scarcely available on the web. We propose vip-AnT that induces Audio-Text alignment without using any parallel audio-text data. Our key idea is to share the image modality between bi-modal image-text representations and bi-modal image-audio representations; the image modality functions as a pivot and connects audio and text in a tri-modal embedding space implicitly.In a difficult zero-shot setting with no paired audio-text data, our model demonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio classification tasks, and even surpasses the supervised state of the art for Clotho caption retrieval (with audio queries) by 2.2% R@1. We further investigate cases of minimal audio-text supervision, finding that, e.g., just a few hundred supervised audio-text pairs increase the zero-shot audio classification accuracy by 8% on US8K. However, to match human parity on some zero-shot tasks, our empirical scaling experiments suggest that we would need about 221 a 2M supervised audio-caption pairs. Our work opens up new avenues for learning audio-text connections with little to no parallel audio-text data.1IntroductionEnvironmental sound provides rich perspectives on the physical world. For example, if we hear joyful laughing, a playful scream, and a splash; when not only can visualize literal objects / actions that might have given rise to the audio scene, but also, we can reason about plausible higher-level facets, e.g., a child speeding down a water slide at a waterpark, splashing through the water (see Figure 1). Machines capable of parsing, representing, and describing such environmental sound hold practical promise. For example, according to the National Association of the Deaf as captioning guide, accessible audio caption generation systems should go beyond speech recognition (i.e., identifying speakers and transcribing the literal content of their speech) and provide the textual description of all the sound effects, e.g., a large group of people talking excitedly at a party.1The dominant paradigm for studying machine hearing (Lyon, 2010) has been through human-annotated audio-text data, where text is either free-form audio descriptions (the sound of heavy rain) or tagsets (Salamon et al., 2014; Gemmeke et al., 2017; Kim et al., 2019; Drossos et al., 2020). While naturally aligned audio-text data could be sourced from audio-tag co-occurrences (Font et al., 2013) and from video captioning data (Rohrbach et al., 2015; Xu et al., 2016; Oncescu et al., 2021a), they are either not sufficient enough to relate to environmental sound or limited in their scale and coverage.In this paper, we study large-scale audio-text alignment without paired audio-text (AT) data. Inspired by pivot-based models for unsupervised machine translation (Wu and Wang, 2007; Utiyama and Isahara, 2007) and image pivots (Specia et al., 2016; Hitschler et al., 2016; Nakayama and Nishida, 2017) have been explored in machine translation. Pivot-based models have also been shown to be helpful in learning image-text alignment (Li et al., 2020). We focus on the tri-modal case.
</think>