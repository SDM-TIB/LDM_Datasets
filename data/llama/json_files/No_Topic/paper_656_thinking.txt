>

<think>
To extract datasets from the research paper titled "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 486a496, Dublin, Ireland, August 23-29 2014.486Fast Tweet Retrieval with Compact Binary CodesWeiwei Guo Wei Liua Mona DiabaaComputer Science Department, Columbia University, New York, NY, USAa IBM T. J. Watson Research Center, Yorktown Heights, NY, USAaDepartment of Computer Science, George Washington University, Washington, D.C., USAweiwei@cs.columbia.eduweiliu@us.ibm.commtdiab@gwu.eduAbstractThe most widely used similarity measure in the ield of natural language processing may be co-sine similarity. However, in the context of Twitter, the large scale of massive tweet data inevitablymakes it expensive to perform cosine similarity computations among tremendous data samples.In this paper, we exploit binary coding to tackle the scalability issue, which compresses each datasample into a compact binary code and hence enables highly eficient similarity computations viaHamming distances between the generated codes. In order to yield semantics sensitive binarycodes for tweet data, we design a binarized matrix factorization model and further improve it intwo aspects. First, we force the projection directions employed by the model nearly orthogonal toreduce the redundant information in their resulting binary bits. Second, we leverage the tweetsaneighborhood information to encourage similar tweets to have adjacent binary codes. Evaluatedon a tweet dataset using hashtags to create gold labels in an information retrieval scenario, ourproposed model shows signiicant performance gains over competing methods.1IntroductionTwitter is rapidly gaining worldwide popularity, with 500 million active users generating more than340 million tweets daily1. Massive-scale tweet data which is freely available on the Web contains richlinguistic phenomena and valuable information, therefore making it one of most favorite data sourcesused by a variety of Natural Language Processing (NLP) applications. Successful examples includeirst story detection (Petrovic et al., 2010), local event detection (Agarwal et al., 2012), Twitter eventdiscovery (Benson et al., 2011) and summarization (Chakrabarti and Punera, 2011), etc.In these NLP applications, one of core technical components is tweet similarity computing to searchfor the desired tweets with respect to some sample tweets. For example, in irst story detection (Petrovicet al., 2010), the purpose is to ind an incoming tweet that is expected to report a novel event not revealedby the previous tweets. This is done by measuring cosine similarity between the incoming tweet andeach previous tweet.One obvious issue is that cosine similarity computations among tweet data will become very slow oncethe scale of tweet data grows drastically. In this paper, we investigate the problem of searching for mostsimilar tweets given a query tweet. Speciically, we propose a binary coding approach to render com-putationally eficient tweet comparisons that should beneit practical NLP applications, especially in theface of massive data scenarios. Using the proposed approach, each tweet is compressed into short-lengthbinary bits (i.e., a compact binary code), so that tweet comparisons can be performed substantially fasterthrough measuring Hamming distances between the generated compact codes. Crucially, Hammingdistance computation only involves very cheap NOR and popcount operations instead of ioating-pointoperations needed by cosine similarity computation.Compared to other genres of data, similarity search in tweet data is very challenging due to the shortnature of Twitter messages, that is, a tweet contains too little information for traditional models to extract
</think>