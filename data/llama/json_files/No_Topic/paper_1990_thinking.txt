>

<think>
To extract datasets from the research paper titled "A Low Power, Fully Event-Based Gesture Recognition System" by Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, Jeff Kusnitz, Michael Debole, Steve Esser, Tobi Delbruck, Myron Flickner, and Dharmendra Modha, I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the use of a new hand gesture dataset and a DVS camera.

In the **introduction**, the authors discuss the limitations of traditional frame-based cameras and the advantages of event-based cameras.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention the following datasets:

1. **DvsGesture**: A new hand gesture dataset created using a DVS camera.
2. **DVS128**: A 128 A 128-pixel Dynamic Vision Sensor used to generate input events.

In the **results section**, the authors confirm that these datasets were used for evaluation.

Now, I will check the **references section** to find the full citations for these datasets:

- For **DvsGesture**, the citation is:
  > The DvsGesture dataset comprises 1,342 instances of a set of 11 hand and arm gestures (Fig. 7), grouped in 122 trials collected from 29 subjects under 3 different lighting conditions.

- For **DVS128**, the citation is:
  > The iniLabs DVS128 camera is a 128 A 128-pixel Dynamic Vision Sensor that generates events only when a pixel value changes magnitude by a user-tunable threshold.

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>