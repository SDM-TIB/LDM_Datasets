>

<think>
To extract datasets from the research paper titled "Transactions of the Association for Computational Linguistics, vol. 3, pp. 145a156, 2015. Action Editor: Ryan McDonald.Submission batch: 10/2014; Revision batch 1/2015; Published 3/2015. c2015 Association for Computational Linguistics.145Entity Disambiguation with Web LinksAndrew ChisholmSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiaandy.chisholm.89@gmail.comBen HacheySchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiaben.hachey@gmail.comAbstractEntity disambiguation with Wikipedia relieson structured information from redirect pages,article text, inter-article links, and categories.We explore whether web links can replace acurated encyclopaedia, obtaining entity prior,name, context, and coherence models froma corpus of web pages with links to Wiki-pedia. Experiments compare web link modelsto Wikipedia models on well-known CoNLLand TAC data sets.Results show that using 34 million web linksapproaches Wikipedia performance. Combin-ing web link and Wikipedia models producesthe best-known disambiguation accuracy of88.7 on standard newswire test data.1 IntroductionEntity linking (EL) resolves mentions in text to theircorresponding node in a knowledge base (KB), orNIL if the entity is not in the KB. Wikipedia andrelated semantic resources a Freebase, DBpedia,Yago2a have emerged as general repositories of no-table entities. The availability of Wikipedia, in par-ticular, has driven work on EL, knowledge base pop-ulation (KBP), and semantic search. This literaturedemonstrates that the rich structure of Wikipediaaredirect pages, article text, inter-article links, cat-egories a delivers disambiguation accuracy above85% on newswire (He et al., 2013; Alhelbawy andGaizauskas, 2014). But what disambiguation accu-racy can we expect in the absence of Wikipediaascurated structure?Web links provide much of the same informationas Wikipedia inter-article links: anchors are used toderive alternative names and conditional probabili-ties of entities given names; in-link counts are usedto derive a simple entity popularity measure; thetext surrounding a link is used to derive textual con-text models; and overlap of in-link sources is usedto derive entity cooccurrence models. On the otherhand, web links lack analogues of additional Wiki-pedia structure commonly used for disambiguation,e.g., categories, encyclopaedic descriptions. More-over, Wikipediaas editors ensure a clean and correctknowledge source while web links are a potentiallynoisier annotation source.We explore linking with web links versus Wiki-pedia. Contributions include:(1) a new bench-mark linker that instantiates entity prior probabili-ties, entity given name probabilities, entity contextmodels, and eficient entity coherence models fromWikipedia-derived data sets; (2) an alternative linkerthat derives the same model using only alternativenames and web pages that link to Wikipedia; (3) de-tailed development experiments, including analysisand proiling of Web link data, and a comparison oflink and Wikipedia-derived models.Results suggest that web link accuracy is at least93% of a Wikipedia linker and that web links arecomplementary to Wikipedia, with the best scorescoming from a combination. We argue that these re-sults motivate open publishing of enterprise author-ities and suggest that accumulating incoming linksshould be prioritised at least as highly as addingricher internal structure to an authority.1462 Related workThomas et al. (2014) describe a disambiguation ap-proach that exploits news documents that have beencurated by professional editors. In addition to con-sistently edited text, these include document-leveltags for entities mentioned in the story. Tags areexploited to build textual mention context, assignweights to alternative names, and train a disam-biguator. This leads to an estimated F1 score of78.0 for end-to-end linking to a KB of 32,000 com-panies. Our work is similar, but we replace qual-ity curated news text with web pages and explore alarger KB of more than four million entites. In placeof document-level entity tags, hyperlinks pointing toWikipedia articles are used to build context, nameand coherence models. This is a cheap form of third-party entity annotation with the potential for gener-alisation to any type of web-connected KB. How-ever, it presents an additional challenge in copingwith noise, including prose that lacks editorial over-sight and links with anchor text that do not corre-spond to actual aliases.Li et al. (2013) explore a similar task setting formicroblogs, where short mention contexts exacer-bate sparsity problems for underdeveloped entities.They address the problem by building a topic modelbased on Wikipedia mention link contexts. A boot-strapping approach analogous to query expansionaugments the model using web pages returned fromthe Google search API. Results suggest that thebootstrapping process is beneicial, improving per-formance from approximately 81% to 87% accu-racy. We demonstrate that adding link data leads tosimilar improvements.The cold start task of the Text Analysis Confer-ence is also comparable.1 It evaluates how well sys-tems perform end-to-end NIL detection, clusteringand slot illing. Input includes a large document col-lection and a slot illing schema. Systems return aKB derived from the document collection that con-forms to the schema. The evaluation target is long-tail or local knowledge. The motivation is the sameas our setting, but we focus on cold-start linkingrather than end-to-end KB population.Finally, recent work addresses linking without1http://www.nist.gov/tac/2014/KBP/ColdStart/guidelines.htmland beyond Wikipedia. Jin et al. (2014) describe anunsupervised system for linking to a person KB froma social networking site, and Shen et al. (2014) de-scribe a general approach for arbitrary KBs. Nakas-hole et al. (2013) and Hoffart et al. (2014) add a tem-poral dimension to NIL detection by focusing on dis-covering and typing emerging entities.3 Tasks and artTwo evaluations in particular have driven compar-ative work on EL:the TAC KBP shared tasks andthe Yago2 annotation of CoNLL 2003 NER data. Wedescribe these tasks and their respective evaluationsetup. A brief survey of results outlines the kind ofperformance we hope to achieve with link data. Fortask history, we suggest Hachey et al. (2013) andShen et al. (2014). For an evaluation survey, seeHachey et al. (2014).|D|), number of mentions (Our evaluation setup follows He et al. (2013) forcomparability to their state-of-the-art disambigua-tion results across CoNLL and TAC data. Table 1summarises the data sets used. Columns correspond), number of entitiesto number of documents (), and number of(|E|non-NIL mentions (). The non-NIL mentionnumber represents the set used for evaluation in thedisambiguation experiments here. The table also in-cludes average and standard deviation of the candi-) and the percent-date set cardinality overMKB (ihage of mentions inMKB where the correct resolu-tion is in the candidate set (R). The last columnC(SOA) gives the state-of-the-art score from the liter-ature. Numbers are discussed below.|MKB||M|C3.1 CoNLLCoNLL is a corpus of Reuters newswire annotatedfor whole-document named entity recognition anddisambiguation (Hoffart et al., 2011). CoNLL is pub-lic, free and much larger than most entity annota-tion data sets, making it an excellent evaluation tar-get. It is based on the widely used NER data fromthe CoNLL 2003 shared task (Tjong Kim Sang andMeulder, 2003), building disambiguation on groundtruth mentions. Training and development splitscomprise 1,162 stories from 22-31 August 1996 andthe held-out test split comprises 231 stories from 6-7December 1996.147Data setCoNLL trainCoNLL devCoNLL testTAC trainTAC test|D|9452162311,0401,012|E|4,0801,6441,537456387|M|23,3965,9175,6161,5002,250(%)|MKB|18,505 (79)4,791 (80)4,485 (80)1,070 (71)1,017 (45)iC(I)h69 (194)73 (194)73 (171)23 (28)24 (30)SOARC100 NA79.710010087.694.4 NA81.088.5Table 1: Data sets for disambiguation tasks addressed here. Statistics are described in Section 3.
</think>