>

<think>
To extract datasets from the research paper titled "Eicient Attention Mechanism for Multiple Inputs in Visual Dialog" by Van-Quang Nguyen et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, materials and methods, and results sections** to identify any datasets that are mentioned or referenced. The abstract mentions the VisDial datasets, which suggests that datasets are likely involved.

In the **introduction**, the authors discuss the task of visual dialog and the need for a neural architecture that can handle interactions between multiple inputs. They mention the VisDial datasets as a benchmark for visual dialog tasks.

Next, I will focus on the **materials and methods section**, particularly the subsections that detail the datasets used for evaluation. Here, the authors mention the following datasets:

1. **VisDial v1.0**: A dataset for visual dialog tasks, consisting of 123,287 images with dialog composed of 10 question-answer pairs along with a caption.
2. **VisDial v0.9**: A dataset for visual dialog tasks, consisting of 82,783 images with dialog composed of 10 question-answer pairs along with a caption.
3. **Audio Visual Scene-aware Dialog (AVSD) task**: A dataset for visual dialog tasks, consisting of videos with dialog and audio features.

In the **results section**, the authors confirm that these datasets were used for extrinsic evaluation of their models, which reinforces their relevance.

Now, I will check the **references section** to find the full citations for these datasets:

- For **VisDial v1.0**, the citation is:
  > Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D., Batra, D. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 326a335 (2017)

- For **VisDial v0.9**, the citation is:
  > Das, A., Kottur, S., Moura, J.M., Lee, S., Batra, D. Learning cooperative visual dialog agents with deep reinforcement learning. In Proceedings of the IEEE International Conference on Computer Vision. pp. 2951a2960 (2017)

- For **AVSD task**, the citation is:
  > Hori, C., Alamri, H., Wang, J., Wichern, G., Hori, T., Cherian, A., Marks, T.K., Cartillier, V., Lopes, R.G., Das, A., et al. End-to-end audio visual scene-aware dialog using multimodal attention-based video features. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 2352a2356 (2019)

After gathering this information, I will compile the dataset entries into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>