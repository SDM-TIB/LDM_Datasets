>

<think>
To extract datasets from the research paper titled "On Layer Normalization in the Transformer Architecture" by Ruibin Xionga * 1 2 Yunchang Yang* 3 Di He 4 5 Kai Zheng 4 Shuxin Zheng 5 Chen Xing 6 Huishuai Zhang 5Yanyan Lan 1 2 Liwei Wang 4 3 Tie-Yan Liu 5Abstract1. Introduction0202nuJ92]GL.sc[2v54740.2002:viXraThe Transformer is widely used in natural lan-guage processing tasks. To train a Transformerhowever, one usually needs a carefully designedlearning rate warm-up stage, which is shown tobe crucial to the inal performance but will slowdown the optimization and bring more hyper-parameter tunings. In this paper, we irst studytheoretically why the learning rate warm-up stageis essential and show that the location of layer nor-malization matters. Speciically, we prove withmean ield theory that at initialization, for theoriginal-designed Post-LN Transformer, whichplaces the layer normalization between the resid-ual blocks, the expected gradients of the parame-ters near the output layer are large. Therefore, us-ing a large learning rate on those gradients makesthe training unstable. The warm-up stage is prac-tically helpful for avoiding this problem. On theother hand, our theory also shows that if the layernormalization is put inside the residual blocks(recently proposed as Pre-LN Transformer), thegradients are well-behaved at initialization. Thismotivates us to remove the warm-up stage for thetraining of Pre-LN Transformers. We show in ourexperiments that Pre-LN Transformers withoutthe warm-up stage can reach comparable resultswith baselines while requiring signiicantly lesstraining time and hyper-parameter tuning on awide range of applications.*Equal contribution a Works done while interning at MicrosoftResearch Asia 1CAS Key Laboratory of Network Data Scienceand Technology, Institute of Computing Technolog, ChineseAcademy of Sciences 2University of Chinese Academy of Sci-ences 3Center for Data Science, Peking University, Beijing Insti-tute of Big Data Research 4Key Laboratory of Machine Perception,MOE, School of EECS, Peking University 5Microsoft Research6College of Computer Science, Nankai University. Correspon-dence to: Shuxin Zheng <shuxin.zheng@microsoft.com>, Di He<dihe@microsoft.com>.Proceedings of the 37 th International Conference on MachineLearning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 bythe author(s).
</think>