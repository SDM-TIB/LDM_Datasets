# Llama Base Model Results Directory

This directory contains the output generated by the base Llama-3.2-3B-Instruct model without fine-tuning. These results serve as a baseline for evaluating the improvements achieved through knowledge distillation and fine-tuning.

## Directory Structure

- **json_files/**: Contains JSON files with dataset metadata extracted by the base Llama model
  - Similar structure to the `gs/json_files/` directory, with subdirectories for different research areas
  - Each paper has two associated files:
    - `paper_X.json`: The extracted dataset information in JSON format
    - `paper_X_thinking.txt`: The reasoning process that led to the extraction

## Model Information

The base Llama-3.2-3B-Instruct model:
- Has 3 billion parameters
- Is a general-purpose instruction-tuned language model
- Has not been specifically trained for the dataset extraction task
- Processes papers using the same prompting approach as the fine-tuned model, but without the benefit of task-specific training

## Generation Process

The results in this directory were generated using:
1. The `process_papers.py` script in the `inference/` directory
2. The base Llama-3.2-3B-Instruct model without modifications
3. The same input papers as used for the gold standard
4. The same prompt template and DCAT vocabulary structure

## Performance

The base model achieves:
- An F1 score of 0.65 for dataset identification
- Lower accuracy on challenging fields like creator identification (F1 score of 0.17)
- Processing time of approximately 35 seconds per paper

The results in this directory demonstrate the baseline performance without task-specific fine-tuning, highlighting the impact of knowledge distillation and reasoning preservation in the fine-tuned model.