# Llama Tuned Model Results Directory

This directory contains the output generated by the fine-tuned Llama-3.2-3B-Instruct model, which is the primary model of the MetaMine project. This model has been trained through knowledge distillation from the teacher model (GPT-o4-mini) with reasoning preservation.

## Directory Structure

- **json_files/**: Contains JSON files with dataset metadata extracted by the fine-tuned Llama model
  - Similar structure to the `gs/json_files/` directory, with subdirectories for different research areas
  - Each paper has two associated files:
    - `paper_X.json`: The extracted dataset information in JSON format
    - `paper_X_thinking.txt`: The reasoning process that led to the extraction

## Model Information

The fine-tuned Llama-3.2-3B-Instruct model:
- Has 3 billion parameters
- Has been fine-tuned using LoRA (Low-Rank Adaptation) for parameter-efficient training
- Underwent knowledge distillation from the teacher model (GPT-o4-mini)
- Preserves the reasoning process through the use of the `<think>` tag
- Is specifically optimized for dataset extraction from scientific papers

## Fine-Tuning Process

The model was fine-tuned using:
1. A training dataset of 20,000 papers processed by the teacher model
2. QLoRA fine-tuning with 4-bit quantization
3. Special focus on preserving the reasoning process
4. DeepSpeed for distributed training on 4 NVIDIA H100 GPUs

## Performance

The fine-tuned model achieves:
- An F1 score of 0.74 for dataset identification (compared to 0.65 for the base model)
- Significantly higher accuracy on challenging fields like creator identification (F1 score of 0.59)
- Processing time of approximately 35 seconds per paper

The results in this directory demonstrate the effectiveness of knowledge distillation and reasoning preservation in improving the model's ability to extract structured dataset metadata from scientific papers.