

<think>
To extract datasets from the research paper titled "Discourse-Aware Neural Extractive Text Summarization" by Jiacheng Xu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract discusses the use of discourse-aware neural summarization models and mentions two datasets, which indicates that datasets are indeed present.Next, I will focus on the **experiments section** where the authors typically describe the datasets used for evaluation. In this section, they explicitly mention two datasets:1. **New York Times (NYT)**: This dataset is used for evaluating the proposed model's performance in text summarization. The authors provide statistics about the dataset, including the number of documents and the average number of sentences, EDUs, and tokens.2. **CNN and Daily Mail (CNNDM)**: This dataset is another benchmark for evaluating the model's performance in text summarization. The authors also provide statistics about this dataset, including the number of samples for training, validation, and testing.After identifying the datasets, I will check the **References section** to retrieve the full citations for each dataset:- For **New York Times (NYT)**, the citation is: > Evan Sandhaus. *The New York Times Annotated Corpus*. Linguistic Data Consortium, Philadelphia, 2008.- For **CNN and Daily Mail (CNNDM)**, the citation is: > Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. *Teaching Machines to Read and Comprehend*. In Advances in Neural Information Processing Systems, 28, 1693a1701, 2015.Now that I have the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>