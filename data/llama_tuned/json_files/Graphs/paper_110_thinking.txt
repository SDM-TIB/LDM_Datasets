<think>
To extract datasets from the research paper titled "Aligning Cross-Lingual Entities with Multi-Aspect Information" by Hsiu-Wei Yang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract discusses multilingual knowledge graphs and cross-lingual entity alignment, which suggests that datasets related to these topics may be included.Next, I will focus on the **experiments section**, specifically **section 4.1 (Datasets and Settings)**, where the authors explicitly mention the datasets used for evaluation. Here, they refer to two benchmark datasets: **DBP15K** and **DBP100K**. I will extract details about these datasets, including their characteristics and the number of inter-lingual links (ILLs) they contain.In **section 4.1**, the authors provide statistics about the datasets:1. **DBP15K**: This dataset contains 15,000 ILLs and is divided into three subsets: Chinese-English (ZH-EN), Japanese-English (JA-EN), and French-English (FR-EN).2. **DBP100K**: This dataset contains 100,000 ILLs and also includes the same subsets as DBP15K.Now, I will check the **References section** to find the full citations for these datasets:- For **DBP15K**, the citation is: > Zequn Sun, Wei Hu, and Chengkai Li. *Cross-lingual entity alignment via joint attribute learning*. In Proceedings of the International Semantic Web Conference (ISWC), pages 1-15, 2017.- For **DBP100K**, the citation is: > Zequn Sun, Wei Hu, and Chengkai Li. *Cross-lingual entity alignment via joint attribute learning*. In Proceedings of the International Semantic Web Conference (ISWC), pages 1-15, 2017.After gathering this information, I will compile the dataset entries, ensuring that I include the dataset names, descriptions, and full citations. This will provide a clear and structured overview of the datasets used in the research.
</think>