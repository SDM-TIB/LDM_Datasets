

<think>
To extract datasets from the research paper titled "Bilingual Sentence Selection for Statistical Machine Translation" by Guillem GascÃ³ et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract discusses the challenges of training statistical machine translation systems and hints at the use of specific datasets for evaluation, which suggests that datasets are present.Next, I will focus on the **experiments section** where the authors typically describe the datasets used for their experiments. In this paper, they mention using two corpora for their experiments:1. **TED Corpus**: This is an English-French corpus composed of subtitles for a collection of public speeches on various topics. The authors specify that they used the TED training set and the TED test set for their experiments.2. **News Commentary Corpus**: This is a slightly larger English-French corpus in the news domain. The authors mention using the News Commentary corpus for their experiments, specifically the 2009 test set.Now, I will check the **References section** to find the full citations for these datasets:- For the **TED Corpus**, the citation is: > Michael Paul, Marcello Federico, and Sebastian Stolcke. *Overview of the IWSLT 2010 evaluation campaign*. In Proceedings of the IWSLT 2010, pages 1a28, Paris, France, December 2010.- For the **News Commentary Corpus**, the citation is: > Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. *Findings of the 2009 Workshop on Statistical Machine Translation*. In Proceedings of the WSMT, pages 1a28, Athens, Greece, March 2009.After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a clear and structured overview of the datasets used in the research paper.
</think>