

<think>
To extract datasets from the research paper titled "Robust Noisy Correspondence Learning with Equivariant Similarity Consistency" by Yuchen Yang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, methodology, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on three widely used datasets, which suggests that specific datasets will be detailed later in the paper.Next, I will focus on the **experiments section** where the authors explicitly mention the datasets used for evaluation. In this section, they list the datasets as follows:1. **Flickr30K**: This dataset contains 31,014 images with five captions each, collected from the Flickr website. The authors specify that they used 1,014 images for model validation, 1,000 for model testing, and 29,000 for model training.2. **MS-COCO**: This dataset is widely used in cross-modal learning and contains 123,287 images, with each image associated with five captions. The authors mention that they used 5,000 images for modal validation, 5,000 for model testing, and 113,287 for model training.3. **Conceptual Captions**: This dataset is a large-scale real-world dataset containing about 3,334,173 images with a single caption each. The authors note that they used a smaller version of this dataset, CC152K, which consists of 1,000 images for model validation, 1,000 for model testing, and 150,000 images for model training.Now, I will check the **References section** to retrieve the full citations for these datasets:- For **Flickr30K**, the citation is: > Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmayer. *From image descriptions to visual denotations: News similarity metrics for semantic inference over event descriptions*. Transactions of the Association for Computational Linguistics, 2:67a78, 2014.- For **MS-COCO**, the citation is: > TY Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P DollAr, CL Zitnick, et al. *Microsoft coco: Common objects in context*. 2014.- For **Conceptual Captions**, the citation is: > Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. *Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning*. In ACL, pages 2556a2565, 2018.With this information, I will compile the dataset entries, ensuring that each dataset is accurately described and includes the full citation. This will provide a comprehensive overview of the datasets used in the research paper.
</think>