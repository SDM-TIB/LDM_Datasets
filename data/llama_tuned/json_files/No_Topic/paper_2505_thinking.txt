

<think>
To extract datasets from the research paper titled "Online Multi-Object Tracking via Robust Collaborative Model and Sample Selection" by Mohamed A. Naiel et al., I will follow a systematic approach to ensure I capture all relevant information accurately.First, I will read through the **abstract, introduction, and experimental results sections** to identify any datasets mentioned. The abstract provides a brief overview of the research, and the introduction often outlines the datasets used for evaluation.Next, I will focus on the **experimental results section**, specifically looking for a subsection that details the datasets used. In this paper, the authors mention evaluating their method on several benchmark datasets, which indicates that datasets are indeed present.In the **experimental results section**, I find the following datasets mentioned:1. **PETS09-S2L1**: This dataset consists of 799 frames of 768 × 576 pixels recorded at 7 frames per second with medium crowd density. The ground truth (GT) data is provided by [45, 46, 47].2. **PETS09-S2L2**: Similar to PETS09-S2L1, this dataset contains 442 frames with the same resolution and frame rate but includes heavy crowd density and illumination changes. The GT data is also provided by [45, 46, 47].3. **UCF Parking Lot (UCF-PL)**: This dataset consists of 998 frames of 1920 × 1080 pixels recorded at 29 frames per second with medium crowd density and long-term occlusions. The detection results are based on the GT data provided by [48].4. **Soccer**: This sequence contains 155 frames of 960 × 544 pixels recorded at 3 to 5 frames per second, with challenging factors like heavy occlusion and scale changes. The GT data is provided by [13].5. **Town Center**: This dataset consists of 4500 frames of 1080 × 1920 pixels recorded at 25 frames per second, containing medium crowd density and heavy occlusions. The GT data is provided by [43].6. **Urban and Sunny sequences from LISA 2010**: These sequences contain car images collected at 30 frames per second from a camera mounted on a moving vehicle. The GT data is provided by [44].Now, I will check the **References section** to gather the full citations for each dataset:- For **PETS09-S2L1**, the citation is: > Yang, B., & Nevatia, R. (2012). Multi-target tracking by online learning of nonlinear motion patterns and robust appearance models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1918a1925.- For **PETS09-S2L2**, the citation is: > Yang, B., & Nevatia, R. (2012). Multi-target tracking by online learning of nonlinear motion patterns and robust appearance models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1918a1925.- For **UCF-PL**, the citation is: > Dollár, P., Belongie, S., & Perona, P. (2010). The fastest pedestrian detector in the west. In British Machine Vision Conference (BMVC), 1-8.- For **Soccer**, the citation is: > Wu, Y., Tong, X., Zhang, Y., Lu, H., & Zhang, Y. (2008). Boosted interactively distributed particle filter for automatic multi-object tracking. In Proceedings of the IEEE International Conference on Image Processing (ICIP), 1844a1847.- For **Town Center**, the citation is: > Benfold, B., & Reid, I. (2011). Stable multi-target tracking in real-time surveillance video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3457a3464.- For **Urban and Sunny sequences from LISA 2010**, the citation is: > Sivaraman, S., & Trivedi, M. (2010). A general active-learning framework for on-road vehicle recognition and tracking. IEEE Transactions on Intelligent Transportation Systems, 11(2), 267a276.After gathering this information, I will compile the dataset entries, ensuring that each dataset is accurately described and includes the full citation. This will provide a comprehensive overview of the datasets used in the research paper.
</think>