[
    {
        "dcterms:creator": [
            "W. Dai",
            "S. Cahyawijaya",
            "Z. Liu",
            "P. Fung"
        ],
        "dcterms:description": "CI-AVSR is a dataset for in-car command recognition in the Cantonese language, consisting of 4,984 samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese speakers, augmented with 10 common in-car background noises.",
        "dcterms:title": "CI-AVSR",
        "dcterms:issued": "2021",
        "dcterms:language": "Cantonese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Cantonese",
            "Audio-Visual Speech Recognition",
            "In-car commands",
            "Multimodal dataset"
        ],
        "dcat:landingPage": "https://github.com/HLTCHKUST/CI-AVSR",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video",
        "mls:task": [
            "Speech Recognition",
            "Multimodal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "T. Afouras",
            "J. S. Chung",
            "A. Senior",
            "O. Vinyals",
            "A. Zisserman"
        ],
        "dcterms:description": "LRS2-BBC is a large-scale dataset collected from the British Broadcasting Corporation (BBC), containing thousands of hours of spoken sentences with transcripts.",
        "dcterms:title": "LRS2-BBC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Audio-Visual Speech Recognition",
            "BBC",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Text",
        "mls:task": [
            "Speech Recognition",
            "Multimodal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "C. Sagonas",
            "E. Antonakos",
            "G. Tzimiropoulos",
            "S. Zafeiriou",
            "M. Pantic"
        ],
        "dcterms:description": "The iBUG 300-W Facedataset is used for face tracking and annotation, providing a large collection of images of faces in-the-wild.",
        "dcterms:title": "iBUG 300-W Facedataset",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Face Tracking"
        ],
        "dcat:keyword": [
            "Face dataset",
            "Image dataset",
            "Face tracking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Face Tracking",
            "Object Detection"
        ]
    },
    {
        "dcterms:creator": [
            "J. W. Jenness",
            "N. D. Lerner",
            "S. Mazor",
            "J. S. Osberg",
            "B. C. Tefft"
        ],
        "dcterms:description": "The dataset contains 30 minutes of data collected from car-driving on two US roads, focusing on the performance of vehicle voice control interfaces.",
        "dcterms:title": "Car-driving dataset",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "User Interaction"
        ],
        "dcat:keyword": [
            "Car-driving",
            "Voice control",
            "User interaction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition",
            "User Interaction"
        ]
    },
    {
        "dcterms:creator": [
            "J. IvaneckÂ´y",
            "S. Mehlhase"
        ],
        "dcterms:description": "This dataset is recorded by 10 speakers and includes commands for physically disabled drivers, addressing the ASR task.",
        "dcterms:title": "In-car speech dataset",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Accessibility"
        ],
        "dcat:keyword": [
            "In-car speech",
            "Disabled drivers",
            "ASR task"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "T. Afouras",
            "J. S. Chung",
            "A. Senior",
            "O. Vinyals",
            "A. Zisserman"
        ],
        "dcterms:description": "LRS2-BBC is a large-scale dataset containing thousands of hours of spoken sentences from various BBC programs, along with face tracks from TED and TEDx videos.",
        "dcterms:title": "LRS2-BBC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Audio-Visual Speech Recognition",
            "BBC",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Text",
        "mls:task": [
            "Speech Recognition",
            "Multimodal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "L. Stappen",
            "A. Baird",
            "L. Schumann",
            "B. Schuller"
        ],
        "dcterms:description": "The MuSe-CaR dataset is used for predicting sentiment classes based on audio-visual recordings from car review videos.",
        "dcterms:title": "MuSe-CaR",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sentiment Analysis",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Audio-Visual Speech Recognition",
            "Car reviews",
            "Sentiment analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "W. Dai",
            "S. Cahyawijaya",
            "Z. Liu",
            "P. Fung"
        ],
        "dcterms:description": "Cantonese CommonVoice is a dataset used for training and evaluating speech recognition models, specifically for the Cantonese language.",
        "dcterms:title": "Cantonese CommonVoice",
        "dcterms:issued": "2020",
        "dcterms:language": "Cantonese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Cantonese",
            "Speech recognition",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "G. I. Winata",
            "S. Cahyawijaya",
            "Z. Lin",
            "Z. Liu",
            "P. Fung"
        ],
        "dcterms:description": "The dataset is used for code-switched speech recognition, focusing on the interaction between multiple languages.",
        "dcterms:title": "Code-switched speech recognition dataset",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Code-switching"
        ],
        "dcat:keyword": [
            "Code-switching",
            "Speech recognition",
            "Multilingual"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition"
        ]
    }
]