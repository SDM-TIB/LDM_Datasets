[
    {
        "dcterms:creator": [
            "Peter Young",
            "Alice Lai",
            "Micah Hodosh",
            "Julia Hockenmayer"
        ],
        "dcterms:description": "The Flickr30K dataset contains 31,014 images with five captions each, collected from the Flickr website. The authors specify that they used 1,014 images for model validation, 1,000 for model testing, and 29,000 for model training.",
        "dcterms:title": "Flickr30K",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Cross-modal Matching"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Captioning",
            "Cross-modal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Cross-modal Matching"
        ]
    },
    {
        "dcterms:creator": [
            "TY Lin",
            "M Maire",
            "S Belongie",
            "J Hays",
            "P Perona",
            "D Ramanan",
            "C L Zitnick"
        ],
        "dcterms:description": "This dataset is widely used in cross-modal learning, containing 123,287 images, with each image associated with five captions. The authors specify that they used 5,000 images for modal validation, 5,000 for model testing, and 113,287 for model training.",
        "dcterms:title": "MS-COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object detection",
            "Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection",
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Piyush Sharma",
            "Nan Ding",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "dcterms:description": "Conceptual Captions is a large-scale real-world dataset containing about 3,334,173 images with a single caption each. The authors note that they used a smaller version of this dataset, CC152K, which consists of 1,000 images for model validation, 1,000 for model testing, and 150,000 images for model training.",
        "dcterms:title": "Conceptual Captions",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Cross-modal Matching"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Captioning",
            "Noisy correspondence"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Cross-modal Matching"
        ]
    }
]