

<think>
To extract datasets from the research paper titled "Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs" by Hong-You Chen et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations accurately.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract discusses the use of glyphs for out-of-vocabulary word embeddings, which suggests that datasets related to Chinese characters or linguistic resources may be involved.Next, I will focus on the **experiments section** where the authors typically describe the datasets used for evaluation. In this paper, they mention using the **Sinica Corpus 4.0** for training their model. This is a significant dataset for Chinese NLP tasks, and I need to gather more details about it.In the **experiments section**, the authors specify that they pre-trained their model on the **Sinica Corpus 4.0**, which includes approximately 10 million tokens. This dataset is crucial for their experiments, and I need to ensure I have the correct citation for it.I will also check the **References section** to find the full citation for the Sinica Corpus 4.0. The citation is as follows:> Frederick Liu, Han Lu, Chieh Lo, and Graham Ng. *Learning character-level compositionality with visual features*. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2059a2068. Association for Computational Linguistics, 2017.Now, I will compile the dataset entry for the Sinica Corpus 4.0, ensuring to include the full citation as required. This will provide a clear and comprehensive overview of the dataset used in the research.Finally, I will prepare the dataset entry in the specified format, ensuring that all relevant details are included for future reference or processing.
</think>