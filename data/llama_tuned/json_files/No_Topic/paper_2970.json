[
    {
        "dcterms:creator": [
            "Frederick Liu",
            "Han Lu",
            "Chieh Lo",
            "Graham Ng"
        ],
        "dcterms:description": "The Sinica Corpus 4.0 is a large corpus that includes about 10 million tokens, used for training word embeddings in Chinese NLP tasks.",
        "dcterms:title": "Sinica Corpus 4.0",
        "dcterms:issued": "2017",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Chinese Language"
        ],
        "dcat:keyword": [
            "Chinese corpus",
            "Word embeddings",
            "NLP"
        ],
        "dcat:landingPage": "http://asbc.iis.sinica.edu.tw/index",
        "dcterms:hasVersion": "4.0",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding",
            "NLP Tasks"
        ]
    },
    {
        "dcterms:creator": [
            "Frederick Liu",
            "Han Lu",
            "Chieh Lo",
            "Graham Ng"
        ],
        "dcterms:description": "The Wikipedia Title dataset is a collection of 593K Chinese articles categorized into 12 classes based on their titles, used for evaluating out-of-vocabulary word embeddings.",
        "dcterms:title": "Wikipedia Title dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Text classification",
            "Out-of-vocabulary words"
        ],
        "dcat:landingPage": "https://github.com/fxsjy/jieba",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Rami Al-Rfou",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "dcterms:description": "Polyglot is a dataset that provides distributed word representations for multilingual NLP tasks, facilitating the use of word embeddings across languages.",
        "dcterms:title": "Polyglot",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multilingual NLP"
        ],
        "dcat:keyword": [
            "Multilingual",
            "Word embeddings",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Rongchao Yin",
            "Quan Wang",
            "Peng Li",
            "Rui Li",
            "Bin Wang"
        ],
        "dcterms:description": "The Cangjie input method dictionary provides high-quality features for character-level embeddings, enhancing the performance of NLP tasks.",
        "dcterms:title": "Cangjie input method dictionary",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Character Embeddings"
        ],
        "dcat:keyword": [
            "Character embeddings",
            "Cangjie",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Frederick Liu",
            "Han Lu",
            "Chieh Lo",
            "Graham Ng"
        ],
        "dcterms:description": "The Jieba toolkit is used for segmenting Chinese text into words, facilitating tasks involving word-level processing.",
        "dcterms:title": "Jieba toolkit",
        "dcterms:issued": "2017",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Segmentation"
        ],
        "dcat:keyword": [
            "Chinese text segmentation",
            "Jieba",
            "NLP"
        ],
        "dcat:landingPage": "https://github.com/fxsjy/jieba",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]