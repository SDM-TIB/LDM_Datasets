[
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "VQA 2.0 consists of over 1 million questions about images from COCO, used for visual question answering.",
        "dcterms:title": "VQA 2.0",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Image Captioning",
            "COCO"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "VCR consists of 290k questions derived from 110k movie scenes, focusing on visual commonsense reasoning.",
        "dcterms:title": "VCR",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Common Sense Reasoning"
        ],
        "dcat:keyword": [
            "Visual Commonsense Reasoning",
            "Movie Scenes",
            "Multi-Choice Questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Alane Suhr",
            "Stephanie Zhou",
            "Iris Zhang",
            "Huajun Bai",
            "Yoav Artzi"
        ],
        "dcterms:description": "NLVR2 is a dataset for joint reasoning about natural language and images, focusing on semantic diversity and visual reasoning.",
        "dcterms:title": "NLVR2",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual Reasoning",
            "Natural Language",
            "Image Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Bryan A Plummer",
            "Liwei Wang",
            "Chris M Cervantes",
            "Juan C Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "dcterms:description": "Flickr30K Entities tests the ability of systems to ground phrases in captions to bounding regions in the image.",
        "dcterms:title": "Flickr30K",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Region-to-Phrase Grounding",
            "Image Captioning",
            "Bounding Regions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Region-to-Phrase Grounding"
        ]
    }
]