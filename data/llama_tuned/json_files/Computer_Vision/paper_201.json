[
    {
        "dcterms:creator": [
            "Ciprian Chelba",
            "Tomas Mikolov",
            "Mike Schuster",
            "Qi Ge",
            "Thorsten Brants",
            "Phillipp Koehn",
            "Tony Robinson"
        ],
        "dcterms:description": "A large language modeling dataset with almost one billion tokens and a vocabulary of over 800K words, where words appearing less than 3 times are replaced with a special unknown symbol.",
        "dcterms:title": "Google Billion Word Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1312.3005",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language modeling",
            "Large dataset",
            "Token count",
            "Vocabulary size"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "S. Merity",
            "C. Xiong",
            "J. Bradbury",
            "R. Socher"
        ],
        "dcterms:description": "A smaller dataset of over 100M tokens with a vocabulary of about 200K words, where sentences are consecutive, allowing models to condition on larger contexts.",
        "dcterms:title": "WikiText-103",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language modeling",
            "Consecutive sentences",
            "Large dataset",
            "Contextual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]