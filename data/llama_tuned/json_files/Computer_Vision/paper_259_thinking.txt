

<think>
To extract datasets from the research paper titled "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue" by Jaewoo Ahn and Byeongchang Kim, I will follow a systematic approach to ensure I capture all relevant information accurately.First, I will read through the **abstract, introduction, methods, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors achieved state-of-the-art performance on the **Wizard of Wikipedia** and **Holl-E datasets**, which suggests these datasets are crucial for their experiments.Next, I will focus on the **introduction** to gather more context about these datasets. The introduction mentions the Wizard of Wikipedia as a benchmark for knowledge-grounded dialogue, which confirms its significance. It also introduces the Holl-E dataset as another dataset used for validation.In the **methods section**, the authors provide detailed descriptions of both datasets:1. **Wizard of Wikipedia**: This dataset contains 18,430 dialogues for training, 1,948 for validation, and 1,933 for testing. It includes ground-truth labels for knowledge selection and is used to evaluate the algorithm's ability to solve the two subproblems of knowledge selection and response generation.2. **Holl-E**: This dataset consists of 7,228 dialogues for training, 930 for validation, and 913 for testing. It includes a single document per dialogue, with an average of 58 sentences on average. The authors mention that they collected ground-truth knowledge annotations for this dataset to improve its quality.Now, I will check the **experiments section** to confirm the datasets' usage in the evaluation process. The authors state that they evaluate their model on both datasets, reinforcing their importance in the research.Next, I will look at the **references section** to find the full citations for these datasets:- For the **Wizard of Wikipedia**, the citation is: > Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. *Wizard of Wikipedia: Knowledge-Powered Conversational Agents*. In ICLR, 2019.- For the **Holl-E**, the citation is: > Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M Khapra. *Towards Exploiting Background Knowledge for Building Conversation Systems*. In EMNLP, 2018.After gathering this information, I will compile the dataset entries, ensuring to include the full citations for each dataset as required. This will provide a comprehensive overview of the datasets used in the research, ready for further processing or review.
</think>