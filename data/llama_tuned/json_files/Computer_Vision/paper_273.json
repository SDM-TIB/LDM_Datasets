[
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "A multi-task benchmark and analysis platform for natural language understanding tasks including sentiment analysis, NLI, and text similarity evaluation.",
        "dcterms:title": "General Language Understanding Evaluation (GLUE)",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "https://gluebenchmark.com/",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Natural Language Understanding",
            "Multi-task",
            "Benchmark"
        ],
        "dcat:landingPage": "https://gluebenchmark.com/",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sentiment Analysis",
            "Natural Language Inference",
            "Text Similarity Evaluation"
        ]
    }
]