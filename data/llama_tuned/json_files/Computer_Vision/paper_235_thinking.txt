

<think>
To extract datasets from the research paper titled "All-in-One Image-Grounded Conversational Agents" by Da Jua et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, methods, and experiments sections** to identify any datasets mentioned. The abstract discusses the development of an image-grounded conversational agent, which suggests that datasets are likely involved in the experiments.Next, I will focus on the **introduction** to see if the authors mention any specific datasets used for training or evaluation. The introduction mentions various tasks and datasets, but I need to look for specific names and details.In the **tasks section**, the authors describe several tasks, including image captioning, visual question answering, and dialogue tasks. They list the datasets used for these tasks, which are crucial for my extraction. The datasets mentioned include:1. **COCO Captions**: This dataset is used for image captioning, requiring models to predict captions that factually summarize the scene.2. **Flickr30k**: Another captioning dataset with 29k training images and 145k captions.3. **Personality Captions (PC)**: This dataset models human style in image captions, containing 187k training images with one caption each.4. **Image Chat**: An extension of Personality Captions that includes human conversations based on images, with 187k images and 356k total conversation turns.5. **Image Chat QA (ICQA)**: This dataset extracts question-answer pairs from the Image Chat dataset to evaluate conversational image-grounded questions.6. **VQA**: A dataset for visual question answering, containing 83k training images and 444k QA pairs.Next, I will check the **related work section** to see if the authors reference any additional datasets or sources that may have been used in their experiments. However, the focus remains on the datasets explicitly mentioned in the paper.After gathering the dataset names and descriptions, I will consult the **References section** to retrieve the full citations for each dataset:- For **COCO Captions**, the citation is: > Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr DollAr, and C Lawrence Zitnick. *Microsoft COCO Captions: A Large-Scale Hierarchical Image Captioning Dataset*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.- For **Flickr30k**, the citation is: > Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. *From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions*. Transactions of the Association for Computational Linguistics, 2:67a78, 2014.- For **Personality Captions (PC)**, the citation is: > Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason Weston. *Engaging Image Captioning via Personality*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.- For **Image Chat**, the citation is: > Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason Weston. *Engaging Image Chat: Modeling Personality in Grounded Dialogue*. arXiv preprint arXiv:1811.00945, 2018.- For **Image Chat QA (ICQA)**, the citation is: > Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason Weston. *The Dialogue Decathlon: Open-Domain Knowledge and Image-Grounded Conversational Agents*. arXiv preprint arXiv:1911.03768, 2019.- For **VQA**, the citation is: > Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. *Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering*. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their references from the paper.
</think>