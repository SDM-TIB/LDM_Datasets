[
    {
        "dcterms:creator": [
            "Xinlei Chen",
            "Hao Fang",
            "Tsung-Yi Lin",
            "Ramakrishna Vedantam",
            "Saurabh Gupta",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "COCO Captions dataset requires that a model predicts a caption that factually summarizes the scene, containing about 83k training images and 414k captions.",
        "dcterms:title": "COCO Captions",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image captions",
            "Scene understanding",
            "Visual recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Young",
            "Alice Lai",
            "Micah Hodosh",
            "Julia Hockenmaier"
        ],
        "dcterms:description": "Flickr30k is a captioning dataset with factual summaries, containing 29k training images and 145k captions.",
        "dcterms:title": "Flickr30k",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image captions",
            "Flickr dataset",
            "Visual recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Kurt Shuster",
            "Samuel Humeau",
            "Antoine Bordes",
            "Jason Weston"
        ],
        "dcterms:description": "Personality Captions (PC) attempts to model human style when people speak about images, containing about 187k training images with one caption each.",
        "dcterms:title": "Personality Captions (PC)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Human Style Modeling"
        ],
        "dcat:keyword": [
            "Image captions",
            "Personality modeling",
            "Human style"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Kurt Shuster",
            "Samuel Humeau",
            "Antoine Bordes",
            "Jason Weston"
        ],
        "dcterms:description": "Image Chat is an extension of the Personality Captions dataset to full dialogue, containing 187k images and 356k total conversation turns.",
        "dcterms:title": "Image Chat",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Dialogue Systems"
        ],
        "dcat:keyword": [
            "Image captions",
            "Dialogue generation",
            "Conversational AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Dialogue Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "VQA Visual QA is a task involving open-ended questions about images requiring understanding of vision, language, and commonsense knowledge.",
        "dcterms:title": "VQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual QA",
            "Image understanding",
            "Commonsense knowledge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Nasrin Mostafazadeh",
            "Chris Brockett",
            "Bill Dolan",
            "Michel Galley",
            "Jianfeng Gao",
            "Image-Georgios Spithourakis",
            "Lucy Vanderwende"
        ],
        "dcterms:description": "Image-Grounded Conversations (IGCQ and IGCQA) is a conversational dataset between pairs of humans given an image, focusing on context, question, and response.",
        "dcterms:title": "Image-Grounded Conversations (IGCQ and IGCQA)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Conversational dialogue",
            "Image-grounded conversations",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Conversational Dialogue Generation"
        ]
    }
]