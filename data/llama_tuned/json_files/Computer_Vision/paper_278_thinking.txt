

<think>
To extract datasets from the research paper titled "Model-based Hand Pose Estimation for Generalized Hand Shape with Appearance Normalization" by Jan Wahlke et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and dataset processing sections** to identify any datasets mentioned. The abstract indicates that the authors evaluate their method on the **Hands 2017 challenge dataset** and the **NYU dataset**, which suggests these are the datasets used for evaluation.Next, I will focus on the **dataset processing section** where the authors provide detailed descriptions of the datasets. Here, they mention:1. **Hands 2017 Challenge Dataset**: This dataset is composed of 957032 depth images of five different hands, with a test set consisting of 295510 depth images. The authors describe it as the largest and most diverse dataset available for hand pose estimation.2. **NYU Dataset**: This dataset contains 72,757 training images of a single subject's hand and 8,252 test images that include a second hand shape. The authors note that the NYU dataset annotates 36 joints, which is relevant for their evaluation.Now, I will check the **References section** to find the full citations for these datasets:- For the **Hands 2017 Challenge Dataset**, the citation is: > S. Yuan, Q. Ye, G. Garcia-Hernando, and T.-K. Kim. *The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation*. arXiv preprint arXiv:1707.02237, 2017.- For the **NYU Dataset**, the citation is: > J. Tompson, M. Stein, Y. Lecun, and K. Perlin. *Real-time continuous pose recovery of human hands using convolutional networks*. ACM Transactions on Graphics, 33(5):169, 2014.After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a clear and structured overview of the datasets used in the research paper.
</think>