[
    {
        "dcterms:creator": [
            "Stanislaw Antol",
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Margaret Mitchell",
            "Dhruv Batra",
            "C Lawrence Zitnick",
            "Devi Parikh"
        ],
        "dcterms:description": "The goal of visual question answering (VQA) is to answer a natural language question related to an image. The dataset contains an average of 5.4 questions per image and the total amount of questions is 1.1M.",
        "dcterms:title": "VQA",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual question answering",
            "Image captioning",
            "Natural language questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Drew A Hudson",
            "Christopher D Manning"
        ],
        "dcterms:description": "The task of GQA is the same as VQA, but GQA requires more reasoning skills, such as spatial understanding and multi-step inference. The dataset contains 22M questions generated from ground truth image scene graphs.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Compositional question answering",
            "Image scene graphs",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and text",
        "mls:task": [
            "Compositional Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alane Suhr",
            "Stephanie Zhou",
            "Iris Zhang",
            "Huajun Bai",
            "Yoav Artzi"
        ],
        "dcterms:description": "NLVR2 is a challenging visual reasoning dataset where each datum consists of two related natural images and one natural language statement. The task is to predict whether the statement correctly describes the two images or not.",
        "dcterms:title": "NLVR2",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual reasoning",
            "Image and language",
            "Natural language statements"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and text",
        "mls:task": [
            "Visual Reasoning"
        ]
    }
]