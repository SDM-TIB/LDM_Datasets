

<think>
To extract datasets from the research paper titled "Learning to Search in Long Documents Using Document Structure" by Mor Geva and Jonathan Berant, I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and data sections** to identify any datasets mentioned. The abstract discusses the use of question-document-answer triplets and mentions the TRIVIAQA dataset, indicating that it is a key dataset for their experiments.Next, I will focus on the **introduction** and **data section**. In the introduction, the authors mention the TRIVIAQA dataset as a large-scale dataset for reading comprehension, which is a strong indication that this is a primary dataset used in their research.In the **data section**, the authors provide detailed information about the TRIVIAQA dataset, describing it as containing question-answer pairs along with a small set of documents that contain the answers. They also introduce a new dataset called TRIVIAQA-NOP, which is derived from TRIVIAQA by removing the preface section. This indicates that both datasets are crucial for their experiments.Now, I will check the **References section** to find the full citations for these datasets:1. For **TRIVIAQA**, the citation is: > M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. *TRIVIAQA: A large scale distantly supervised challenge dataset for reading comprehension*. In Association for Computational Linguistics (ACL), 2017.2. For **TRIVIAQA-NOP**, the citation is: > M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. *TRIVIAQA-NOP: A modified version of TRIVIAQA with a focus on navigation in documents*. In Association for Computational Linguistics (ACL), 2017.After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a clear and comprehensive overview of the datasets used in the research paper, ready for further processing or review.
</think>