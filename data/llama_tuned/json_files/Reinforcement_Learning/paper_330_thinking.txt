

<think>
To extract datasets from the research paper titled "3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds" by Fangyu Liu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract provides a brief overview of the method but does not explicitly mention datasets. Therefore, I will focus on the introduction and experiments sections for more detailed information.In the **introduction**, the authors discuss the challenges of semantic parsing in 3D point clouds but do not specify any datasets. I will proceed to the **experiments section** where datasets are typically detailed.Next, I will look for a dedicated **datasets section** or any mention of datasets in the **experiments**. In this paper, the authors mention two datasets used for their experiments:1. **Stanford 3D Semantic Parsing Dataset**: This dataset is described as containing 3D scans from Matterport scanners in 6 areas, including 271 rooms, totaling 6020 square meters. It is fully annotated for 12 semantic classes, including structural elements and commonly found items. The authors specify that they randomly chose 70% of the rooms for training and the rest for testing.2. **SUNCG Dataset**: This dataset consists of 45,622 house models, 2,549 object files with corresponding material files, and many texture files. It is annotated for 84 classes. The authors mention that they converted the data into 3D cloud points for their experiments.Now, I will check the **References section** to find the full citations for these datasets:- For the **Stanford 3D Semantic Parsing Dataset**, the citation is: > I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. *3D semantic parsing of large-scale indoor spaces*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, 2, 6, 7.- For the **SUNCG Dataset**, the citation is: > S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. *Semantic scene completion from a single depth image*. Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition, 2017.After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a clear and structured overview of the datasets used in the research paper.
</think>