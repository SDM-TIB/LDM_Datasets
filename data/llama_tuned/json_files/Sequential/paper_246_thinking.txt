

<think>
To extract datasets from the research paper titled "Deep Learning Techniques for Humor Detection in Hindi-English Code-Mixed Tweets" by Sushmitha Reddy Sane et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and methods sections** to identify any datasets mentioned. The abstract indicates that the authors created a corpus for bilingual word embeddings using Twitter data, which suggests that a dataset is involved.Next, I will focus on the **introduction** and **corpus creation section**. Here, the authors describe the process of creating a bilingual word embeddings corpus using Twitter data, specifically mentioning that around 200k tweets were extracted from the 1000 most common words. This indicates that the dataset is derived from Twitter data, but I need to find the specific name or reference for this dataset.In the **corpus creation section**, the authors provide details about the dataset, stating that it contains 3543 annotated tweets, with 1755 labeled as humorous and 1698 as non-humorous. This section confirms the dataset's structure and purpose.Now, I will check the **references section** to find the full citation for the dataset. The authors reference a benchmark dataset used for evaluating their models, which is crucial for providing a complete citation.The relevant citation for the dataset is:- For the benchmark dataset, the citation is: > Ankush Khandelwal, Sahil Swami, Syed S Akhtar, and Manish Shrivastava. *Humor detection in English-Hindi code-mixed social media content: Corpus and baseline system*. arXiv preprint arXiv:1806.05513, 2018.With this information, I can now summarize the datasets extracted from the paper:1. **Twitter Corpus**: A dataset created by extracting around 200k tweets using the 1000 most common words from the training corpus. It contains 3543 annotated tweets, with 1755 labeled as humorous and 1698 as non-humorous.2. **Benchmark Dataset**: A dataset published online that contains 3543 annotated tweets, used for evaluating the effectiveness of bilingual word embeddings and proposed deep learning models.Finally, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a clear and comprehensive overview of the datasets used in the research paper.
</think>