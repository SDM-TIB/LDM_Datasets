

<think>
To extract datasets from the research paper titled "Evaluating Coherence in Dialogue Systems using Entailment" by Nouha Dziria et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and methods sections** to identify any datasets mentioned. The abstract discusses the challenges of evaluating dialogue systems and hints at the use of datasets for training and evaluation, which suggests that specific datasets will be detailed later in the paper.Next, I will focus on the **introduction** and **experiments sections**. In the introduction, the authors mention the need for a reliable evaluation metric for dialogue systems, which implies that datasets are crucial for their experiments. I will look for explicit mentions of datasets in these sections.In the **experiments section**, the authors describe the datasets used for training and evaluation. They mention two datasets:1. **Persona-Chat Conversational Data**: This dataset is used to create a synthetic inference dataset for evaluating dialogue systems. The authors describe how they derived utterances from this dataset to create their own dataset, which is crucial for understanding its application in their experiments.2. **MultiNLI Corpus**: This dataset is referenced as a source for constructing contradictory instances in their synthetic dataset. The authors explain how they used human annotations from Amazon Mechanical Turk to annotate the MultiNLI corpus, which is essential for understanding its role in their experiments.Now, I will check the **References section** to gather the full citations for these datasets:- For **Persona-Chat Conversational Data**, the citation is: > Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. *Personalizing dialogue agents: I have a dog, do you have pets too?*. arXiv preprint arXiv:1801.07243, 2018.- For **MultiNLI Corpus**, the citation is: > Adina Williams, Nikita Nangia, and Samuel Bowman. *A broad-coverage challenge corpus for sentence understanding through inference*. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1112a1122, 2018.After identifying the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their references from the paper.
</think>