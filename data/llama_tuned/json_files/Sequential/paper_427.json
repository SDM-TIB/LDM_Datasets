[
    {
        "dcterms:creator": [
            "Cody Coleman",
            "Deepak Narayanan",
            "Daniel Kang",
            "Tian Zhao",
            "Jian Zhang",
            "Luigi Nardi",
            "Peter Bailis",
            "Kunle Olukotun",
            "Chris RA",
            "Matei Zaharia"
        ],
        "dcterms:description": "A benchmark suite for end-to-end deep learning training and inference, focusing on question answering tasks.",
        "dcterms:title": "DAWNbench",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Deep Learning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Question answering",
            "Deep learning",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset designed for machine comprehension of text, containing 100,000+ questions.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Text comprehension",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Robin Jia",
            "Percy Liang"
        ],
        "dcterms:description": "An extension of SQuAD that includes unanswerable questions.",
        "dcterms:title": "SQuAD v2",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Unanswerable questions",
            "Text comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S. Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large-scale distantly supervised challenge dataset for reading comprehension.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Reading comprehension",
            "Distant supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Matthew Dunn",
            "Levent Sagun",
            "Mike Higgins",
            "V Ugur Guney",
            "Volkan Cirik",
            "Kyunghyun Cho"
        ],
        "dcterms:description": "A dataset augmented with context from a search engine, designed for question answering.",
        "dcterms:title": "SearchQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Search engine data",
            "Contextual information"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Koéciský",
            "Jonathan Schwarz",
            "Phil Blunsom",
            "Chris Dyer",
            "Karl Moritz Hermann",
            "Gábor Melis",
            "Edward Grefenstette"
        ],
        "dcterms:description": "A reading comprehension challenge dataset focused on narrative questions.",
        "dcterms:title": "NarrativeQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Narrative questions",
            "Reading comprehension",
            "Conversational AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Eunsol Choi",
            "He He",
            "Mohit Iyyer",
            "Mark Yatskar",
            "Wen-tau Yih",
            "Yejin Choi",
            "Percy Liang",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A dataset for question answering in context.",
        "dcterms:title": "QuAC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Contextual questions",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Matthew Kelcey",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina N. Toutanova",
            "Llion Jones",
            "Ming-Wei Chang",
            "Andrew Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "A benchmark for question answering research, focusing on natural questions.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Natural questions",
            "Question answering",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yi Yang",
            "Wen-tau Yih",
            "Christopher Meek"
        ],
        "dcterms:description": "A challenge dataset for open-domain question answering.",
        "dcterms:title": "WikiQA",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Open-domain questions",
            "Question answering",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Danqi Chen",
            "Adam Fisch",
            "Jason Weston",
            "Antoine Bordes"
        ],
        "dcterms:description": "A dataset for reading comprehension that employs a variety of features for question answering.",
        "dcterms:title": "DrQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Question answering",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alexander H Miller",
            "Will Feng",
            "Adam Fisch",
            "Jiasen Lu",
            "Dhruv Batra",
            "Antoine Bordes",
            "Devi Parikh",
            "Jason Weston"
        ],
        "dcterms:description": "A dialog research software platform for natural language processing tasks.",
        "dcterms:title": "ParlAI",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Dialogue Systems"
        ],
        "dcat:keyword": [
            "Dialogue systems",
            "Natural language processing",
            "Research platform"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]