

<think>
To extract datasets from the research paper titled "NLP4IF 2019 Shared Task: Propaganda Detection using Ensemble Deep Learning Models" by Hani Al-Omari et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.First, I will read through the **abstract, introduction, and dataset sections** to identify any datasets mentioned. The abstract indicates that the proposed model was applied on a dataset from the NLP4IF 2019 challenge, which suggests that a specific dataset is involved.Next, I will focus on the **dataset section** (section 3) where the authors provide detailed information about the dataset used for their experiments. Here, they describe the dataset as consisting of 350 training articles and 61 development articles, totaling 411 articles. They mention that the dataset is part of the NLP4IF 2019 Task 1 Sentence Level Classification (SLC) and provide a breakdown of the number of sentences labeled as propaganda and non-propaganda.I will also check the **References section** to find the full citation for the dataset. The authors reference the dataset as being provided by the NLP4IF 2019 challenge, which is a shared task. However, they do not provide a specific citation for the dataset itself, as it is part of a challenge rather than a standalone publication.Now, I will compile the information I have gathered into a structured format, ensuring to include the dataset name, description, and any relevant details about its use in the research. The dataset is described as follows:- **NLP4IF 2019 Dataset**: This dataset consists of 350 training articles and 61 development articles, with a total of 411 articles. It is used for sentence-level classification to detect propaganda in news articles.Finally, I will prepare the dataset entry for the NLP4IF 2019 Dataset, ensuring that I include the necessary details and the citation for the challenge, as it is the source of the dataset information.
</think>