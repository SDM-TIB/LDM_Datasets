[
    {
        "dcterms:creator": [
            "Michael Volske",
            "Martin Potthast",
            "Shahbaz Syed",
            "Benno Stein"
        ],
        "dcterms:description": "The TL;DR corpus consists of approximately 3 million content-summary pairs mined from Reddit, used for the TL;DR challenge.",
        "dcterms:title": "TL;DR Reddit corpus",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reddit",
            "Text summarization",
            "Content-summary pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Karl Moritz Hermann",
            "Tomás Kociský",
            "Edward Grefenstette",
            "Lasse Espeholt",
            "Will Kay",
            "Mustafa Teach-Suleyman",
            "Phil Blunsom"
        ],
        "dcterms:description": "The CNN/Daily Mail corpus contains pairs of online news articles and their summaries, with approximately 287,000 training pairs, 13,368 validation pairs, and 11,490 testing pairs.",
        "dcterms:title": "CNN/Daily Mail corpus",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "News articles",
            "Summarization",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    }
]