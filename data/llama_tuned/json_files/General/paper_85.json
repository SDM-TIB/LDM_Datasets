[
    {
        "dcterms:creator": [
            "Kevin Clark",
            "Urvashi Khandelwal",
            "Omer Levy",
            "Christopher D Manning"
        ],
        "dcterms:description": "The dataset contains 992 sequences extracted from Wikipedia, where each sequence consists of two consecutive paragraphs, in the form of [CLS] paragraph1 [SEP] paragraph2 [SEP]. Each sequence consists of up to 128 tokens, with an average of 122 tokens.",
        "dcterms:title": "BERT-base dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Translation"
        ],
        "dcat:keyword": [
            "BERT",
            "Attention mechanisms",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Analysis",
            "Machine Translation"
        ]
    },
    {
        "dcterms:creator": [
            "David Vilar",
            "Maja PopoviÄ‡",
            "Hermann Ney"
        ],
        "dcterms:description": "The dataset is used for training a Transformer-based NMT system for German-to-English translation.",
        "dcterms:title": "Europarl v7 corpus",
        "dcterms:issued": "2006",
        "dcterms:language": "German, English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Translation"
        ],
        "dcat:keyword": [
            "NMT",
            "German-to-English",
            "Translation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Translation"
        ]
    }
]