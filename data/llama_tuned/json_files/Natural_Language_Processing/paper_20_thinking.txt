

<think>
To extract datasets from the research paper titled "Question Generation from Paragraphs: A Tale of Two Hierarchical Models" by Vishwajeet Kumar et al., I will follow a systematic approach to ensure I capture all relevant information accurately.First, I will read through the **abstract, introduction, and experimental setup sections** to identify any datasets mentioned. The abstract indicates that the authors conducted empirical evaluation on two datasets, which suggests that specific names will be provided.Next, I will focus on the **experimental setup section** (section 4.1) where the authors explicitly mention the datasets used for their experiments. Here, they state that they performed experiments on the **SQuAD and MSMARCO datasets**. This is a clear indication that these datasets are crucial for the research.In the **experimental setup section**, the authors provide details about the SQuAD dataset, mentioning that it contains 536 Wikipedia articles and over 100,000 questions posed by crowd-workers. They also describe the MSMARCO dataset, which consists of passages retrieved from web documents and contains questions that are anonymized versions of Bing queries.Now, I will check the **References section** to find the full citations for these datasets:1. For **SQuAD**, the citation is: > Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. *SQuAD: 100,000+ questions for machine comprehension of text*. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383a2392, 2016.2. For **MSMARCO**, the citation is: > Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. *MS MARCO: A human generated machine reading comprehension dataset*. arXiv preprint arXiv:1611.09268, 2016.After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a comprehensive overview of the datasets used in the research paper, ready for further processing or review.
</think>