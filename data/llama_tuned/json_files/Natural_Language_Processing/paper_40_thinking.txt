

<think>
To extract datasets from the research paper titled "Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning" by Roei Schuster et al., I will follow a systematic approach to ensure I capture all relevant datasets accurately.First, I will read through the **abstract, introduction, methods, and results sections** to identify any datasets mentioned. The abstract discusses the use of word embeddings and mentions the training of embeddings on public corpora, which suggests that datasets are involved.Next, I will focus on the **datasets section** (if it exists) or any relevant subsections that detail the datasets used in the experiments. In this paper, the authors mention using a **full Wikipedia text dump** for their experiments, which is a significant dataset for natural language processing tasks.In the **experiments section**, the authors specify that they used a full Wikipedia text dump, which is a well-known dataset in NLP research. They also mention using a **Twitter stream archive** for their NER experiments, which is another dataset relevant to their work.I will also check the **references section** to find the full citations for these datasets:1. For the **Wikipedia dataset**, the citation is: > J. Pennington, R. Socher, and C. D. Manning. *GloVe: Global vectors for word representation*. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 1-10, 2014.2. For the **Twitter stream archive**, the citation is: > archive.org, a Twitter stream, oct 2018, https://archive.org/download/arc [41] aa, a Neural word embedding as implicit matrix factorization*. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS), 2014.After gathering this information, I will compile the dataset entries, ensuring to include the full citations for each dataset as required. This will provide a clear and structured overview of the datasets used in the research paper.
</think>