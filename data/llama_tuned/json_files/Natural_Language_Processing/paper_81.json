[
    {
        "dcterms:creator": [
            "Karl M. Hermann",
            "Tomas Kocisky",
            "Edward Grefenstette",
            "Lasse Espeholt",
            "Will Kay",
            "Mustafa Suleyman",
            "Phil Blunsom"
        ],
        "dcterms:description": "A dataset containing 287,226 documents with summaries for training, 13,368 for validation, and 11,490 for test, used for training and evaluating the model.",
        "dcterms:title": "CNN/Dailymail",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Document Summarization"
        ],
        "dcat:keyword": [
            "Document summarization",
            "Text dataset",
            "Summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Document Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Greg Durrett",
            "Taylor Berg-Kirkpatrick",
            "Dan Klein"
        ],
        "dcterms:description": "A subset of the New York Times dataset, created by removing documents with summaries shorter than 50 words, containing 137,778 documents for training, 17,222 for validation, and 17,223 for test.",
        "dcterms:title": "NYT50",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Document Summarization"
        ],
        "dcat:keyword": [
            "Document summarization",
            "Text dataset",
            "Summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Document Summarization"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset containing 6,626,842 documents and 2,854 million words, created by combining the CNNDM dataset and the English Gigaword dataset, used for pre-training the model.",
        "dcterms:title": "GIGA-CM",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Document Summarization"
        ],
        "dcat:keyword": [
            "Document summarization",
            "Text dataset",
            "Pre-training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Document Summarization"
        ]
    }
]