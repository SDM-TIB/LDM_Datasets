

<think>
To extract datasets from the research paper titled "Structured Pruning of BERT-based Question Answering Models" by J.S. McCarley and Rishav Chakravarti, I will follow a systematic approach to ensure I capture all relevant information accurately.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract discusses the use of BERT-based models for question answering and mentions two datasets: **SQuAD 2.0** and **Natural Questions (NQ)**. This indicates that these datasets are crucial for the experiments conducted.Next, I will focus on the **experiments section** where the authors provide more details about the datasets used. They explicitly state that they evaluate their method on **SQuAD 2.0** and **Natural Questions (NQ)**, confirming their relevance.In the **experiments section**, the authors describe the datasets in detail:1. **SQuAD 2.0**: This dataset consists of questions from Wikipedia passages, annotated by human annotators. It is used for evaluating question answering models.2. **Natural Questions (NQ)**: This dataset contains Google search queries with answers from Wikipedia pages, also annotated by human annotators. It is designed to mitigate observational bias in question answering.Now, I will check the **References section** to find the full citations for these datasets:- For **SQuAD 2.0**, the citation is: > Pranav Rajpurkar, Robin Jia, and Percy Liang. *Know what you don't know: Unanswerable questions for SQuAD*. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784a789, Melbourne, Australia, 2018.- For **Natural Questions (NQ)**, the citation is: > Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Lion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. *Natural questions: A benchmark for question answering research*. Transactions of the Association for Computational Linguistics, 2019.After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a clear and comprehensive overview of the datasets used in the research paper.
</think>