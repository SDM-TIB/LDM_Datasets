

<think>
To extract datasets from the research paper titled "Multi-modality Latent Interaction Network for Visual Question Answering" by Peng Gao et al., I will follow a systematic approach to ensure I capture all relevant information accurately.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on two datasets, which suggests that specific datasets will be detailed later in the paper.Next, I will focus on the **experiments section** (section 4) where the authors explicitly mention the datasets used for their evaluations. Here, they state that they conducted experiments on **VQA v2.0** and **TDIUC** datasets. This is a clear indication that these datasets are crucial for the research.In the **experiments section**, the authors provide details about the datasets, including the number of images and questions. They mention that VQA v2.0 contains 82,743 images, 443,757 questions, and 81,434 test images, while TDIUC has 1.6 million questions divided into 12 categories.Now, I will check the **References section** to find the full citations for these datasets. The citations are as follows:- For **VQA v2.0**, the citation is: > Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. *VQA: Visual Question Answering*. In Proceedings of the IEEE International Conference on Computer Vision, pages 2425a2433, 2015.- For **TDIUC**, the citation is: > Kushal Kaie and Christopher Kanan. *An analysis of visual question answering algorithms*. In Proceedings of the IEEE International Conference on Computer Vision, pages 1965a1973, 2017.With this information, I will compile the dataset entries, ensuring to include the dataset names, descriptions, and full citations. This structured approach will help in accurately documenting the datasets used in the research paper.
</think>