

<think>
To extract datasets from the research paper titled "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization" by Xingxing Zhang, Furu Wei, and Ming Zhou, I will follow a systematic approach.First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors achieved state-of-the-art performance on two datasets, which suggests that specific datasets will be detailed later in the paper.Next, I will focus on the **experiments section** (section 4), where the authors typically describe the datasets used for their evaluations. In this section, they mention two datasets:1. **CNN/Dailymail Dataset**: This dataset is used for training and evaluating the model. The authors provide details about the number of documents, summaries, and the preprocessing steps taken.2. **New York Times Dataset**: This dataset is also used for evaluation, and the authors describe how they created a subset of the dataset (NYT50) by filtering out documents with summaries shorter than 50 words.After identifying the datasets, I will check the **References section** to find the full citations for these datasets:- For the **CNN/Dailymail Dataset**, the citation is: > Hermann, K. M., Liu, P. J., & Manning, C. D. (2015). Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems (pp. 1693-1701).- For the **New York Times Dataset**, the citation is: > Durrett, G., & Xu, J. (2016). Learning-based single-document summarization with compression and anaphoricity constraints. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1998-2008.Now that I have the dataset names and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their citations from the paper.
</think>