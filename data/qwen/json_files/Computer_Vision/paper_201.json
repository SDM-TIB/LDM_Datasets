[
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset with over 100 million tokens and a vocabulary of about 200,000 words. It is used for language modeling, conditioning on entire paragraphs.",
        "dcterms:title": "WikiText-103",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language modeling",
            "Paragraph context",
            "Vocabulary"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A large dataset with almost one billion tokens and a vocabulary of over 800,000 words. It is based on an English corpus of 30 million sentences, shuffled to avoid bias.",
        "dcterms:title": "Google Billion Words",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language modeling",
            "Large scale",
            "Vocabulary"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]