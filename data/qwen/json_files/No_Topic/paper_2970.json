[
    {
        "dcterms:creator": [
            "Liu, Han",
            "Liu, Frederick",
            "Lu, Chieh",
            "Neubig, Graham"
        ],
        "dcterms:description": "A dataset containing Chinese Wikipedia titles used to evaluate the model's performance on rare and out-of-vocabulary words.",
        "dcterms:title": "Wikipedia Title Dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Chinese NLP"
        ],
        "dcat:keyword": [
            "Wikipedia titles",
            "OOV words",
            "Chinese NLP"
        ],
        "dcat:landingPage": "https://github.com/frederick0329/Wikipedia-Title-Classifier",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Word Embedding"
        ]
    },
    {
        "dcterms:creator": [
            "Al-Rfou, Rami",
            "Perozzi, Bryan",
            "Skiena, Steven"
        ],
        "dcterms:description": "A multilingual dataset providing word representations for various languages, used for sanity checks and visualization in the paper.",
        "dcterms:title": "Polyglot Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "Multilingual",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multilingual NLP",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Multilingual embeddings",
            "Word representations"
        ],
        "dcat:landingPage": "https://github.com/aboSamoor/polyglot",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Representation",
            "Cross-lingual Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Chen, Hong-You",
            "Yu, Sz-Han",
            "Lin, Shou-De"
        ],
        "dcterms:description": "A large-scale Chinese corpus used for training word embeddings, containing about 10 million tokens.",
        "dcterms:title": "Sinica Corpus 4.0",
        "dcterms:issued": "2013",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Chinese NLP",
            "Corpus"
        ],
        "dcat:keyword": [
            "Chinese corpus",
            "Word embeddings"
        ],
        "dcat:landingPage": "http://asbc.iis.sinica.edu.tw/indexreadme.htm",
        "dcterms:hasVersion": "4.0",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding",
            "Language Modeling"
        ]
    }
]