[
    {
        "dcterms:creator": [
            "He Bai",
            "Peng Shi",
            "Jimmy Lin",
            "Yuqing Xie",
            "Luchen Tan",
            "Kun Xiong",
            "Wen Gao",
            "Ming Li"
        ],
        "dcterms:description": "A large word-level dataset with 103 million tokens, 28,000 articles, and an average length of 3.6 thousand tokens per article, used for language modeling experiments.",
        "dcterms:title": "WikiText-103",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language modeling",
            "Perplexity",
            "Segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Wikimedia Foundation"
        ],
        "dcterms:description": "A large corpus of English Wikipedia articles used for pre-training language models.",
        "dcterms:title": "English Wikipedia",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Corpora"
        ],
        "dcat:keyword": [
            "Pre-training",
            "Text corpus"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pre-training"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Zhu",
            "R. Kiros",
            "R. S. Zemel",
            "R. Urtasun",
            "A. Torralba",
            "S. Fidler"
        ],
        "dcterms:description": "A large text corpus of books used for pre-training language models.",
        "dcterms:title": "BookCorpus",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Corpora"
        ],
        "dcat:keyword": [
            "Pre-training",
            "Text corpus"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pre-training"
        ]
    }
]