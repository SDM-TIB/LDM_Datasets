<think>
Alright, I'm trying to figure out how to extract the datasets mentioned in the provided research paper. The paper is about a Multimodal Semantic Transformer Network for the AVSD task. Let me go through the paper step by step.

First, I'll read the abstract. It mentions the AVSD task, which stands for Audio Visual Scene-aware Dialog. The authors propose a new model but don't mention any specific datasets yet.

Moving on to the introduction, they talk about the AVSD task and reference some previous works. They mention DSTC7 and DSTC8 datasets. DSTC stands for Dialog State Tracking Challenge, so these are likely datasets used for evaluating dialog systems.

In the Related Work section, they discuss previous models and datasets. They reference MTN (Multimodal Transformer Networks) and mention that their experiments are on the DSTC7 and DSTC8 datasets. So, DSTC7 and DSTC8 are definitely datasets used in this paper.

Looking at the Experimental Results section, they provide a table comparing their model's performance on DSTC7 and DSTC8 datasets. This confirms that these are the datasets they used.

Now, I need to find more details about these datasets. The references section might help. The authors cite Alamri et al. 2018 and 2019, which are the papers introducing DSTC7 and DSTC8. From these, I can gather that DSTC7 was introduced in 2018 and DSTC8 in 2019. Both are audio-visual scene-aware dialog datasets, containing video clips, audio, and dialog histories for generating responses.

I don't see any other datasets mentioned in the paper. The authors only reference DSTC7 and DSTC8 in their experiments and discussions.

So, the datasets to extract are DSTC7 and DSTC8. I'll need to create JSON entries for each, including creators, descriptions, titles, issued years, and other relevant metadata based on the information from the paper and references.
</think>