[
    {
        "dcterms:creator": [
            "Clark et al., 2019"
        ],
        "dcterms:description": "A dataset containing 992 sequences extracted from Wikipedia, formatted as [CLS] paragraph1 [SEP] paragraph2 [SEP], used for analyzing BERT's attention mechanism.",
        "dcterms:title": "Clark et al. (2019) Dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Attention Mechanism"
        ],
        "dcat:keyword": [
            "BERT",
            "Attention Analysis",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Attention Mechanism Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Europarl v7"
        ],
        "dcterms:description": "A corpus of parallel texts in German and English used for training a Transformer-based NMT system.",
        "dcterms:title": "Europarl v7 Corpus",
        "dcterms:issued": "2007",
        "dcterms:language": [
            "German",
            "English"
        ],
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Translation",
            "Parallel Corpora"
        ],
        "dcat:keyword": [
            "NMT",
            "German-English",
            "Parallel Texts"
        ],
        "dcat:landingPage": "http://www.statmt.org/europarl/v7",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Neural Machine Translation"
        ]
    },
    {
        "dcterms:creator": [
            "Vilar et al., 2006"
        ],
        "dcterms:description": "A gold alignment dataset used for evaluating word alignments in NMT systems.",
        "dcterms:title": "Vilar et al. (2006) Gold Alignment Dataset",
        "dcterms:issued": "2006",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Translation",
            "Word Alignment"
        ],
        "dcat:keyword": [
            "Word Alignment",
            "Gold Standard"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Alignment Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Hugging Face"
        ],
        "dcterms:description": "A pre-trained BERT model used for experiments in analyzing attention mechanisms.",
        "dcterms:title": "BERT-base",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/huggingface/transformers",
        "dcat:theme": [
            "Natural Language Processing",
            "Pre-trained Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Pre-trained Model",
            "Attention"
        ],
        "dcat:landingPage": "https://github.com/huggingface/transformers",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Attention Mechanism Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Facebook AI Research"
        ],
        "dcterms:description": "A toolkit for sequence modeling used to implement the Transformer-based NMT system.",
        "dcterms:title": "fairseq",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/pytorch/fairseq",
        "dcat:theme": [
            "Machine Translation",
            "Sequence Modeling"
        ],
        "dcat:keyword": [
            "NMT",
            "Transformer",
            "Toolkit"
        ],
        "dcat:landingPage": "https://github.com/pytorch/fairseq",
        "dcterms:hasVersion": "",
        "dcterms:format": "Software",
        "mls:task": [
            "Neural Machine Translation"
        ]
    }
]