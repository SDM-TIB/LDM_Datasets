<think>
Alright, I'm trying to figure out how to extract the datasets mentioned in the provided research paper. The paper is about detecting propaganda in news articles using an ensemble deep learning model. 

First, I'll read through the abstract and introduction to get a sense of what datasets are being used. The abstract mentions that the model was applied on the dataset provided by the NLP4IF 2019 Shared Task, specifically Task 1, which is Sentence Level Classification (SLC). 

Moving on to the introduction, the authors discuss the challenge of limited resources and corpora in this field. They mention using the corpus provided by the NLP4IF 2019 challenge, which consists of news articles with sentences labeled as propagandistic or not. 

In the related work section, they reference several other datasets like the Fake News Challenge (FNC-1) dataset, but these aren't the primary focus of their work. The main dataset they use is from the NLP4IF 2019 task.

Looking at the dataset description in Section 3, the authors provide more details about the NLP4IF 2019 dataset. It includes 350 training articles and 61 development articles, totaling 411 articles. Each article is in plain text, with sentences labeled as propaganda or non-propaganda. 

I need to extract the metadata for this dataset. The creators are the authors of the dataset, which in this case, the paper cites Da San Martino et al., 2019. So, the creators would be Giovanni Da San Martino, Alberto Barrón-Cedeño, and others involved in the NLP4IF 2019 task.

The description should include that it's a corpus of news articles with sentences labeled for propaganda detection. The title is "NLP4IF 2019 Task 1 Dataset." The issued year is 2019. The language is English since the articles are in English. There's no specific identifier like a DOI mentioned, so that field remains empty. The theme is related to natural language processing and propaganda detection. Keywords include propaganda detection, news articles, sentence classification, etc. The landing page isn't provided, so it's empty. The format is text, and the task is sentence-level classification for propaganda detection.

I should ensure that all the metadata fields are correctly filled based on the information provided in the paper. Since the paper doesn't mention other datasets in detail, I'll focus only on the NLP4IF 2019 dataset.
</think>