# Qwen (DeepSeek) Model Results Directory

This directory contains the output generated by the DeepSeek-R1-Distill-Qwen-32B model, which serves as a larger model baseline for comparison with the MetaMine model.

## Directory Structure

- **json_files/**: Contains JSON files with dataset metadata extracted by the DeepSeek Qwen model
  - Similar structure to the `gs/json_files/` directory, with subdirectories for different research areas
  - Each paper has two associated files:
    - `paper_X.json`: The extracted dataset information in JSON format
    - `paper_X_thinking.txt`: The reasoning process that led to the extraction

## Model Information

The DeepSeek-R1-Distill-Qwen-32B model:
- Has 32 billion parameters (10.7× larger than the Llama-3.2-3B model)
- Is a state-of-the-art language model distilled from Qwen
- Represents a much larger and more computationally expensive baseline
- Uses the same prompt template and DCAT vocabulary structure as the other models

## Generation Process

The results in this directory were generated using:
1. The `process_papers.py` script in the `inference/` directory
2. The DeepSeek-R1-Distill-Qwen-32B model
3. The same input papers as used for the gold standard
4. The same prompt template and DCAT vocabulary structure

## Performance

The DeepSeek Qwen model achieves:
- An F1 score of 0.73 for dataset identification
- Lower accuracy on challenging fields like creator identification (F1 score of 0.39)
- Processing time of approximately 120 seconds per paper (3.4× slower than the MetaMine model)

Despite being 10.7× larger, the DeepSeek model performs slightly worse than the fine-tuned MetaMine model in terms of overall F1 score, and significantly worse on certain challenging fields. It's also much slower, requiring more computational resources for inference. These results highlight the effectiveness of knowledge distillation and reasoning preservation in the MetaMine approach, which enables a much smaller model to outperform larger models while being more computationally efficient.