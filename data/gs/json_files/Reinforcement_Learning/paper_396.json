[
    {
        "dcterms:creator": [
            "Alexey Dosovitskiy",
            "German Ros",
            "Felipe Codevilla",
            "Antonio López",
            "Vladlen Koltun"
        ],
        "dcterms:description": "CARLA is an open-source simulator for autonomous driving research, developed to support the training, prototyping, and validation of autonomous driving models. It provides a flexible setup for sensor suites and environmental conditions, along with a variety of urban layouts and digital assets.",
        "dcterms:title": "CARLA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.unrealengine.com",
        "dcat:theme": [
            "Autonomous Driving",
            "Simulation"
        ],
        "dcat:keyword": [
            "Driving simulator",
            "Urban environments",
            "Autonomous vehicles",
            "Sensor simulation"
        ],
        "dcat:landingPage": "http://carla.org",
        "dcterms:hasVersion": "",
        "dcterms:format": "Simulation",
        "mls:task": [
            "Autonomous driving research",
            "Training and validation of driving models"
        ]
    },
    {
        "dcterms:creator": [
            "D. H. Biedermann",
            "M. Ochs",
            "R. Mester"
        ],
        "dcterms:description": "COnGRATS is a dataset used for evaluating visual Advanced Driver Assistance Systems (ADAS) components, providing a benchmark for assessing the performance of various visual perception algorithms.",
        "dcterms:title": "COnGRATS",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Driver Assistance Systems"
        ],
        "dcat:keyword": [
            "ADAS",
            "Visual perception",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of visual perception systems"
        ]
    },
    {
        "dcterms:creator": [
            "C. Chen",
            "A. Seff",
            "A. L. Kornhauser",
            "J. Xiao"
        ],
        "dcterms:description": "DeepDriving is a dataset designed for learning affordance in autonomous driving, providing a collection of driving scenarios to train models for direct perception.",
        "dcterms:title": "DeepDriving",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Autonomous Driving",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Driving scenarios",
            "Perception learning",
            "Autonomous vehicles"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning affordance",
            "Direct perception in driving"
        ]
    },
    {
        "dcterms:creator": [
            "F. Codevilla",
            "M. Müller",
            "A. Dosovitskiy",
            "A. López",
            "V. Koltun"
        ],
        "dcterms:description": "The Driving traces dataset consists of driving data collected from human drivers, used for training deep networks in imitation learning for autonomous driving.",
        "dcterms:title": "Driving traces dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1710.02410",
        "dcat:theme": [
            "Autonomous Driving",
            "Imitation Learning"
        ],
        "dcat:keyword": [
            "Driving data",
            "Imitation learning",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Training deep networks",
            "Imitation learning"
        ]
    },
    {
        "dcterms:creator": [
            "A. Dosovitskiy",
            "V. Koltun"
        ],
        "dcterms:description": "The Learning to act dataset is used for training models to predict future states in autonomous driving, facilitating the development of control strategies based on future predictions.",
        "dcterms:title": "Learning to act dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Autonomous Driving",
            "Predictive Modeling"
        ],
        "dcat:keyword": [
            "Future prediction",
            "Control strategies",
            "Autonomous vehicles"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Predicting future states",
            "Control in autonomous driving"
        ]
    },
    {
        "dcterms:creator": [
            "A. Krizhevsky",
            "I. Sutskever",
            "G. E. Hinton"
        ],
        "dcterms:description": "ImageNet is a large-scale dataset for image classification, widely used for training deep learning models in various computer vision tasks.",
        "dcterms:title": "ImageNet",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Deep learning",
            "Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image classification",
            "Training deep learning models"
        ]
    },
    {
        "dcterms:creator": [
            "G. Ros",
            "L. Sellart",
            "J. Materzynska",
            "D. Vázquez",
            "A. López"
        ],
        "dcterms:description": "SYNTHIA is a dataset that provides a large collection of synthetic images for semantic segmentation of urban scenes, aimed at training and evaluating segmentation algorithms.",
        "dcterms:title": "SYNTHIA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Semantic Segmentation"
        ],
        "dcat:keyword": [
            "Synthetic images",
            "Urban scenes",
            "Semantic segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Semantic segmentation",
            "Training segmentation algorithms"
        ]
    }
]