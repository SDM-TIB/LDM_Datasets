[
    {
        "dcterms:creator": [
            "M. G. Bellemare",
            "Y. Naddaf",
            "J. Veness",
            "M. Bowling"
        ],
        "dcterms:description": "An evaluation platform for general agents, specifically designed for testing reinforcement learning algorithms on Atari 2600 games.",
        "dcterms:title": "Atari 2600 benchmark",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game AI"
        ],
        "dcat:keyword": [
            "Atari games",
            "benchmark",
            "evaluation platform"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. G. Bellemare",
            "Y. Naddaf",
            "J. Veness",
            "M. Bowling"
        ],
        "dcterms:description": "A platform that provides a suite of Atari 2600 games for evaluating the performance of reinforcement learning agents.",
        "dcterms:title": "Arcade Learning Environment",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game AI"
        ],
        "dcat:keyword": [
            "Atari games",
            "evaluation",
            "reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "V. Mnih",
            "K. Kavukcuoglu",
            "D. Silver",
            "A. Graves",
            "I. Antonoglou",
            "D. Wierstra",
            "M. A. Riedmiller"
        ],
        "dcterms:description": "A deep reinforcement learning algorithm that combines Q-learning with deep neural networks to achieve human-level performance in playing Atari games.",
        "dcterms:title": "DQN (Deep Q-Network)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Q-learning",
            "deep learning",
            "Atari games"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Game Playing"
        ]
    },
    {
        "dcterms:creator": [
            "V. Mnih",
            "A. P. Badia",
            "M. Mirza",
            "A. Graves",
            "T. Lillicrap",
            "T. Harley",
            "D. Silver",
            "K. Kavukcuoglu"
        ],
        "dcterms:description": "An algorithm that uses asynchronous updates to improve the efficiency of training deep reinforcement learning agents.",
        "dcterms:title": "A3C (Asynchronous Actor-Critic Agents)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "asynchronous learning",
            "actor-critic",
            "deep reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Game Playing"
        ]
    },
    {
        "dcterms:creator": [
            "H. van Hasselt",
            "A. Guez",
            "D. Silver"
        ],
        "dcterms:description": "An extension of DQN that reduces overestimation bias by using two value functions to decouple action selection from action evaluation.",
        "dcterms:title": "DDQN (Double DQN)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "double Q-learning",
            "bias reduction",
            "reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Game Playing"
        ]
    },
    {
        "dcterms:creator": [
            "T. Schaul",
            "J. Quan",
            "I. Antonoglou",
            "D. Silver"
        ],
        "dcterms:description": "A method that improves the efficiency of experience replay by prioritizing transitions based on their learning potential.",
        "dcterms:title": "Prioritized Experience Replay",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "experience replay",
            "prioritization",
            "reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Z. Wang",
            "T. Schaul",
            "M. Hessel",
            "H. van Hasselt",
            "M. Lanctot",
            "N. de Freitas"
        ],
        "dcterms:description": "A neural network architecture that separates the representation of state values and action advantages to improve learning.",
        "dcterms:title": "Dueling DQN",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "dueling architecture",
            "value representation",
            "reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. G. Bellemare",
            "W. Dabney",
            "R. Munos"
        ],
        "dcterms:description": "A method that learns a distribution of returns instead of just the expected return, providing a richer learning signal.",
        "dcterms:title": "Distributional DQN",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "distributional learning",
            "returns distribution",
            "reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Fortunato",
            "M. G. Azar",
            "B. Piot",
            "J. Menick",
            "I. Osband",
            "A. Graves",
            "V. Mnih",
            "R. Munos",
            "D. Hassabis",
            "O. Pietquin",
            "C. Blundell",
            "S. Legg"
        ],
        "dcterms:description": "A method that introduces noise into the network weights to encourage exploration during training.",
        "dcterms:title": "Noisy DQN",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "exploration",
            "noisy networks",
            "reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]