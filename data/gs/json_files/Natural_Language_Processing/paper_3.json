[
    {
        "dcterms:creator": [
            "P. Rajpurkar",
            "J. Zhang",
            "K. Lopyrev",
            "P. Liang"
        ],
        "dcterms:description": "Crowdsourcing workers were shown Wikipedia paragraphs and were asked to author questions about their content. Questions mostly require soft matching of the language in the question to a local context in the text.",
        "dcterms:title": "SQUAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Crowdsourced questions",
            "Machine comprehension",
            "Extractive QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Trischler",
            "T. Wang",
            "X. Yuan",
            "J. Harris",
            "A. Sordoni",
            "P. Bachman",
            "K. Suleman"
        ],
        "dcterms:description": "Crowdsourcing workers were shown a CNN article (longer than SQUAD) and were asked to author questions about its content.",
        "dcterms:title": "NEWSQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Crowdsourced questions",
            "News articles",
            "Machine comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Dunn",
            "L. Sagun",
            "M. Higgins",
            "U. Guney",
            "V. Cirik",
            "K. Cho"
        ],
        "dcterms:description": "Trivia questions were taken from Jeopardy! TV show, and contexts are web snippets retrieved from Google search engine for those questions, with an average of 50 snippets per question.",
        "dcterms:title": "SEARCHQA",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Trivia questions",
            "Web snippets",
            "Machine comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "Trivia questions were crawled from the web. In one variant of TRIVIAQA (termed TQA-W), Wikipedia pages related to the questions are provided for each question.",
        "dcterms:title": "TRIVIAQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Trivia questions",
            "Web snippets",
            "Machine comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Yang",
            "P. Qi",
            "S. Zhang",
            "Y. Bengio",
            "W. W. Cohen",
            "R. Salakhutdinov",
            "C. D. Manning"
        ],
        "dcterms:description": "Crowdsourcing workers were shown pairs of related Wikipedia paragraphs and asked to author questions that require multi-hop reasoning over the paragraphs.",
        "dcterms:title": "HOTPOTQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Multi-hop reasoning",
            "Crowdsourced questions",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Bao",
            "N. Duan",
            "Z. Yan",
            "M. Zhou",
            "T. Zhao"
        ],
        "dcterms:description": "Questions are real Google web queries crawled from Google Suggest, originally constructed for querying the KB Freebase. However, the dataset was also used as a RC task with retrieved web snippets.",
        "dcterms:title": "CQ",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Web queries",
            "Knowledge base",
            "Machine comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Talmor",
            "J. Berant"
        ],
        "dcterms:description": "Crowdsourcing workers were shown compositional formal queries against Freebase and were asked to re-phrase them in natural language. Thus, questions require multi-hop reasoning.",
        "dcterms:title": "CWQ",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Compositional queries",
            "Natural language",
            "Machine comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "1.1",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Welbl",
            "P. Stenetorp",
            "S. Riedel"
        ],
        "dcterms:description": "Questions are entity-relation pairs from Freebase, and are not phrased in natural language. Multiple Wikipedia paragraphs are given as context, and the dataset was constructed such that multi-hop reasoning is needed for answering the question.",
        "dcterms:title": "WIKIHOP",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Entity-relation pairs",
            "Multi-hop reasoning",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Dua",
            "Y. Wang",
            "P. Dasigi",
            "G. Stanovsky",
            "S. Singh",
            "M. Gardner"
        ],
        "dcterms:description": "Contexts are Wikipedia paragraphs and questions are authored by crowdsourcing workers. This dataset focuses on quantitative reasoning.",
        "dcterms:title": "DROP",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Quantitative reasoning",
            "Crowdsourced questions",
            "Machine comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]