[
    {
        "dcterms:creator": [
            "W. Kay",
            "J. Carreira",
            "K. Simonyan",
            "B. Zhang",
            "C. Hillier",
            "S. Vijayanarasimhan",
            "F. Viola",
            "T. Green",
            "T. Back",
            "P. Natsev",
            "M. Suleyman",
            "A. Zisserman"
        ],
        "dcterms:description": "The Kinetics Human Action Video Dataset consists of 400 different human activity classes with 10 seconds of videos. These activities cover a broad range of movements, such as sports, basic body motions, eating or cooking related activities, hobbies, and communicative motions.",
        "dcterms:title": "Kinetics Human Action Video Dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Human action",
            "Activity classification"
        ],
        "dcat:landingPage": "https://github.com/deepmind/kinetics-i3d/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The IAPR TC-12 dataset consists of 200K images and their captions. The features of these images are extracted through a VGG-128 image recognition network.",
        "dcterms:title": "IAPR TC-12 Dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Recognition",
            "Captioning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Image captions",
            "Visual recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Iyyer",
            "V. Manjunatha",
            "J. L. Boyd-Graber",
            "H. D. III"
        ],
        "dcterms:description": "Pre-trained Glove embeddings are used to represent the co-occurrence relationships of the words in the trained in-domain dataset.",
        "dcterms:title": "Glove Word Embeddings",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Text representation",
            "Natural language"
        ],
        "dcat:landingPage": "https://nlp.stanford.edu/projects/glove/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification"
        ]
    },
    {
        "dcterms:creator": [
            "G. Collell",
            "M.-F. Moens"
        ],
        "dcterms:description": "Pre-trained feature vectors from Collell et al. are used to bridge modalities in cross-modal mappings.",
        "dcterms:title": "Pre-trained Feature Vectors from Collell et al.",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-Modal Learning"
        ],
        "dcat:keyword": [
            "Feature vectors",
            "Cross-modal mapping"
        ],
        "dcat:landingPage": "https://github.com/gcollell/neural-cross-modal-maps",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]