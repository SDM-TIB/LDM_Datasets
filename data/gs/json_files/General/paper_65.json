[
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "The VQA v2.0 dataset reduces answer bias compared to VQA v1.0 and contains an average of 5.4 questions per image, totaling 1.1 million questions.",
        "dcterms:title": "VQA v2.0",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Image understanding",
            "Question answering",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Drew A Hudson",
            "Christopher D Manning"
        ],
        "dcterms:description": "GQA is a dataset for compositional question answering over real-world images, requiring more reasoning skills than VQA.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Compositional reasoning",
            "Image understanding",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alane Suhr",
            "Stephanie Zhou",
            "Iris Zhang",
            "Huajun Bai",
            "Yoav Artzi"
        ],
        "dcterms:description": "NLVR2 is a challenging visual reasoning dataset where each datum consists of two related natural images and one natural language statement.",
        "dcterms:title": "NLVR2",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Natural language reasoning",
            "Image comparison",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "MS COCO is a large-scale dataset containing images with annotations for object detection, segmentation, and captioning.",
        "dcterms:title": "MS COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object recognition",
            "Image segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Object Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "Visual Genome connects language and vision using crowdsourced dense image annotations, providing a rich dataset for visual understanding.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Visual Understanding"
        ],
        "dcat:keyword": [
            "Dense annotations",
            "Image understanding",
            "Language and vision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Annotation"
        ]
    },
    {
        "dcterms:creator": [
            "Yuke Zhu",
            "Oliver Groth",
            "Michael Bernstein",
            "Li Fei-Fei"
        ],
        "dcterms:description": "VG-QA is a dataset for grounded question answering in images, focusing on visual reasoning tasks.",
        "dcterms:title": "VG-QA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Grounded question answering",
            "Visual reasoning",
            "Image understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]