[
    {
        "dcterms:creator": [
            "J. Thorne",
            "A. Vlachos",
            "C. Christodoulopoulos",
            "A. Mittal"
        ],
        "dcterms:description": "The FEVER dataset provides a benchmark for evidence-based claim verification, consisting of 185K generated claims labeled as 'SUPPORTED', 'REFUTED', or 'NOT ENOUGH INFO'. It is used to extract evidence sentences from a Wikipedia dump to classify claims.",
        "dcterms:title": "FEVER",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/1803.05355",
        "dcat:theme": [
            "Natural Language Processing",
            "Fact Verification"
        ],
        "dcat:keyword": [
            "Claim Verification",
            "Evidence Extraction",
            "Fact Checking"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1803.05355",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Claim Verification",
            "Evidence Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "T. Nguyen",
            "M. Rosenberg",
            "X. Song",
            "J. Gao",
            "S. Tiwary",
            "R. Majumder",
            "L. Deng"
        ],
        "dcterms:description": "MS MARCO is a human-generated machine reading comprehension dataset used for passage re-ranking tasks.",
        "dcterms:title": "MS MARCO",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/1611.09268",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Reading Comprehension"
        ],
        "dcat:keyword": [
            "Machine Reading",
            "Passage Ranking",
            "Comprehension"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1611.09268",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Passage Re-ranking"
        ]
    },
    {
        "dcterms:creator": [
            "S. R. Bowman",
            "G. Angeli",
            "C. Potts",
            "C. D. Manning"
        ],
        "dcterms:description": "The Stanford Natural Language Inference (SNLI) dataset is a large annotated corpus for learning natural language inference.",
        "dcterms:title": "Stanford Natural Language Inference (SNLI)",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/1508.05326",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "Inference",
            "Natural Language Understanding",
            "Textual Entailment"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1508.05326",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "A. Williams",
            "N. Nangia",
            "S. Bowman"
        ],
        "dcterms:description": "Multi-Genre NLI (MultiNLI) is a broad-coverage challenge corpus for sentence understanding through inference.",
        "dcterms:title": "Multi-Genre NLI (MultiNLI)",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "Inference",
            "Textual Entailment",
            "Sentence Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A Wikipedia dump consisting of 50K popular Wikipedia pages used for evidence extraction in the FEVER dataset.",
        "dcterms:title": "Wikipedia dump (50K popular Wikipedia pages)",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Wikipedia",
            "Text Data",
            "Knowledge Base"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]