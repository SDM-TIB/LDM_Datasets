To extract datasets from the research paper titled "Alternative Weighting Schemes for ELMo Embeddings" by Nils Reimers and Iryna Gurevych, I will follow a systematic approach to identify and document each dataset mentioned in the paper, ensuring to include full citations for each.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets that are explicitly mentioned or described. The abstract provides a brief overview of the research focus, and the introduction often outlines the context and significance of the datasets used.

Next, I will focus on **section 4 (Evaluation of Weighting Schemes)**, where the authors detail their experiments. Here, they mention using several datasets for their evaluations, including:

1. **Arguments Dataset**: This dataset is used for argument component detection in persuasive essays, specifically 402 essays. The authors mention the development and test sets consist of 80 randomly selected essays each.

2. **ACE Entities/Events**: The ACE 2005 dataset consists of 599 annotated documents from various domains. The authors specify that they used 90 randomly selected documents for both the development and test sets.

3. **POS Dataset**: This dataset is derived from Universal Dependencies v. 1.3 for English, with a reduced training set of the first 500 sentences.

4. **Chunking Dataset**: The authors refer to the CoNLL 2000 shared task dataset for chunking.

5. **NER Dataset**: This dataset is from the CoNLL 2003 shared task on named entity recognition.

6. **GENIA NER Dataset**: This dataset consists of 2000 annotated Medline abstracts for bio-entity recognition, with a development set of 400 abstracts and a test set of 404 abstracts.

7. **WNUT16 Dataset**: This dataset is from the WNUT16 shared task on Named Entity Recognition over Twitter, with training data of 2,394 annotated tweets, development data of 1,000 tweets, and test data of 3,856 tweets.

8. **Stanford Sentiment Treebank (SST-5)**: This dataset is used for sentiment analysis.

9. **SNLI Dataset**: This dataset is used for natural language inference tasks.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset. The citations are crucial for proper attribution and to allow others to access the datasets.

The full citations I will extract include:

- **Arguments Dataset**: 
  > Stab, C., & Gurevych, I. (2017). Parsing Argumentation Structures in Persuasive Essays. *Computational Linguistics*, 43(3), 619–659.

- **ACE Entities/Events**: 
  > Walker, C., Strassel, S., Medero, J., & Maeda, K. (2005). ACE 2005 Multilingual Training Corpus. LDC Catalog No.: LDC2006T06.

- **POS Dataset**: 
  > Universal Dependencies. (2016). Universal Dependencies v. 1.3.

- **Chunking Dataset**: 
  > Tjong Kim Sang, E. F., & Veenstra, J. (2000). Representing Text in the CoNLL 2000 Shared Task. In *Proceedings of the CoNLL-2000*.

- **NER Dataset**: 
  > Tjong Kim Sang, E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In *Proceedings of CoNLL-2003*.

- **GENIA NER Dataset**: 
  > Kim, J.-D., Ohta, T., Tsuruoka, Y., Tateisi, Y., & Collier, N. (2004). Introduction to the Bio-Entity Recognition Task at JNLPBA. In *Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications, JNLPBA ’04*.

- **WNUT16 Dataset**: 
  > Strauss, B., Toma, B., Ritter, A., de Marneffe, M.-C., & Xu, W. (2016). Results of the WNUT16 Named Entity Recognition Shared Task. In *Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)*.

- **Stanford Sentiment Treebank (SST-5)**: 
  > Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., & Potts, C. (2013). Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing*.

- **SNLI Dataset**: 
  > Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A Large Annotated Corpus for Learning Natural Language Inference. In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*.

Finally, I will compile the dataset entries with their respective citations into a structured format for further processing or review.