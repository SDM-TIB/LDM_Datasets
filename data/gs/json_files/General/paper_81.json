[
    {
        "dcterms:creator": [
            "K. M. Hermann",
            "T. Kocisky",
            "E. Grefenstette",
            "L. Espeholt",
            "W. Kay",
            "M. Suleyman",
            "P. Blunsom"
        ],
        "dcterms:description": "The CNN/Dailymail dataset is used for training extractive summarization models, containing document-summary pairs.",
        "dcterms:title": "CNN/Dailymail",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Summarization"
        ],
        "dcat:keyword": [
            "Extractive summarization",
            "Document summarization",
            "Neural networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Document Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "G. Durrett",
            "T. Berg-Kirkpatrick",
            "D. Klein"
        ],
        "dcterms:description": "The New York Times dataset is used for summarization tasks, specifically for creating the NYT50 subset which includes documents with summaries longer than 50 words.",
        "dcterms:title": "New York Times",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Summarization"
        ],
        "dcat:keyword": [
            "Extractive summarization",
            "Document summarization",
            "Neural networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "NYT50",
        "dcterms:format": "Text",
        "mls:task": [
            "Document Summarization"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "GIGA-CM dataset includes documents sampled from the English Gigaword dataset and the training split of the CNN/Dailymail dataset, used for pre-training the HIBERT model.",
        "dcterms:title": "GIGA-CM",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Pre-training"
        ],
        "dcat:keyword": [
            "Document dataset",
            "Pre-training",
            "Neural networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pre-training"
        ]
    },
    {
        "dcterms:creator": [
            "G. Durrett",
            "T. Berg-Kirkpatrick",
            "D. Klein"
        ],
        "dcterms:description": "NYT50 is a subset of the New York Times dataset, created by filtering documents with summaries shorter than 50 words.",
        "dcterms:title": "NYT50",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Summarization"
        ],
        "dcat:keyword": [
            "Extractive summarization",
            "Document summarization",
            "Neural networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Document Summarization"
        ]
    }
]