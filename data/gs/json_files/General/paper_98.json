[
    {
        "dcterms:creator": [
            "Aitor Gonzalez-Agirre",
            "Montserrat Marimon",
            "Ander Intxaurrondo",
            "Obdulia Rabal",
            "Marta Villegas",
            "Martin Krallinger"
        ],
        "dcterms:description": "The PharmaCoNER corpus is the first Named Entity Recognition (NER) task to recognize chemical and protein entities from Spanish biomedical texts.",
        "dcterms:title": "PharmaCoNER corpus",
        "dcterms:issued": "2019",
        "dcterms:language": "Spanish",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Biomedical NLP",
            "Named Entity Recognition"
        ],
        "dcat:keyword": [
            "Pharmaceutical entities",
            "Chemical recognition",
            "Protein recognition",
            "Spanish biomedical texts"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Named Entity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT is a pre-trained deep bidirectional transformer model designed for language understanding.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Language model",
            "Deep learning",
            "Bidirectional transformers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "Multilingual BERT is a variant of BERT pre-trained on Wikipedia in multiple languages.",
        "dcterms:title": "Multilingual BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multilingual model",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Jinhyuk Lee",
            "Wonjin Yoon",
            "Sungdong Kim",
            "Donghyeon Kim",
            "Sunkyu Kim",
            "Chan Ho So",
            "Jaewoo Kang"
        ],
        "dcterms:description": "BioBERT is a pre-trained biomedical language representation model for biomedical text mining.",
        "dcterms:title": "BioBERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1901.08746",
        "dcat:theme": [
            "Biomedical NLP"
        ],
        "dcat:keyword": [
            "Biomedical text mining",
            "Language representation model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Biomedical text mining"
        ]
    }
]