[
    {
        "dcterms:creator": [
            "S. Shahrampour",
            "A. Rakhlin",
            "A. Jadbabaie"
        ],
        "dcterms:description": "This dataset involves multi-armed bandits in multi-agent networks, focusing on the interactions and strategies among agents competing for resources.",
        "dcterms:title": "Multi-armed bandits in multi-agent networks",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Agent Systems",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Multi-armed bandits",
            "Multi-agent networks",
            "Resource allocation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "E. Hillel",
            "Z. S. Karnin",
            "T. Koren",
            "R. Lempel",
            "O. Somekh"
        ],
        "dcterms:description": "This dataset addresses distributed exploration strategies in multi-armed bandits, focusing on how agents can explore options in a decentralized manner.",
        "dcterms:title": "Distributed exploration in multi-armed bandits",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Distributed Learning",
            "Exploration Strategies"
        ],
        "dcat:keyword": [
            "Distributed exploration",
            "Multi-armed bandits",
            "Decentralized systems"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Landgren",
            "V. Srivastava",
            "N. E. Leonard"
        ],
        "dcterms:description": "This dataset focuses on distributed cooperative decision-making in multi-armed bandits, exploring both frequentist and Bayesian algorithms.",
        "dcterms:title": "Distributed cooperative decision-making in multiarmed bandits",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cooperative Learning",
            "Decision Making"
        ],
        "dcat:keyword": [
            "Cooperative decision-making",
            "Multi-armed bandits",
            "Frequentist algorithms",
            "Bayesian algorithms"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Cesa-Bianchi",
            "C. Gentile",
            "Y. Mansour",
            "A. Minora"
        ],
        "dcterms:description": "This dataset examines delay and cooperation in non-stochastic bandits, focusing on the implications of delays in decision-making processes.",
        "dcterms:title": "Delay and cooperation in non-stochastic bandits",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Non-stochastic Processes",
            "Cooperation"
        ],
        "dcat:keyword": [
            "Non-stochastic bandits",
            "Delay",
            "Cooperation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "B. Szorenyi",
            "R. Busa-Fekete",
            "I. Hegedus",
            "R. Orm´andi",
            "M. Jelasity",
            "B. K´egl"
        ],
        "dcterms:description": "This dataset presents gossip-based distributed stochastic bandit algorithms, focusing on how agents can communicate and share information in a decentralized manner.",
        "dcterms:title": "Gossip-based distributed stochastic bandit algorithms",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Gossip Protocols",
            "Stochastic Processes"
        ],
        "dcat:keyword": [
            "Gossip-based algorithms",
            "Distributed systems",
            "Stochastic bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Korda",
            "B. Sz¨or´enyi",
            "L. Shuai"
        ],
        "dcterms:description": "This dataset involves distributed clustering of linear bandits in peer-to-peer networks, focusing on how agents can cluster their actions in a decentralized manner.",
        "dcterms:title": "Distributed clustering of linear bandits in peer to peer networks",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Clustering",
            "Peer-to-Peer Networks"
        ],
        "dcat:keyword": [
            "Distributed clustering",
            "Linear bandits",
            "Peer-to-peer networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Rosenski",
            "O. Shamir",
            "L. Szlak"
        ],
        "dcterms:description": "This dataset presents a multi-player bandits approach using a musical chairs strategy, focusing on how players can optimize their choices in a competitive environment.",
        "dcterms:title": "Multi-player bandits–a musical chairs approach",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Multi-Player Systems"
        ],
        "dcat:keyword": [
            "Multi-player bandits",
            "Musical chairs",
            "Competitive strategies"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Nayyar",
            "D. Kalathil",
            "R. Jain"
        ],
        "dcterms:description": "This dataset focuses on regret-optimal learning in decentralized multi-player multi-armed bandits, exploring strategies for minimizing regret in a competitive setting.",
        "dcterms:title": "On regret-optimal learning in decentralized multi-player multi-armed bandits",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Decentralized Learning",
            "Regret Minimization"
        ],
        "dcat:keyword": [
            "Decentralized learning",
            "Multi-player bandits",
            "Regret optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Kalathil",
            "N. Nayyar",
            "R. Jain"
        ],
        "dcterms:description": "This dataset addresses decentralized learning for multiplayer multiarmed bandits, focusing on strategies for learning in a decentralized manner.",
        "dcterms:title": "Decentralized learning for multiplayer multiarmed bandits",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Decentralized Learning",
            "Multi-armed Bandits"
        ],
        "dcat:keyword": [
            "Decentralized learning",
            "Multiarmed bandits",
            "Learning strategies"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Liu",
            "Q. Zhao"
        ],
        "dcterms:description": "This dataset involves distributed learning in multi-armed bandit scenarios with multiple players, focusing on collaborative strategies for learning.",
        "dcterms:title": "Distributed learning in multi-armed bandit with multiple players",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Collaborative Learning",
            "Multi-armed Bandits"
        ],
        "dcat:keyword": [
            "Distributed learning",
            "Multi-armed bandits",
            "Multiple players"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Vakili",
            "K. Liu",
            "Q. Zhao"
        ],
        "dcterms:description": "This dataset explores deterministic sequencing of exploration and exploitation in multi-armed bandit problems, focusing on optimal strategies for decision-making.",
        "dcterms:title": "Deterministic sequencing of exploration and exploitation for multi-armed bandit problems",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Decision Making",
            "Multi-armed Bandits"
        ],
        "dcat:keyword": [
            "Deterministic sequencing",
            "Exploration",
            "Exploitation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "L. Lai",
            "H. Jiang",
            "H. V. Poor"
        ],
        "dcterms:description": "This dataset presents a competitive multi-armed bandit framework for medium access in cognitive radio networks, focusing on resource allocation strategies.",
        "dcterms:title": "Medium access in cognitive radio networks: A competitive multi-armed bandit framework",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Radio",
            "Resource Allocation"
        ],
        "dcat:keyword": [
            "Cognitive radio networks",
            "Medium access",
            "Competitive bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Anandkumar",
            "N. Michael",
            "A. K. Tang",
            "A. Swami"
        ],
        "dcterms:description": "This dataset focuses on distributed algorithms for learning and cognitive medium access with logarithmic regret, exploring efficient strategies for resource allocation.",
        "dcterms:title": "Distributed algorithms for learning and cognitive medium access with logarithmic regret",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Medium Access",
            "Distributed Learning"
        ],
        "dcat:keyword": [
            "Distributed algorithms",
            "Cognitive medium access",
            "Logarithmic regret"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "K. Liu",
            "Q. Zhao"
        ],
        "dcterms:description": "This dataset examines learning in a changing world through restless multiarmed bandits with unknown dynamics, focusing on adaptive learning strategies.",
        "dcterms:title": "Learning in a changing world: Restless multiarmed bandit with unknown dynamics",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Adaptive Learning",
            "Dynamic Environments"
        ],
        "dcat:keyword": [
            "Restless bandits",
            "Unknown dynamics",
            "Adaptive strategies"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "O. Avner",
            "S. Mannor"
        ],
        "dcterms:description": "This dataset explores concurrent bandits in cognitive radio networks, focusing on strategies for managing multiple users and resources.",
        "dcterms:title": "Concurrent bandits and cognitive radio networks",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Radio",
            "Concurrent Learning"
        ],
        "dcat:keyword": [
            "Concurrent bandits",
            "Cognitive radio",
            "Resource management"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Evirgen",
            "A. Kose"
        ],
        "dcterms:description": "This dataset investigates the effect of communication on noncooperative multiplayer multi-armed bandit problems, focusing on how communication impacts decision-making.",
        "dcterms:title": "The effect of communication on noncooperative multiplayer multi-armed bandit problems",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Communication",
            "Noncooperative Games"
        ],
        "dcat:keyword": [
            "Communication effects",
            "Noncooperative bandits",
            "Multiplayer strategies"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "L. Besson",
            "E. Kaufmann"
        ],
        "dcterms:description": "This dataset revisits multi-player bandits, exploring new strategies and insights into the dynamics of multiplayer bandit problems.",
        "dcterms:title": "Multi-player bandits revisited",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Multi-Player Systems"
        ],
        "dcat:keyword": [
            "Multi-player bandits",
            "Revisiting strategies",
            "Game dynamics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Cohen",
            "A. H´eliou",
            "P. Mertikopoulos"
        ],
        "dcterms:description": "This dataset explores learning with bandit feedback in potential games, focusing on how feedback influences learning dynamics.",
        "dcterms:title": "Learning with bandit feedback in potential games",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Potential Games",
            "Learning Dynamics"
        ],
        "dcat:keyword": [
            "Bandit feedback",
            "Potential games",
            "Learning dynamics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "O. Avner",
            "S. Mannor"
        ],
        "dcterms:description": "This dataset addresses multi-user lax communications through a multi-armed bandit approach, focusing on strategies for managing communication resources.",
        "dcterms:title": "Multi-user lax communications: a multi-armed bandit approach",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Communication",
            "Resource Management"
        ],
        "dcat:keyword": [
            "Multi-user communications",
            "Lax strategies",
            "Multi-armed bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]