To extract datasets from the research paper titled "DSC-IIT ISM at WNUT-2020 Task 2: Detection of COVID-19 informative tweets using RoBERTa" by Sirigireddy Dhanalaxmi et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and dataset sections** to identify any datasets mentioned. The abstract indicates that the authors evaluated their model on a public dataset, which suggests that there is a dataset to extract.

Next, I will focus on **section 3 (Dataset)**, where the authors provide detailed information about the datasets used. They mention that the training and validation data consists of 7000 and 1000 samples, respectively, and that the test dataset consists of 12000 tweets. This section will likely contain the specifics I need.

In this section, I will note the following details:
- The training dataset has 3303 informative and 3697 uninformative tweets.
- The validation dataset has 472 informative and 528 uninformative tweets.
- The test dataset consists of 12000 tweets, with 2000 selected for final evaluation.

The authors do not provide a specific name for the dataset, but they refer to it as a "public dataset" used in the W-NUT workshop. I will need to find the full citation for this dataset in the **References section**.

In the references, I will look for the paper by Nguyen et al. (2020), which is likely the source of the dataset used for the W-NUT-2020 Task 2. The citation I find is:
> Dat Quoc Nguyen, Thanh Vu, Afshin Rahimi, Mai Hoang Dao, Linh The Nguyen, and Long Doan. *WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets*. In Proceedings of the 6th Workshop on Noisy User-generated Text, pages 412â€“413, Online, Nov 19, 2020. Association for Computational Linguistics.

Now that I have identified the dataset and its citation, I will summarize the dataset details:
- The dataset is used for detecting informative tweets related to COVID-19.
- It includes training, validation, and test splits with specific counts for informative and uninformative tweets.

Finally, I will compile the dataset information and its citation into a structured format for further processing or review.