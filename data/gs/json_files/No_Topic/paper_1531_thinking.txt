To extract datasets from the research paper titled "Modelling Regular Subcategorization Changes in German Particle Verbs" by Stefan Bott and Sabine Schulte im Walde, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experimental setup sections** to identify any datasets mentioned. The abstract provides a brief overview of the research focus, but I need to look for specific datasets used in the experiments.

In the **introduction**, the authors discuss the syntactic transfer problem and the methodology they are using, but they do not explicitly mention any datasets yet. I will continue to the **experimental setup** section, where they describe the selection of a set of 32 particle verbs (PVs) listed in Fleischer and Barz (2012). This indicates that there is a dataset involved, but I need to find more details.

In **section 3.1 (Automatic Classification)**, the authors mention using a lemmatized and tagged version of the SDeWaC corpus (Faaß and Eckart, 2013) as training data. This is a significant dataset for their experiments, and I will note its details.

Next, I will check the **references section** to gather full citations for the datasets mentioned:

1. **SDeWaC Corpus**: The citation is:
   > Faaß, Gertrud, and Eckart, Kerstin. *SdeWaC – a Corpus of Parsable Sentences from the Web*. In Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology, Darmstadt, Germany, 2013.

2. **Fleischer and Barz (2012)**: The citation is:
   > Fleischer, Wolfgang, and Barz, Irmhild. *Wortbildung der deutschen Gegenwartssprache*. Walter de Gruyter, 4th edition, 2012.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing. This will ensure that I have accurately captured the datasets used in the research and their corresponding citations.