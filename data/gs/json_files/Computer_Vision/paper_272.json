[
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A reading comprehension dataset consisting of over 100,000 questions based on a set of Wikipedia articles, designed to test machine comprehension of text.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Machine Comprehension",
            "Text Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Dheeru Dua",
            "Yizhong Wang",
            "Pradeep Dasigi",
            "Gabriel Stanovsky",
            "Sameer Singh",
            "Matt Gardner"
        ],
        "dcterms:description": "A reading comprehension benchmark that requires discrete reasoning over paragraphs, designed to evaluate models' ability to perform complex reasoning tasks.",
        "dcterms:title": "DROP",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Discrete Reasoning",
            "Question Answering",
            "Machine Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    }
]