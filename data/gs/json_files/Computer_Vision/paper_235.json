[
    {
        "dcterms:creator": [
            "Xinlei Chen",
            "Hao Fang",
            "Tsung-Yi Lin",
            "Ramakrishna Vedantam",
            "Saurabh Gupta",
            "Piotr Dollár",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "The COCO Captions dataset requires that a model, given an image, predicts a caption that factually summarizes the scene. In the dataset used for the 2015 challenge, there are about 83k training images and 414k captions.",
        "dcterms:title": "COCO Captions",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1504.00325",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Captioning",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Young",
            "Alice Lai",
            "Micah Hodosh",
            "Julia Hockenmaier"
        ],
        "dcterms:description": "Flickr30k is a captioning dataset with factual summaries, consisting of 29k training images and 145k captions.",
        "dcterms:title": "Flickr30k",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Captioning",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Kurt Shuster",
            "Samuel Humeau",
            "Hexiang Hu",
            "Antoine Bordes",
            "Jason Weston"
        ],
        "dcterms:description": "Personality Captions attempts to model human style when people speak about images, consisting of about 187k training images, each with a given style label out of 215 possible styles.",
        "dcterms:title": "Personality Captions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Personality modeling",
            "Conversational style"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Conversational Response Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Kurt Shuster",
            "Samuel Humeau",
            "Antoine Bordes",
            "Jason Weston"
        ],
        "dcterms:description": "Image Chat is an extension of the Personality Captions dataset to full dialogue, consisting of human-human conversations based on images and traits.",
        "dcterms:title": "Image Chat",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Image Dialogue"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Dialogue",
            "Conversational style"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Nasrin Mostafazadeh",
            "Chris Brockett",
            "Bill Dolan",
            "Michel Galley",
            "Jianfeng Gao",
            "Georgios Spithourakis",
            "Lucy Vanderwende"
        ],
        "dcterms:description": "Image Chat QA is the extraction of all the question-answer pairs that appear in the Image Chat dataset, consisting of about 20k training questions.",
        "dcterms:title": "Image Chat QA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Question answering",
            "Dialogue"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "Visual Genome connects language and vision using crowdsourced dense image annotations, consisting of 108k images with 2.8 million attributes.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Dense annotations",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Annotation"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "VQA is a task involving open-ended questions about images which require an understanding of vision, language, and commonsense knowledge to answer.",
        "dcterms:title": "VQA (Visual Question Answering)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Question answering",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pierre-Emmanuel Mazaré",
            "Samuel Humeau",
            "Martin Raison",
            "Antoine Bordes"
        ],
        "dcterms:description": "The pushshift.io Reddit dataset consists of 174 million (context, response) pairs extracted from Reddit, used for training personalized dialogue agents.",
        "dcterms:title": "pushshift.io Reddit",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Generation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Dialogue dataset",
            "Conversational AI",
            "Reddit"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jia Deng",
            "Wei Dong",
            "Richard Socher",
            "Li-Jia Li",
            "Kai Li",
            "Li Fei-Fei"
        ],
        "dcterms:description": "ImageNet is a large-scale hierarchical image database consisting of 1.28 million images.",
        "dcterms:title": "ImageNet",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Classification",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Classification",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Dhruv Mahajan",
            "Ross Girshick",
            "Vignesh Ramanathan",
            "Kaiming He",
            "Manohar Paluri",
            "Yixuan Li",
            "Ashwin Bharambe",
            "Laurens van der Maaten"
        ],
        "dcterms:description": "Instagram images dataset consists of 3.5 billion images used for exploring the limits of weakly supervised pretraining.",
        "dcterms:title": "Instagram images",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Classification",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Weakly supervised learning",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Samuel Humeau",
            "Kurt Shuster",
            "Marie-Anne Lachaux",
            "Jason Weston"
        ],
        "dcterms:description": "Toronto Books dataset consists of 150 million (context, response) pairs extracted from Wikipedia and is used for pre-training text encoders.",
        "dcterms:title": "Toronto Books",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Pre-training",
            "Dialogue"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation"
        ]
    }
]