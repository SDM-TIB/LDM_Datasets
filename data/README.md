# üìä Data Directory

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

This directory contains all data files used and generated by the MetaMine project.

## üìÅ Directory Structure

- **aws/**: Amazon Mechanical Turk annotation files
  - `Batch_5281193_batch_results.csv`: Raw annotation results from the first batch
  - `Batch_5281193_reordered.csv`: Reordered columns for better readability
  - `Batch_5282286_batch_results.csv`: Raw annotation results from the second batch
  - `Batch_5282286_reordered.csv`: Reordered columns for better readability

- **gs/**: Gold standard datasets
  - `combined_papers_metadata.csv`: Metadata for all papers in the gold standard
  - `datasets.json`: Combined dataset information from all sources
  - `downloaded_papers.json`: Information about the downloaded papers
  - `json_files/`: Directory containing extracted dataset information
    - `Computer_Vision/`: Extracted datasets from computer vision papers
    - `General/`: Extracted datasets from general ML papers
    - `Graphs/`: Extracted datasets from graph-related papers
    - `Natural_Language_Processing/`: Extracted datasets from NLP papers
  - `paper_urls.txt`: URLs of papers used in the gold standard
  - `pdfs/`: Directory for downloaded PDF files (not included due to copyright restrictions)
  - `processing_summary.json`: Summary of the processing statistics
  - `txt_files/`: Directory for extracted text from PDFs (not included due to copyright restrictions)

- **llama/**: Generated output from the base Llama model
  - `json_files/`: JSON files containing extracted dataset information

- **llama_tuned/**: Generated output from the fine-tuned Llama model
  - `json_files/`: JSON files containing extracted dataset information

- **qwen/**: Generated output from the DeepSeek Qwen model
  - `json_files/`: JSON files containing extracted dataset information

## üîÑ Data Flow

1. Papers are first selected randomly from the Papers With Code repository (via `1_choose_papers_randomly.py`)
2. Selected papers are downloaded as PDFs (via `2_download_papers.py`)
3. PDFs are converted to text (via `3_pdf2txt.py`)
4. Papers are processed by the teacher model to extract dataset information (via `4_process_papers_fine_tune.py`)
5. Human annotators verify a subset of the extracted information (stored in `aws/`)
6. Dataset information from different sources is combined (via `5_combine_datasets.py`)

## üìù Notes

- The `pdfs/` and `txt_files/` directories are not included due to copyright restrictions. To replicate the project, you'll need to download the papers yourself using the provided scripts.
- Each `json_file` contains structured dataset information according to the DCAT vocabulary, including fields like creators, description, title, publication date, language, identifier, theme, keywords, and more.
- For each paper, there are two files:
  - `paper_X.json`: The extracted dataset information in JSON format
  - `paper_X_thinking.txt`: The reasoning process used to extract the information