To extract datasets from the research paper titled "Activity Graph Transformer for Temporal Action Localization" by Megha Nawhal and Greg Mori, I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract** and **introduction** sections to get an overview of the datasets used for evaluation. In the abstract, the authors mention evaluating their model on three challenging datasets: **THUMOS14**, **Charades**, and **EPIC-Kitchens100**. This indicates that these datasets are central to the research.

Next, I will look for a dedicated **Datasets** section or any relevant mentions in the **experiments** section. In the experiments section, the authors provide detailed descriptions of the datasets:

1. **THUMOS14**: This dataset contains 200 videos in the training set and 213 videos in the testing set, with 20 action categories. The average number of action instances per video is 15, with about 8% overlap among instances.

2. **Charades**: This is a large-scale dataset with 9,848 videos of daily indoor activities, featuring 157 action categories. Videos in this dataset have an average of 6 action instances, with a high overlap of approximately 79%.

3. **EPIC-Kitchens100**: This dataset consists of 700 egocentric videos of daily kitchen activities, containing 289 noun classes and 97 verb classes. Each video has an average of 128 action instances, with about 28% overlap.

After identifying the datasets, I will check the **References** section for full citations. The citations for the datasets are as follows:

- **THUMOS14**:
  > Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar. *THUMOS challenge: Action recognition with a large number of classes*. http://crcv.ucf.edu/THUMOS14/, 2014.

- **Charades**:
  > Gunnar A. Sigurdsson, GÃ¼l Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. *Hollywood in homes: Crowdsourcing data collection for activity understanding*. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.

- **EPIC-Kitchens100**:
  > Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. *Rescaling egocentric vision*. CoRR, abs/2006.13256, 2020.

Now that I have gathered the necessary information about the datasets and their citations, I will compile this into a structured format for further use or processing. This includes ensuring that each dataset is clearly described along with its full citation.