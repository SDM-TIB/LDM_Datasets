[
    {
        "dcterms:creator": [
            "Stephen Merity",
            "Nitish Shirish Keskar",
            "Richard Socher"
        ],
        "dcterms:description": "The largest available word-level language modeling benchmark with long-term dependency, containing 103M training tokens from 28K articles, allowing testing of long-term dependency modeling.",
        "dcterms:title": "WikiText-103",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1708.02182",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Word-level",
            "Language modeling",
            "Long-term dependency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "MultiMedia LLC"
        ],
        "dcterms:description": "A dataset containing 100M bytes of unprocessed Wikipedia text, used for evaluating language models.",
        "dcterms:title": "enwik8",
        "dcterms:issued": "2009",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Character-level",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "MultiMedia LLC"
        ],
        "dcterms:description": "A dataset containing 100M processed Wikipedia characters, created by lowering case the text and removing any character other than the 26 letters a through z, and space.",
        "dcterms:title": "text8",
        "dcterms:issued": "2009",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Character-level",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Ciprian Chelba",
            "Tomas Mikolov",
            "Mike Schuster",
            "Qi Ge",
            "Thorsten Brants",
            "Phillipp Koehn",
            "Tony Robinson"
        ],
        "dcterms:description": "A dataset designed to measure progress in statistical language modeling, containing one billion words.",
        "dcterms:title": "One Billion Word",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1312.3005",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Word-level",
            "Language modeling",
            "Statistical language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Geoffrey Zweig"
        ],
        "dcterms:description": "A dataset used for language modeling, containing 1M training tokens.",
        "dcterms:title": "Penn Treebank",
        "dcterms:issued": "2012",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Word-level",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]