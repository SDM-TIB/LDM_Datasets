[
    {
        "dcterms:creator": [
            "Anna Primpeli",
            "Ralph Peeters",
            "Christian Bizer"
        ],
        "dcterms:description": "The WDC training dataset and gold standard for large-scale product matching, used for training Transformer-based matchers for product offers.",
        "dcterms:title": "WDC training dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Product Matching",
            "E-commerce"
        ],
        "dcat:keyword": [
            "Product offers",
            "Training dataset",
            "Entity matching"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Entity Matching"
        ]
    },
    {
        "dcterms:creator": [
            "Alexis Conneau",
            "Kartikay Khandelwal",
            "Naman Goyal",
            "Vishrav Chaudhary",
            "Guillaume Wenzek"
        ],
        "dcterms:description": "A web corpus used for unsupervised cross-lingual representation learning, providing a large dataset for training multilingual models.",
        "dcterms:title": "CommonCrawl web corpus",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1911.02116",
        "dcat:theme": [
            "Cross-lingual Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Web corpus",
            "Cross-lingual representation",
            "Unsupervised learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A pre-trained model for language understanding, used for various NLP tasks including entity matching.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Transformers",
            "Pre-trained model",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Entity Matching"
        ]
    },
    {
        "dcterms:creator": [
            "Alexis Conneau",
            "Kartikay Khandelwal",
            "Naman Goyal",
            "Vishrav Chaudhary",
            "Guillaume Wenzek"
        ],
        "dcterms:description": "A multilingual model trained on a diverse set of languages, used for cross-lingual tasks.",
        "dcterms:title": "XLM-R",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1911.02116",
        "dcat:theme": [
            "Cross-lingual Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multilingual model",
            "Cross-lingual representation",
            "Transformers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A German version of the BERT model, adapted for German language tasks.",
        "dcterms:title": "German BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "German",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "German language",
            "Transformers",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Entity Matching"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A multilingual version of BERT, designed to handle multiple languages in NLP tasks.",
        "dcterms:title": "Multilingual BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Cross-lingual Learning"
        ],
        "dcat:keyword": [
            "Multilingual model",
            "Transformers",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Entity Matching"
        ]
    },
    {
        "dcterms:creator": [
            "Muhao Chen",
            "Yingtao Tian",
            "Kai-Wei Chang",
            "Steven Skiena",
            "Carlo Zaniolo"
        ],
        "dcterms:description": "Knowledge graphs used for cross-lingual entity alignment, providing a framework for aligning entities across different languages.",
        "dcterms:title": "Knowledge Graphs",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Knowledge Graphs",
            "Cross-lingual Learning"
        ],
        "dcat:keyword": [
            "Entity alignment",
            "Knowledge graphs",
            "Cross-lingual"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]