[
    {
        "dcterms:creator": [
            "E. F. Tjong Kim Sang",
            "F. De Meulder"
        ],
        "dcterms:description": "The CoNLL-03 dataset provides a mainstream NER benchmark in the English language. For few-shot trials, training points are sampled uniformly at random from the full training set.",
        "dcterms:title": "CoNLL-03",
        "dcterms:issued": "2003",
        "dcterms:language": "English",
        "dcterms:identifier": "https://www.aclweb.org/anthology/W03-0419",
        "dcat:theme": [
            "Natural Language Processing",
            "Named Entity Recognition"
        ],
        "dcat:keyword": [
            "NER",
            "benchmark",
            "few-shot learning"
        ],
        "dcat:landingPage": "https://www.aclweb.org/anthology/W03-0419",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Named Entity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Rahimi",
            "Y. Li",
            "T. Cohn"
        ],
        "dcterms:description": "The WikiAnn dataset is a multilingual NER benchmark used to test the effect of resource availability in the pretraining phase. For few-shot experiments, a fixed training set of size N = 100 is sampled for each language separately.",
        "dcterms:title": "WikiAnn",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.aclweb.org/anthology/P19-1015",
        "dcat:theme": [
            "Natural Language Processing",
            "Named Entity Recognition"
        ],
        "dcat:keyword": [
            "multilingual",
            "NER",
            "resource availability"
        ],
        "dcat:landingPage": "https://www.aclweb.org/anthology/P19-1015",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Named Entity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "S. Merity",
            "C. Xiong",
            "J. Bradbury",
            "R. Socher"
        ],
        "dcterms:description": "The WikiText-2 dataset is a subset of the English Wikipedia developed for use in long-term dependency language modeling. It contains the first 2 million words of a dump of the English Wikipedia at the time of creation.",
        "dcterms:title": "WikiText-2",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "language modeling",
            "Wikipedia",
            "text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Masked Language Modeling"
        ]
    }
]