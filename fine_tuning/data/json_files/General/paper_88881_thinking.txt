To extract datasets from the research paper titled "See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks" by Xiankai Lu et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract states that the authors conducted extensive experiments over three large benchmarks, which suggests that datasets are involved.

Next, I will focus on **section 4 (Experiments)**, where the authors typically describe the datasets used for their evaluations. In this section, they mention three datasets:

1. **DAVIS16**: This dataset consists of 50 videos with pixel-wise annotations for video object segmentation. It is a well-known benchmark in the field.

2. **FBMS**: This dataset contains 59 video sequences with sparse annotations, specifically 720 frames that are labeled. It is used for evaluating video object segmentation methods.

3. **Youtube-Objects**: This dataset includes 126 video sequences across 10 object categories, with more than 20,000 frames in total. It is another popular benchmark for video object segmentation.

To ensure I have the correct details, I will check the **References section** for full citations of these datasets:

- For **DAVIS16**, the citation is:
  > Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. *A benchmark dataset and evaluation methodology for video object segmentation*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2016, 2016.

- For **FBMS**, the citation is:
  > Peter Ochs, Jitendra Malik, and Thomas Brox. *Segmentation of moving objects by long term video analysis*. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 36(6):1187â€“1200, 2014.

- For **Youtube-Objects**, the citation is:
  > Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio Ferrari. *Learning object class detectors from weakly annotated video*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2012, 2012.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets used in the research and their corresponding references.