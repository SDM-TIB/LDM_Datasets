[
    {
        "dcterms:creator": [
            "M. Mahoney"
        ],
        "dcterms:description": "The text8 dataset consists of the first 108 characters of a Wikipedia article dump from 2006, after removing punctuation and markup and converting to lowercase, leaving spaces and the 26 characters of the English alphabet.",
        "dcterms:title": "text8",
        "dcterms:issued": "2006",
        "dcterms:language": "English",
        "dcterms:identifier": "https://cs.fit.edu/~mmahoney/compression/textdata.html",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Character-level modeling",
            "Wikipedia",
            "Text dataset"
        ],
        "dcat:landingPage": "https://cs.fit.edu/~mmahoney/compression/textdata.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "S. Merity",
            "N. S. Keskar",
            "J. N. Socher"
        ],
        "dcterms:description": "The WikiText-2 dataset contains words from 720 Wikipedia articles with an average length of over 3500 words per article and a total vocabulary of over 33,000 words.",
        "dcterms:title": "WikiText-2",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "http://arxiv.org/abs/1609.07843",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Word-level modeling",
            "Wikipedia",
            "Text dataset"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1609.07843",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]