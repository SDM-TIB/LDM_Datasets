[
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A pre-trained model for language understanding that uses deep bidirectional transformers.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "abs/1810.04805",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Transformers",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jeff Wu",
            "Rewon Child",
            "David Luan",
            "Dario Amodei",
            "Ilya Sutskever"
        ],
        "dcterms:description": "An unsupervised multitask learner that generates human-like text.",
        "dcterms:title": "GPT-2",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "dcat:keyword": [
            "Language model",
            "Text generation",
            "Unsupervised learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "Mohammad Shoeybi",
            "Mostofa Patwary",
            "Raul Puri",
            "Patrick LeGresley",
            "Jared Casper",
            "Bryan Catanzaro"
        ],
        "dcterms:description": "A framework for training multi-billion parameter language models using model parallelism.",
        "dcterms:title": "Megatron-LM",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Parallelism"
        ],
        "dcat:keyword": [
            "Language model",
            "Model parallelism",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Colin Raffel",
            "Noam Shazeer",
            "Adam Roberts",
            "Katherine Lee",
            "Sharan Narang",
            "Michael Matena",
            "Yanqi Zhou",
            "Wei Li",
            "Peter J. Liu"
        ],
        "dcterms:description": "A unified framework for transfer learning with a text-to-text transformer.",
        "dcterms:title": "T5",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Transfer Learning"
        ],
        "dcat:keyword": [
            "Text-to-text",
            "Transfer learning",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification",
            "Translation"
        ]
    },
    {
        "dcterms:creator": [
            "Microsoft"
        ],
        "dcterms:description": "A 17-billion-parameter language model developed by Microsoft.",
        "dcterms:title": "Turing-NLG",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language model",
            "Large-scale model",
            "Deep learning"
        ],
        "dcat:landingPage": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language generation"
        ]
    }
]