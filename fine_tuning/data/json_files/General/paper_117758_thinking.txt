To extract datasets from the research paper titled "On the Effectiveness of Automated Metrics for Text Generation Systems" by Pius von Däniken et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract** and **introduction** sections to see if any datasets are mentioned. The abstract highlights the application of the theory on **WMT 21** and **Spot-The-Bot evaluation data**, which suggests that these may be datasets used in the research.

Next, I will look for specific sections that discuss datasets in detail. The **experiments** section is likely to contain information about the datasets used for evaluation. I will check if there is a dedicated section for datasets or if they are mentioned within the context of the experiments.

Upon reviewing the paper, I find that the authors explicitly mention two datasets:

1. **WMT 21 Dataset**: This dataset is used in the context of evaluating machine translation systems. The authors refer to it in the experiments section, indicating that it is a well-known benchmark in the field.

2. **Spot-The-Bot Dataset**: This dataset is used for evaluating dialogue systems, as mentioned in the experiments section. The authors describe how it contains dialogues between two systems, which are classified by humans.

Now, I will gather the full citations for these datasets from the **References section** of the paper:

- For the **WMT 21 Dataset**, the citation is:
  > Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. *Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain*. In Proceedings of the Sixth Conference on Machine Translation, pages 733–774, Online, 2021.

- For the **Spot-The-Bot Dataset**, the citation is:
  > Jan Deriu, Don Tuggener, Pius von Däniken, Jon Ander Campos, Alvaro Rodrigo, Thiziri Belkacem, Aitor Soroa, Eneko Agirre, and Mark Cieliebak. *Spot the bot: A robust and efficient framework for the evaluation of conversational dialogue systems*. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3971–3984, Online, 2020.

After confirming the datasets and their citations, I will compile this information into a structured format that clearly outlines each dataset along with its citation. This will ensure that the datasets are accurately represented and can be referenced appropriately in future work.