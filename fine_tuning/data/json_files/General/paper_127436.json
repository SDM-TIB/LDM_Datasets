[
    {
        "dcterms:creator": [
            "Aakanksha Chowdhery",
            "Sharan Narang",
            "Jacob Devlin",
            "Maarten Bosma",
            "Gaurav Mishra",
            "Adam Roberts",
            "Paul Barham",
            "Hyung Won Chung",
            "Charles Sutton",
            "Sebastian Gehrmann"
        ],
        "dcterms:description": "A primary test set consisting of 360 descriptions spanning four large language models and 30 diverse entities, used for evaluating self-contradictions in generated text.",
        "dcterms:title": "MainTestSet",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2204.02311",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language Models",
            "Self-Contradiction",
            "Text Generation"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2204.02311",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation",
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Aakanksha Chowdhery",
            "Sharan Narang",
            "Jacob Devlin",
            "Maarten Bosma",
            "Gaurav Mishra",
            "Adam Roberts",
            "Paul Barham",
            "Hyung Won Chung",
            "Charles Sutton",
            "Sebastian Gehrmann"
        ],
        "dcterms:description": "A secondary test set containing a comprehensive set of 100 entities, used for further evaluation of self-contradictions in generated text.",
        "dcterms:title": "2ndTestSet",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language Models",
            "Self-Contradiction",
            "Text Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation",
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Mallen",
            "Akari Asai",
            "Victor Zhong",
            "Rajarshi Das",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "dcterms:description": "A benchmark dataset for evaluating the effectiveness of language models in question answering tasks, focusing on the detection of self-contradictions.",
        "dcterms:title": "PopQA",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://doi.org/10.18653/v1/2023.acl-long.546",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Language Models",
            "Question Answering",
            "Self-Contradiction"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/2023.acl-long.546",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Evaluation"
        ]
    }
]