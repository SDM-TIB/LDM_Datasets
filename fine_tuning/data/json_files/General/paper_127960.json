[
    {
        "dcterms:creator": [
            "J. F. Gemmeke",
            "D. P. Ellis",
            "D. Freedman",
            "A. Jansen",
            "W. Lawrence",
            "R. C. Moore",
            "M. Plakal",
            "M. Ritter"
        ],
        "dcterms:description": "AudioSet is one of the largest publicly available audio corpora, containing over 5000 hours of audio data distributed in 2 million 10-second weakly annotated YouTube clips spanning 527 classes.",
        "dcterms:title": "AudioSet",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Event Classification"
        ],
        "dcat:keyword": [
            "Audio dataset",
            "YouTube clips",
            "Weakly annotated",
            "Audio events"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "AS-5k",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio Classification"
        ]
    },
    {
        "dcterms:creator": [
            "S.-w. Yang",
            "P.-H. Chi",
            "Y.-S. Chuang",
            "C.-I. Jeff Lai",
            "K. Lakhotia",
            "Y. Y. Lin",
            "A. T. Liu",
            "J. Shi",
            "X. Chang",
            "G.-T. Lin",
            "T.-H. Huang",
            "W.-C. Tseng",
            "K.-t. Lee",
            "D.-R. Liu",
            "Z. Huang",
            "S. Dong",
            "S.-W. Li",
            "S. Watanabe",
            "A. Mohamed",
            "H.-y. Lee"
        ],
        "dcterms:description": "SUPERB is a benchmark for evaluating speech processing performance across various tasks.",
        "dcterms:title": "SUPERB",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Processing"
        ],
        "dcat:keyword": [
            "Speech dataset",
            "Benchmark",
            "Speech recognition",
            "Speech tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Speech Recognition",
            "Speech Processing"
        ]
    },
    {
        "dcterms:creator": [
            "J. Turian",
            "J. Shier",
            "H. R. Khan",
            "B. Raj",
            "B. W. Schuller",
            "C. J. Steinmetz",
            "C. Malloy",
            "G. Tzanetakis",
            "G. Velarde",
            "K. McNally",
            "M. Henry",
            "N. Pinto",
            "C. Noufi",
            "C. Clough",
            "D. Herremans",
            "E. Fonseca",
            "J. Engel",
            "J. Salamon",
            "P. Esling",
            "P. Manocha",
            "S. Watanabe",
            "Z. Jin",
            "Y. Bisk"
        ],
        "dcterms:description": "HEAR is a benchmark consisting of 19 tasks spanning diverse audio domains of speech, music, and environmental sounds.",
        "dcterms:title": "HEAR",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Representation Evaluation"
        ],
        "dcat:keyword": [
            "Audio dataset",
            "Benchmark",
            "Speech",
            "Music",
            "Environmental sounds"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Audio Classification",
            "Speech Recognition",
            "Music Classification"
        ]
    },
    {
        "dcterms:creator": [
            "J. Engel",
            "C. Resnick",
            "A. Roberts",
            "S. Dieleman",
            "D. Eck",
            "K. Simonyan",
            "M. Norouzi"
        ],
        "dcterms:description": "NSynth Pitch 5h is a dataset for pitch classification tasks, containing audio samples of musical notes.",
        "dcterms:title": "NSynth Pitch 5h",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Classification"
        ],
        "dcat:keyword": [
            "Pitch classification",
            "Music dataset",
            "Audio synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Pitch Classification"
        ]
    },
    {
        "dcterms:creator": [
            "H. Cao",
            "D. G. Cooper",
            "M. K. Keutmann",
            "R. C. Gur",
            "A. Nenkova",
            "R. Verma"
        ],
        "dcterms:description": "CREMA-D is a crowd-sourced emotional multimodal actors dataset for emotion recognition tasks.",
        "dcterms:title": "CREMA-D",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition"
        ],
        "dcat:keyword": [
            "Emotion dataset",
            "Multimodal",
            "Crowd-sourced"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "K. J. Piczak"
        ],
        "dcterms:description": "ESC-50 is a dataset for environmental sound classification, containing 50 classes of environmental sounds.",
        "dcterms:title": "ESC-50",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Environmental Sound Classification"
        ],
        "dcat:keyword": [
            "Environmental sounds",
            "Sound classification",
            "Audio dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Sound Classification"
        ]
    },
    {
        "dcterms:creator": [
            "F.-R. St√∂ter",
            "S. Chakrabarty",
            "E. Habets",
            "B. Edler"
        ],
        "dcterms:description": "LibriCount is a dataset for speaker count estimation, providing audio samples for estimating the number of speakers in a given audio clip.",
        "dcterms:title": "LibriCount",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speaker Count Estimation"
        ],
        "dcat:keyword": [
            "Speaker count",
            "Audio dataset",
            "Estimation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Speaker Count Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "E. Fonseca",
            "X. Favory",
            "J. Pons",
            "F. Font",
            "X. Serra"
        ],
        "dcterms:description": "FSD50K is an open dataset of human-labeled sound events, designed for audio tagging tasks.",
        "dcterms:title": "FSD50K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Tagging"
        ],
        "dcat:keyword": [
            "Sound events",
            "Audio dataset",
            "Human-labeled"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio Tagging"
        ]
    },
    {
        "dcterms:creator": [
            "P. Warden"
        ],
        "dcterms:description": "Speech Commands is a dataset for limited-vocabulary speech recognition, containing audio samples of spoken commands.",
        "dcterms:title": "Speech Commands",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition"
        ],
        "dcat:keyword": [
            "Speech dataset",
            "Limited vocabulary",
            "Audio commands"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Anantapadmanabhan",
            "A. Bellur",
            "H. A. Murthy"
        ],
        "dcterms:description": "Mridangam Stroke is a dataset for modal analysis and transcription of strokes of the mridangam, a traditional Indian percussion instrument.",
        "dcterms:title": "Mridangam Stroke",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Analysis"
        ],
        "dcat:keyword": [
            "Mridangam",
            "Percussion",
            "Stroke analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Music Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "A. Anantapadmanabhan",
            "A. Bellur",
            "H. A. Murthy"
        ],
        "dcterms:description": "Mridangam Tonic is a dataset for modal analysis and transcription of tonic of the mridangam, a traditional Indian percussion instrument.",
        "dcterms:title": "Mridangam Tonic",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Analysis"
        ],
        "dcat:keyword": [
            "Mridangam",
            "Percussion",
            "Tonic analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Music Analysis"
        ]
    }
]