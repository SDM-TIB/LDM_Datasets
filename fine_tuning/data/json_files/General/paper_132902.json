[
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozière",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar"
        ],
        "dcterms:description": "LLaMA-7B is a foundational language model designed for various natural language processing tasks, optimized for efficiency and performance.",
        "dcterms:title": "LLaMA-7B",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Natural language processing",
            "LLaMA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "J. Ainslie",
            "J. Lee-Thorp",
            "M. de Jong",
            "Y. Zemlyanskiy",
            "F. Lebrón",
            "S. Sanghai"
        ],
        "dcterms:description": "LLaMA-GQA is a variant of the LLaMA model that incorporates generalized multi-query attention mechanisms for improved performance on multi-query tasks.",
        "dcterms:title": "LLaMA-GQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.13245",
        "dcat:theme": [
            "Natural Language Processing",
            "Attention Mechanisms"
        ],
        "dcat:keyword": [
            "Generalized multi-query attention",
            "LLaMA",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozière",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar"
        ],
        "dcterms:description": "LLaMA-33H is a variant of the LLaMA model featuring 33 attention heads, designed for enhanced performance in complex tasks.",
        "dcterms:title": "LLaMA-33H",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Attention heads",
            "LLaMA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozière",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar"
        ],
        "dcterms:description": "LLaMA-16H is a variant of the LLaMA model with 16 attention heads, aimed at balancing performance and computational efficiency.",
        "dcterms:title": "LLaMA-16H",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Attention heads",
            "LLaMA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozière",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar"
        ],
        "dcterms:description": "LLaMA-8H is a variant of the LLaMA model featuring 8 attention heads, designed for specific applications requiring fewer heads.",
        "dcterms:title": "LLaMA-8H",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Attention heads",
            "LLaMA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozière",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar"
        ],
        "dcterms:description": "LLaMA-4H is a variant of the LLaMA model with 4 attention heads, optimized for lightweight applications.",
        "dcterms:title": "LLaMA-4H",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Attention heads",
            "LLaMA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozière",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar"
        ],
        "dcterms:description": "LLaMA-2H is a variant of the LLaMA model featuring 2 attention heads, suitable for minimal resource environments.",
        "dcterms:title": "LLaMA-2H",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Attention heads",
            "LLaMA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    }
]