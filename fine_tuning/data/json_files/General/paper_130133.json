[
    {
        "dcterms:creator": [],
        "dcterms:description": "A comprehensive benchmark for evaluating Chinese large language models, encompassing user queries and ratings derived from a model battle platform.",
        "dcterms:title": "SuperCLUE",
        "dcterms:issued": "",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Chinese LLMs",
            "User Preferences",
            "Benchmarking"
        ],
        "dcat:landingPage": "https://www.CLUEbenchmarks.com",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset containing user-model interactions with user-reported ratings from a model battle platform, allowing users to evaluate the performance of different models.",
        "dcterms:title": "CArena",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "User Evaluation",
            "Model Comparison"
        ],
        "dcat:keyword": [
            "User Ratings",
            "Model Interaction",
            "Chinese LLMs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "User Preference Evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset containing open-ended questions designed to evaluate models' instruction-following and multi-turn conversational abilities.",
        "dcterms:title": "OPEN",
        "dcterms:issued": "",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Open-ended Questioning",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Open-ended Questions",
            "Multi-turn Dialogue",
            "Instruction Following"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Conversational Evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset containing closed-ended questions derived from open-ended questions, aimed at evaluating model performance through multi-choice formats.",
        "dcterms:title": "CLOSE",
        "dcterms:issued": "",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Closed-ended Questioning",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "Closed-ended Questions",
            "Multi-choice Evaluation",
            "Model Performance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Language Understanding",
            "Multitask Evaluation"
        ],
        "dcat:keyword": [
            "Multitask Learning",
            "Language Understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "A. Lewkowycz",
            "A. Slone",
            "A. Andreassen",
            "D. Freeman",
            "E. S. Dyer",
            "G. Mishra",
            "G. Gur-Ari",
            "J. Lee",
            "J. Sohl-dickstein",
            "K. Chiafullo",
            "L. B. Fedus",
            "N. Fiedel",
            "R. Liu",
            "V. Misra",
            "V. V. Ramasesh"
        ],
        "dcterms:description": "A technical report quantifying and extrapolating the capabilities of language models beyond imitation.",
        "dcterms:title": "Big-Bench",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Model Evaluation",
            "Capability Assessment"
        ],
        "dcat:keyword": [
            "Language Models",
            "Evaluation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Liang",
            "R. Bommasani",
            "T. Lee",
            "D. Tsipras",
            "D. Soylu",
            "M. Yasunaga",
            "Y. Zhang",
            "D. Narayanan",
            "Y. Wu",
            "A. Kumar",
            "B. Newman",
            "B. Yuan",
            "B. Yan",
            "C. Zhang",
            "C. Cosgrove",
            "C. D. Manning",
            "C. RÃ©",
            "D. Acosta-Navas",
            "D. A. Hudson",
            "E. Zelikman",
            "E. Durmus",
            "F. Ladhak",
            "F. Rong",
            "H. Yao",
            "J. Wang",
            "K. Santhanam",
            "L. Orr",
            "L. Zheng",
            "M. Yuksekgonul",
            "M. Suzgun",
            "N. Kim",
            "N. Guha",
            "N. Chatterji",
            "O. Khattab",
            "P. Henderson",
            "Q. Huang",
            "R. Chi",
            "S. M. Xie",
            "S. Santurkar",
            "S. Ganguli",
            "T. Hashimoto",
            "T. Icard",
            "T. Zhang",
            "V. Chaudhary",
            "W. Wang",
            "X. Li",
            "Y. Mai",
            "Y. Zhang",
            "Y. Koreeda"
        ],
        "dcterms:description": "A holistic evaluation framework for language models.",
        "dcterms:title": "HELM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2211.09110",
        "dcat:theme": [
            "Language Model Evaluation",
            "Holistic Assessment"
        ],
        "dcat:keyword": [
            "Evaluation Framework",
            "Language Models",
            "Assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "H. Zeng"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask Chinese understanding.",
        "dcterms:title": "MMCU",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.12986",
        "dcat:theme": [
            "Chinese Language Understanding",
            "Multitask Evaluation"
        ],
        "dcat:keyword": [
            "Chinese Understanding",
            "Benchmark",
            "Multitask Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "W. Zhong",
            "R. Cui",
            "Y. Guo",
            "Y. Liang",
            "S. Lu",
            "Y. Wang",
            "A. S. S. Saied",
            "W. Chen",
            "N. Duan"
        ],
        "dcterms:description": "A human-centric benchmark for evaluating foundation models.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.06364",
        "dcat:theme": [
            "Human-Centric Evaluation",
            "Foundation Models"
        ],
        "dcat:keyword": [
            "Evaluation",
            "Human-Centric",
            "Foundation Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Huang",
            "Y. Bai",
            "Z. Zhu",
            "J. Zhang",
            "J. Zhang",
            "T. Su",
            "J. Liu",
            "C. Lv",
            "Y. Zhang",
            "J. Lei",
            "Y. Fu",
            "M. Sun",
            "J. He"
        ],
        "dcterms:description": "A multi-level multi-discipline Chinese evaluation suite for foundation models.",
        "dcterms:title": "C-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.08322",
        "dcat:theme": [
            "Chinese Evaluation",
            "Foundation Models"
        ],
        "dcat:keyword": [
            "Evaluation Suite",
            "Chinese Language",
            "Foundation Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "L. Zheng",
            "W.-L. Chiang",
            "Y. Sheng",
            "S. Zhuang",
            "Z. Wu",
            "Y. Zhuang",
            "Z. Lin",
            "Z. Li",
            "D. Li",
            "E. P. Xing",
            "H. Zhang",
            "J. E. Gonzalez",
            "I. Stoica"
        ],
        "dcterms:description": "A benchmark for evaluating models' multi-turn conversational and instruction-following abilities.",
        "dcterms:title": "MT-bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.05685",
        "dcat:theme": [
            "Conversational AI",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "Multi-turn Evaluation",
            "Conversational Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]