[
    {
        "dcterms:creator": [
            "D. Soboleva",
            "F. Al-Khateeb",
            "R. Myers",
            "J. R. Steeves",
            "J. Hestness",
            "N. Dey"
        ],
        "dcterms:description": "A large open-source corpus created for training language models based on RedPajama, derived by cleaning and deduplicating the original RedPajama corpus.",
        "dcterms:title": "SlimPajama",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "language model",
            "open-source corpus",
            "RedPajama",
            "tokenized data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Li",
            "L. B. allal",
            "Y. Zi",
            "N. Muennighoff",
            "D. Kocetkov",
            "C. Mou",
            "M. Marone",
            "C. Akiki",
            "J. LI",
            "J. Chim",
            "Q. Liu",
            "E. Zheltonozhskii",
            "T. Y. Zhuo",
            "T. Wang",
            "O. Dehaene",
            "J. Lamy-Poirier",
            "J. Monteiro",
            "N. Gontier",
            "M.-H. Yee",
            "L. K. Umapathi",
            "J. Zhu",
            "B. Lipkin",
            "M. Oblokulov",
            "Z. Wang",
            "R. Murthy",
            "J. T. Stillerman",
            "S. S. Patel",
            "D. Abulkhanov",
            "M. Zocca",
            "M. Dey",
            "Z. Zhang",
            "U. Bhattacharyya",
            "W. Yu",
            "S. Luccioni",
            "P. Villegas",
            "F. Zhdanov",
            "T. Lee",
            "N. Timor",
            "J. Ding",
            "C. S. Schlesinger",
            "H. Schoelkopf",
            "J. Ebert",
            "T. Dao",
            "M. Mishra",
            "A. Gu",
            "C. J. Anderson",
            "B. Dolan-Gavitt",
            "D. Contractor",
            "S. Reddy",
            "D. Fried",
            "D. Bahdanau",
            "Y. Jernite",
            "C. M. Ferrandis",
            "S. Hughes",
            "T. Wolf",
            "A. Guha",
            "L. V. Werra",
            "H. de Vries"
        ],
        "dcterms:description": "A dataset collected to train StarCoder, comprising approximately 250 billion tokens across 86 programming languages, including GitHub issues and text-code pairs.",
        "dcterms:title": "Starcoderdata",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Code Generation"
        ],
        "dcat:keyword": [
            "code dataset",
            "programming languages",
            "GitHub issues",
            "text-code pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of machines to complete sentences, featuring challenging examples.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "sentence completion",
            "commonsense reasoning",
            "NLP benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "A dataset for open book question answering, featuring questions that require reasoning and knowledge.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "open book",
            "question answering",
            "reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Sakaguchi",
            "R. L. Bras",
            "C. Bhagavatula",
            "Y. Choi"
        ],
        "dcterms:description": "An adversarial challenge dataset based on the Winograd schema, designed to test commonsense reasoning at scale.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "adversarial dataset",
            "commonsense reasoning",
            "Winograd schema"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "A dataset for the AI2 Reasoning Challenge, designed to evaluate question answering capabilities.",
        "dcterms:title": "ARC-Easy",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "question answering",
            "reasoning challenge",
            "AI2"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "A dataset for the AI2 Reasoning Challenge, designed to evaluate question answering capabilities with more challenging questions.",
        "dcterms:title": "ARC-Challenge",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "question answering",
            "reasoning challenge",
            "AI2"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "C. Clark",
            "K. Lee",
            "M.-W. Chang",
            "T. Kwiatkowski",
            "M. Collins",
            "K. Toutanova"
        ],
        "dcterms:description": "A dataset exploring the difficulty of natural yes/no questions, designed for binary question answering.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "yes/no questions",
            "binary classification",
            "NLP benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "R. Zellers",
            "J. Gao",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset for reasoning about physical commonsense in natural language, designed to evaluate physical reasoning capabilities.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "physical commonsense",
            "reasoning",
            "NLP benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring a model's world knowledge and problem-solving capabilities across various subjects.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "multitask learning",
            "world knowledge",
            "problem-solving"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Sch√§rli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. Le",
            "E. Chi",
            "D. Zhou",
            "J. Wei"
        ],
        "dcterms:description": "A subset of challenging tasks from the BIG-Bench benchmark designed to measure a language model's abilities in complex instruction following.",
        "dcterms:title": "BIG-Bench Hard (BBH)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "BIG-Bench",
            "complex tasks",
            "instruction following"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Dua",
            "Y. Wang",
            "P. Dasigi",
            "G. Stanovsky",
            "S. Singh",
            "M. Gardner"
        ],
        "dcterms:description": "A reading comprehension benchmark requiring discrete reasoning over paragraphs.",
        "dcterms:title": "DROP",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "reading comprehension",
            "discrete reasoning",
            "NLP benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Q. Zheng",
            "X. Xia",
            "X. Zou",
            "Y. Dong",
            "S. Wang",
            "Y. Xue",
            "L. Shen",
            "Z. Wang",
            "A. Wang",
            "Y. Li",
            "T. Su",
            "Z. Yang",
            "J. Tang"
        ],
        "dcterms:description": "A benchmark for measuring a model's programming capabilities, used to evaluate code generation.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Code Generation"
        ],
        "dcat:keyword": [
            "programming",
            "code generation",
            "NLP benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]