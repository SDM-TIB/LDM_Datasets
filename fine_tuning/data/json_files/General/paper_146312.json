[
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "GLUE is a multi-task benchmark for evaluating natural language understanding systems, consisting of various tasks including sentiment analysis, linguistic acceptability, and textual entailment.",
        "dcterms:title": "GLUE",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Natural Language Understanding",
            "Benchmark",
            "Multi-task"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Warstadt",
            "Amanpreet Singh",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "CoLA is a dataset for assessing the grammatical acceptability of English sentences, where the task is to predict whether a sentence is grammatically correct.",
        "dcterms:title": "CoLA (Corpus of Linguistic Acceptability)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Linguistics",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Grammatical Acceptability",
            "Linguistic Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Grammatical Acceptability Judgments"
        ]
    },
    {
        "dcterms:creator": [
            "Adina Williams",
            "Nikita Nangia",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "MNLI is a broad-coverage challenge corpus for sentence understanding through inference, designed to evaluate models on natural language inference tasks.",
        "dcterms:title": "MNLI (Multi-Genre Natural Language Inference)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Inference",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Inference",
            "Sentence Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "William B. Dolan",
            "Chris Brockett"
        ],
        "dcterms:description": "MRPC is a dataset for paraphrase identification, where the task is to determine if two sentences are paraphrases of each other.",
        "dcterms:title": "MRPC (Microsoft Research Paraphrase Corpus)",
        "dcterms:issued": "2005",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Paraphrase Identification",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Paraphrase",
            "Sentence Similarity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Paraphrase Identification"
        ]
    },
    {
        "dcterms:creator": [
            "Richard Socher",
            "Alex Perelygin",
            "Jean Wu",
            "Jason Chuang",
            "Christopher D. Manning",
            "Andrew Y. Ng",
            "Christopher Potts"
        ],
        "dcterms:description": "SST-2 is a binary sentiment classification dataset derived from movie reviews, where the task is to predict the sentiment of the reviews.",
        "dcterms:title": "SST-2 (Stanford Sentiment Treebank)",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sentiment Analysis",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Sentiment Analysis",
            "Movie Reviews"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sentiment Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Daniel M. Cer",
            "Mona T. Diab",
            "Eneko Agirre",
            "IÃ±igo Lopez-Gazpio",
            "Lucia Specia"
        ],
        "dcterms:description": "STS-B is a benchmark for evaluating semantic textual similarity, where the task is to assess how similar two pieces of text are on a scale from 1 to 5.",
        "dcterms:title": "STS-B (Semantic Textual Similarity Benchmark)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Similarity",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Text Similarity",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Semantic Textual Similarity"
        ]
    },
    {
        "dcterms:creator": [
            "Zihan Chen",
            "Hongbo Zhang",
            "Xiaoji Zhang",
            "Leqi Zhao"
        ],
        "dcterms:description": "QQP is a dataset containing question pairs from Quora, where the task is to determine whether two questions are semantically equivalent.",
        "dcterms:title": "QQP (Quora Question Pairs)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.kaggle.com/c/quora-question-pairs",
        "dcat:theme": [
            "Question Similarity",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question Pairs",
            "Semantic Equivalence"
        ],
        "dcat:landingPage": "https://www.kaggle.com/c/quora-question-pairs",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Similarity"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "QNLI is a dataset derived from SQuAD, converted into a binary classification task to determine if a context contains the answer to a question.",
        "dcterms:title": "QNLI (Question Natural Language Inference)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Inference",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Luisa Bentivogli",
            "Bernardo Magnini",
            "Ido Dagan",
            "Hoa Trang Dang",
            "Danilo Giampiccolo"
        ],
        "dcterms:description": "RTE is a binary entailment task that evaluates whether a hypothesis can be inferred from a premise.",
        "dcterms:title": "RTE (Recognizing Textual Entailment)",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Textual Entailment",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Entailment",
            "Inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Textual Entailment"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "SQuAD 1.1 is a reading comprehension dataset consisting of questions posed on Wikipedia articles, where the answer is a segment of text from the article.",
        "dcterms:title": "SQuAD 1.1 (Stanford Question Answering Dataset)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Robin Jia",
            "Percy Liang"
        ],
        "dcterms:description": "SQuAD 2.0 builds on SQuAD 1.1 by adding unanswerable questions, combining them with the original questions to create a more challenging dataset.",
        "dcterms:title": "SQuAD 2.0",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Unanswerable Questions",
            "Reading Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    }
]