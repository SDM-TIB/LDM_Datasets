[
    {
        "dcterms:creator": [
            "R. Hecht-Nielsen"
        ],
        "dcterms:description": "The Kolmogorov-Arnold representation theorem demonstrates that every continuous function can be represented by a specific neural network structure with two hidden layers.",
        "dcterms:title": "Kolmogorov-Arnold representation theorem",
        "dcterms:issued": "1987",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Function approximation",
            "Neural networks",
            "Representation theorem"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Bader"
        ],
        "dcterms:description": "Space-filling curves are continuous functions that map a one-dimensional interval onto a multi-dimensional space, effectively filling the space.",
        "dcterms:title": "Space-filling curves",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Computational Science"
        ],
        "dcat:keyword": [
            "Fractals",
            "Continuous functions",
            "Mapping"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Kupers"
        ],
        "dcterms:description": "The Cantor set is a classic example of a fractal, constructed by iteratively removing the middle third of a segment, resulting in a set that is uncountably infinite yet has measure zero.",
        "dcterms:title": "Cantor set",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Fractals"
        ],
        "dcat:keyword": [
            "Fractal geometry",
            "Set theory",
            "Measure theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Z. Shen"
        ],
        "dcterms:description": "Deep ReLU networks are neural networks that utilize the ReLU activation function across multiple layers, allowing for efficient approximation of complex functions.",
        "dcterms:title": "Deep ReLU networks",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Neural Networks",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Deep learning",
            "ReLU activation",
            "Function approximation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Schmidt-Hieber"
        ],
        "dcterms:description": "Neural networks with ReLU activation function are used for nonparametric regression, providing a flexible framework for modeling complex relationships in data.",
        "dcterms:title": "Neural networks with ReLU activation function",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistics",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Nonparametric regression",
            "Deep learning",
            "ReLU networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Pinkus"
        ],
        "dcterms:description": "The approximation theory of the MLP model in neural networks explores the capabilities and limitations of multilayer perceptrons in approximating functions.",
        "dcterms:title": "Approximation theory of the MLP model in neural networks",
        "dcterms:issued": "1999",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Function approximation",
            "Neural networks",
            "MLP model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]