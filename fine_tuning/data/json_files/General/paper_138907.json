[
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "An instruction-following llama model designed to enhance the ability of language models to follow human instructions.",
        "dcterms:title": "Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction-following",
            "Llama model",
            "Language model training"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "dcterms:description": "A dataset for instruction tuning generated using GPT-4, aimed at enhancing the instruction-following capabilities of language models.",
        "dcterms:title": "Alpaca-GPT4",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction tuning",
            "GPT-4",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Boyu Zhang",
            "Hongyang Yang",
            "Xiao-Yang Liu"
        ],
        "dcterms:description": "A dataset for financial sentiment analysis, focusing on instruction tuning of general-purpose large language models.",
        "dcterms:title": "FinGPT",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Finance",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Financial sentiment analysis",
            "Instruction tuning",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Tianyu Han",
            "Lisa C Adams",
            "Jens-Michalis Papaioannou",
            "Paul Grundmann",
            "Tom Oberhauser",
            "Alexander LÃ¶ser",
            "Daniel Truhn",
            "Keno K Bressem"
        ],
        "dcterms:description": "An open-source collection of medical conversational AI models and training data.",
        "dcterms:title": "MedAlpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards",
        "dcat:theme": [
            "Medical",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Medical AI",
            "Conversational AI",
            "Language model training"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Medical Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Sahil Chaudhary"
        ],
        "dcterms:description": "An instruction-following llama model specifically designed for code generation tasks.",
        "dcterms:title": "Code-Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/sahil280114/codealpaca",
        "dcat:theme": [
            "Programming",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Code generation",
            "Instruction-following",
            "Language model training"
        ],
        "dcat:landingPage": "https://github.com/sahil280114/codealpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Xiang Yue",
            "Xingwei Qu",
            "Ge Zhang",
            "Yao Fu",
            "Wenhao Huang",
            "Huan Sun",
            "Yu Su",
            "Wenhu Chen"
        ],
        "dcterms:description": "A dataset aimed at building math generalist models through hybrid instruction tuning.",
        "dcterms:title": "MathInstruct",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Math instruction",
            "Instruction tuning",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Guanming Yao",
            "Wei Zhu",
            "Yuan Ni",
            "Guotong Xie",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "dcterms:description": "A dataset designed to enhance language models with high-quality feedback.",
        "dcterms:title": "UltraFeedback",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Feedback Learning"
        ],
        "dcat:keyword": [
            "Feedback",
            "Language model training",
            "Instruction tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Feedback Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan"
        ],
        "dcterms:description": "A dataset for training language models with reinforcement learning from human feedback, focusing on helpfulness and harmlessness.",
        "dcterms:title": "HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/datasets/Anthropic/hh-rlhf",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reinforcement learning",
            "Human feedback",
            "Language model training"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/Anthropic/hh-rlhf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Value Alignment"
        ]
    }
]