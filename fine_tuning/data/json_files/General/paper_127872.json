[
    {
        "dcterms:creator": [
            "J. Deng",
            "W. Dong",
            "R. Socher",
            "L.-J. Li",
            "K. Li",
            "L. Fei-Fei"
        ],
        "dcterms:description": "A large-scale hierarchical image database used for training deep learning models in computer vision tasks.",
        "dcterms:title": "ImageNet",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Large-scale",
            "Hierarchical",
            "Deep learning"
        ],
        "dcat:landingPage": "https://image-net.org/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset used for pre-training the BERT-Large model, consisting of English Wikipedia and BooksCorpus.",
        "dcterms:title": "English Wikipedia and BooksCorpus",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/NVIDIA/DeepLearningExamples/",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Pre-training",
            "BERT"
        ],
        "dcat:landingPage": "https://github.com/NVIDIA/DeepLearningExamples/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A benchmark for evaluating natural language understanding tasks, consisting of multiple tasks and datasets.",
        "dcterms:title": "GLUE benchmark",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "https://gluebenchmark.com/",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Natural Language Understanding",
            "Multi-task"
        ],
        "dcat:landingPage": "https://gluebenchmark.com/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset used for training deep learning models, consisting of 100 classes of images.",
        "dcterms:title": "CIFAR-100",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Classification",
            "Small images"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A large pre-trained transformer model used for various natural language processing tasks.",
        "dcterms:title": "BERT-Large",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Transformer",
            "Pre-trained model",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "Large",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A larger version of the BERT model with 4 billion parameters.",
        "dcterms:title": "BERT-4B",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Transformer",
            "Pre-trained model",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "4B",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding"
        ]
    }
]