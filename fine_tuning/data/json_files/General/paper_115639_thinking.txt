To extract datasets from the research paper titled "Frame-Subtitle Self-Supervision for Multi-Modal Video Question Answering" by Jiong Wang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors conducted extensive experiments on **TVQA** and **TVQA+** datasets, which suggests these are the primary datasets used in their research.

Next, I will look for specific details about these datasets in the **experiments section**. Here, the authors provide descriptions of both datasets:

1. **TVQA**: This dataset is described as a large-scale multi-modal video QA dataset based on 6 popular TV shows, consisting of 152.5K QA pairs from 21.8K video clips, with various splits for training, validation, and testing.

2. **TVQA+**: This is a subset of the TVQA dataset that includes frame-level bounding box annotations, linking video objects to visual concepts in questions and answers. It also refines the temporal boundaries for more precise annotations.

To ensure I have the correct citations, I will check the **References section** of the paper. The citations for the datasets are as follows:

- For **TVQA**, the citation is:
  > Lei, J., Yu, L., Bansal, M., & Berg, T. L. (2018). TVQA: Localized, compositional video question answering. In EMNLP.

- For **TVQA+**, the citation is:
  > Lei, J., Yu, L., Berg, T. L., & Bansal, M. (2020). TVQA+: Spatio-temporal grounding for video question answering. In ACL.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing. This will include the dataset names, descriptions, and full citations to ensure clarity and completeness in the extraction process.