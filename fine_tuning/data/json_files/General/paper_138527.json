[
    {
        "dcterms:creator": [
            "B. Deng",
            "W. Wang",
            "F. Feng",
            "Y. Deng",
            "Q. Wang",
            "X. He"
        ],
        "dcterms:description": "The Semi-Automatic Attack Prompt (SAP) dataset series comprises various sets of attack prompts designed for research, including collections like SAP5, SAP10, SAP20, SAP30, and SAP200, with a total of 2,120 task prompts.",
        "dcterms:title": "Semi-Automatic Attack Prompt (SAP)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "doi:10.18653/v1/2023.findings-emnlp.143",
        "dcat:theme": [
            "Natural Language Processing",
            "Security in AI"
        ],
        "dcat:keyword": [
            "Attack prompts",
            "Red teaming",
            "Large language models"
        ],
        "dcat:landingPage": "https://aclanthology.org/2023.findings-emnlp.143",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Attack prompt generation",
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "MMLU is a comprehensive multitask accuracy test for text-based models, encompassing 57 diverse tasks that span elementary mathematics, US history, computer science, law, and more.",
        "dcterms:title": "MMLU (Massive Multitask Language Understanding)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multitask evaluation",
            "Model performance",
            "Text-based tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Multitask learning"
        ]
    },
    {
        "dcterms:creator": [
            "D.A. Hudson",
            "C.D. Manning"
        ],
        "dcterms:description": "GQA is a novel dataset designed to overcome limitations in model evaluation by presenting compositional questions tied to real-world images, comprising 22 million questions related to everyday imagery.",
        "dcterms:title": "GQA (Generalized Question Answering)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Compositional questions",
            "Visual reasoning",
            "Image-based questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering",
            "Visual reasoning"
        ]
    }
]