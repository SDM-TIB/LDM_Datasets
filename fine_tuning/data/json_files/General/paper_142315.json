[
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Saurav Kadavath",
            "Akul Arora",
            "Steven Basart",
            "Eric Tang",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring mathematical problem solving capabilities.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Mathematical problem solving",
            "Benchmark",
            "Assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of models to complete sentences.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Sentence Completion"
        ],
        "dcat:keyword": [
            "Sentence completion",
            "Natural language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence completion"
        ]
    },
    {
        "dcterms:creator": [
            "Yixin Nie",
            "Adina Williams",
            "Emily Dinan",
            "Mohit Bansal",
            "Jason Weston",
            "Douwe Kiela"
        ],
        "dcterms:description": "A benchmark for adversarial natural language inference.",
        "dcterms:title": "ANLI",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Inference"
        ],
        "dcat:keyword": [
            "Natural language inference",
            "Adversarial examples",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural language inference"
        ]
    },
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Vineet Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano",
            "Christopher Hesse",
            "John Schulman"
        ],
        "dcterms:description": "A dataset for training models to solve math word problems.",
        "dcterms:title": "GSM-8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Word Problems"
        ],
        "dcat:keyword": [
            "Math word problems",
            "Benchmark",
            "Training dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Di Jin",
            "Eileen Pan",
            "Nassim Oufattole",
            "Wei-Hung Weng",
            "Hanyi Fang",
            "Peter Szolovits"
        ],
        "dcterms:description": "A large-scale open domain question answering dataset derived from medical exams.",
        "dcterms:title": "MedQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Medical",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Medical question answering",
            "Open domain",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "Amin Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "dcterms:description": "A human-centric benchmark for evaluating foundation models.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Foundation Models"
        ],
        "dcat:keyword": [
            "Model evaluation",
            "Benchmark",
            "Human-centric"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S. Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large scale distantly supervised challenge dataset for reading comprehension.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Challenge dataset",
            "Distant supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "A dataset for the AI2 Reasoning Challenge, focusing on question answering.",
        "dcterms:title": "Arc-C",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reasoning"
        ],
        "dcat:keyword": [
            "Question answering",
            "Reasoning challenge",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset for reasoning about physical commonsense in natural language.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Physical commonsense",
            "Reasoning",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan",
            "Nicholas Joseph",
            "Saurav Kadavath",
            "Jackson Kernion",
            "Tom Conerly",
            "Sheer El-Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Tristan Hume",
            "Scott Johnston",
            "Shauna Kravec",
            "Liane Lovitt",
            "Neel Nanda",
            "Catherine Olsson",
            "Dario Amodei",
            "Tom Brown",
            "Jack Clark",
            "Sam McCandlish",
            "Chris Olah",
            "Ben Mann",
            "Jared Kaplan"
        ],
        "dcterms:description": "Datasets for training a helpful and harmless assistant with reinforcement learning from human feedback.",
        "dcterms:title": "Helpfulness and Harmlessness Preference Datasets",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "AI Safety"
        ],
        "dcat:keyword": [
            "Reinforcement learning",
            "AI safety",
            "Preference datasets"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "AI safety"
        ]
    }
]