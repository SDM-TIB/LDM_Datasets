[
    {
        "dcterms:creator": [
            "S. Baltes",
            "L. Dumani",
            "C. Treude",
            "S. Diehl"
        ],
        "dcterms:description": "SOTorrent is used for reconstructing and analyzing the evolution of Stack Overflow posts, containing all posts from July 2008 to December 2020, with extracted code snippets and preserved formatting.",
        "dcterms:title": "SOTorrent",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Software Engineering",
            "Data Analysis"
        ],
        "dcat:keyword": [
            "Stack Overflow",
            "Dataset",
            "Code Snippets",
            "Evolution Analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A large-scale dataset created by the authors containing 890,000 question posts covering eight programming languages, used to validate the effectiveness of their approach.",
        "dcterms:title": "Large-scale dataset with 890,000 question posts",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Software Engineering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Dataset",
            "Question Posts",
            "Programming Languages"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Wang",
            "W. Wang",
            "S. Joty",
            "S.C. Hoi"
        ],
        "dcterms:description": "CodeT5 is a pre-trained Transformer model designed for code understanding and generation, which serves as the backbone for the proposed approach.",
        "dcterms:title": "CodeT5",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2109.00859",
        "dcat:theme": [
            "Machine Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Code Understanding",
            "Code Generation",
            "Transformer Model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Robertson",
            "H. Zaragoza"
        ],
        "dcterms:description": "BM25 is a probabilistic relevance framework used for information retrieval, which the authors mention as a baseline method for comparison.",
        "dcterms:title": "BM25",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Relevance Ranking",
            "Information Retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "W. Ahmad",
            "S. Chakraborty",
            "B. Ray",
            "K.W. Chang"
        ],
        "dcterms:description": "PLBART is a unified pre-training model for program understanding and generation, which is used as a baseline in the experiments.",
        "dcterms:title": "PLBART",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Program Understanding",
            "Code Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Z. Gao",
            "X. Xia",
            "J. Grundy",
            "D. Lo",
            "Y.F. Li"
        ],
        "dcterms:description": "Code2Que is an end-to-end model proposed for generating Stack Overflow titles from code snippets, which serves as a baseline for comparison.",
        "dcterms:title": "Code2Que",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Software Engineering"
        ],
        "dcat:keyword": [
            "Title Generation",
            "Code Snippets"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "F. Zhang",
            "J. Keung",
            "X. Yu",
            "Z. Xie",
            "Z. Yang",
            "C. Ma",
            "Z. Zhang"
        ],
        "dcterms:description": "CCBERT is a model that improves Stack Overflow question title generation by incorporating a copying mechanism and bi-modal information.",
        "dcterms:title": "CCBERT",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "10.1016/j.infsof.2022.106922",
        "dcat:theme": [
            "Natural Language Processing",
            "Software Engineering"
        ],
        "dcat:keyword": [
            "Title Generation",
            "Copy Mechanism",
            "Bi-modal Information"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Liu",
            "G. Yang",
            "X. Chen",
            "C. Yu"
        ],
        "dcterms:description": "SOTitle is a transformer-based approach for generating post titles for Stack Overflow, which is also used as a baseline in the experiments.",
        "dcterms:title": "SOTitle",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2202.09789",
        "dcat:theme": [
            "Natural Language Processing",
            "Software Engineering"
        ],
        "dcat:keyword": [
            "Title Generation",
            "Transformer Model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]