To extract datasets from the research paper titled "Evaluation of Video-Based rPPG in Challenging Environments: Artifact Mitigation and Network Resilience" by Nhi Nguyen et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and methods sections** to identify any mentions of datasets. The abstract provides a brief overview of the study and may highlight the datasets used for evaluation. The introduction typically discusses the context and significance of the research, which may include references to specific datasets.

Next, I will focus on **section 3 (Evaluation Framework)**, where the authors explicitly mention the datasets used in their experiments. This section is likely to contain detailed descriptions of each dataset, including their characteristics and how they were utilized in the study.

In this paper, the authors mention **seven publicly available rPPG databases**. I will extract the following datasets along with their descriptions:

1. **COHFACE**: Contains video recordings of 40 subjects captured at 20 Hz with a resolution of 640x480 pixels. The database comprises 160 videos, each lasting approximately 1 minute, with reference physiological data recorded using medical-grade equipment.

2. **LGI-PPGI**: Features 24 videos of 6 users engaged in various scenarios, recorded at 640x480 pixels and 25 fps. Reference ground-truth measurements were obtained using a CMS50E pulse oximeter device synchronized at 60 Hz.

3. **MAHNOB**: A multimodal database designed for emotion recognition, featuring 27 participants recorded with various cameras. It includes 527 facial videos with corresponding reference physiological signals recorded at 60 frames per second using an ECG sensor.

4. **PURE**: Consists of videos of 10 subjects performing controlled head motions captured at 30 Hz and a resolution of 640x480 pixels. Reference pulse data was collected using a pulox CMS50E fingertip pulse oximeter.

5. **UBFC-rPPG 1 & 2**: Contains 50 videos synchronized with a pulse oximeter finger-clip sensor, recorded at a frame rate of 28-30Hz and a resolution of 640x480 pixels in uncompressed 8-bit RGB format.

6. **UCLA-rPPG**: Includes 98 subjects with 4-5 videos per subject, recorded at 30 fps and lasting about 1 minute each. A total of 488 videos were included, synchronized with ground truth heart rate data.

7. **UBFC-Phys**: Comprises videos of 56 subjects recorded at 35 fps and a resolution of 1024x1024 pixels. Reference ground truth data were collected using an Empatica E4 wristband.

After identifying the datasets, I will refer to the **References section** to find the full citations for each dataset. This is crucial for proper attribution and to allow others to access the datasets.

The citations I will extract include:

- **COHFACE**: 
  > Heusch, G., Anjos, A., & Marcel, S. (2017). A reproducible study on remote heart rate measurement. 

- **LGI-PPGI**: 
  > Pilz, C. S., Zaunseder, S., Krajewski, J., & Blazek, V. (2018). Local group invariance for heart rate estimation from face videos in the wild. 

- **MAHNOB**: 
  > Soleymani, M., Lichtenauer, J., Pun, T., & Pantic, M. (2012). A multimodal database for affect recognition and implicit tagging. 

- **PURE**: 
  > Stricker, R., MÃ¼ller, S., & Gross, H.-M. (2014). Non-contact video-based pulse rate measurement on a mobile service robot. 

- **UBFC-rPPG 1 & 2**: 
  > Bobbia, S., Macwan, R., Benezeth, Y., & Mansouri, A. (2019). Unsupervised skin tissue segmentation for remote photoplethysmography. 

- **UCLA-rPPG**: 
  > Wang, Z., Ba, Y., Chari, P., & Bozkurt, O. D. (2022). Synthetic generation of face videos with plethysmograph physiology. 

- **UBFC-Phys**: 
  > Meziati Sabour, R., Benezeth, Y., De Oliveira, P., & Chappe, J. (2021). UBFC-Phys: A multimodal database for psychophysiological studies of social stress. 

Finally, I will compile this information into a structured format that clearly presents each dataset along with its citation, ensuring that all relevant details are included for future reference.