To extract datasets from the research paper titled "Improving Dual-Encoder Training through Dynamic Indexes for Negative Mining" by Nicholas Monath et al., I will follow a systematic approach to identify and document the datasets mentioned in the text.

First, I will start by reading through the **abstract, introduction, and experiments sections** of the paper. The abstract provides a brief overview of the research and may mention datasets used for evaluation. The introduction often elaborates on the context and significance of the datasets in relation to the research objectives.

In the **introduction**, I will look for any explicit mentions of datasets. The authors may refer to specific datasets that are central to their experiments or methodology. 

Next, I will focus on the **experiments section**, which typically contains detailed information about the datasets used for testing the proposed methods. Here, I will look for sections that describe the datasets, including their names, sizes, and any relevant characteristics.

From my reading, I note that the authors mention two datasets in the experiments section: **Natural Questions (NQ)** and **MSMARCO**. I will extract the relevant details for each dataset:

1. **Natural Questions (NQ)**: This dataset is described as a passage retrieval dataset with over 21 million targets and approximately 60,000 training examples. It is used for evaluating the performance of the proposed method.

2. **MSMARCO**: This dataset contains 8.8 million targets and 500,000 training examples. It is also used for passage retrieval tasks.

Now, I will check the **References section** of the paper to find the full citations for these datasets. This is crucial for proper attribution and to provide readers with the necessary information to access the datasets.

- For **Natural Questions (NQ)**, the citation is:
  > Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M. W. (2019). Natural Questions: A Benchmark for Question Answering Research. *Transactions of the Association for Computational Linguistics (TACL)*.

- For **MSMARCO**, the citation is:
  > Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et al. (2016). MS MARCO: A Human Generated Machine Reading Comprehension Dataset. *arXiv preprint arXiv:1611.09268*.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is clearly described and properly cited. This will provide a comprehensive overview of the datasets used in the research paper, facilitating further exploration and understanding of the work.