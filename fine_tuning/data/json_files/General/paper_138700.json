[
    {
        "dcterms:creator": [
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "dcterms:description": "A dataset containing 160 forbidden questions categorized into 16 violation categories, designed to evaluate the effectiveness of various jailbreak methods on large language models.",
        "dcterms:title": "Forbidden Question Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning Security"
        ],
        "dcat:keyword": [
            "Jailbreak prompts",
            "Violation categories",
            "Large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluating jailbreak attacks"
        ]
    },
    {
        "dcterms:creator": [
            "Andy Zou",
            "Zifan Wang",
            "J. Zico Kolter",
            "Matt Fredrikson"
        ],
        "dcterms:description": "A benchmark dataset consisting of harmful strings and behaviors, used to evaluate the effectiveness of adversarial attacks on aligned language models.",
        "dcterms:title": "AdvBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Adversarial Machine Learning"
        ],
        "dcat:keyword": [
            "Adversarial attacks",
            "Language models",
            "Harmful content"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluating adversarial attacks"
        ]
    },
    {
        "dcterms:creator": [
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "dcterms:description": "A dataset built based on the usage policy from OpenAI, comprising hundreds of samples across more than ten violation scenarios to evaluate human-based jailbreak prompts.",
        "dcterms:title": "Forbidden_Question_Set",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning Security"
        ],
        "dcat:keyword": [
            "Jailbreak prompts",
            "Violation categories",
            "Large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluating jailbreak attacks"
        ]
    }
]