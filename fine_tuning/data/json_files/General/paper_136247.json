[
    {
        "dcterms:creator": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "Amin Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "dcterms:description": "AGIEval is a human-centric benchmark designed for evaluating foundation models, focusing on their performance in various tasks.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2304.06364",
        "dcat:theme": [
            "Benchmarking",
            "Foundation Models"
        ],
        "dcat:keyword": [
            "Evaluation",
            "Foundation Models",
            "Human-Centric"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2304.06364",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Taicheng Guo",
            "Kehan Guo",
            "Bozhao Nan",
            "Zhenwen Liang",
            "Zhichun Guo",
            "Nitesh V. Chawla",
            "Olaf Wiest",
            "Xiangliang Zhang"
        ],
        "dcterms:description": "ChemLLMBench is a comprehensive benchmark assessing the capabilities of large language models in chemistry across eight tasks.",
        "dcterms:title": "ChemLLMBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2305.18365",
        "dcat:theme": [
            "Chemistry",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Chemistry",
            "Large Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2305.18365",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Liangtai Sun",
            "Yang Han",
            "Zihan Zhao",
            "Da Ma",
            "Zhennan Shen",
            "Baocai Chen",
            "Lu Chen",
            "Kai Yu"
        ],
        "dcterms:description": "SciEval is a multi-level evaluation benchmark for large language models, specifically designed for scientific research.",
        "dcterms:title": "SCIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2308.13149",
        "dcat:theme": [
            "Scientific Research",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Scientific Evaluation",
            "Large Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2308.13149",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "dcterms:description": "HaluEval is a large-scale benchmark for evaluating hallucinations in large language models.",
        "dcterms:title": "HaluEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2305.11747",
        "dcat:theme": [
            "Evaluation",
            "Hallucination Detection"
        ],
        "dcat:keyword": [
            "Hallucination",
            "Large Language Models",
            "Evaluation"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2305.11747",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Shima Imani",
            "Liang Du",
            "Harsh Shrivastava"
        ],
        "dcterms:description": "MultiArith is a dataset designed for evaluating mathematical reasoning capabilities of large language models.",
        "dcterms:title": "MultiArith",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/2023.acl-industry.4",
        "dcat:theme": [
            "Mathematics",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Mathematical Reasoning",
            "Large Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/2023.acl-industry.4",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tianyi Zhang",
            "Varsha Kishore",
            "Felix Wu",
            "Kilian Q. Weinberger",
            "Yoav Artzi"
        ],
        "dcterms:description": "BERTScore is a method for evaluating text generation using BERT embeddings.",
        "dcterms:title": "BERTScore",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://openreview.net/forum?id=SkeHuCVFDr",
        "dcat:theme": [
            "Text Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Text Generation",
            "Evaluation",
            "BERT"
        ],
        "dcat:landingPage": "https://openreview.net/forum?id=SkeHuCVFDr",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Kishore Papineni",
            "Salim Roukos",
            "Todd Ward",
            "Wei-Jing Zhu"
        ],
        "dcterms:description": "BLEU is a method for automatic evaluation of machine translation.",
        "dcterms:title": "BLEU",
        "dcterms:issued": "2001",
        "dcterms:language": "",
        "dcterms:identifier": "http://dx.doi.org/10.3115/1073083.1073135",
        "dcat:theme": [
            "Machine Translation",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Machine Translation",
            "Evaluation",
            "BLEU"
        ],
        "dcat:landingPage": "http://dx.doi.org/10.3115/1073083.1073135",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Yen-Ting Lin",
            "Yun-Nung Chen"
        ],
        "dcterms:description": "LLM-Eval is a unified multi-dimensional automatic evaluation framework for open-domain conversations with large language models.",
        "dcterms:title": "LLM-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2305.13711",
        "dcat:theme": [
            "Evaluation",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Open-Domain Conversations",
            "Evaluation",
            "Large Language Models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2305.13711",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]