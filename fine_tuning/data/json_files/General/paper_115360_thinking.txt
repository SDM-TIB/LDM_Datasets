To extract datasets from the research paper titled "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers" by Zengyuan Guo et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on several popular benchmarks, which suggests that datasets are involved.

Next, I will focus on the **experiments section** where the authors explicitly mention the datasets used for evaluation. In this section, they list two datasets:

1. **PubTabNet**: This dataset is described as a large-scale collection containing 500,777 training images, 9,115 validating images, and 9,138 testing images. It consists of complex tables extracted from scientific documents.

2. **SynthTable**: This dataset is noted for covering unconstrained tables in natural scenes, containing 1,000 training images and 1,000 testing images. The tables in this dataset have various orientations, sizes, and types of separators.

To ensure I have the correct citations for these datasets, I will check the **References section** of the paper. The citations for the datasets are as follows:

- For **PubTabNet**, the citation is:
  > Zhong, X., ShafieiBavani, E., & Jimeno Yepes, A. (2020). *Image-based table recognition: data, model, and evaluation*. In European Conference on Computer Vision. Springer, pp. 564–580.

- For **SynthTable**, the citation is:
  > Qasim, S. R., Mahmood, H., & Shafait, F. (2019). *Rethinking table recognition using graph neural networks*. In 2019 International Conference on Document Analysis and Recognition (ICDAR). IEEE, pp. 142–147.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review.