[
    {
        "dcterms:creator": [
            "Alexey Dosovitskiy",
            "Lucas Beyer",
            "Alexander Kolesnikov",
            "Dirk Weissenborn",
            "Xiaohua Zhai",
            "Thomas Unterthiner",
            "Mostafa Dehghani",
            "Matthias Minderer",
            "Georg Heigold",
            "Sylvain Gelly"
        ],
        "dcterms:description": "A large-scale dataset used for training image recognition models, consisting of millions of labeled images across thousands of categories.",
        "dcterms:title": "ImageNet-1k",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1145/3605573.3605613",
        "dcat:theme": [
            "Computer Vision",
            "Image Recognition"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Image classification",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Wikimedia Foundation"
        ],
        "dcterms:description": "A dataset containing a vast amount of textual data extracted from Wikipedia, useful for various natural language processing tasks.",
        "dcterms:title": "Wikipedia",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://dumps.wikimedia.org",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Data"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Language modeling",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling",
            "Text Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Denis Paperno",
            "Germán Kruszewski",
            "Angeliki Lazaridou",
            "Quan Pham",
            "Raffaella Bernardi",
            "Sandro Pezzelle",
            "Marco Baroni",
            "Gemma Boleda",
            "Raquel Fernández"
        ],
        "dcterms:description": "A dataset designed for word prediction tasks that require understanding of broader discourse context.",
        "dcterms:title": "LAMBADA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/P16-1144",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Word prediction",
            "Discourse context",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Prediction"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A pre-trained model for natural language understanding that utilizes deep bidirectional transformers.",
        "dcterms:title": "BERT-Base",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Transformer model",
            "Natural language understanding",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Text Classification",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom B Brown",
            "Benjamin Mann",
            "Nick Ryder",
            "Melanie Subbiah",
            "Jared Kaplan",
            "Prafulla Dhariwal",
            "Arvind Neelakantan",
            "Pranav Shyam",
            "Girish Sastry",
            "Amanda Askell"
        ],
        "dcterms:description": "A language model that is capable of few-shot learning, trained on a diverse range of internet text.",
        "dcterms:title": "GPT-2",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "https://doi.org/10.1145/3605573.3605613",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "dcat:keyword": [
            "Language model",
            "Few-shot learning",
            "Text generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Text Generation",
            "Language Modeling"
        ]
    }
]