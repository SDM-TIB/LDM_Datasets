[
    {
        "dcterms:creator": [
            "Pengcheng Yin",
            "Bowen Deng",
            "Edgar Chen",
            "Bogdan Vasilescu",
            "Graham Neubig"
        ],
        "dcterms:description": "A dataset containing aligned code and natural language pairs mined from Stack Overflow, used for evaluating code generation models.",
        "dcterms:title": "CoNaLa",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Code generation",
            "Natural language",
            "Stack Overflow"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Human Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Mark Chen",
            "Jerry Tworek",
            "Heewoo Jun",
            "Qiming Yuan",
            "Henrique Ponde de Oliveira Pinto",
            "Jared Kaplan",
            "Harri Edwards",
            "Yuri Burda",
            "Nicholas Joseph",
            "Greg Brockman"
        ],
        "dcterms:description": "A benchmark dataset for evaluating the functional correctness of code generation models, consisting of natural language goals, input-output test cases, and reference solutions.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2107.03374",
        "dcat:theme": [
            "Code Generation",
            "Functional Correctness"
        ],
        "dcat:keyword": [
            "Code evaluation",
            "Functional correctness",
            "Python"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Functional Correctness Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Qinkai Zheng",
            "Xiao Xia",
            "Xu Zou",
            "Yuxiao Dong",
            "Shan Wang",
            "Yufei Xue",
            "Zihan Wang",
            "Lei Shen",
            "Andi Wang",
            "Yang Li"
        ],
        "dcterms:description": "An extended version of the HumanEval dataset that includes generated code samples for evaluating code generation models across multiple programming languages.",
        "dcterms:title": "HumanEval-X",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2303.17568",
        "dcat:theme": [
            "Code Generation",
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "Code generation",
            "Multilingual",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Qinkai Zheng",
            "Xiao Xia",
            "Xu Zou",
            "Yuxiao Dong",
            "Shan Wang",
            "Yufei Xue",
            "Zihan Wang",
            "Lei Shen",
            "Andi Wang",
            "Yang Li"
        ],
        "dcterms:description": "An extended version of the HumanEval dataset that includes generated code samples for evaluating code generation models across multiple programming languages.",
        "dcterms:title": "HumanEval-X (with generated code)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2303.17568",
        "dcat:theme": [
            "Code Generation",
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "Code generation",
            "Multilingual",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation Evaluation"
        ]
    }
]