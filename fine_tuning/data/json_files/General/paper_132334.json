[
    {
        "dcterms:creator": [
            "Richard Sproat",
            "Navdeep Jaitly"
        ],
        "dcterms:description": "The Google TN dataset is comprised of around 1.1 billion words from Wikipedia, providing sizeable coverage of common text normalization domains. It includes individual tokens paired with domain tags and a target normalization generated with Google's Kestrel TN system.",
        "dcterms:title": "Google TN dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Normalization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Text normalization",
            "Wikipedia",
            "Natural language processing",
            "Large datasets"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text normalization",
            "Natural language processing tasks"
        ]
    }
]