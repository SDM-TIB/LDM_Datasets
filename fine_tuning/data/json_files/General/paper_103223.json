[
    {
        "dcterms:creator": [
            "Alex Wang",
            "Yada Pruksachatkun",
            "Nikita Nangia",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel R Bowman"
        ],
        "dcterms:description": "A multi-task benchmark and analysis platform for natural language understanding.",
        "dcterms:title": "GLUE Benchmark",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1804.07461",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "NLP",
            "Benchmark",
            "Multi-task",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1606.05250",
        "dcat:theme": [
            "Machine Reading",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Machine Comprehension",
            "Question Answering",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Christopher Clark",
            "Kenton Lee",
            "Ming-Wei Chang",
            "Tom Kwiatkowski",
            "Michael Collins",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A dataset exploring the surprising difficulty of natural yes/no questions.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1905.10044",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Yes/No Questions",
            "Dataset",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Daniel Khashabi",
            "Snigdha Chaturvedi",
            "Michael Roth",
            "Shyam Upadhyay",
            "Dan Roth"
        ],
        "dcterms:description": "A challenge set for reading comprehension over multiple sentences.",
        "dcterms:title": "MultiRC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-sentence Comprehension",
            "Dataset",
            "Reading Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    }
]