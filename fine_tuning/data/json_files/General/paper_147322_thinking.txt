To extract datasets from the research paper titled "Attention & Frequency Learning for SISR" by Dinh Phu Tran, Dao Duy Hung, and Daeyoung Kim, I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, methodology, and experiments sections** of the paper. The abstract mentions the use of a new architecture for Single Image Super-Resolution (SISR) and hints at experimental findings, which may involve specific datasets.

In the **introduction**, the authors discuss the limitations of existing methods and the need for improved datasets for training and evaluation. This section may provide insights into the datasets they used or referenced.

Next, I will focus on the **experiments section** (section 4), where the authors typically describe the datasets used for training and evaluation. Here, they mention using the **DF2K dataset** for training, which is a combination of the DIV2K and Flicker2K datasets. This is a crucial dataset for their experiments.

The authors also specify five benchmark datasets used for evaluation: **Set5, Set14, BSD100, Urban100, and Manga109**. I will take note of these datasets as they are essential for understanding the context of the experiments.

Now, I will check the **References section** to find the full citations for each dataset mentioned:

1. **DF2K Dataset** (combination of DIV2K and Flicker2K):
   - DIV2K citation:
     > E. Agustsson and R. Timofte. *Ntire 2017 challenge on single image super-resolution: Dataset and study*. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1122–1131, 2017. doi: 10.1109/CVPRW.2017.150.
   - Flicker2K citation:
     > Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. *Image super-resolution using very deep residual channel attention networks*. In Proceedings of the European Conference on Computer Vision (ECCV), pages 286–301, 2018.

2. **Set5 Dataset**:
   - Citation:
     > D. Martin, C. Fowlkes, D. Tal, and J. Malik. *A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics*. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416–423, 2001. doi: 10.1109/ICCV.2001.937655.

3. **Set14 Dataset**:
   - Citation:
     > Z. Zeyde, M. Elad, and M. Protter. *On single image scale-up using sparse representations*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 711–730, 2010. doi: 10.1109/CVPR.2010.553993.

4. **BSD100 Dataset**:
   - Citation:
     > D. Martin, C. Fowlkes, D. Tal, and J. Malik. *A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics*. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416–423, 2001. doi: 10.1109/ICCV.2001.937655.

5. **Urban100 Dataset**:
   - Citation:
     > J. Huang, A. Singh, and N. Ahuja. *Single image super-resolution from transformed self-exemplars*. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5197–5206, 2015. doi: 10.1109/CVPR.2015.7299156.

6. **Manga109 Dataset**:
   - Citation:
     > Y. Matsui, K. Ito, Y. Aramaki, A. Fujimoto, T. Ogawa, T. Yamasaki, and K. Aizawa. *Sketch-based manga retrieval using manga109 dataset*. Multimedia Tools and Applications, 76(20):21811–21838, 2016. doi: 10.1007/s11042-016-4020-z.

After gathering all this information, I will compile the dataset entries into a structured format for further processing or review.