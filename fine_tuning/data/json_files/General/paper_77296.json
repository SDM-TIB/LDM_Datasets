[
    {
        "dcterms:creator": [
            "A. L. Maas",
            "A. Y. Hannun",
            "A. Y. Ng"
        ],
        "dcterms:description": "The LReLU is one of the most popular activation functions, which allows a small, non-zero, constant gradient when the unit is not active.",
        "dcterms:title": "LReLU",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Activation function",
            "Neural networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "G. Klambauer",
            "T. Unterthiner",
            "A. Mayr",
            "S. Hochreiter"
        ],
        "dcterms:description": "The SELU is a self-normalizing activation function that helps to maintain the mean and variance of the activations.",
        "dcterms:title": "SELU",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1706.02515",
        "dcat:theme": [],
        "dcat:keyword": [
            "Activation function",
            "Self-normalizing"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1706.02515",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]