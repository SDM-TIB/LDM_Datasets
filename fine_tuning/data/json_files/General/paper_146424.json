[
    {
        "dcterms:creator": [
            "Pengyu Cheng",
            "Jiawen Xie",
            "Ke Bai",
            "Yong Dai",
            "Nan Du"
        ],
        "dcterms:description": "A dataset designed for learning customized human preferences, focusing on domain-specific tasks.",
        "dcterms:title": "Domain-Specific Preference Dataset (DSP)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2309.03126",
        "dcat:theme": [
            "Preference Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Customized preferences",
            "Domain-specific tasks",
            "Human feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan",
            "Nicholas Joseph",
            "Saurav Kadavath",
            "Jackson Kernion",
            "Tom Conerly",
            "Sheer El Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Tristan Hume",
            "Scott Johnston",
            "Shauna Kravec",
            "Liane Lovitt",
            "Neel Nanda",
            "Catherine Olsson",
            "Dario Amodei",
            "Tom B. Brown",
            "Jack Clark",
            "Sam McCandlish",
            "Chris Olah",
            "Benjamin Mann",
            "Jared Kaplan"
        ],
        "dcterms:description": "A dataset for training language models to be helpful and harmless, utilizing reinforcement learning from human feedback.",
        "dcterms:title": "HH-RLHF Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Helpfulness",
            "Harmlessness",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Helpfulness Evaluation",
            "Harmlessness Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B Hashimoto"
        ],
        "dcterms:description": "A training set consisting of 52,000 instruction-following examples for fine-tuning language models.",
        "dcterms:title": "52K Alpaca Training Set",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Following",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "Instruction-following",
            "Training set",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    }
]