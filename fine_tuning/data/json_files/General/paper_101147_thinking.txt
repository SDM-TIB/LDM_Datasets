To extract datasets from the research paper titled "Space-time Mixing Attention for Video Transformer" by Adrian Bulat et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and results sections** to identify any datasets mentioned. The abstract provides a brief overview of the paper's contributions, and the introduction typically outlines the context and significance of the work, which may include references to datasets.

In the **results section**, the authors explicitly mention the datasets used for training and evaluation. Here, they list three datasets: **Kinetics-400**, **Something-Something-v2 (SSv2)**, and **Epic Kitchens-100 (Epic-100)**. I will take note of the details provided for each dataset, including their characteristics and the context in which they were used.

Next, I will summarize the datasets as follows:

1. **Kinetics-400**: This dataset consists of short video clips (approximately 10 seconds long) sourced from YouTube, labeled with 400 action classes. The paper mentions that the version used contains around 261,000 clips.

2. **Something-Something-v2 (SSv2)**: This dataset includes 220,487 short videos (2 to 6 seconds long) depicting humans performing predefined actions with everyday objects. The authors highlight that this dataset requires strong temporal modeling due to consistent objects and backgrounds across action classes.

3. **Epic Kitchens-100 (Epic-100)**: This dataset is an egocentric action recognition dataset with over 90,000 action segments spanning 100 hours of recordings in natural environments. It includes labels for 97 verb and 300 noun classes.

After identifying the datasets, I will refer to the **References section** to find the full citations for each dataset:

- For **Kinetics**, the citation is:
  > Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. *The Kinetics Human Action Video Dataset*. arXiv preprint arXiv:1705.06950, 2017.

- For **Something-Something-v2**, the citation is:
  > Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Freund, Peter Yianilos, Moritz Mueller-Freitag, et al. *The "Something Something" Video Database for Learning and Evaluating Visual Common Sense*. In Proceedings of the IEEE International Conference on Computer Vision, pages 5842â€“5850, 2017.

- For **Epic Kitchens-100**, the citation is:
  > Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. *Scaling Egocentric Vision: The Epic-Kitchens Dataset*. In ECCV, 2018.

Finally, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited, ready for structured output or further processing.