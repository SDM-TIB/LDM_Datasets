[
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT is a pre-trained model that uses deep bidirectional transformers for language understanding, powering various web services such as search and translation.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Transformer",
            "Language Understanding",
            "Pre-trained Model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Text Classification",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yinhan Liu",
            "Myle Ott",
            "Naman Goyal",
            "Jingfei Du",
            "Mandar Joshi",
            "Danqi Chen",
            "Omer Levy",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "dcterms:description": "RoBERTa is a robustly optimized version of BERT, designed to improve performance through better training techniques.",
        "dcterms:title": "RoBERTa",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Transformer",
            "Language Understanding",
            "Pre-trained Model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Text Classification",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Victor Sanh",
            "Lysandre Debut",
            "Julien Chaumond",
            "Thomas Wolf"
        ],
        "dcterms:description": "DistilBERT is a distilled version of BERT that is smaller, faster, cheaper, and lighter, while retaining much of BERT's performance.",
        "dcterms:title": "DistilBERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Transformer",
            "Model Distillation",
            "Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Text Classification",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Thomas Wolf",
            "Lysandre Debut",
            "Victor Sanh",
            "Julien Chaumond",
            "Clement Delangue",
            "Anthony Moi",
            "Pierric Cistac",
            "Tim Rault",
            "RÃ©mi Louf",
            "Morgan Funtowicz",
            "Joe Davison",
            "Sam Shleifer",
            "Patrick von Platen",
            "Clara Ma",
            "Yacine Jernite",
            "Julien Plu",
            "Canwen Xu",
            "Teven Le Scao",
            "Sylvain Gugger",
            "Mariama Drame",
            "Quentin Lhoest",
            "Alexander M. Rush"
        ],
        "dcterms:description": "The HuggingFace Inference Benchmark provides a state-of-the-art implementation of numerous Transformer-based NLP models and includes an easy-to-use inference benchmark.",
        "dcterms:title": "HuggingFace Inference Benchmark",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Inference",
            "Transformer Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation",
            "Inference Performance"
        ]
    }
]