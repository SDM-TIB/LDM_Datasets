[
    {
        "dcterms:creator": [
            "S. Lin",
            "J. Hilton",
            "O. Evans"
        ],
        "dcterms:description": "A multiple choice benchmark designed to check the truthfulness of large language models by testing them on questions across 38 different categories like health and politics where humans might provide incorrect responses due to implicit biases or incorrect beliefs.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2109.07958",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Bias",
            "Multiple Choice"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Talmor",
            "J. Herzig",
            "N. Lourie",
            "J. Berant"
        ],
        "dcterms:description": "A multiple choice benchmark testing commonsense reasoning, with challenging associations extracted from ConceptNet to find many target concepts for a single source concept.",
        "dcterms:title": "CommonsenseQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense",
            "Multiple Choice",
            "Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "A multiple choice dataset requiring multi-step reasoning over common and commonsense knowledge requiring deeper understanding of a diverse set of topics.",
        "dcterms:title": "OpenbookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Open Book Question Answering"
        ],
        "dcat:keyword": [
            "Open Book",
            "Multiple Choice",
            "Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Jin",
            "E. Pan",
            "N. Oufattole",
            "W.-H. Weng",
            "H. Fang",
            "P. Szolovits"
        ],
        "dcterms:description": "A challenging dataset testing medical knowledge with questions based on the United States Medical License Exams (USMLE) and other medical board exams.",
        "dcterms:title": "MedQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.13081",
        "dcat:theme": [
            "Medical",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Medical Knowledge",
            "USMLE",
            "Multiple Choice"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A massive benchmark covering 57 subjects from a diverse set of areas including STEM, humanities, and social sciences. This benchmark tests both more rudimentary and more advanced sets of knowledge for these topics covering great breadth and depth.",
        "dcterms:title": "MMLU (Massive Multitask Language Understanding)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multitask",
            "Benchmark",
            "Knowledge Assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]