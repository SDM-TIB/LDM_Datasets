[
    {
        "dcterms:creator": [
            "S. R. Bowman",
            "G. Angeli",
            "C. Potts",
            "C. D. Manning"
        ],
        "dcterms:description": "The SNLI (Stanford Natural Language Inference) dataset is considered the golden classic of NLI benchmarking, with 570k examples based on image captions from the Flickr30k corpus, where the captions serve as premises for natural language inference.",
        "dcterms:title": "Stanford NLI (SNLI)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1508.05326",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "NLI",
            "Inference",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "A. Wang",
            "A. Singh",
            "J. Michael",
            "F. Hill",
            "O. Levy",
            "S. R. Bowman"
        ],
        "dcterms:description": "The QNLI dataset is a natural language inference task derived from the Stanford Question Answering Dataset (SQuAD), where each question is combined with sentences from the context to determine entailment.",
        "dcterms:title": "Question-answering NLI (QNLI)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1804.07461",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "NLI",
            "Question Answering",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "A. Williams",
            "N. Nangia",
            "S. R. Bowman"
        ],
        "dcterms:description": "The MNLI (Multi-Genre NLI) dataset is modeled after the SNLI dataset and includes 433k examples from ten different sources or genres, providing a diverse range of data for evaluating NLI models.",
        "dcterms:title": "Multi-Genre NLI (MultiNLI)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1704.05426",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "NLI",
            "Multi-Genre",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Nie",
            "A. Williams",
            "E. Dinan",
            "M. Bansal",
            "J. Weston",
            "D. Kiela"
        ],
        "dcterms:description": "The ANLI (Adversarial NLI) dataset is a next-generation NLI benchmark that employs a unique data collection process called Human-And-Model-in-the Loop Entailment Training (HAMLET), focusing on challenging examples.",
        "dcterms:title": "Adversarial NLI (ANLI)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1910.14599",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "NLI",
            "Adversarial",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "S. Soni",
            "M. Gudala",
            "A. Pajouhi",
            "K. Roberts"
        ],
        "dcterms:description": "RadQA is a radiology question-answering dataset containing 6,148 question-answer evidence pairs derived from realistic radiology reports, aimed at improving comprehension of radiology reports.",
        "dcterms:title": "RadQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://aclanthology.org/2022.lrec-1.672",
        "dcat:theme": [
            "Medical Informatics",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Radiology",
            "Question Answering",
            "Dataset"
        ],
        "dcat:landingPage": "https://aclanthology.org/2022.lrec-1.672",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. E. Johnson",
            "T. J. Pollard",
            "L. Shen",
            "L. W. H. Lehman",
            "M. Feng",
            "M. Ghassemi"
        ],
        "dcterms:description": "MIMIC-III is a freely accessible critical care database that includes a wide range of clinical data, including radiology reports, aimed at supporting research in critical care.",
        "dcterms:title": "MIMIC-III",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Medical Informatics",
            "Clinical Data"
        ],
        "dcat:keyword": [
            "Critical Care",
            "Clinical Database",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Clinical Research"
        ]
    },
    {
        "dcterms:creator": [
            "C. Y. Lin"
        ],
        "dcterms:description": "ROUGE is a package for automatic evaluation of summaries, widely used for evaluating the quality of generated text against reference summaries.",
        "dcterms:title": "ROUGE",
        "dcterms:issued": "2004",
        "dcterms:language": "",
        "dcterms:identifier": "https://aclanthology.org/W04-1013",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Evaluation"
        ],
        "dcat:keyword": [
            "Evaluation",
            "Summarization",
            "Dataset"
        ],
        "dcat:landingPage": "https://aclanthology.org/W04-1013",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Evaluation"
        ]
    }
]