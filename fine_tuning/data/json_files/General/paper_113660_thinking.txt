To extract datasets from the research paper titled "Egocentric Scene Understanding via Multimodal Spatial Rectifier" by Tien Do et al., I will follow a systematic approach to identify and document each dataset mentioned in the text.

First, I will read through the **abstract, introduction, and methods sections** to identify any datasets that are explicitly mentioned. The abstract indicates that the authors evaluate their method on several datasets, including **EPIC-KITCHENS**, **FPHA**, and a new dataset called **EDINA**. This suggests that I should look for detailed descriptions of these datasets.

Next, I will focus on the **introduction** section, where the authors discuss the challenges of egocentric scene understanding and mention the datasets used for training and evaluation. They specifically highlight the limitations of existing datasets like **ScanNet** and **NYUv2**, which are also mentioned in the context of their work.

In the **method section**, particularly in **section 4 (EDINA Dataset)**, the authors provide a comprehensive description of the **EDINA** dataset. They mention that it consists of over 550,000 synchronized RGBD frames collected from various indoor activities, which is crucial for understanding the dataset's purpose and scope.

I will also check the **experiments section** where the authors discuss the datasets used for evaluation. They mention using **EDINA**, **FPHA**, and **EPIC-KITCHENS** for their experiments, confirming their relevance to the study.

Now, I will gather the full citations for each dataset mentioned in the paper from the **References section**:

1. **EDINA (Egocentric Depth on everyday INdoor Activities)**:
   - Tien Do, Khiem Vuong, Stergios I. Roumeliotis, and Hyun Soo Park. *Egocentric Depth on everyday INdoor Activities*. Available at: https://github.com/tien-d/EgoDepthNormal.

2. **EPIC-KITCHENS**:
   - Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. *Scaling egocentric vision: The epic-kitchens dataset*. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.

3. **FPHA (First Person Hand Action)**:
   - Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. *First-person hand action benchmark with RGB-D videos and 3D hand pose annotations*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

4. **ScanNet**:
   - Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie√üner. *ScanNet: Richly-annotated 3D reconstructions of indoor scenes*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

With these citations, I will now compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a clear and comprehensive overview of the datasets used in the research.