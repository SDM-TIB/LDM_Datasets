[
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "A dataset containing images of common objects in context, widely used for evaluating image captioning and object detection models.",
        "dcterms:title": "MS-COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object detection",
            "Image captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Object Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Jordi Pont-Tuset",
            "Jasper Uijlings",
            "Soravit Changpinyo",
            "Radu Soricut",
            "Vittorio Ferrari"
        ],
        "dcterms:description": "A dataset that connects vision and language through detailed localized narratives, providing longer descriptions than traditional datasets.",
        "dcterms:title": "Localized Narratives",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Text description",
            "Vision-language connection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning",
            "Vision-Language Tasks"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A new holistic benchmark introduced in the paper, consisting of over 1600 English prompts designed to evaluate model capabilities across various categories and difficulty aspects.",
        "dcterms:title": "PartiPrompts (P2)",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Image Generation",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Prompt dataset",
            "Evaluation benchmark",
            "Text-to-image synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text-to-Image Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Christoph Schuhmann",
            "Richard Vencu",
            "Romain Beaumont",
            "Robert Kaczmarczyk",
            "Clayton Mullis",
            "Aarush Katta",
            "Theo Coombes",
            "Jenia Jitsev",
            "Aran Komatsuzaki"
        ],
        "dcterms:description": "An open dataset containing 400 million image-text pairs filtered using CLIP, aimed at facilitating research in multimodal learning.",
        "dcterms:title": "LAION-400M",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2111.02114",
        "dcat:theme": [
            "Computer Vision",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Image-text pairs",
            "Open dataset",
            "CLIP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text Pair",
        "mls:task": [
            "Image-Text Matching",
            "Multimodal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Xiaohua Zhai",
            "Alexander Kolesnikov",
            "Neil Houlsby",
            "Lucas Beyer"
        ],
        "dcterms:description": "A large-scale dataset containing images with text annotation labels, used for training vision transformers.",
        "dcterms:title": "JFT-4B",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Text annotations",
            "Vision transformers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text Pair",
        "mls:task": [
            "Image Classification",
            "Object Recognition"
        ]
    }
]