[
    {
        "dcterms:creator": [
            "D. Damen",
            "H. Doughty",
            "G. M. Farinella",
            "S. Fidler",
            "A. Furnari",
            "E. Kazakos",
            "D. Moltisanti",
            "J. Munro",
            "T. Perrett",
            "W. Price",
            "M. Wray"
        ],
        "dcterms:description": "A dataset designed for egocentric vision, containing a variety of kitchen scenes to facilitate the study of interaction intention and behavior prediction.",
        "dcterms:title": "Epic-Kitchens-100",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Egocentric Video Analysis"
        ],
        "dcat:keyword": [
            "Egocentric vision",
            "Kitchen scenes",
            "Interaction intention",
            "Behavior prediction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Interaction intention prediction",
            "Behavior forecasting"
        ]
    },
    {
        "dcterms:creator": [
            "K. Grauman",
            "A. Westbury",
            "E. Byrne",
            "Z. Chavis",
            "A. Furnari",
            "R. Girdhar",
            "J. Hamburger",
            "H. Jiang",
            "M. Liu",
            "X. Liu"
        ],
        "dcterms:description": "A large-scale dataset comprising 3,000 hours of egocentric video footage, covering a wide range of activities to support research in action recognition and prediction.",
        "dcterms:title": "EGO4D",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Egocentric video",
            "Activity recognition",
            "Action prediction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action recognition",
            "Interaction prediction"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Li",
            "M. Liu",
            "J. M. Rehg"
        ],
        "dcterms:description": "A dataset that focuses on the joint learning of gaze and actions in first-person videos, providing insights into how gaze influences interaction.",
        "dcterms:title": "EGTEA Gaze+",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Gaze Estimation"
        ],
        "dcat:keyword": [
            "First-person video",
            "Gaze prediction",
            "Action recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Gaze estimation",
            "Action prediction"
        ]
    }
]