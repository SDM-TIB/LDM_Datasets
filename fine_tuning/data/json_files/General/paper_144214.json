[
    {
        "dcterms:creator": [
            "Stephanie Lin",
            "Jacob Hilton",
            "Owain Evans"
        ],
        "dcterms:description": "A benchmark assessing the modelâ€™s reliability by evaluating its ability to accurately respond to questions based on factual information.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reliability",
            "Question Answering",
            "Factual Information"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Thomas Hartvigsen",
            "Saadia Gabriel",
            "Hamid Palangi",
            "Maarten Sap",
            "Dipankar Ray",
            "Ece Kamar"
        ],
        "dcterms:description": "A large-scale machine-generated dataset for adversarial and implicit hate speech detection.",
        "dcterms:title": "ToxiGen",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Hate Speech Detection",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Hate Speech",
            "Adversarial Detection",
            "Implicit Bias"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Hate Speech Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Jwala Dhamala",
            "Tony Sun",
            "Varun Kumar",
            "Satyapriya Krishna",
            "Yada Pruksachatkun",
            "Kai-Wei Chang",
            "Rahul Gupta"
        ],
        "dcterms:description": "Dataset and metrics for measuring biases in open-ended language generation.",
        "dcterms:title": "BOLD",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Bias",
            "Language Generation",
            "Open-ended Responses"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Measurement"
        ]
    },
    {
        "dcterms:creator": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su",
            "Junteng Liu",
            "Chuancheng Lv",
            "Yikai Zhang",
            "Jiayi Lei"
        ],
        "dcterms:description": "A multi-level multi-discipline Chinese evaluation suite for foundation models.",
        "dcterms:title": "C-eval",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "arXiv:2305.08322",
        "dcat:theme": [
            "Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Chinese Language",
            "Evaluation Suite",
            "Foundation Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Zhi-Rui Tam",
            "Ya-Ting Pai",
            "Yen-Wei Lee",
            "Sega Cheng",
            "Hong-Han Shuai"
        ],
        "dcterms:description": "An improved traditional Chinese evaluation suite for foundation models.",
        "dcterms:title": "TMMLU Plus",
        "dcterms:issued": "2024",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Chinese Language",
            "Evaluation Suite",
            "Foundation Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "A benchmark for evaluating question answering capabilities.",
        "dcterms:title": "ARC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1803.05457",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "A benchmark for evaluating commonsense reasoning capabilities.",
        "dcterms:title": "Hellaswag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Commonsense Reasoning",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multi-task language understanding.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-task Learning",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-task Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P. Xing",
            "Hao Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "dcterms:description": "A benchmark for evaluating language models in multi-conversation scenarios.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-conversation",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-conversation Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P. Xing",
            "Hao Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "dcterms:description": "A benchmark for evaluating language models in multi-conversation scenarios, specifically for Japanese.",
        "dcterms:title": "MT-Bench-JP",
        "dcterms:issued": "2023",
        "dcterms:language": "Japanese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Japanese Language",
            "Multi-conversation",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-conversation Evaluation"
        ]
    }
]