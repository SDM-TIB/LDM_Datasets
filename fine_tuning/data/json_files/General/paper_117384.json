[
    {
        "dcterms:creator": [],
        "dcterms:description": "A diverse and explainable textual evidence-based complex question answering dataset with explicit reasoning chains. It consists of two subtasks: answer generation and evidence chains extraction, and contains higher diversity for multi-hop questions with varying depths, 12 reasoning types, and 78 relations.",
        "dcterms:title": "ReasonChainQA",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Complex Question Answering",
            "Evidence Chains",
            "Multi-hop Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Answer Generation",
            "Evidence Extraction"
        ]
    },
    {
        "dcterms:creator": [
            "J. Thorne",
            "M. Yazdani",
            "M. Saeidi",
            "F. Silvestri",
            "S. Riedel",
            "A. Halevy"
        ],
        "dcterms:description": "A dataset that constructs knowledge triples to sentence level evidence and introduces database queries into complex text QA tasks.",
        "dcterms:title": "WIKINLDB",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Knowledge Triples",
            "Database Queries",
            "Textual Evidence"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Complex Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Yang",
            "P. Qi",
            "S. Zhang",
            "Y. Bengio",
            "W. Cohen",
            "R. Salakhutdinov",
            "C. D. Manning"
        ],
        "dcterms:description": "A dataset for diverse, explainable multi-hop question answering.",
        "dcterms:title": "HotpotQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-hop Reasoning",
            "Explainable QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-hop Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Dua",
            "Y. Wang",
            "P. Dasigi",
            "G. Stanovsky",
            "S. Singh",
            "M. Gardner"
        ],
        "dcterms:description": "A reading comprehension benchmark requiring discrete reasoning over paragraphs.",
        "dcterms:title": "DROP",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Discrete Reasoning",
            "Reading Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "J. Weston",
            "A. Bordes",
            "S. Chopra",
            "A. M. Rush",
            "B. van MerriÃ«nboer",
            "A. Joulin",
            "T. Mikolov"
        ],
        "dcterms:description": "A set of prerequisite toy tasks for AI-complete question answering.",
        "dcterms:title": "bAbI",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Toy Tasks",
            "AI-complete QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Shi",
            "S. Cao",
            "L. Pan",
            "Y. Xiang",
            "L. Hou",
            "J. Li",
            "H. Zhang",
            "B. He"
        ],
        "dcterms:description": "A dataset with explicit compositional programs for complex question answering over knowledge base.",
        "dcterms:title": "KQA-Pro",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Compositional Programs",
            "Knowledge Base QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Complex Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "P. Qi",
            "H. Lee",
            "T. Sido",
            "C. D. Manning"
        ],
        "dcterms:description": "A dataset for answering open-domain questions of varying reasoning steps from text.",
        "dcterms:title": "BeerQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Open-domain QA",
            "Varying Reasoning Steps"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Open-domain Question Answering"
        ]
    }
]