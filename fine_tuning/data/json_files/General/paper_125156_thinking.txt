To extract datasets from the research paper titled "HADR: Applying Domain Randomization for Generating Synthetic Multimodal Dataset for Hand Instance Segmentation in Cluttered Industrial Environments" by Stefan Grushko et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and methods sections** to identify any datasets mentioned. The abstract indicates that the study provides a new synthetic dataset for hand detection applications, which suggests that a dataset is indeed present.

Next, I will focus on the **introduction** where the authors discuss the need for large, accurately labeled datasets for training deep learning models. They mention that they generated a synthetic dataset using domain randomization techniques, which is a key point to note.

In the **methods section**, I will look for specific details about the dataset. The authors describe the dataset generation process, including the use of a simulation platform (CoppeliaSim) and the characteristics of the generated dataset. They mention that a total of **117,000 images** were generated, which is significant information.

The paper also references existing datasets in the **related work section**. I will look for any comparisons made with other datasets, such as EgoHands, HandSeg, DenseHands, Rendered Hand Pose (RHD), and ObMan. Each of these datasets has specific characteristics and citation details that I need to extract.

Now, I will compile the full citations for each dataset mentioned in the paper:

1. **HaDR Dataset** (the synthetic dataset created by the authors):
   > Grushko, S., Chlebek, J., Vysocký, A., & Prokop, P. (2023). HADR: Applying Domain Randomization for Generating Synthetic Multimodal Dataset for Hand Instance Segmentation in Cluttered Industrial Environments. Preprint.

2. **EgoHands Dataset**:
   > Bambach, S., Lee, S., Crandall, D. J., & Yu, C. (2015). Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 1949–1957. doi:10.1109/ICCV.2015.226.

3. **HandSeg Dataset**:
   > Bojja, A. K., Mueller, F., Malireddi, S. R., Oberweger, M., Lepetit, V., Theobalt, C., Yi, K. M. (2018). HandSeg: An Automatically Labeled Dataset for Hand Segmentation from Depth Images. arXiv:1711.05944.

4. **DenseHands Dataset**:
   > Mueller, F., Bernard, F., Sotnychenko, O., Mehta, D., Theobalt, C. (2019). Real-time Pose and Shape Reconstruction of Two Interacting Hands With a Single Depth Camera. ACM Transactions on Graphics, 38(4):1–13. doi:10.1145/3306346.3322958.

5. **Rendered Hand Pose (RHD) Dataset**:
   > Zimmermann, C., & Brox, T. (2017). Learning to Estimate 3D Hand Pose from Single RGB Images. arXiv:1705.01389.

6. **ObMan Dataset**:
   > Hasson, Y., Varol, G., Tzionas, D., Kalevatykh, I., Black, M. J., Laptev, I., & Schmid, C. (2019). Learning joint reconstruction of hands and manipulated objects. arXiv:1904.05767.

After gathering this information, I will ensure that I have accurately captured the dataset names, descriptions, and full citations as required. This will allow me to present a comprehensive overview of the datasets referenced in the paper.