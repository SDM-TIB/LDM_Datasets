[
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan",
            "Nicholas Joseph",
            "Saurav Kadavath",
            "Jackson Kernion",
            "Tom Conerly",
            "Sheer El-Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Tristan Hume",
            "Scott Johnston",
            "Shauna Kravec",
            "Liane Lovitt",
            "Neel Nanda",
            "Catherine Olsson",
            "Dario Amodei",
            "Tom Brown",
            "Jack Clark",
            "Sam McCandlish",
            "Chris Olah",
            "Ben Mann",
            "Jared Kaplan"
        ],
        "dcterms:description": "The HH-RLHF dataset consists of 112,000 training samples and 12,500 test samples, where each sample includes a prompt and two responses, with one being preferred over the other based on crowd worker assessments of helpfulness and harmlessness.",
        "dcterms:title": "HH-RLHF (Helpful and Harmless)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/datasets/Dahoas/full-hh-rlhf",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Alignment",
            "Human Feedback",
            "Language Models",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/Dahoas/full-hh-rlhf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation",
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Kawin Ethayarajh",
            "Yejin Choi",
            "Swabha Swayamdipta"
        ],
        "dcterms:description": "The SHP dataset consists of 349,000 training samples and 36,800 test samples, where each sample contains a prompt paired with two responses, annotated to indicate which response is more preferred.",
        "dcterms:title": "Stanford Human Preferences (SHP)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Preference Learning"
        ],
        "dcat:keyword": [
            "Human Preferences",
            "Response Comparison",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning",
            "Response Generation"
        ]
    }
]