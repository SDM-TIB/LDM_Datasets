[
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset consisting of approximately 10,000 posts related to Jewish topics, collected using the Twitter API, and annotated for anti-Semitic sentiment.",
        "dcterms:title": "Twitter Dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Twitter",
            "Hate Speech",
            "Anti-Semitism",
            "Social Media"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Hate Speech Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT is a pre-trained transformer model that enables a nuanced understanding of language through bidirectional training.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1810.04805",
        "dcat:theme": [],
        "dcat:keyword": [
            "Transformer",
            "Language Understanding",
            "NLP"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1810.04805",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Natural Language Processing"
        ]
    },
    {
        "dcterms:creator": [
            "Victor Sanh",
            "Lysandre Debut",
            "Julien Chaumond",
            "Thomas Wolf"
        ],
        "dcterms:description": "DistilBERT is a distilled version of BERT that is smaller, faster, and cheaper while retaining 97% of BERT's language understanding capabilities.",
        "dcterms:title": "DistilBERT",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1910.01108",
        "dcat:theme": [],
        "dcat:keyword": [
            "Transformer",
            "Distillation",
            "NLP"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1910.01108",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Natural Language Processing"
        ]
    },
    {
        "dcterms:creator": [
            "Yinhan Liu",
            "Myle Ott",
            "Naman Goyal",
            "Jingfei Du",
            "Mandar Joshi",
            "Danqi Chen",
            "Omer Levy",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "dcterms:description": "RoBERTa is an optimized version of BERT that improves performance by modifying key hyperparameters and training on more data.",
        "dcterms:title": "RoBERTa",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1907.11692",
        "dcat:theme": [],
        "dcat:keyword": [
            "Transformer",
            "Optimization",
            "NLP"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1907.11692",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Natural Language Processing"
        ]
    },
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Louis Martin",
            "Kevin Stone",
            "et al."
        ],
        "dcterms:description": "LLaMA-2 is a large-scale transformer model designed for generative and multitasking capabilities across multiple languages.",
        "dcterms:title": "LLaMA-2",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2307.09288",
        "dcat:theme": [],
        "dcat:keyword": [
            "Transformer",
            "Generative Model",
            "NLP"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2307.09288",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Natural Language Processing"
        ]
    },
    {
        "dcterms:creator": [
            "Edward J. Hu",
            "Yelong Shen",
            "Phillip Wallis",
            "et al."
        ],
        "dcterms:description": "LoRA is a method for adapting large language models efficiently by introducing low-rank matrices to modify only a small fraction of the model's parameters.",
        "dcterms:title": "LoRA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2106.09685",
        "dcat:theme": [],
        "dcat:keyword": [
            "Model Adaptation",
            "Efficiency",
            "NLP"
        ],
        "dcat:landingPage": "arXiv:2106.09685",
        "dcterms:hasVersion": "",
        "dcterms:format": "Method",
        "mls:task": []
    }
]