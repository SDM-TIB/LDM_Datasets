[
    {
        "dcterms:creator": [],
        "dcterms:description": "OMGEval is the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages, providing 804 open-ended questions covering a wide range of important capabilities such as general knowledge and logical reasoning, with questions verified by human annotators.",
        "dcterms:title": "OMGEval",
        "dcterms:issued": "",
        "dcterms:language": "Chinese, Russian, French, Spanish, Arabic",
        "dcterms:identifier": "https://github.com/blcuicall/OMGEval",
        "dcat:theme": [
            "Multilingual Evaluation",
            "Generative Evaluation"
        ],
        "dcat:keyword": [
            "Open-ended questions",
            "Multilingual capabilities",
            "Cultural localization"
        ],
        "dcat:landingPage": "https://github.com/blcuicall/OMGEval",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multilingual Question Answering",
            "Cultural Context Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Xuechen Li",
            "Tianyi Zhang",
            "Yann Dubois",
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "AlpacaEval is an automatic evaluator of instruction-following models, utilizing a dataset comprising 805 instructions and evaluation data from various projects, including Self-instruct, Open Assistant, and Vicuna.",
        "dcterms:title": "AlpacaEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/alpaca_eval",
        "dcat:theme": [
            "Automated Evaluation",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "Evaluation benchmark",
            "Instruction-following models",
            "Automated scoring"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/alpaca_eval",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "MMLU is a benchmark for measuring massive multitask language understanding, designed to evaluate the performance of language models across a wide range of tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multitask evaluation",
            "Language models",
            "Performance measurement"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Alexis Conneau",
            "Kartikay Khandelwal",
            "Naman Goyal",
            "Vishrav Chaudhary",
            "Guillaume Wenzek",
            "Francisco Guzmán",
            "Edouard Grave",
            "Myle Ott",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "dcterms:description": "XNLI is a benchmark for evaluating the model's proficiency in grasping textual meanings across multiple languages.",
        "dcterms:title": "XNLI",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1911.02116",
        "dcat:theme": [
            "Cross-lingual Evaluation",
            "Natural Language Understanding"
        ],
        "dcat:keyword": [
            "Textual meaning",
            "Cross-lingual representation",
            "Benchmarking"
        ],
        "dcat:landingPage": "arXiv:1911.02116",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yinfei Yang",
            "Yuan Zhang",
            "Chris Tar",
            "Jason Baldridge"
        ],
        "dcterms:description": "PAWS-X is a cross-lingual adversarial dataset for paraphrase identification, designed to challenge models in recognizing paraphrases across languages.",
        "dcterms:title": "PAWS-X",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1908.11828",
        "dcat:theme": [
            "Paraphrase Identification",
            "Cross-lingual Evaluation"
        ],
        "dcat:keyword": [
            "Adversarial dataset",
            "Paraphrase recognition",
            "Cross-lingual tasks"
        ],
        "dcat:landingPage": "arXiv:1908.11828",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Paraphrase Identification"
        ]
    },
    {
        "dcterms:creator": [
            "Edoardo Maria Ponti",
            "Goran Glavaš",
            "Olga Majewska",
            "Qianchu Liu",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "dcterms:description": "XCOPA is a multilingual dataset for causal common-sense reasoning, designed to evaluate models' ability to make inferential judgments based on given premises.",
        "dcterms:title": "XCOPA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2005.00333",
        "dcat:theme": [
            "Common-Sense Reasoning",
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "Causal reasoning",
            "Multilingual dataset",
            "Inference tasks"
        ],
        "dcat:landingPage": "arXiv:2005.00333",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Causal Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Jonathan H Clark",
            "Eunsol Choi",
            "Michael Collins",
            "Dan Garrette",
            "Tom Kwiatkowski",
            "Vitaly Nikolaev",
            "Jennimaria Palomaki"
        ],
        "dcterms:description": "TyDi-QA is a benchmark for information-seeking question answering in typologically diverse languages, designed to evaluate models' performance in understanding and answering questions.",
        "dcterms:title": "TyDi-QA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "Information-seeking",
            "Diverse languages",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Patrick Lewis",
            "Barlas Oğuz",
            "Ruty Rinott",
            "Sebastian Riedel",
            "Holger Schwenk"
        ],
        "dcterms:description": "MLQA evaluates cross-lingual extractive question answering, providing a benchmark for assessing models' ability to answer questions based on context in multiple languages.",
        "dcterms:title": "MLQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1910.07475",
        "dcat:theme": [
            "Extractive Question Answering",
            "Cross-lingual Evaluation"
        ],
        "dcat:keyword": [
            "Extractive QA",
            "Cross-lingual tasks",
            "Benchmarking"
        ],
        "dcat:landingPage": "arXiv:1910.07475",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mikel Artetxe",
            "Sebastian Ruder",
            "Dani Yogatama"
        ],
        "dcterms:description": "XQuAD is a dataset designed to evaluate the cross-lingual transferability of monolingual representations, focusing on the ability of models to answer questions in different languages.",
        "dcterms:title": "XQuAD",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1910.11856",
        "dcat:theme": [
            "Cross-lingual Evaluation",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Transferability",
            "Cross-lingual tasks",
            "Question answering"
        ],
        "dcat:landingPage": "arXiv:1910.11856",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Shayne Longpre",
            "Yi Lu",
            "Joachim Daiber"
        ],
        "dcterms:description": "MKQA is a linguistically diverse benchmark for multilingual open domain question answering, designed to evaluate models' performance across various languages and cultural contexts.",
        "dcterms:title": "MKQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Open Domain Question Answering",
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "Linguistic diversity",
            "Open domain QA",
            "Multilingual tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Open Domain Question Answering"
        ]
    }
]