To extract the datasets mentioned in the research paper titled "TransCrowd: weakly-supervised crowd counting with transformers" by Dingkang Liang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** of the paper. The abstract mentions that the proposed method is evaluated on **five benchmark datasets**, which indicates that there are multiple datasets to extract.

Next, I will focus on **section 4.2 (Dataset)**, where the authors provide detailed descriptions of the datasets used in their experiments. Here, I will identify each dataset, its characteristics, and any relevant statistics provided by the authors.

The datasets mentioned in the paper are:

1. **NWPU-Crowd**: This dataset consists of 5,109 images with 2,133,375 instances annotated. The images are split into training, validation, and testing sets.

2. **JHU-CROWD++**: This dataset contains 2,722 training images, 500 validation images, and 1,600 testing images, with a total number of people in each image ranging from 0 to 25,791.

3. **UCF-QNRF**: This dataset includes 1,535 images captured from unconstrained crowd scenes, with about one million annotations. The training set consists of 1,201 images, and the testing set consists of 334 images.

4. **ShanghaiTech**: This dataset contains 1,198 crowd images with 330,165 annotations, divided into two parts: Part A and Part B.

5. **UCF CC 50**: A smaller dataset containing 50 images with an average of 1,280 individuals per image, captured in various events.

6. **WorldExpo’10**: This dataset consists of 1,132 surveillance videos from 108 cameras, with a total of 199,923 annotations across 3,980 images.

After identifying the datasets, I will then refer to the **References section** of the paper to find the full citations for each dataset. The citations are as follows:

- **NWPU-Crowd**: 
  > Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li. *NWPU-Crowd: A large-scale benchmark for crowd counting and localization*. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.

- **JHU-CROWD++**: 
  > Vishwanath A Sindagi and Vishal M Patel. *JHU-CROWD++: Large-scale crowd counting dataset and a benchmark method*. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.

- **UCF-QNRF**: 
  > Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak Shah. *Composition loss for counting, density map estimation and localization in dense crowds*. In Proceedings of the European Conference on Computer Vision, 2018.

- **ShanghaiTech**: 
  > Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. *Single-image crowd counting via multi-column convolutional neural network*. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, 2016.

- **UCF CC 50**: 
  > Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. *Multi-source multi-scale counting in extremely dense crowd images*. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, 2013.

- **WorldExpo’10**: 
  > Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, and Errui Ding. *Perspective-guided convolution networks for crowd counting*. In Proceedings of the IEEE International Conference on Computer Vision, 2019.

Finally, I will compile the dataset entries with their descriptions and citations into a structured format for further processing or review.