To extract datasets from the research paper titled "Implicit Neural Representations for Variable Length Human Motion Generation" by Pablo Cervantes et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors evaluated their method on several datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on **section 4.2 (Datasets)**, where the authors explicitly list the datasets used for their experiments. Here, they mention three datasets:

1. **HumanAct12**: This dataset consists of 1,191 motion clips and a total of 90,099 frames, with action labels for 12 actions. The sequence lengths range from 8 to 470 frames.

2. **NTU-RGBD (NTU13)**: This dataset originally contains pose annotations from a Microsoft Kinect sensor and label annotations for 120 actions. The authors refer to a refined subset of 13 actions, where each action label has between 286 and 309 samples, and the sequence lengths range from 20 to 201 frames.

3. **UESTC**: This dataset includes 40 action classes, 40 subjects, and 25,000 samples. It is noted as the largest dataset used in their experiments, with a train/test split and sequence lengths ranging from 24 to 2,891 time steps.

After identifying the datasets, I will check the **References section** to find the full citations for each dataset:

- For **HumanAct12**, the citation is:
  > Guo, C., Zuo, X., Wang, S., Zou, S., Sun, Q., Deng, A., Gong, M., Cheng, L. (2020). *Action2Motion: Conditioned generation of 3D human motions*. In Proceedings of the 28th ACM International Conference on Multimedia (MM ’20).

- For **NTU-RGBD**, the citation is:
  > Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L.Y., Kot, A.C. (2019). *NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding*. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10), 2684–2701.

- For **UESTC**, the citation is:
  > Ji, Y., Xu, F., Yang, Y., Shen, F., Shen, H.T., Zheng, W.S. (2018). *A large-scale RGB-D database for arbitrary-view human action recognition*. In Proceedings of the 26th ACM International Conference on Multimedia (MM ’18).

Now that I have gathered the necessary information, I will compile the dataset entries with their respective citations to ensure clarity and completeness in the extraction process.