To extract datasets from the research paper titled "Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical Fusion for Multimodal Affect Recognition" by Yaoting Wang et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations accurately.

First, I will read through the **abstract, introduction, and experiments sections** of the paper. The abstract mentions that the authors evaluated their method on five benchmark datasets, which indicates that there are multiple datasets to extract.

Next, I will focus on the **experiments section** (specifically section 4) where the authors describe the datasets used for their evaluations. In section 4.1, they explicitly list the datasets:

1. **CMU-MOSI**: This dataset is described as containing video clips from YouTube, annotated with sentiment scores ranging from -3 to 3, with a total of 2,199 samples.

2. **CMU-MOSEI**: Similar to CMU-MOSI, this dataset also consists of video clips from YouTube, but it has a larger total of 23,454 samples.

3. **IEMOCAP**: This dataset is focused on emotion recognition and includes 4,453 samples, annotated with four emotions: happy, sad, angry, and neutral.

4. **UR-FUNNY**: This dataset is collected from TED talk videos and contains 10,000 instances (5,000 humor and 5,000 non-humor) from 1,741 distinct speakers.

5. **MUStARD**: This dataset consists of 690 video segments from popular TV shows, annotated for sarcasm detection.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset:

- For **CMU-MOSI**, the citation is:
  > Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. *Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph*. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236–2246, 2018.

- For **CMU-MOSEI**, the citation is the same as above, as it is part of the same research.

- For **IEMOCAP**, the citation is:
  > Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. *IEMOCAP: Interactive emotional dyadic motion capture database*. Language Resources and Evaluation, 42(4):335–359, 2008.

- For **UR-FUNNY**, the citation is:
  > Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, et al. *UR-FUNNY: A multimodal language dataset for understanding humor*. arXiv preprint arXiv:1904.06618, 2019.

- For **MUStARD**, the citation is:
  > Santiago Castro, Devamanyu Hazarika, Verónica Pérez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. *Towards multimodal sarcasm detection (an _obviously_ perfect paper)*. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4619–4629, 2019.

Now that I have gathered all the necessary information about the datasets and their citations, I will compile this information into a structured format for further use or analysis.