[
    {
        "dcterms:creator": [
            "A. Zadeh",
            "R. Zellers",
            "E. Pincus",
            "L. Morency"
        ],
        "dcterms:description": "The MOSI dataset consists of 2,199 multimodal sequence samples with video, audio, and text modalities. Each multimodal sample has a uniform label that ranges from -3 to 3, representing emotions from strongly negative to strongly positive.",
        "dcterms:title": "MOSI",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Emotion Recognition"
        ],
        "dcat:keyword": [
            "Multimodal dataset",
            "Emotion recognition",
            "Video",
            "Audio",
            "Text"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Multimodal",
        "mls:task": [
            "Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Zadeh",
            "P. P. Liang",
            "S. Poria",
            "E. Cambria",
            "L. Morency"
        ],
        "dcterms:description": "The MOSEI dataset comprises 22,851 video clips collected from YouTube with spontaneous expressions, head poses, occlusions, illuminations, and so on. Each sample is manually annotated with a sentiment score from -3 (strongly negative) to 3 (strongly positive).",
        "dcterms:title": "MOSEI",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Emotion Recognition"
        ],
        "dcat:keyword": [
            "Multimodal dataset",
            "Emotion recognition",
            "Video",
            "Sentiment analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Multimodal",
        "mls:task": [
            "Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "C. Busso",
            "M. Bulut",
            "C. Lee",
            "A. Kazemzadeh",
            "E. Mower",
            "S. Kim",
            "S. S. Narayanan"
        ],
        "dcterms:description": "The IEMOCAP dataset includes video, audio, and text transcriptions and 12 hours of video recordings of situational dialogues. The data is divided into five sessions with a total of 10,039 samples and 9 emotion categories.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Emotion Recognition"
        ],
        "dcat:keyword": [
            "Multimodal dataset",
            "Emotion recognition",
            "Video",
            "Audio",
            "Text"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Multimodal",
        "mls:task": [
            "Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Wang",
            "L. Guan"
        ],
        "dcterms:description": "The RML dataset is an audio-visual emotion dataset containing 720 video samples across six different languages. Each data sample is labeled as one of six emotions, and the training and testing sets are organized in a speaker-independent manner.",
        "dcterms:title": "RML",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Emotion Recognition"
        ],
        "dcat:keyword": [
            "Audio-visual dataset",
            "Emotion recognition",
            "Video",
            "Audio"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Multimodal",
        "mls:task": [
            "Emotion Recognition"
        ]
    }
]