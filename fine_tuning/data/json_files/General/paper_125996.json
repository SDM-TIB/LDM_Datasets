[
    {
        "dcterms:creator": [],
        "dcterms:description": "A machine-generated, conflict-induced dataset designed to encourage research on knowledge conflicts in retrieval-augmented models.",
        "dcterms:title": "MACNOISE",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Knowledge Conflict",
            "Retrieval-Augmented Models",
            "Machine-Generated Dataset"
        ],
        "dcat:landingPage": "https://github.com/wjdghks950/Discern-and-Answer",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Open-Domain Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "A benchmark dataset for question answering research, consisting of real queries and corresponding Wikipedia documents.",
        "dcterms:title": "Natural Questions (NQ)",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Benchmark Dataset",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Open-Domain Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large-scale distantly supervised challenge dataset for reading comprehension, sourced from trivia and quiz-league websites.",
        "dcterms:title": "TriviaQA (TQA)",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Trivia",
            "Challenge Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Shayne Longpre",
            "Kartik Perisetla",
            "Anthony Chen",
            "Nikhil Ramesh",
            "Chris DuBois",
            "Sameer Singh"
        ],
        "dcterms:description": "A dataset that explores entity-based knowledge conflicts in question answering.",
        "dcterms:title": "Entity Perturbation",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Knowledge Conflicts",
            "Entity Perturbation",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Open-Domain Question Answering"
        ]
    }
]