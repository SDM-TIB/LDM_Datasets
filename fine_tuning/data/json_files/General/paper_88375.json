[
    {
        "dcterms:creator": [
            "A. Rastogi",
            "X. Zang",
            "S. Sunkara",
            "R. Gupta",
            "P. Khaitan"
        ],
        "dcterms:description": "The dataset is used for schema-guided dialogue state tracking, focusing on predicting intents and slots in user turns based on a predefined schema.",
        "dcterms:title": "DSTC 8 Track 4 Test Dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1909.05855",
        "dcat:theme": [
            "Dialogue Systems",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Dialogue State Tracking",
            "Schema-guided",
            "Multi-domain"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue State Tracking"
        ]
    },
    {
        "dcterms:creator": [
            "P. Rajpurkar",
            "J. Zhang",
            "K. Lopyrev",
            "P. Liang"
        ],
        "dcterms:description": "SQuAD is a large-scale dataset for machine comprehension of text, containing over 100,000 questions based on a set of Wikipedia articles.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Text Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "G. Lai",
            "Q. Xie",
            "H. Liu",
            "Y. Yang",
            "E. Hovy"
        ],
        "dcterms:description": "RACE is a large-scale reading comprehension dataset derived from English examinations, designed to evaluate the reading comprehension abilities of models.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Examinations",
            "Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Yang",
            "Z. Dai",
            "Y. Yang",
            "J. Carbonell",
            "R. Salakhutdinov",
            "Q. V. Le"
        ],
        "dcterms:description": "XLNet is a generalized autoregressive pretraining model for language understanding, which outperforms BERT on various NLP tasks.",
        "dcterms:title": "XLNet",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1906.08237",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Language Understanding",
            "Pretraining"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "J. Devlin",
            "M.-W. Chang",
            "K. Lee",
            "K. Toutanova"
        ],
        "dcterms:description": "BERT is a pre-trained deep bidirectional transformer model for language understanding, achieving state-of-the-art results on various NLP tasks.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Language Understanding",
            "Transformers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "C.-S. Wu",
            "A. Madotto",
            "E. Hosseini-Asl",
            "C. Xiong",
            "R. Socher",
            "P. Fung"
        ],
        "dcterms:description": "TRADE-DST is a transferable multi-domain state generator for task-oriented dialogue systems, designed to improve dialogue state tracking.",
        "dcterms:title": "TRADE-DST",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Systems"
        ],
        "dcat:keyword": [
            "Task-oriented Dialogue",
            "State Tracking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue State Tracking"
        ]
    },
    {
        "dcterms:creator": [
            "N. Mrkˇsi´c",
            "D. ´O S´eaghdha",
            "B. Thomson",
            "M. Gaˇsi´c",
            "P.-H. Su",
            "D. Vandyke",
            "T.-H. Wen",
            "S. Young"
        ],
        "dcterms:description": "The Multi-domain Dialogue State Tracking Dataset is designed for training dialogue state tracking models across multiple domains.",
        "dcterms:title": "Multi-domain Dialogue State Tracking Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Systems"
        ],
        "dcat:keyword": [
            "Multi-domain",
            "State Tracking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue State Tracking"
        ]
    }
]