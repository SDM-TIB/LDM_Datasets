[
    {
        "dcterms:creator": [
            "I. Beltagy",
            "K. Lo",
            "A. Cohan"
        ],
        "dcterms:description": "A pretrained language model specifically designed for scientific text, which is utilized as a feature extractor in the classification models.",
        "dcterms:title": "SciBERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Pretrained model",
            "Scientific text",
            "Text classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "J. Devlin",
            "M.-W. Chang",
            "K. Lee",
            "K. Toutanova"
        ],
        "dcterms:description": "A pretrained language model that uses deep bidirectional transformers for language understanding, serving as a foundational model for various NLP tasks.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1810.04805",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Pretrained model",
            "Language understanding",
            "Text classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "F. Feng",
            "Y. Yang",
            "D. Cer",
            "N. Arivazhagan",
            "W. Wang"
        ],
        "dcterms:description": "A language-agnostic sentence embedding model that can be used for various NLP tasks across different languages.",
        "dcterms:title": "Language-agnostic BERT",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language-agnostic",
            "Sentence embedding",
            "Text classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset containing all publications of EP patent applications and patent specifications from 1978 until January 2022, used to create a labeled training dataset for patent classification.",
        "dcterms:title": "EP full text data",
        "dcterms:issued": "",
        "dcterms:language": "English, French, German",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Patent Data",
            "Text Analytics"
        ],
        "dcat:keyword": [
            "Patent applications",
            "Full text data",
            "Text classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Data preparation",
            "Text classification"
        ]
    }
]