[
    {
        "dcterms:creator": [
            "Sven Bambach",
            "Stefan Lee",
            "David J. Crandall",
            "Chen Yu"
        ],
        "dcterms:description": "The EgoHands dataset includes 4.8K images of two people’s interactions, where pixel-level segmentation masks for each hand were manually annotated.",
        "dcterms:title": "EgoHands Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "10.1109/ICCV.2015.226",
        "dcat:theme": [
            "Computer Vision",
            "Hand Detection"
        ],
        "dcat:keyword": [
            "Hand segmentation",
            "Egocentric interactions",
            "Image dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Hand Detection",
            "Instance Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Cristina Nuzzi",
            "Simone Pasinetti",
            "Roberto Pagani",
            "Gabriele Coffetti",
            "Giovanna Sansoni"
        ],
        "dcterms:description": "The RGB-D HANDS dataset comprises samples with single hand gestures, focusing on gesture classification without segmentation masks.",
        "dcterms:title": "RGB-D HANDS Dataset",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "10.1016/j.dib.2021.106791",
        "dcat:theme": [
            "Computer Vision",
            "Gesture Recognition"
        ],
        "dcat:keyword": [
            "RGB-D dataset",
            "Hand gestures",
            "Human-robot interaction"
        ],
        "dcat:landingPage": "https://www.sciencedirect.com/science/article/pii/S2352340921000755",
        "dcterms:hasVersion": "",
        "dcterms:format": "RGB-D",
        "mls:task": [
            "Gesture Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Abhishake Kumar Bojja",
            "Franziska Mueller",
            "Sri Raghu Malireddi",
            "Markus Oberweger",
            "Vincent Lepetit",
            "Christian Theobalt",
            "Kwang Moo Yi",
            "Andrea Tagliasacchi"
        ],
        "dcterms:description": "The HandSeg dataset contains 150K samples with random hand gestures in front of a depth camera, with ground truth annotations created automatically using color gloves.",
        "dcterms:title": "HandSeg Dataset",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1711.05944",
        "dcat:theme": [
            "Computer Vision",
            "Hand Segmentation"
        ],
        "dcat:keyword": [
            "Depth images",
            "Hand gestures",
            "Automatic labeling"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1711.05944",
        "dcterms:hasVersion": "",
        "dcterms:format": "Depth",
        "mls:task": [
            "Hand Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Franziska Mueller",
            "Micah Davis",
            "Florian Bernard",
            "Oleksandr Sotnychenko",
            "Mickeal Verschoor",
            "Miguel A. Otaduy",
            "Dan Casas",
            "Christian Theobalt"
        ],
        "dcterms:description": "The DenseHands dataset consists of depth image samples augmented with RGB-encoded segmentation masks representing the vertices of a MANO hand model.",
        "dcterms:title": "DenseHands Dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "10.1145/3306346.3322958",
        "dcat:theme": [
            "Computer Vision",
            "Hand Pose Estimation"
        ],
        "dcat:keyword": [
            "Depth images",
            "Hand shape reconstruction",
            "Segmentation masks"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2106.08059",
        "dcterms:hasVersion": "",
        "dcterms:format": "Depth",
        "mls:task": [
            "Hand Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "Christian Zimmermann",
            "Thomas Brox"
        ],
        "dcterms:description": "The Rendered Hand Pose Dataset (RHD) is designed for estimating 3D hand pose from single RGB images, utilizing 3D character models matched with a parameterizable hand model.",
        "dcterms:title": "Rendered Hand Pose Dataset (RHD)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1705.01389",
        "dcat:theme": [
            "Computer Vision",
            "3D Hand Pose Estimation"
        ],
        "dcat:keyword": [
            "3D hand pose",
            "Rendered images",
            "Synthetic dataset"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1705.01389",
        "dcterms:hasVersion": "",
        "dcterms:format": "RGB",
        "mls:task": [
            "3D Hand Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "Yana Hasson",
            "Gül Varol",
            "Dimitrios Tzionas",
            "Igor Kalevatykh",
            "Michael J. Black",
            "Ivan Laptev",
            "Cordelia Schmid"
        ],
        "dcterms:description": "The ObMan dataset is a fully synthetic dataset consisting of RGB-D images created using 3D models of human characters holding various objects, with annotations for hand key points and segmentation masks.",
        "dcterms:title": "ObMan Dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1904.05767",
        "dcat:theme": [
            "Computer Vision",
            "Hand and Object Interaction"
        ],
        "dcat:keyword": [
            "RGB-D images",
            "Synthetic dataset",
            "Hand-object interaction"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1904.05767",
        "dcterms:hasVersion": "",
        "dcterms:format": "RGB-D",
        "mls:task": [
            "Hand and Object Interaction"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The COCO dataset provides a large-scale dataset for object detection, segmentation, and captioning, containing images with complex scenes and annotations.",
        "dcterms:title": "COCO Dataset",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "10.1007/978-3-319-10602-1_48",
        "dcat:theme": [
            "Computer Vision",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Object detection",
            "Segmentation",
            "Image dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection",
            "Segmentation"
        ]
    }
]