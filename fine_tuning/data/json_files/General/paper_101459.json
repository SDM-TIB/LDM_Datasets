[
    {
        "dcterms:creator": [
            "Jia Deng",
            "Wei Dong",
            "Richard Socher",
            "Li-Jia Li",
            "Kai Li",
            "Li Fei-Fei"
        ],
        "dcterms:description": "A large-scale hierarchical image database used for pretraining convolutional networks in computer vision tasks.",
        "dcterms:title": "ImageNet",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Hierarchical classification",
            "Visual recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification",
            "Object Detection",
            "Semantic Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "R. Socher"
        ],
        "dcterms:description": "A dataset used for pretraining language models, particularly for tasks involving natural language processing.",
        "dcterms:title": "WikiText-103",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1609.07843",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Language modeling",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Pedro Javier Ortiz Suárez",
            "Laurent Romary",
            "Benoît Sagot"
        ],
        "dcterms:description": "A multilingual corpus used for training language models, particularly for mid-resource languages.",
        "dcterms:title": "OSCAR",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multilingual Models"
        ],
        "dcat:keyword": [
            "Multilingual dataset",
            "Language modeling",
            "Contextualized embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Mathilde Caron",
            "Ishan Misra",
            "Julien Mairal",
            "Priya Goyal",
            "Piotr Bojanowski",
            "Armand Joulin"
        ],
        "dcterms:description": "A self-supervised learning technique that learns image representations by contrasting cluster assignments.",
        "dcterms:title": "SwAV",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Self-Supervised Learning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Self-supervised learning",
            "Visual representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Representation Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Zhen-Zhong Lan",
            "Mingda Chen",
            "Sebastian Goodman",
            "Kevin Gimpel",
            "Piyush Sharma",
            "Radu Soricut"
        ],
        "dcterms:description": "A lite BERT model designed for self-supervised learning of language representations.",
        "dcterms:title": "ALBERT",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Representation"
        ],
        "dcat:keyword": [
            "Language model",
            "Self-supervised learning",
            "Text representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Learning@home team"
        ],
        "dcterms:description": "A library for decentralized deep learning.",
        "dcterms:title": "Hivemind",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/learning-at-home/hivemind",
        "dcat:theme": [
            "Distributed Computing",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Decentralized learning",
            "Collaborative training",
            "Deep learning framework"
        ],
        "dcat:landingPage": "https://github.com/learning-at-home/hivemind",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Divyanshu Kakwani",
            "Anoop Kunchukuttan",
            "Satish Golla",
            "Gokul N.C.",
            "Avik Bhattacharyya",
            "Mitesh M. Khapra",
            "Pratyush Kumar"
        ],
        "dcterms:description": "Monolingual corpora, evaluation benchmarks, and pre-trained multilingual language models for Indian languages.",
        "dcterms:title": "IndicNLPSuite",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multilingual Models"
        ],
        "dcat:keyword": [
            "Indian languages",
            "Language models",
            "Evaluation benchmarks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Kushal Jain",
            "Adwait Deshpande",
            "Kumar Shridhar",
            "Felix Laumann",
            "Ayushman Dash"
        ],
        "dcterms:description": "An analysis of transformer language models for Indian languages.",
        "dcterms:title": "bnRoBERTa",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Representation"
        ],
        "dcat:keyword": [
            "Bengali language",
            "Language model",
            "Text representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Alexis Conneau",
            "Kartikay Khandelwal",
            "Naman Goyal",
            "Vishrav Chaudhary",
            "Guillaume Wenzek",
            "Francisco Guzmán",
            "Edouard Grave",
            "Myle Ott",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "dcterms:description": "A cross-lingual transformer-based masked language model pretrained on 100 languages.",
        "dcterms:title": "XLM-R",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Cross-Lingual Models"
        ],
        "dcat:keyword": [
            "Cross-lingual representation",
            "Language model",
            "Multilingual training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]