[
    {
        "dcterms:creator": [
            "Matthew S. Dryer",
            "Martin Haspelmath"
        ],
        "dcterms:description": "The World Atlas of Language Structures (WALS) aims at classifying languages of the world on the basis of typological criteria at all levels of linguistic analysis. Languages in WALS are represented as categorial feature-value vectors.",
        "dcterms:title": "World Atlas of Language Structures (WALS)",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Linguistics",
            "Language Typology"
        ],
        "dcat:keyword": [
            "Language classification",
            "Typological features",
            "Linguistic analysis"
        ],
        "dcat:landingPage": "https://wals.info/",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Philipp Koehn"
        ],
        "dcterms:description": "The EuroParl corpus is a large-scale parallel corpus of proceedings of the European Parliament, used for statistical machine translation.",
        "dcterms:title": "EuroParl corpus",
        "dcterms:issued": "2005",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Translation",
            "Parallel Corpus"
        ],
        "dcat:keyword": [
            "Parallel corpus",
            "Statistical machine translation",
            "European Parliament proceedings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Machine Translation"
        ]
    },
    {
        "dcterms:creator": [
            "José Cañete",
            "Gabriel Chaperon",
            "Rodrigo Fuentes",
            "Jou-Hui Ho",
            "Hojin Kang",
            "Jorge Pérez"
        ],
        "dcterms:description": "A Spanish pre-trained BERT model and evaluation data.",
        "dcterms:title": "Spanish pre-trained BERT model",
        "dcterms:issued": "2020",
        "dcterms:language": "Spanish",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Spanish language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Stefan Dumitrescu",
            "Andrei-Marius Avram",
            "Sampo Pyysalo"
        ],
        "dcterms:description": "The birth of Romanian BERT, a pre-trained BERT model for the Romanian language.",
        "dcterms:title": "Romanian BERT",
        "dcterms:issued": "2020",
        "dcterms:language": "Romanian",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Romanian language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "John Koutsikakis",
            "Ilias Chalkidis",
            "Prodromos Malakasiotis",
            "Ion Androutsopoulos"
        ],
        "dcterms:description": "Greek-BERT: The Greeks visiting Sesame Street, a pre-trained BERT model for the Greek language.",
        "dcterms:title": "Greek BERT",
        "dcterms:issued": "2020",
        "dcterms:language": "Greek",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Greek language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Stefan Schweter"
        ],
        "dcterms:description": "BERTurk - BERT models for Turkish language.",
        "dcterms:title": "Turkish BERT",
        "dcterms:issued": "2020",
        "dcterms:language": "Turkish",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Turkish language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Antti Virtanen",
            "Jenna Kanerva",
            "Rami Ilo",
            "Jouni Luoma",
            "Juhani Luotolahti",
            "Tapio Salakoski",
            "Filip Ginter",
            "Sampo Pyysalo"
        ],
        "dcterms:description": "Multilingual is not enough: BERT for Finnish, a pre-trained BERT model for the Finnish language.",
        "dcterms:title": "Finnish BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "Finnish",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Finnish language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT: Pre-training of deep bidirectional transformers for language understanding, the original BERT model for English.",
        "dcterms:title": "English BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "English language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Wietse de Vries",
            "Andreas van Cranenburgh",
            "Arianna Bisazza",
            "Tommaso Caselli",
            "Gertjan van Noord",
            "Malvina Nissim"
        ],
        "dcterms:description": "Bertje: A Dutch BERT model, a pre-trained BERT model for the Dutch language.",
        "dcterms:title": "Dutch BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "Dutch",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Dutch language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yuri Kuratov",
            "Mikhail Y. Arkhipov"
        ],
        "dcterms:description": "Adaptation of deep bidirectional multilingual transformers for Russian language, a pre-trained BERT model for the Russian language.",
        "dcterms:title": "Russian BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "Russian",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "BERT",
            "Russian language",
            "Pre-trained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Language Understanding"
        ]
    }
]