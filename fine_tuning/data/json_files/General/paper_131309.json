[
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A multi-task benchmark and analysis platform for natural language understanding, used to evaluate the performance of models on various NLP tasks.",
        "dcterms:title": "GLUE benchmark",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "10.18653/v1/W18-5446",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "NLP tasks",
            "Benchmark",
            "Model evaluation"
        ],
        "dcat:landingPage": "https://aclanthology.org/W18-5446",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Alex Warstadt",
            "Amanpreet Singh",
            "Samuel R Bowman"
        ],
        "dcterms:description": "A dataset for assessing the acceptability of sentences in English, used to evaluate models on linguistic acceptability tasks.",
        "dcterms:title": "CoLA (Corpus of Linguistic Acceptability)",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1805.12471",
        "dcat:theme": [
            "Natural Language Processing",
            "Linguistic Acceptability"
        ],
        "dcat:keyword": [
            "Linguistic acceptability",
            "Sentence evaluation",
            "NLP dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Linguistic acceptability judgment"
        ]
    },
    {
        "dcterms:creator": [
            "Daniel Cer",
            "Mona Diab",
            "Eneko Agirre",
            "Inigo Lopez-Gazpio",
            "Lucia Specia"
        ],
        "dcterms:description": "A benchmark for evaluating semantic textual similarity across multiple languages, used to assess the similarity between pairs of sentences.",
        "dcterms:title": "STS-B (Semantic Textual Similarity Benchmark)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1708.00055",
        "dcat:theme": [
            "Natural Language Processing",
            "Semantic Similarity"
        ],
        "dcat:keyword": [
            "Semantic textual similarity",
            "NLP benchmark",
            "Cross-lingual evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Semantic similarity evaluation"
        ]
    }
]