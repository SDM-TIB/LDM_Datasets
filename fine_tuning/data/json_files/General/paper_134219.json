[
    {
        "dcterms:creator": [
            "Aaron Gokaslan",
            "Vanya Cohen"
        ],
        "dcterms:description": "OpenWebText is a dataset that serves as an alternative to the Common Crawl dataset, containing web text data for training language models.",
        "dcterms:title": "OpenWebText",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Web Text Data"
        ],
        "dcat:keyword": [
            "Web text",
            "Language model training",
            "Common Crawl alternative"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Brown",
            "B. Mann",
            "N. Ryder",
            "M. Subbiah",
            "J. D. Kaplan",
            "P. Dhariwal",
            "A. Neelakantan",
            "P. Shyam",
            "G. Sastry",
            "A. Askell"
        ],
        "dcterms:description": "C4 is a large-scale dataset used for training language models, consisting of web pages and documents.",
        "dcterms:title": "C4",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Web Data"
        ],
        "dcat:keyword": [
            "Language model training",
            "Web pages",
            "Document dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "W. Foundation"
        ],
        "dcterms:description": "A dump of Wikipedia data from 2022, used for various natural language processing tasks.",
        "dcterms:title": "Wikipedia dump (2022)",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "https://dumps.wikimedia.org",
        "dcat:theme": [
            "Natural Language Processing",
            "Encyclopedic Data"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Text corpus",
            "Natural language processing"
        ],
        "dcat:landingPage": "https://dumps.wikimedia.org",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Gerlach",
            "F. Font-Clos"
        ],
        "dcterms:description": "A standardized corpus derived from Project Gutenberg for statistical analysis of natural language and quantitative linguistics.",
        "dcterms:title": "Gutenberg",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Literary Data"
        ],
        "dcat:keyword": [
            "Project Gutenberg",
            "Text corpus",
            "Statistical analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Zhu",
            "R. Kiros",
            "R. Zemel",
            "R. Salakhutdinov",
            "R. Urtsun",
            "A. Torralba",
            "S. Fidler"
        ],
        "dcterms:description": "BookCorpus is a dataset containing the text of books, used for training models to understand narrative and story-like structures.",
        "dcterms:title": "BookCorpus",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Literary Data"
        ],
        "dcat:keyword": [
            "Books",
            "Text corpus",
            "Narrative understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Taori",
            "I. Gulrajani",
            "T. Zhang",
            "Y. Dubois",
            "X. Li",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "Alpaca is a dataset consisting of instructions and demonstrations generated for instruction-following tasks in language models.",
        "dcterms:title": "Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Demonstrations",
            "Language model training"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    }
]