[
    {
        "dcterms:creator": [
            "D. Soudry",
            "E. Hoffer"
        ],
        "dcterms:description": "The dataset is used to study the loss surface of multilayer neural networks with ReLU activations, focusing on the behavior of local minima.",
        "dcterms:title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1702.05777",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Loss Surface",
            "ReLU Networks",
            "Local Minima"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Tian"
        ],
        "dcterms:description": "The dataset is utilized to analyze the population gradient for two-layered ReLU networks and its implications for convergence.",
        "dcterms:title": "An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Population Gradient",
            "ReLU Networks",
            "Convergence Analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "K. Zhong",
            "Z. Song",
            "P. Jain",
            "P. L. Bartlett",
            "I. S. Dhillon"
        ],
        "dcterms:description": "The dataset is used to establish recovery guarantees for one-hidden-layer neural networks, focusing on their performance in learning tasks.",
        "dcterms:title": "Recovery guarantees for one-hidden-layer neural networks",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Recovery Guarantees",
            "One-Hidden-Layer Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "P. Zhou",
            "J. Feng"
        ],
        "dcterms:description": "The dataset is referenced in the context of analyzing the landscape of deep learning algorithms, particularly focusing on their optimization properties.",
        "dcterms:title": "The landscape of deep learning algorithms",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1705.07038",
        "dcat:theme": [
            "Machine Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Optimization Landscape",
            "Deep Learning Algorithms"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "A. Brutzkus",
            "A. Globerson",
            "E. Malach",
            "S. Shalev-Shwartz"
        ],
        "dcterms:description": "The dataset is used to demonstrate that SGD can learn over-parameterized networks that generalize well on linearly separable data.",
        "dcterms:title": "SGD learns over-parameterized networks that provably generalize on linearly separable data",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1710.10174",
        "dcat:theme": [
            "Machine Learning",
            "Generalization"
        ],
        "dcat:keyword": [
            "SGD",
            "Over-Parameterized Networks",
            "Generalization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "P. Chaudhari",
            "A. Choromanska",
            "S. Soatto",
            "Y. LeCun"
        ],
        "dcterms:description": "The dataset is referenced in the context of biasing gradient descent into wide valleys to improve optimization in deep learning.",
        "dcterms:title": "Entropy-SGD: Biasing gradient descent into wide valleys",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Gradient Descent",
            "Wide Valleys",
            "Optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "A. Choromanska",
            "M. Henaff",
            "M. Mathieu",
            "G. B. Arous",
            "Y. LeCun"
        ],
        "dcterms:description": "The dataset is used to analyze the loss surfaces of multilayer networks, focusing on their properties and implications for optimization.",
        "dcterms:title": "The loss surfaces of multilayer networks",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Loss Surfaces",
            "Multilayer Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "S. S. Du",
            "J. D. Lee",
            "Y. Tian"
        ],
        "dcterms:description": "The dataset is referenced in the context of understanding when a convolutional filter is easy to learn, contributing to the analysis of neural networks.",
        "dcterms:title": "When is a convolutional filter easy to learn?",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1709.06129",
        "dcat:theme": [
            "Machine Learning",
            "Convolutional Networks"
        ],
        "dcat:keyword": [
            "Convolutional Filters",
            "Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "P. Frasconi",
            "M. Gori",
            "A. Tesi"
        ],
        "dcterms:description": "The dataset is used to analyze the successes and failures of backpropagation in neural networks, providing insights into their training dynamics.",
        "dcterms:title": "Successes and failures of backpropagation: A theoretical",
        "dcterms:issued": "1997",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Backpropagation",
            "Neural Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Gori",
            "A. Tesi"
        ],
        "dcterms:description": "The dataset is referenced in the context of analyzing local minima in backpropagation, contributing to the understanding of optimization in neural networks.",
        "dcterms:title": "On the problem of local minima in backpropagation",
        "dcterms:issued": "1992",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Local Minima",
            "Backpropagation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "K. Kawaguchi"
        ],
        "dcterms:description": "The dataset is used to explore the concept of deep learning without poor local minima, contributing to the theoretical understanding of neural networks.",
        "dcterms:title": "Deep learning without poor local minima",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Local Minima",
            "Deep Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "N. S. Keskar",
            "D. Mudigere",
            "J. Nocedal",
            "M. Smelyanskiy",
            "P. T. P. Tang"
        ],
        "dcterms:description": "The dataset is referenced in the context of large-batch training for deep learning, focusing on the generalization gap and sharp minima.",
        "dcterms:title": "On large-batch training for deep learning: Generalization gap and sharp minima",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Large-Batch Training",
            "Generalization Gap"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "T. Laurent",
            "J. H. von Brecht"
        ],
        "dcterms:description": "The dataset is used to analyze deep linear neural networks with arbitrary loss, focusing on the properties of local minima.",
        "dcterms:title": "Deep linear neural networks with arbitrary loss: All local minima are global",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Deep Linear Networks",
            "Global Minima"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Li",
            "Y. Yuan"
        ],
        "dcterms:description": "The dataset is referenced in the context of convergence analysis of two-layer neural networks with ReLU activation.",
        "dcterms:title": "Convergence analysis of two-layer neural networks with relu activation",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Convergence Analysis",
            "ReLU Activation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "S. Mei",
            "Y. Bai",
            "A. Montanari"
        ],
        "dcterms:description": "The dataset is used to study the landscape of empirical risk for non-convex losses, contributing to the understanding of optimization in neural networks.",
        "dcterms:title": "The landscape of empirical risk for non-convex losses",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1607.06534",
        "dcat:theme": [
            "Machine Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Empirical Risk",
            "Non-Convex Losses"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "I. Safran",
            "O. Shamir"
        ],
        "dcterms:description": "The dataset is referenced in the context of analyzing the quality of the initial basin in overspecified neural networks.",
        "dcterms:title": "On the quality of the initial basin in overspecified neural networks",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Overspecified Networks",
            "Initial Basin"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Soltanolkotabi"
        ],
        "dcterms:description": "The dataset is used to study the learning dynamics of ReLU networks via gradient descent.",
        "dcterms:title": "Learning relus via gradient descent",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Learning",
            "Gradient Descent"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning"
        ]
    }
]