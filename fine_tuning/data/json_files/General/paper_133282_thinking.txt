To extract datasets from the research paper titled "HAARNET: LARGE-SCALE LINEAR-MORPHOLOGICAL HYBRID NETWORK FOR RGB-D SEMANTIC SEGMENTATION" by Rick Groenendijk et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract mentions that HaarNet is evaluated on a set of sizeable real-world datasets, which indicates that datasets are involved.

Next, I will focus on the **experiments section** (section 4), where the authors describe the datasets used for their experiments. In this section, they explicitly mention three datasets:

1. **NYUv2**: This dataset is used for indoor semantic segmentation and consists of 795 training images and 654 test images, with 40 semantic classes.

2. **2D-3D-S**: This dataset is also for indoor semantic segmentation, containing 52,903 training images and 17,593 test images, with 13 semantic classes.

3. **CityScapes**: This dataset is used for outdoor semantic segmentation, comprising 2,975 training images and 500 test images, with 30 training classes and 19 test classes.

To ensure accuracy, I will also check the **References section** for full citations of these datasets. The relevant citations are:

- For **NYUv2**, the citation is:
  > Gupta, S., Girshick, R., Arbeláez, P., Malik, J. (2014). Learning rich features from RGB-D images for object detection and segmentation. In ECCV. pp. 345–360. Springer.

- For **2D-3D-S**, the citation is:
  > Armeni, I., Sax, S., Zamir, A.R., Savarese, S. (2017). Joint 2D-3D-Semantic Data for Indoor Scene Understanding. arXiv preprint arXiv:1702.01105.

- For **CityScapes**, the citation is:
  > Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B. (2016). The Cityscapes Dataset for Semantic Urban Scene Understanding. In CVPR.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review.