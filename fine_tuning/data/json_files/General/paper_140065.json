[
    {
        "dcterms:creator": [
            "Kunishou"
        ],
        "dcterms:description": "Japanese-Alpaca is fine-tuned with instruction data translated from English Alpaca via GPT-3.5, aimed at improving instruction-following capabilities in Japanese.",
        "dcterms:title": "Japanese-Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "Japanese",
        "dcterms:identifier": "https://github.com/kunishou/Japanese-Alpaca-LoRA",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction data",
            "Japanese language",
            "Fine-tuning"
        ],
        "dcat:landingPage": "https://github.com/kunishou/Japanese-Alpaca-LoRA",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "Stanford Alpaca is an instruction-following model based on LLaMA, providing a dataset for fine-tuning language models.",
        "dcterms:title": "Stanford Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction data",
            "Fine-tuning",
            "Language models"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Yi Chen",
            "Rui Wang",
            "Haiyun Jiang",
            "Shuming Shi",
            "Ruifeng Xu"
        ],
        "dcterms:description": "Vicuna is a dataset used for exploring the use of large language models for reference-free text quality evaluation.",
        "dcterms:title": "Vicuna",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Quality Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Quality evaluation",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Text Quality Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "MT Alpaca is a baseline instruction dataset machine-translated from Stanford Alpaca in English, used for fine-tuning language models.",
        "dcterms:title": "MT Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Machine translation",
            "Instruction data",
            "Fine-tuning"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Hirokazu Kiyomaru"
        ],
        "dcterms:description": "The Japanese Vicuna QA Benchmark is a repository containing a set of questions designed for evaluating Japanese language models.",
        "dcterms:title": "Japanese Vicuna QA Benchmark",
        "dcterms:issued": "2023",
        "dcterms:language": "Japanese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "QA Benchmark",
            "Japanese language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "GPT-4 Self-Instruct Data is a dataset generated by the authors using GPT-4 to create instruction data with minimal human effort.",
        "dcterms:title": "GPT-4 Self-Instruct Data",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Self-instruction",
            "Instruction data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "GPT-3.5 (Davinci-003) is a language model used as a baseline for comparison in the evaluation of instruction-following capabilities.",
        "dcterms:title": "GPT-3.5 (Davinci-003)",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://doi.org/10.48550/arXiv.2303.08774",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Language model",
            "Baseline"
        ],
        "dcat:landingPage": "https://doi.org/10.48550/arXiv.2303.08774",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]