[
    {
        "dcterms:creator": [
            "C. Clark",
            "K. Lee",
            "M.-W. Chang",
            "T. Kwiatkowski",
            "M. Collins",
            "K. Toutanova"
        ],
        "dcterms:description": "A reading comprehension dataset focused on naturally occurring yes/no questions, each instance includes a question, an excerpt from a passage, and an answer.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1905.10044",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Yes/No Questions",
            "Reading Comprehension",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "R. Zellers",
            "J. Gao",
            "Y. Choi"
        ],
        "dcterms:description": "A benchmark for evaluating and studying the capacity of natural language models in comprehending physical commonsense understanding.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Physical Interaction",
            "Commonsense Reasoning",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Sap",
            "H. Rashkin",
            "D. Chen",
            "R. LeBras",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset designed to measure the social and emotional intelligence of computational models through multiple-choice question answering.",
        "dcterms:title": "SIQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1904.09728",
        "dcat:theme": [
            "Natural Language Processing",
            "Social Intelligence"
        ],
        "dcat:keyword": [
            "Social Interaction",
            "Emotional Intelligence",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "A benchmark for physically situated commonsense natural language inference, comprising four-way multiple-choice problems.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1905.07830",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Natural Language Inference",
            "Commonsense Reasoning",
            "Multiple Choice"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "K. Sakaguchi",
            "R. L. Bras",
            "C. Bhagavatula",
            "Y. Choi"
        ],
        "dcterms:description": "A benchmark for commonsense reasoning consisting of a set of expert-crafted pronoun resolution problems.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Pronoun Resolution",
            "Commonsense Reasoning",
            "Natural Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "A compilation of natural science questions divided into a Challenge Set and an Easy Set.",
        "dcterms:title": "ARC",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1803.05457",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Natural Science",
            "Question Answering",
            "Challenge Set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "A dataset consisting of approximately 6000 questions designed for open book question answering.",
        "dcterms:title": "OBQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1809.02789",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Open Book",
            "Question Answering",
            "Knowledge Integration"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. S. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "A closed-book question answering benchmark encompassing over 650,000 question-answer evidence triples.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1705.03551",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Closed-Book",
            "Question Answering",
            "Evidence-Based"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "G. Lai",
            "Q. Xie",
            "H. Liu",
            "Y. Yang",
            "E. Hovy"
        ],
        "dcterms:description": "A large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1704.04683",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Examinations",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "X. Li",
            "T. Zhang",
            "Y. Dubois",
            "R. Taori",
            "I. Gulrajani",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "A single-turn benchmark featuring 805 questions across various topics, evaluated by GPT-4.",
        "dcterms:title": "AlpacaEval",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/tatsu-lab/alpaca_eval",
        "dcat:theme": [
            "Natural Language Processing",
            "Chat Evaluation"
        ],
        "dcat:keyword": [
            "Chat Evaluation",
            "Instruction-Following",
            "Benchmark"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/alpaca_eval",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Chat Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "L. Zheng",
            "W.-L. Chiang",
            "Y. Sheng",
            "S. Zhuang",
            "Z. Wu",
            "Y. Zhuang",
            "Z. Lin",
            "Z. Li",
            "D. Li",
            "E. Xing"
        ],
        "dcterms:description": "A multi-turn benchmark comprising 160 questions spanning eight diverse knowledge domains.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2306.05685",
        "dcat:theme": [
            "Natural Language Processing",
            "Chat Evaluation"
        ],
        "dcat:keyword": [
            "Multi-Turn",
            "Chat Evaluation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Chat Evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Generated dataset for fine-tuning and evaluating models for API calling tasks, as described in the paper.",
        "dcterms:title": "API Calling Dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "API Calling",
            "Task Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "API Calling"
        ]
    }
]