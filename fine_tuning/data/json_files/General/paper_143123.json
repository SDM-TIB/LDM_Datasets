[
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset used for evaluating language models in a few-shot setting, focusing on the ability to answer questions based on context.",
        "dcterms:title": "ARC-C",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Few-shot learning",
            "Language model evaluation",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset designed for evaluating language models in a few-shot context, focusing on answering questions based on provided information.",
        "dcterms:title": "ARC-E",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Few-shot learning",
            "Language model evaluation",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset for evaluating models on binary question answering tasks, focusing on whether a given statement is true or false.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Binary classification",
            "Question answering",
            "True/False"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset for evaluating models on commonsense reasoning tasks, where models must choose the most plausible continuation of a given context.",
        "dcterms:title": "Hellaswag",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Context continuation",
            "Language model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset for evaluating models on open-book question answering tasks, where models must answer questions based on a provided context.",
        "dcterms:title": "OBQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Open-book question answering",
            "Context-based reasoning",
            "Language model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset for evaluating models on physical reasoning tasks, where models must choose the correct answer based on physical principles.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Physical reasoning",
            "Question answering",
            "Language model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Physical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset for evaluating models on commonsense reasoning tasks, focusing on ambiguous pronouns and their resolutions.",
        "dcterms:title": "Winogrande",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Pronoun resolution",
            "Language model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Jonathan Tow",
            "Baber Abbasi",
            "Stella Biderman",
            "Sid Black",
            "Anthony DiPofi",
            "Charles Foster",
            "Laurence Golding",
            "Jeffrey Hsu",
            "Alain Le Noac’h",
            "Haonan Li",
            "Kyle McDonell",
            "Niklas Muennighoff",
            "Chris Ociepa",
            "Jason Phang",
            "Laria Reynolds",
            "Hailey Schoelkopf",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Eric Tang",
            "Anish Thite",
            "Ben Wang",
            "Kevin Wang",
            "Andy Zou"
        ],
        "dcterms:description": "A dataset for evaluating models on scientific question answering tasks, where models must answer questions based on scientific knowledge.",
        "dcterms:title": "SciQ",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Scientific reasoning",
            "Question answering",
            "Language model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    }
]