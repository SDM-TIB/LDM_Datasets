[
    {
        "dcterms:creator": [
            "Yue Zhang",
            "Ming Zhang",
            "Haipeng Yuan",
            "Shichun Liu",
            "Yongyao Shi",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "dcterms:description": "A dataset designed for evaluating large language models (LLMs) across various criteria including accuracy, fluency, informativeness, logical coherence, and harmlessness, with a total of 243,337 manual annotations and 57,511 automatic evaluation results.",
        "dcterms:title": "LLMEval",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/llmeval",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Large Language Models",
            "Evaluation",
            "Manual Annotation",
            "Automatic Evaluation"
        ],
        "dcat:landingPage": "https://github.com/llmeval",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yue Zhang",
            "Ming Zhang",
            "Haipeng Yuan",
            "Shichun Liu",
            "Yongyao Shi",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "dcterms:description": "A subset of the LLMEval dataset designed to evaluate LLMs using 17 different types of questions including classification, code, conversation, and more.",
        "dcterms:title": "LLMEval-1",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Evaluation",
            "Question Types",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "1",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yue Zhang",
            "Ming Zhang",
            "Haipeng Yuan",
            "Shichun Liu",
            "Yongyao Shi",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "dcterms:description": "A subset of the LLMEval dataset focused on evaluating LLMs in specialized domains with questions from 12 academic subjects.",
        "dcterms:title": "LLMEval-2",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Specialized Domains",
            "Academic Subjects",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "2",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Liang",
            "R. Bommasani",
            "T. Lee",
            "D. Tsipras",
            "D. Soylu",
            "M. Yasunaga",
            "Y. Zhang",
            "D. Narayanan",
            "Y. Wu",
            "A. Kumar",
            "B. Newman",
            "B. Yuan",
            "B. Yan",
            "Y. Zhang",
            "C. Cosgrove",
            "C. D. Manning",
            "C. RÂ´e",
            "D. Acosta-Navas",
            "D. A. Hudson",
            "E. Zelikman",
            "E. Durmus",
            "F. Ladhak",
            "F. Rong",
            "H. Ren",
            "H. Yao",
            "J. Wang",
            "K. Santhanam",
            "L. Orr",
            "L. Zheng",
            "M. Yuksekgonul",
            "M. Suzgun",
            "N. Kim",
            "N. Guha",
            "N. Chatterji",
            "O. Khattab",
            "P. Henderson",
            "Q. Huang",
            "R. Chi",
            "S. M. Xie",
            "S. Santurkar",
            "S. Ganguli",
            "T. Hashimoto",
            "T. Icard",
            "T. Zhang",
            "V. Chaudhary",
            "W. Wang",
            "X. Li",
            "Y. Mai",
            "Y. Zhang",
            "Y. Koreeda"
        ],
        "dcterms:description": "A comprehensive evaluation framework for language models that combines various existing datasets for holistic assessment.",
        "dcterms:title": "HELM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2211.09110",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Evaluation Framework",
            "Language Models",
            "Holistic Assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding through multiple-choice questions.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multitask Evaluation",
            "Language Understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Huang",
            "Y. Bai",
            "Z. Zhu",
            "J. Zhang",
            "J. Zhang",
            "T. Su",
            "J. Liu",
            "C. Lv",
            "Y. Zhang",
            "J. Lei",
            "Y. Fu",
            "M. Sun",
            "J. He"
        ],
        "dcterms:description": "A multi-level, multi-discipline evaluation suite for foundation models in Chinese.",
        "dcterms:title": "C-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.08322",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Chinese Evaluation",
            "Foundation Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "W. Zhong",
            "R. Cui",
            "Y. Guo",
            "Y. Liang",
            "S. Lu",
            "Y. Wang",
            "A. Saied",
            "W. Chen",
            "N. Duan"
        ],
        "dcterms:description": "A human-centric benchmark for evaluating foundation models.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.06364",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Human-Centric Evaluation",
            "Foundation Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Zhang",
            "V. Kishore",
            "F. Wu",
            "K. Q. Weinberger",
            "Y. Artzi"
        ],
        "dcterms:description": "A method for evaluating text generation using BERT.",
        "dcterms:title": "BERTScore",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1904.09675",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Text Generation",
            "Evaluation",
            "BERT"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Dubois",
            "X. Li",
            "R. Taori",
            "T. Zhang",
            "I. Gulrajani",
            "J. Ba",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "A simulation framework for methods that learn from human feedback.",
        "dcterms:title": "AlpacaFarm",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.14387",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Human Feedback",
            "Simulation Framework",
            "Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "L. Zheng",
            "W.-L. Chiang",
            "Y. Sheng",
            "S. Zhuang",
            "Z. Wu",
            "Y. Zhuang",
            "Z. Lin",
            "Z. Li",
            "D. Li",
            "E. P. Xing",
            "J. E. Gonzalez",
            "I. Stoica"
        ],
        "dcterms:description": "A framework for judging LLMs using MT-Bench and Chatbot Arena.",
        "dcterms:title": "Chatbot Arena",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.05685",
        "dcat:theme": [
            "Language Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "LLM Evaluation",
            "Benchmark",
            "Chatbot"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]