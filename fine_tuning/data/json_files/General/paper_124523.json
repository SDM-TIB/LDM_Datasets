[
    {
        "dcterms:creator": [
            "A. R. Fabbri",
            "W. Krýscínski",
            "B. McCann",
            "C. Xiong",
            "R. Socher",
            "D. Radev"
        ],
        "dcterms:description": "SummEval is a benchmark that compares different evaluation methods for summarization, providing human ratings for fluency, coherence, consistency, and relevance.",
        "dcterms:title": "SummEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Summarization evaluation",
            "Human ratings",
            "Fluency",
            "Coherence",
            "Consistency",
            "Relevance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Mehri",
            "M. Eskenazi"
        ],
        "dcterms:description": "Topical-Chat is a testbed for meta-evaluating different evaluators on dialogue response generation systems that use knowledge, providing human ratings on naturalness, coherence, engagingness, and groundedness.",
        "dcterms:title": "Topical-Chat",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Generation",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Dialogue evaluation",
            "Naturalness",
            "Coherence",
            "Engagingness",
            "Groundedness"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Response Generation Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "A. Wang",
            "K. Cho",
            "M. Lewis"
        ],
        "dcterms:description": "QAGS is a benchmark for evaluating hallucinations in summarization tasks, measuring the consistency dimension of summaries across different summarization datasets.",
        "dcterms:title": "QAGS",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Factual consistency",
            "Summarization evaluation",
            "Hallucinations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Summarization Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "K. M. Hermann",
            "T. Kocisky",
            "E. Grefenstette",
            "L. Espeholt",
            "W. Kay",
            "M. Suleyman",
            "P. Blunsom"
        ],
        "dcterms:description": "CNN/DailyMail is a dataset used for training and evaluating models for text summarization, containing news articles and their corresponding summaries.",
        "dcterms:title": "CNN/DailyMail",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "News Articles"
        ],
        "dcat:keyword": [
            "News summarization",
            "Text summarization dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "S. Narayan",
            "S. B. Cohen",
            "M. Lapata"
        ],
        "dcterms:description": "XSum is a dataset designed for extreme summarization, where the goal is to produce a single-sentence summary of a news article.",
        "dcterms:title": "XSum",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization"
        ],
        "dcat:keyword": [
            "Extreme summarization",
            "Single-sentence summary"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "T. Zhang",
            "F. Ladhak",
            "E. Durmus",
            "P. Liang",
            "K. McKeown",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "The dataset from Zhang et al. (2023) benchmarks large language models for news summarization, providing a comparison of human-written and LLM-generated summaries.",
        "dcterms:title": "Zhang et al. (2023) dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "News summarization",
            "Large language models",
            "Human evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization Evaluation"
        ]
    }
]