[
    {
        "dcterms:creator": [
            "M. Tapaswi",
            "Y. Zhu",
            "R. Stiefelhagen",
            "A. Torralba",
            "R. Urtasun",
            "S. Fidler"
        ],
        "dcterms:description": "The MovieQA dataset consists of 408 movies and 14,944 questions, requiring systems to demonstrate story comprehension by successfully answering multiple choice questions relating to videos and subtitles taken from movies.",
        "dcterms:title": "MovieQA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-modal Question Answering",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Movie dataset",
            "Question answering",
            "Multi-modal context"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Kembhavi",
            "M. Seo",
            "D. Schwenk",
            "J. Choi",
            "A. Farhadi",
            "H. Hajishirzi"
        ],
        "dcterms:description": "TextbookQA consists of lessons from middle school science curricula with both textual and diagrammatic context, designed for multimodal machine comprehension.",
        "dcterms:title": "TextbookQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-modal Question Answering",
            "Educational Assessment"
        ],
        "dcat:keyword": [
            "Textbook dataset",
            "Question answering",
            "Multi-modal context"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "K.-M. Kim",
            "M.-O. Heo",
            "S.-H. Choi",
            "B.-T. Zhang"
        ],
        "dcterms:description": "PororoQA is constructed from children cartoon Pororo with video, dialogue, and description, aimed at video story question answering.",
        "dcterms:title": "PororoQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-modal Question Answering",
            "Children's Education"
        ],
        "dcat:keyword": [
            "Children's cartoon dataset",
            "Question answering",
            "Video story comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "L. Jiang",
            "J. Liang",
            "L. Cao",
            "Y. Kalantidis",
            "S. Farfade",
            "A. Hauptmann"
        ],
        "dcterms:description": "MemexQA is a visual question answering dataset that integrates visual and textual information for answering questions about visual content.",
        "dcterms:title": "MemexQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1708.01336",
        "dcat:theme": [
            "Visual Question Answering",
            "Multi-modal Learning"
        ],
        "dcat:keyword": [
            "Visual dataset",
            "Question answering",
            "Memory networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Liang",
            "L. Jiang",
            "L. Cao",
            "L.-J. Li",
            "A. Hauptmann"
        ],
        "dcterms:description": "JHMDB is a dataset for visual question answering that focuses on focal visual-text attention for understanding personal photos and descriptions.",
        "dcterms:title": "JHMDB",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Question answering",
            "Action understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "S. Antol",
            "A. Agrawal",
            "J. Lu",
            "M. Mitchell",
            "D. Batra",
            "C. L. Zitnick",
            "D. Parikh"
        ],
        "dcterms:description": "VQA is a dataset designed for visual question answering, where the task is to answer questions based on images.",
        "dcterms:title": "VQA",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Question answering",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    }
]