To extract datasets from the research paper titled "Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation" by Ayan Kumar Bhunia et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract discusses the challenges of text recognition and hints at the use of benchmark datasets, which suggests that specific datasets may be referenced later in the paper.

Next, I will focus on the **experiments section** (section 4) where the authors typically describe the datasets used for training and evaluation. In this section, the authors mention using several datasets for both Scene Text Recognition (STR) and Handwriting Text Recognition (HTR). 

I will specifically look for the following datasets mentioned in the paper:

1. **Synth90k**: This dataset is used for training STR models and is described as containing 8 million images.
2. **SynthText**: Another dataset for STR training, containing 6 million images.
3. **IIIT5K-Words**: This dataset consists of 5000 cropped words from Google image search, used for evaluation.
4. **Street View Text (SVT)**: This dataset includes 647 images collected from Google Street View, known for its challenging conditions.
5. **ICDAR 2013 (IC13)**: Contains 848 cropped word patches, primarily regular images.
6. **ICDAR 2015 (IC15)**: This dataset has 2077 irregular word images.
7. **CUTE80**: A dataset with high-resolution images containing curved text.
8. **IAM**: A dataset for HTR containing 1,15,320 words.
9. **RIMES**: Another HTR dataset with 66,982 words.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset. The citations are crucial for proper attribution and to allow others to access the datasets.

Here are the full citations for the datasets mentioned:

- **Synth90k**: 
  > Gupta, A., Vedaldi, A., & Zisserman, A. (2016). Synthetic data for text localisation in natural images. In CVPR.

- **SynthText**: 
  > Gupta, A., Vedaldi, A., & Zisserman, A. (2016). Synthetic data for text localisation in natural images. In CVPR.

- **IIIT5K-Words**: 
  > Mishra, A., Alahari, K., & Jawahar, C. V. (2012). Scene text recognition using higher order language priors. In BMVC.

- **Street View Text (SVT)**: 
  > Wang, K., Babenko, B., & Belongie, S. (2011). End-to-end scene text recognition. In ICCV.

- **ICDAR 2013 (IC13)**: 
  > Karatzas, D., Shafait, F., Uchida, S., et al. (2013). ICDAR 2013 robust reading competition. In ICDAR.

- **ICDAR 2015 (IC15)**: 
  > Karatzas, D., et al. (2015). ICDAR 2015 competition on robust reading. In ICDAR.

- **CUTE80**: 
  > Risnumawan, A., Shivakumara, P., Chan, C. S., & Tan, C. L. (2014). A robust arbitrary text detection system for natural scene images. Expert Systems with Applications.

- **IAM**: 
  > Marti, U.-V., & Bunke, H. (2002). The IAM-database: An English sentence database for offline handwriting recognition. In IJDAR.

- **RIMES**: 
  > G. G. (2009). RIMES evaluation campaign for handwritten mail processing. In ICFHR.

Now that I have gathered all the necessary information about the datasets and their citations, I will compile this into a structured format for further use or processing.