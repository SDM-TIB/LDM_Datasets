[
    {
        "dcterms:creator": [
            "J. Lei",
            "L. Yu",
            "M. Bansal",
            "T. L. Berg"
        ],
        "dcterms:description": "TVQA is a large-scale multi-modal video QA dataset based on 6 popular TV shows spanning 3 genres: medical dramas, sitcoms, and crime shows. It consists of 152.5K QA pairs from 21.8K video clips, spanning over 460 hours of video. The dataset requires systems to jointly comprehend subtitles-based dialogue to correctly answer questions and localize relevant moments within video frames.",
        "dcterms:title": "TVQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering"
        ],
        "dcat:keyword": [
            "Multi-modal",
            "Video QA",
            "Temporal localization",
            "Subtitles"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Temporal Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "J. Lei",
            "L. Yu",
            "T. L. Berg",
            "M. Bansal"
        ],
        "dcterms:description": "TVQA+ is a subset of the TVQA dataset that augments it with frame-level bounding box annotations, linking video objects to visual concepts in questions and answers. It refines the temporal boundary so that the temporal annotations are more precise than in TVQA.",
        "dcterms:title": "TVQA+",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering"
        ],
        "dcat:keyword": [
            "Multi-modal",
            "Video QA",
            "Object grounding",
            "Temporal localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Temporal Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "M. Tapaswi",
            "Y. Zhu",
            "R. Stiefelhagen",
            "A. Torralba",
            "R. Urtasun",
            "S. Fidler"
        ],
        "dcterms:description": "MovieQA is a dataset designed for understanding stories in movies through question-answering. It includes various questions related to movie plots and requires reasoning about the visual and textual content.",
        "dcterms:title": "MovieQA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering"
        ],
        "dcat:keyword": [
            "Movie understanding",
            "Question Answering",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Jang",
            "Y. Song",
            "Y. Yu",
            "K. Kim",
            "G. Kim"
        ],
        "dcterms:description": "TGIF-QA is a dataset aimed at spatio-temporal reasoning in visual question answering. It combines deep appearance and motion features with spatial and temporal attention for accurate question answering.",
        "dcterms:title": "TGIF-QA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering"
        ],
        "dcat:keyword": [
            "Spatio-temporal reasoning",
            "Visual Question Answering",
            "Motion features"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]