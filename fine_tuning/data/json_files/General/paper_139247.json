[
    {
        "dcterms:creator": [
            "Ondrej Bojar",
            "Christian Buck",
            "Christian Federmann",
            "Barry Haddow",
            "Philipp Koehn",
            "Johannes Leveling",
            "Christof Monz",
            "Pavel Pecina",
            "Matt Post",
            "Herve Saint-Amand",
            "Radu Soricut",
            "Lucia Specia",
            "Ale≈° Tamchyna"
        ],
        "dcterms:description": "A dataset for statistical machine translation from English to French, used to evaluate translation quality.",
        "dcterms:title": "WMT14 (En-Fr)",
        "dcterms:issued": "2014",
        "dcterms:language": "English, French",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Translation"
        ],
        "dcat:keyword": [
            "Translation",
            "Language Models",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Translation"
        ]
    },
    {
        "dcterms:creator": [
            "Alon Talmor",
            "Jonathan Herzig",
            "Nicholas Lourie",
            "Jonathan Berant"
        ],
        "dcterms:description": "A question answering challenge targeting commonsense knowledge, designed to evaluate models' understanding of everyday situations.",
        "dcterms:title": "CommonsenseQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Commonsense Knowledge",
            "Natural Language Understanding",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Hector Levesque",
            "Ernest Davis",
            "Leora Morgenstern"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of AI systems to understand and resolve ambiguities in natural language.",
        "dcterms:title": "WSC",
        "dcterms:issued": "2012",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Understanding"
        ],
        "dcat:keyword": [
            "Ambiguity Resolution",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "C. Xu",
            "et al."
        ],
        "dcterms:description": "A large-scale Chinese natural language inference dataset designed to evaluate models' understanding of logical entailment.",
        "dcterms:title": "CMNLI",
        "dcterms:issued": "2020",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "arXiv:2010.05444",
        "dcat:theme": [
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "Chinese Language",
            "Natural Language Inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard Hovy"
        ],
        "dcterms:description": "A large-scale reading comprehension dataset derived from examinations, designed to evaluate models' reading comprehension abilities.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1704.04683",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Todor Mihaylov",
            "Peter Clark",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "dcterms:description": "A dataset for open book question answering, where models must answer questions using a provided knowledge base.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Open Book Question Answering",
            "Knowledge Base"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Christopher Clark",
            "Kenton Lee",
            "Ming-Wei Chang",
            "Tom Kwiatkowski",
            "Michael Collins",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A dataset exploring the difficulty of natural yes/no questions, designed to evaluate models' binary classification abilities.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1905.10044",
        "dcat:theme": [
            "Binary Question Answering"
        ],
        "dcat:keyword": [
            "Yes/No Questions",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Binary Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Hector Levesque",
            "Ernest Davis",
            "Leora Morgenstern"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of AI systems to understand and resolve ambiguities in natural language.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2012",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Understanding"
        ],
        "dcat:keyword": [
            "Ambiguity Resolution",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su",
            "Junteng Liu",
            "Chuancheng Lv",
            "Yikai Zhang",
            "Jiayi Lei",
            "Yao Fu",
            "Maosong Sun",
            "Junxian He"
        ],
        "dcterms:description": "A multi-level multi-discipline Chinese evaluation suite for foundation models, designed to assess various capabilities.",
        "dcterms:title": "C-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "arXiv:2305.08322",
        "dcat:theme": [
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Chinese Language",
            "Evaluation",
            "Foundation Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding across various domains.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Language Understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multitask Learning"
        ]
    }
]