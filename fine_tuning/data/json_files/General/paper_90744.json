[
    {
        "dcterms:creator": [
            "Pradeep Dasigi",
            "Nelson F. Liu",
            "Ana Marasovic",
            "Noah A. Smith",
            "Matt Gardner"
        ],
        "dcterms:description": "QUOREF is a reading comprehension dataset with questions requiring coreferential reasoning, where 78% of the questions cannot be answered without coreference resolution.",
        "dcterms:title": "QUOREF",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Coreferential Reasoning"
        ],
        "dcat:keyword": [
            "Coreference",
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Adam Fisch",
            "Alon Talmor",
            "Robin Jia",
            "Minjoon Seo",
            "Eunsol Choi",
            "Danqi Chen"
        ],
        "dcterms:description": "MRQA is a dataset that includes paragraphs from different sources and questions with various styles, used to evaluate the performance of models in diverse domains.",
        "dcterms:title": "MRQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Diverse Domains"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "SQuAD is a dataset containing over 100,000 questions for machine comprehension of text, designed for evaluating reading comprehension systems.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Machine Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Matthew Dunn",
            "Levent Sagun",
            "Mike Higgins",
            "V. Ugur Güneş",
            "Volkan Cirik",
            "Kyunghyun Cho"
        ],
        "dcterms:description": "NewsQA is a question answering dataset that is augmented with context from a search engine, designed to evaluate reading comprehension.",
        "dcterms:title": "NewsQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Search Engine"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Matthew Dunn",
            "Levent Sagun",
            "Mike Higgins",
            "V. Ugur Güneş",
            "Volkan Cirik",
            "Kyunghyun Cho"
        ],
        "dcterms:description": "SearchQA is a new question answering dataset augmented with context from a search engine, designed to evaluate reading comprehension.",
        "dcterms:title": "SearchQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Search Engine"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S. Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "TriviaQA is a large-scale distantly supervised challenge dataset for reading comprehension, designed to evaluate question answering systems.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Distant Supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Zhilin Yang",
            "Peng Qi",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "William W. Cohen",
            "Ruslan Salakhutdinov",
            "Christopher D. Manning"
        ],
        "dcterms:description": "HotpotQA is a dataset for diverse, explainable multi-hop question answering, requiring models to synthesize information from multiple sources.",
        "dcterms:title": "HotpotQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Multi-hop Reasoning",
            "Explainable AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-hop Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur P. Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "Natural Questions is a benchmark for question answering research, providing a large dataset of questions and answers from real user queries.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yuan Yao",
            "Deming Ye",
            "Peng Li",
            "Xu Han",
            "Yankai Lin",
            "Zhenghao Liu",
            "Zhiyuan Liu",
            "Lixin Huang",
            "Jie Zhou",
            "Maosong Sun"
        ],
        "dcterms:description": "DocRED is a large-scale document-level relation extraction dataset that requires models to extract relations between entities by synthesizing information from the entire document.",
        "dcterms:title": "DocRED",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Relation Extraction"
        ],
        "dcat:keyword": [
            "Document-level",
            "Relation Extraction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Relation Extraction"
        ]
    },
    {
        "dcterms:creator": [
            "James Thorne",
            "Christos Christodoulopoulos",
            "Andreas Vlachos"
        ],
        "dcterms:description": "FEVER is a large-scale dataset for fact extraction and verification, consisting of claims and evidence from Wikipedia.",
        "dcterms:title": "FEVER",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fact Verification"
        ],
        "dcat:keyword": [
            "Fact Extraction",
            "Verification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Fact Verification"
        ]
    },
    {
        "dcterms:creator": [
            "Kellie Webster",
            "Marta Recasens",
            "Vera Axelrod",
            "Jason Baldridge"
        ],
        "dcterms:description": "GAP is a balanced corpus of gendered ambiguous pronouns, designed to study gender bias in coreference resolution.",
        "dcterms:title": "GAP",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Coreference Resolution"
        ],
        "dcat:keyword": [
            "Gender Bias",
            "Pronoun Resolution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Coreference Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "Altaf Rahman",
            "Vincent Ng"
        ],
        "dcterms:description": "DPR is a dataset for resolving complex cases of definite pronouns, used in the Winograd Schema Challenge.",
        "dcterms:title": "DPR",
        "dcterms:issued": "2012",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Coreference Resolution"
        ],
        "dcat:keyword": [
            "Pronoun Resolution",
            "Winograd Schema"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Coreference Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "Ernest Davis",
            "Leora Morgenstern",
            "Charles L. Ortiz Jr."
        ],
        "dcterms:description": "WSC is the first Winograd Schema Challenge dataset, designed to evaluate coreference resolution systems.",
        "dcterms:title": "WSC",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Coreference Resolution"
        ],
        "dcat:keyword": [
            "Winograd Schema",
            "Coreference Resolution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Coreference Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "Rachel Rudinger",
            "Jason Naradowsky",
            "Brian Leonard",
            "Benjamin Van Durme"
        ],
        "dcterms:description": "Winogender is a dataset designed to study gender bias in coreference resolution, focusing on gendered ambiguous pronouns.",
        "dcterms:title": "Winogender",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Coreference Resolution"
        ],
        "dcat:keyword": [
            "Gender Bias",
            "Coreference Resolution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Coreference Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "Hector J. Levesque"
        ],
        "dcterms:description": "PDP is a dataset for the Winograd Schema Challenge, focusing on commonsense reasoning and coreference resolution.",
        "dcterms:title": "PDP",
        "dcterms:issued": "2011",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Coreference Resolution"
        ],
        "dcat:keyword": [
            "Commonsense Reasoning",
            "Coreference Resolution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Coreference Resolution"
        ]
    }
]