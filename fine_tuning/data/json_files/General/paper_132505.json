[
    {
        "dcterms:creator": [
            "Leo Gao",
            "Stella Biderman",
            "Sid Black",
            "Laurence Golding",
            "Travis Hoppe",
            "Charles Foster",
            "Jason Phang",
            "Horace He",
            "Anish Thite",
            "Noa Nabeshima"
        ],
        "dcterms:description": "An 800GB dataset of diverse text for language modeling, used as a calibration set in the context of quantization methods.",
        "dcterms:title": "The Pile",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2101.00027",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Language modeling",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Vineet Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano",
            "Christopher Hesse",
            "John Schulman"
        ],
        "dcterms:description": "A dataset containing grade school math questions, used as a calibration set for quantization methods.",
        "dcterms:title": "GSM8k",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2110.14168",
        "dcat:theme": [
            "Mathematics",
            "Education"
        ],
        "dcat:keyword": [
            "Math dataset",
            "Word problems",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Chen, J.",
            "Tworek, J.",
            "Jun, H.",
            "Yuan, Q.",
            "Pinto, H. P. d. O.",
            "Kaplan, J.",
            "Edwards, H.",
            "Burda, Y.",
            "Joseph, N.",
            "Brockman, G."
        ],
        "dcterms:description": "A dataset that includes hand-crafted programming problems written in Python, used for evaluating code generation capabilities.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2107.03374",
        "dcat:theme": [
            "Programming",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Code dataset",
            "Programming problems",
            "Evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Vineet Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano",
            "Christopher Hesse",
            "John Schulman"
        ],
        "dcterms:description": "A crowd-sourced entry-level Python problem set, used as a calibration set for quantization methods.",
        "dcterms:title": "MBPP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2108.07732",
        "dcat:theme": [
            "Programming",
            "Education"
        ],
        "dcat:keyword": [
            "Python dataset",
            "Programming problems",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Stella Biderman",
            "Sid Black",
            "Laurence Golding",
            "Travis Hoppe",
            "Charles Foster",
            "Jason Phang",
            "Horace He",
            "Anish Thite",
            "Noa Nabeshima"
        ],
        "dcterms:description": "A dataset for evaluating multi-task language understanding, used as a calibration set for quantization methods.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multi-task dataset",
            "Language understanding",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Ronan Le Bras",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset for reasoning about physical commonsense in natural language, used as a calibration set for quantization methods.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense dataset",
            "Reasoning",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset for evaluating commonsense reasoning abilities, used as a calibration set for quantization methods.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense dataset",
            "Reasoning",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "An adversarial Winograd schema challenge dataset, used as a calibration set for quantization methods.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense dataset",
            "Reasoning",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "A dataset for evaluating reasoning abilities in question answering, used as a calibration set for quantization methods.",
        "dcterms:title": "ARC-easy",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering dataset",
            "Reasoning",
            "Calibration set"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    }
]