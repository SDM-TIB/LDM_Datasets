[
    {
        "dcterms:creator": [
            "Alexander R. Fabbri",
            "Wojciech Krýscínski",
            "Bryan McCann",
            "Caiming Xiong",
            "Richard Socher",
            "Dragomir Radev"
        ],
        "dcterms:description": "The human-annotated part of the dataset consists of 100 texts, each text is accompanied by 17 summaries generated by 17 systems. All 1700 text-summary pairs are annotated (on scale 1 to 5) by 3 experts for 4 qualities: consistency, relevance, coherence, and fluency.",
        "dcterms:title": "SummEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/Yale-LILY/SummEval",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Summarization evaluation",
            "Expert scores",
            "Text-summary pairs"
        ],
        "dcat:landingPage": "https://github.com/Yale-LILY/SummEval",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Summarization Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Oleg Vasilyev",
            "Vedant Dharnidharka",
            "John Bohannon"
        ],
        "dcterms:description": "A dataset for human-free quality estimation of document summaries.",
        "dcterms:title": "BLANC",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/PrimerAI/blanc",
        "dcat:theme": [
            "Quality Estimation",
            "Text Summarization"
        ],
        "dcat:keyword": [
            "Quality estimation",
            "Document summaries",
            "Human-free evaluation"
        ],
        "dcat:landingPage": "https://github.com/PrimerAI/blanc",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Quality Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "Annie Louis",
            "Ani Nenkova"
        ],
        "dcterms:description": "A method for automatically evaluating content selection in summarization without human models.",
        "dcterms:title": "Jensen-Shannon",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation Metrics",
            "Text Summarization"
        ],
        "dcat:keyword": [
            "Content selection",
            "Automatic evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Content Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Thomas Scialom",
            "Sylvain Lamprier",
            "Benjamin Piwowarski",
            "Jacopo Staiano"
        ],
        "dcterms:description": "A dataset that provides unsupervised metrics for reinforced summarization models.",
        "dcterms:title": "SummaQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Unsupervised metrics",
            "Reinforced summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Summarization Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yang Gao",
            "Wei Zhao",
            "Steffen Eger"
        ],
        "dcterms:description": "A dataset aimed at developing new unsupervised evaluation metrics for multi-document summarization.",
        "dcterms:title": "SUPERT",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation Metrics",
            "Multi-document Summarization"
        ],
        "dcat:keyword": [
            "Unsupervised evaluation",
            "Multi-document summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Summarization Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Kishore Papineni",
            "Salim Roukos",
            "Todd Ward",
            "Wei-Jing Zhu"
        ],
        "dcterms:description": "A method for automatic evaluation of machine translation.",
        "dcterms:title": "BLEU",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Translation",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Automatic evaluation",
            "Machine translation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Machine Translation Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Chin-Yew Lin"
        ],
        "dcterms:description": "A package for automatic evaluation of summaries.",
        "dcterms:title": "ROUGE",
        "dcterms:issued": "2004",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Automatic evaluation",
            "Summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Summarization Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Tianyi Zhang",
            "Varsha Kishore",
            "Felix Wu",
            "Kilian Q. Weinberger",
            "Yoav Artzi"
        ],
        "dcterms:description": "A method for evaluating text generation using BERT.",
        "dcterms:title": "BERTScore",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1904.09675",
        "dcat:theme": [
            "Text Generation",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Text evaluation",
            "BERT"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Text Generation Evaluation"
        ]
    }
]