[
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large scale distantly supervised challenge dataset for reading comprehension, used to evaluate closed-book question answering performance.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Closed-book QA",
            "Distant supervision",
            "Reading comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "A benchmark for question answering research that provides a dataset of questions and answers.",
        "dcterms:title": "Natural Questions (NQ)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Jonathan Berant",
            "Andrew Chou",
            "Roy Frostig",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset for semantic parsing on Freebase from question-answer pairs.",
        "dcterms:title": "WebQuestions (WQ)",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Semantic Parsing"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Semantic Parsing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Machine Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Sheng Zhang",
            "Xiaodong Liu",
            "Jingjing Liu",
            "Jianfeng Gao",
            "Kevin Duh",
            "Benjamin Van Durme"
        ],
        "dcterms:description": "A dataset designed to bridge the gap between human and machine commonsense reading comprehension.",
        "dcterms:title": "ReCoRD",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense Reading",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]