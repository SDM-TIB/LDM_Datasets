[
    {
        "dcterms:creator": [
            "Niklas Muennighoff",
            "Qian Liu",
            "Armel Zebaze",
            "Qinkai Zheng",
            "Binyuan Hui",
            "Terry Yue Zhuo",
            "Swayam Singh",
            "Xiangru Tang",
            "Leandro von Werra",
            "Shayne Longpre"
        ],
        "dcterms:description": "HumanEvalPack extends Python problems of the Humaneval Benchmark to five additional commonly used programming languages, namely JavaScript, Java, Go, C++, and Rust, to test three coding tasks: generation, explanation, and fixing.",
        "dcterms:title": "HumanEvalPack",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Code Understanding"
        ],
        "dcat:keyword": [
            "Python",
            "JavaScript",
            "Java",
            "Go",
            "C++",
            "Rust",
            "code tasks",
            "generation",
            "explanation",
            "fixing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation",
            "Code Explanation",
            "Code Fixing"
        ]
    },
    {
        "dcterms:creator": [
            "Daya Guo",
            "Canwen Xu",
            "Nan Duan",
            "Jian Yin",
            "Julian McAuley"
        ],
        "dcterms:description": "Long Code Completion (LCC) tests a model’s ability to predict the next line of code from long repository-based context for Python, Java, and C#. The benchmark’s context length spans 1/2K through 8K+ tokens.",
        "dcterms:title": "Long Code Completion (LCC)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2306.14893",
        "dcat:theme": [
            "Code Completion"
        ],
        "dcat:keyword": [
            "code completion",
            "long context",
            "Python",
            "Java",
            "C#"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2306.14893",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Completion"
        ]
    },
    {
        "dcterms:creator": [
            "Tianyang Liu",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "dcterms:description": "RepoBench-P tests the model’s next line code completion ability for long-context input, focusing on repository-level code auto-completion systems.",
        "dcterms:title": "RepoBench-P",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2306.03091",
        "dcat:theme": [
            "Code Completion"
        ],
        "dcat:keyword": [
            "repository-level",
            "code auto-completion",
            "long context"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2306.03091",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Completion"
        ]
    },
    {
        "dcterms:creator": [
            "Jiawei Liu",
            "Jia Le Tian",
            "Vijay Daita",
            "Yuxiang Wei",
            "Yifeng Ding",
            "Yuhan Katherine Wang",
            "Jun Yang",
            "Lingming Zhang"
        ],
        "dcterms:description": "RepoQA is an advanced Needle-in-the-Haystack test that focuses on testing LLMs’ capabilities on long-context code understanding and retrieval.",
        "dcterms:title": "RepoQA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2406.06025",
        "dcat:theme": [
            "Code Understanding",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "long-context",
            "code understanding",
            "retrieval"
        ],
        "dcat:landingPage": "arXiv:2406.06025",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Understanding",
            "Information Retrieval"
        ]
    }
]