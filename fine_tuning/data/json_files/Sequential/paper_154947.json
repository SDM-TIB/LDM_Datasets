[
    {
        "dcterms:creator": [
            "Stephen Merity"
        ],
        "dcterms:description": "The ENWIK8 dataset is 100M UTF encoded bytes that were from an XML dump of the English version of Wikipedia, used for character modeling tasks.",
        "dcterms:title": "ENWIK8",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Character Modeling"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Character prediction",
            "Compression task"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Character Prediction"
        ]
    }
]