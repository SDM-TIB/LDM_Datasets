To extract datasets from the research paper titled "Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks" by Behzad Hasani and Mohammad H. Mahoor, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the proposed method is evaluated using **four publicly available databases**, which suggests that specific datasets will be listed.

Next, I will focus on **section 4 (Experiments and results)**, particularly **subsection 4.1 (Face databases)**, where the authors describe the datasets used for evaluation. Here, they explicitly mention four databases:

1. **MMI Database**: This database contains more than 20 subjects with various ethnicities and includes sequences of facial expressions transitioning from neutral to apex and back to neutral. The authors extracted static frames resulting in 11,500 images.

2. **CK+ Database**: The extended Cohn-Kanade database consists of 593 videos from 123 subjects, with 327 sequences labeled for facial expressions. The sequences start from a neutral state and end at the apex of one of the six basic expressions.

3. **FERA Database**: The GEMEP-FERA database is a subset of the GEMEP corpus, containing 87 image sequences from 7 subjects, annotated with action units and holistic expressions. The authors extracted around 7,000 images from this database.

4. **DISFA Database**: This database consists of recordings of spontaneous facial expressions from 27 subjects, coded for action unit intensity. The authors mention that they obtained around 89,000 images from this database.

After identifying the datasets, I will check the **References section** for full citations of these datasets. The citations are as follows:

- For **MMI Database**:
  > M. Pantic, M. Valstar, R. Rademaker, and L. Maat. *Web-based database for facial expression analysis*. In 2005 IEEE international conference on multimedia and Expo, pages 5–pp. IEEE, 2005.

- For **CK+ Database**:
  > P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews. *The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-speciﬁed expression*. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, pages 94–101. IEEE, 2010.

- For **FERA Database**:
  > T. Bänziger and K. R. Scherer. *Introducing the geneva multimodal emotion portrayal (gemep) corpus*. Blueprint for affective computing: A sourcebook, pages 271–294, 2010.

- For **DISFA Database**:
  > S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and J. F. Cohn. *Disfa: A spontaneous facial action intensity database*. IEEE Transactions on Affective Computing, 4(2):151–160, 2013.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review.