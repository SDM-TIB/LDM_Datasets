To extract datasets from the research paper titled "CNN-LSTM models for Multi-Speaker Source Separation using Bayesian Hyper Parameter Optimization" by Jeroen Zegers and Hugo Van Hamme, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract discusses deep learning approaches for multi-speaker source separation but does not specify any datasets. Therefore, I will need to look deeper into the paper.

Next, I will focus on **section 4 (Experiments)**, which typically contains details about the datasets used for training and testing. In this section, the authors mention that the experiments were conducted on two-speaker mixtures created from the **Wall Street Journal 0 (WSJ0) corpus**. This indicates that the WSJ0 corpus is a dataset used in their experiments.

I will also check the **References section** to find the full citation for the WSJ0 corpus. The relevant citation is:

- For **Wall Street Journal 0 (WSJ0) corpus**, the citation is:
  > J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe. "Deep clustering: Discriminative embeddings for segmentation and separation." In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 31â€“35, 2016.

Since the paper does not mention any other datasets explicitly, I will conclude that the WSJ0 corpus is the primary dataset used for the experiments.

Now, I will compile the dataset entry for the WSJ0 corpus, ensuring to include the full citation as required. 

After this, I will prepare the dataset entry in the specified format for further processing or review.