[
    {
        "dcterms:creator": [
            "Y. Wu",
            "M. Schuster",
            "Z. Chen",
            "Q. V. Le",
            "M. Norouzi",
            "W. Macherey",
            "M. Krikun",
            "Y. Cao",
            "Q. Gao",
            "K. Macherey"
        ],
        "dcterms:description": "The WMT’14 dataset contains 850M words from English-French parallel corpora of UN, Europarl, news commentary, and crawled corpora, used for machine translation tasks.",
        "dcterms:title": "WMT’14 Dataset",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "http://www.statmt.org/wmt14/translation-task.html",
        "dcat:theme": [
            "Machine Translation"
        ],
        "dcat:keyword": [
            "Translation",
            "Parallel Corpora",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "http://www.statmt.org/wmt14/translation-task.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Translation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Grusky",
            "M. Naaman",
            "Y. Artzi"
        ],
        "dcterms:description": "The CNN-Daily Mail dataset contains around 287K news articles along with 2 to 4 highlights (summary) for each news article, used for text summarization.",
        "dcterms:title": "CNN-Daily Mail Dataset",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "https://summari.es/newsroom.pdf",
        "dcat:theme": [
            "Text Summarization"
        ],
        "dcat:keyword": [
            "Summarization",
            "News Articles",
            "Extractive Strategies"
        ],
        "dcat:landingPage": "https://summari.es/newsroom.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "D. Graff",
            "J. Kong",
            "K. Chen",
            "K. Maeda"
        ],
        "dcterms:description": "The Gigaword dataset contains more than 8M news articles from multiple news agencies, used for headline generation and text summarization.",
        "dcterms:title": "Gigaword Dataset",
        "dcterms:issued": "2003",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Headline Generation"
        ],
        "dcat:keyword": [
            "News Articles",
            "Text Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization",
            "Headline Generation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Rajpurkar",
            "J. Zhang",
            "K. Lopyrev",
            "P. Liang"
        ],
        "dcterms:description": "The Stanford Question Answering Dataset (SQuAD) contains over 100K pairs of questions and answers for reading comprehension tasks.",
        "dcterms:title": "Stanford Question Answering Dataset (SQuAD)",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "https://rajpurkar.github.io/SQuAD-explorer/",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "https://rajpurkar.github.io/SQuAD-explorer/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "TriviaQA is a large-scale dataset for reading comprehension, containing 650K triples of questions, answers, and evidences.",
        "dcterms:title": "TriviaQA Dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "http://nlp.cs.washington.edu/triviaqa/",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "http://nlp.cs.washington.edu/triviaqa/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Tiedemann"
        ],
        "dcterms:description": "The OpenSubtitles dataset contains conversations between movie characters for over 20K movies in 20 languages, used for dialogue generation.",
        "dcterms:title": "OpenSubtitles Dataset",
        "dcterms:issued": "",
        "dcterms:language": "Multiple",
        "dcterms:identifier": "http://opus.nlpl.eu/",
        "dcat:theme": [
            "Dialogue Generation"
        ],
        "dcat:keyword": [
            "Dialogue",
            "Movie Scripts"
        ],
        "dcat:landingPage": "http://opus.nlpl.eu/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Generation"
        ]
    },
    {
        "dcterms:creator": [
            "V. Zhong",
            "C. Xiong",
            "R. Socher"
        ],
        "dcterms:description": "WikiSQL is a dataset for semantic parsing that contains 80,654 hand-annotated questions and SQL queries across 24,241 tables from Wikipedia.",
        "dcterms:title": "WikiSQL Dataset",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/salesforce/WikiSQL",
        "dcat:theme": [
            "Semantic Parsing"
        ],
        "dcat:keyword": [
            "SQL Queries",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "https://github.com/salesforce/WikiSQL",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Parsing"
        ]
    },
    {
        "dcterms:creator": [
            "J. McAuley",
            "R. Pandey",
            "J. Leskovec"
        ],
        "dcterms:description": "The Amazon Product Review dataset contains over 82 million product reviews, used for sentiment analysis.",
        "dcterms:title": "Amazon Product Review Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "http://jmcauley.ucsd.edu/data/amazon/",
        "dcat:theme": [
            "Sentiment Analysis"
        ],
        "dcat:keyword": [
            "Product Reviews",
            "Sentiment Analysis"
        ],
        "dcat:landingPage": "http://jmcauley.ucsd.edu/data/amazon/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "S. R. Bowman",
            "G. Angeli",
            "C. Potts",
            "C. D. Manning"
        ],
        "dcterms:description": "The Stanford Natural Language Inference (SNLI) dataset contains 570K human-written English sentence pairs labeled for entailment, contradiction, and neutral.",
        "dcterms:title": "Stanford Natural Language Inference (SNLI) Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "https://nlp.stanford.edu/projects/snli/",
        "dcat:theme": [
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "Inference",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "https://nlp.stanford.edu/projects/snli/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "M. Palmer",
            "D. Gildea",
            "P. Kingsbury"
        ],
        "dcterms:description": "The Proposition Bank (PropBank) dataset contains an annotated corpus of semantic roles, used for semantic role labeling.",
        "dcterms:title": "Proposition Bank (PropBank) Dataset",
        "dcterms:issued": "2005",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Role Labeling"
        ],
        "dcat:keyword": [
            "Semantic Roles",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Role Labeling"
        ]
    },
    {
        "dcterms:creator": [
            "J. R. Finkel",
            "T. Grenager",
            "C. Manning"
        ],
        "dcterms:description": "The Freebase dataset contains billions of triples representing entities and their relationships, used for relation extraction.",
        "dcterms:title": "Freebase Dataset",
        "dcterms:issued": "2005",
        "dcterms:language": "English",
        "dcterms:identifier": "https://old.datahub.io/dataset/freebase",
        "dcat:theme": [
            "Relation Extraction"
        ],
        "dcat:keyword": [
            "Entities",
            "Relationships"
        ],
        "dcat:landingPage": "https://old.datahub.io/dataset/freebase",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Relation Extraction"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The OntoNotes 5.0 dataset is a large-scale corpus that includes various linguistic annotations, used for tasks like pronoun resolution.",
        "dcterms:title": "OntoNotes 5.0 Dataset",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://catalog.ldc.upenn.edu/LDC2013T19",
        "dcat:theme": [
            "Pronoun Resolution"
        ],
        "dcat:keyword": [
            "Linguistic Annotations",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "https://catalog.ldc.upenn.edu/LDC2013T19",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pronoun Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "T.-Y. Lin",
            "M. Maire",
            "S. Belongie",
            "J. Hays",
            "P. Perona",
            "D. Ramanan",
            "P. Dollár",
            "C. L. Zitnick"
        ],
        "dcterms:description": "The COCO dataset is designed for object detection, segmentation, and image captioning, containing around 330K images.",
        "dcterms:title": "COCO Dataset",
        "dcterms:issued": "2014",
        "dcterms:language": "English",
        "dcterms:identifier": "http://cocodataset.org/",
        "dcat:theme": [
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Images",
            "Object Detection",
            "Segmentation"
        ],
        "dcat:landingPage": "http://cocodataset.org/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "J. Xu",
            "T. Mei",
            "T. Yao",
            "Y. Rui"
        ],
        "dcterms:description": "The MSR-VTT dataset consists of 10K videos each containing 20 human-annotated captions, used for video captioning.",
        "dcterms:title": "MSR-VTT Dataset",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Captioning"
        ],
        "dcat:keyword": [
            "Videos",
            "Captions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "V. Panayotov",
            "G. Chen",
            "D. Povey",
            "S. Khudanpur"
        ],
        "dcterms:description": "The LibriSpeech ASR Corpus is composed of 1000 hours of segmented and aligned English speech derived from audiobooks, used for speech recognition.",
        "dcterms:title": "LibriSpeech ASR Corpus",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "http://www.openslr.org/12/",
        "dcat:theme": [
            "Speech Recognition"
        ],
        "dcat:keyword": [
            "Speech",
            "Audio"
        ],
        "dcat:landingPage": "http://www.openslr.org/12/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition"
        ]
    }
]