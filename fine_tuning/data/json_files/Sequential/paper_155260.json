[
    {
        "dcterms:creator": [
            "C. Busso",
            "M. Bulut",
            "C.-C. Lee",
            "A. Kazemzadeh",
            "E. Mower",
            "S. Kim",
            "J. N. Chang",
            "S. Lee",
            "S. S. Narayanan"
        ],
        "dcterms:description": "The IEMOCAP dataset consists of five sessions with one male and one female speaker each, used for emotion recognition tasks. It includes speech and corresponding human transcriptions, covering four emotional categories: happy, angry, sad, and neutral.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Emotion dataset",
            "Speech",
            "Text",
            "Multimodal",
            "Emotion categories"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Text",
        "mls:task": [
            "Emotion Classification"
        ]
    }
]