[
    {
        "dcterms:creator": [
            "C. Busso",
            "M. Bulut",
            "C.-C. Lee",
            "A. Kazemzadeh",
            "E. Mower",
            "S. Kim",
            "J. N. Chang",
            "S. Lee",
            "S. S. Narayanan"
        ],
        "dcterms:description": "IEMOCAP (Interactive Emotional Dyadic Motion Capture) is one of the standard datasets for emotion recognition, consisting of twelve hours of audio and video recordings performed by 10 professional actors organized in 5 sessions of dialogues. Each audio sample is an utterance assigned with an emotion label, labeled by six students of USC, with a focus on improvised utterances for emotion prediction.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Speech Processing"
        ],
        "dcat:keyword": [
            "Emotion dataset",
            "Speech emotion recognition",
            "Audio recordings",
            "Video recordings",
            "Actor performances"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    }
]