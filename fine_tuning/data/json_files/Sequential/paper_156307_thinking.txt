To extract datasets from the research paper titled "Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data" by Leiyu Xie et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on three widely-used datasets: **NTU RGB+D 60**, **NTU RGB+D 120**, and **PKU-MMD**. This is a good starting point for my extraction.

Next, I will examine the **introduction** section, where the authors elaborate on the challenges of medical action recognition and mention the datasets again. They specifically state that extensive experimental results were conducted on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets, confirming their relevance.

I will then look at the **datasets section** (Section IV) where the authors provide detailed descriptions of each dataset:

1. **NTU RGB+D 60**: This dataset consists of 56,880 instances for 60 types of human actions recorded from 40 subjects under various conditions. It includes 25 pose landmarks per subject.

2. **NTU RGB+D 120**: An extension of the NTU RGB+D 60 dataset, it contains 114,000 skeleton sequences across 120 action types, recorded from 106 subjects in diverse scenarios. It also includes 25 pose landmarks.

3. **PKU-MMD**: This dataset includes 1,076 long skeleton sequences across 51 action classes, recorded from 66 subjects in three different camera views, with over 20,000 instances of multi-modal data.

Next, I will check the **References section** to find the full citations for these datasets. The citations are as follows:

- For **NTU RGB+D 60**:
  > A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang. *NTU RGB+D: A large scale dataset for 3D human activity analysis*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

- For **NTU RGB+D 120**:
  > J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot. *NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding*. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), vol. 42, no. 10, pp. 2684â€“2701, 2019.

- For **PKU-MMD**:
  > C. Liu, Y. Hu, Y. Li, S. Song, and J. Liu. *PKU-MMD: A large scale benchmark for continuous multi-modal human action understanding*. arXiv preprint arXiv:1703.07475, 2017.

Now that I have identified the datasets and their full citations, I will compile this information into a structured format for further processing. This will ensure that I have accurately captured all relevant details for each dataset mentioned in the paper.