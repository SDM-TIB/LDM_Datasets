[
    {
        "dcterms:creator": [
            "Jigsaw"
        ],
        "dcterms:description": "A multi-label dataset with imbalanced classes containing 1.8 million comments, classified into seven categories of toxicity. The dataset includes identity tagging for a subset of comments to evaluate biases during classification.",
        "dcterms:title": "Civil Comments Dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Toxicity Detection",
            "Hate Speech"
        ],
        "dcat:keyword": [
            "Toxic comments",
            "Hate speech",
            "Multi-label classification",
            "Bias measurement"
        ],
        "dcat:landingPage": "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxicity classification",
            "Bias evaluation"
        ]
    }
]