To extract the datasets from the research paper titled "A Novel Two Stream Decision Level Fusion of Vision and Inertial Sensors Data for Automatic Multimodal Human Activity Recognition System" by Santosh Kumar Yadav et al., I will follow a systematic approach.

First, I will read through the **abstract** and **introduction** sections to identify any datasets that are mentioned. The abstract states that the authors conducted experiments using **four multimodal standard benchmark datasets**. This indicates that there are specific datasets that I need to look for.

Next, I will focus on **section 4.1 (Dataset Details)**, where the authors provide a detailed description of the datasets used in their experiments. Here, they mention the following datasets:

1. **UP-Fall Detection Dataset**: This dataset was published in 2019 and includes videos recorded with 17 adult subjects performing various activities, including falls. It aims to benchmark activity recognition and fall detection solutions.

2. **UTD-MHAD Dataset**: Published in 2015, this dataset consists of synchronized data from RGB videos, depth videos, skeleton positions, and wearable inertial signals, recorded while subjects performed various actions.

3. **Berkeley-MHAD Dataset**: Introduced in 2013, this dataset contains temporally synchronized data from multiple sensors, including optical motion capture systems and accelerometers, capturing various actions performed by subjects.

4. **C-MHAD Dataset**: Released in 2020, this dataset is unique as it captures video and inertial data streams simultaneously in a continuous manner, focusing on smart TV gestures and transition movements.

After identifying these datasets, I will then look into the **References section** to find the full citations for each dataset. The citations are crucial for proper attribution and will be formatted as follows:

- For **UP-Fall Detection Dataset**:
  > L. Martínez-Villasenor, H. Ponce, J. Brieva, E. Moya-Albor, J. Núñez-Martínez, C. Peñafort-Asturiano. *UP-Fall Detection Dataset: A Multimodal Approach*. Sensors, 19(9), 1988, 2019.

- For **UTD-MHAD Dataset**:
  > C. Chen, R. Jafari, N. Kehtarnavaz. *UTD-MHAD: A Multimodal Dataset for Human Action Recognition Utilizing a Depth Camera and a Wearable Inertial Sensor*. In Proceedings of the IEEE International Conference on Image Processing (ICIP), 2015, pp. 168–172.

- For **Berkeley-MHAD Dataset**:
  > F. Ofli, R. Chaudhry, G. Kurillo, R. Vidal, R. Bajcsy. *Berkeley MHAD: A Comprehensive Multimodal Human Action Database*. In Proceedings of the 2013 IEEE Workshop on Applications of Computer Vision (WACV), 2013, pp. 53–60.

- For **C-MHAD Dataset**:
  > H. Wei, P. Chopada, N. Kehtarnavaz. *C-MHAD: Continuous Multimodal Human Action Dataset of Simultaneous Video and Inertial Sensing*. Sensors, 20(10), 2905, 2020.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This systematic approach ensures that I capture all relevant details while maintaining proper attribution to the original sources.