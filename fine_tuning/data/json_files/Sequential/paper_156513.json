[
    {
        "dcterms:creator": [
            "Elazar Gershuni",
            "Yuval Pinter"
        ],
        "dcterms:description": "The Nakdimon dataset is used for training Hebrew diacritization models, providing a diverse range of modern Hebrew texts compiled from various sources.",
        "dcterms:title": "Nakdimon",
        "dcterms:issued": "2022",
        "dcterms:language": "Hebrew",
        "dcterms:identifier": "https://arxiv.org/pdf/2105.05209.pdf",
        "dcat:theme": [
            "Natural Language Processing",
            "Hebrew Language Processing"
        ],
        "dcat:keyword": [
            "Hebrew diacritization",
            "text corpus",
            "modern Hebrew texts"
        ],
        "dcat:landingPage": "https://arxiv.org/pdf/2105.05209.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Diacritization"
        ]
    },
    {
        "dcterms:creator": [
            "Avi Shmidman",
            "Shaltiel Shmidman",
            "Moshe Koppel",
            "Yoav Goldberg"
        ],
        "dcterms:description": "The Dicta dataset provides manually diacritized Hebrew texts, which are used to enhance the quality of diacritization models.",
        "dcterms:title": "Dicta",
        "dcterms:issued": "2020",
        "dcterms:language": "Hebrew",
        "dcterms:identifier": "https://arxiv.org/pdf/2005.03312.pdf",
        "dcat:theme": [
            "Natural Language Processing",
            "Hebrew Language Processing"
        ],
        "dcat:keyword": [
            "Hebrew diacritization",
            "manual diacritization",
            "text corpus"
        ],
        "dcat:landingPage": "https://arxiv.org/pdf/2005.03312.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Diacritization"
        ]
    },
    {
        "dcterms:creator": [
            "Omri Keren",
            "Tal Avinari",
            "Reut Tsarfaty",
            "Omer Levy"
        ],
        "dcterms:description": "TavBERT is a pre-trained model that utilizes character-based tokenization for Hebrew, enhancing the diacritization process by capturing intricate morphological patterns.",
        "dcterms:title": "TavBERT",
        "dcterms:issued": "2022",
        "dcterms:language": "Hebrew",
        "dcterms:identifier": "https://arxiv.org/pdf/2204.04748.pdf",
        "dcat:theme": [
            "Natural Language Processing",
            "Hebrew Language Processing"
        ],
        "dcat:keyword": [
            "Hebrew diacritization",
            "character-based tokenization",
            "pre-trained model"
        ],
        "dcat:landingPage": "https://arxiv.org/pdf/2204.04748.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Diacritization"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT is a pre-trained model that has been widely used for various natural language processing tasks, including diacritization.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/pdf/1810.04805.pdf",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "pre-trained model",
            "language understanding",
            "transformer model"
        ],
        "dcat:landingPage": "https://arxiv.org/pdf/1810.04805.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Sepp Hochreiter",
            "Jurgen Schmidhuber"
        ],
        "dcterms:description": "The LSTM dataset refers to the foundational work on Long Short-Term Memory networks, which are utilized in various sequence processing tasks, including diacritization.",
        "dcterms:title": "LSTM",
        "dcterms:issued": "1997",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.bioinf.jku.at/publications/older/2604.pdf",
        "dcat:theme": [
            "Machine Learning",
            "Sequence Processing"
        ],
        "dcat:keyword": [
            "LSTM",
            "sequence modeling",
            "neural networks"
        ],
        "dcat:landingPage": "https://www.bioinf.jku.at/publications/older/2604.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sequence Processing"
        ]
    },
    {
        "dcterms:creator": [
            "Stav Klein",
            "Reut Tsarfaty"
        ],
        "dcterms:description": "The Word-Pieces dataset explores the adequacy of word-piece tokenization for modeling complex morphology, relevant for Hebrew diacritization tasks.",
        "dcterms:title": "Word-Pieces",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://aclanthology.org/2020.sigmorphon-1.24",
        "dcat:theme": [
            "Natural Language Processing",
            "Morphological Analysis"
        ],
        "dcat:keyword": [
            "word-piece tokenization",
            "morphological modeling",
            "Hebrew language"
        ],
        "dcat:landingPage": "https://aclanthology.org/2020.sigmorphon-1.24",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Morphological Analysis"
        ]
    }
]