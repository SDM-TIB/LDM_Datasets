[
    {
        "dcterms:creator": [],
        "dcterms:description": "The OMG-Empathy Dataset consists of semi-scripted interactions between pairs of individuals, one of whom is an actor recounting a fictional autobiographical story to a listener. The dataset includes 80 videos, each lasting on average 5 minutes 12 seconds, and features continuous annotations of empathic responses from listeners.",
        "dcterms:title": "OMG-Empathy Dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Empathy",
            "Emotion recognition",
            "Multimodal interaction",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion prediction",
            "Empathic response prediction"
        ]
    },
    {
        "dcterms:creator": [
            "M. WÃ¶llmer",
            "M. Kaiser",
            "F. Eyben",
            "B. Schuller",
            "G. Rigoll"
        ],
        "dcterms:description": "The AVEC dataset is used for modeling continuous emotions in an audiovisual affect recognition framework, focusing on the integration of audio and visual features for emotion recognition.",
        "dcterms:title": "Audio/Visual Emotion Challenge (AVEC) Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Affective Computing",
            "Emotion Recognition"
        ],
        "dcat:keyword": [
            "Audio-visual",
            "Emotion recognition",
            "LSTM"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Emotion recognition"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The EmotiW Challenge 2018 dataset is used for evaluating models in the context of emotion recognition in the wild, focusing on multimodal emotion analysis.",
        "dcterms:title": "Emotion Recognition in the Wild (EmotiW) Challenge 2018",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Emotion recognition",
            "Multimodal analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Barros",
            "N. Churamani",
            "E. Lakomkin",
            "H. Sequeira",
            "A. Sutherland",
            "S. Wermter"
        ],
        "dcterms:description": "The OMG-Emotion Behaviour Dataset contains data for predicting a speaker's valence from short one-minute videos, focusing on emotion recognition in behavioral contexts.",
        "dcterms:title": "OMG-Emotion Behaviour Challenge 2018",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Affective Computing",
            "Emotion Recognition"
        ],
        "dcat:keyword": [
            "Behavior analysis",
            "Emotion prediction",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion prediction"
        ]
    },
    {
        "dcterms:creator": [
            "J. Pennington",
            "R. Socher",
            "C. D. Manning"
        ],
        "dcterms:description": "GloVe is a model for distributed word representation that captures global statistical information of words in a corpus, used for various natural language processing tasks.",
        "dcterms:title": "GloVe (Global Vectors for Word Representation)",
        "dcterms:issued": "2014",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Text representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text analysis"
        ]
    },
    {
        "dcterms:creator": [
            "O. M. Parkhi",
            "A. Vedaldi",
            "A. Zisserman"
        ],
        "dcterms:description": "The VGG Face CNN Descriptor Models are used for deep face recognition, providing a framework for identifying and verifying faces in images.",
        "dcterms:title": "VGG Face CNN Descriptor Models",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Face recognition",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Face recognition"
        ]
    },
    {
        "dcterms:creator": [
            "F. Eyben",
            "F. Weninger",
            "F. Gross",
            "B. Schuller"
        ],
        "dcterms:description": "OpenSMILE is an open-source multimedia feature extractor used for extracting audio features from multimedia data, particularly for emotion recognition tasks.",
        "dcterms:title": "OpenSMILE",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Affective Computing",
            "Audio Processing"
        ],
        "dcat:keyword": [
            "Feature extraction",
            "Audio analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Feature extraction"
        ]
    }
]