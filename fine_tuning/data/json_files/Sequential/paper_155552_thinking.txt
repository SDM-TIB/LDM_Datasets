To extract the datasets mentioned in the research paper titled "A CROSS-CORPUS STUDY ON SPEECH EMOTION RECOGNITION" by Rosanna Milner et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and datasets sections** to identify any datasets that are explicitly mentioned. The abstract provides a general overview, but the datasets section will likely contain detailed descriptions.

In the **introduction**, the authors discuss the challenges of speech emotion recognition (SER) and mention the need for various datasets. However, specific dataset names are not provided here, so I will move on to the **datasets section**.

In **section 3 (Datasets)**, the authors clearly outline the four datasets used in their study:

1. **eNTERFACE**: This dataset is described as containing about 1 hour of acted utterances with big-six emotion classes. It includes recordings from 44 speakers, with a total of 1287 segments.

2. **RAVDESS**: This dataset consists of 24 speakers and contains 1.5 hours of speech, covering the big-six emotions plus calm and neutral. The data is split into training and testing sets.

3. **IEMOCAP**: This dataset includes utterances from 10 speakers over 12 hours, with both scripted and improvised sessions to elicit emotions. It contains additional emotion categories beyond the big-six.

4. **MOSEI**: This is the largest dataset mentioned, with around 65 hours of natural emotion data collected from YouTube videos. It includes annotations for the big-six emotions.

Next, I will check the **References section** to find the full citations for each dataset:

- For **eNTERFACE**, the citation is:
  > Olivier Martin, Irene Kotsia, Benoît Macq, and Ioannis Pitas. *The eNTERFACE’05 audio-visual emotion database*. In International Conference on Data Engineering Workshops, ICDE, Atlanta, GA, USA, Apr 3-7, 2006, p. 8.

- For **RAVDESS**, the citation is:
  > Steven R. Livingstone and Frank A. Russo. *The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)*. PLoS ONE, 2018.

- For **IEMOCAP**, the citation is:
  > Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, and Shrikanth Narayanan. *IEMOCAP: interactive emotional dyadic motion capture database*. Language Resources and Evaluation, vol. 42, no. 4, pp. 335–359, 2008.

- For **MOSEI**, the citation is:
  > Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. *Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph*. In Annual Meeting of the Association for Computational Linguistics, ACL, Melbourne, Australia, Jul 15-20, 2018, pp. 2236–2246.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured all necessary details for each dataset mentioned in the paper.