[
    {
        "dcterms:creator": [
            "Antoine Miech",
            "Dimitri Zhukov",
            "Jean-Baptiste Alayrac",
            "Ivan Laptev",
            "Josef Sivic",
            "Makarand Tapaswi"
        ],
        "dcterms:description": "HowTo100M is a large-scale dataset composed of 100 million pairs of video clips and captions from 1.22 million narrated instructional videos, designed to facilitate learning joint video-text embeddings.",
        "dcterms:title": "HowTo100M",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Representation Learning",
            "Self-Supervised Learning"
        ],
        "dcat:keyword": [
            "Instructional Videos",
            "Video Dataset",
            "Text-Video Embedding",
            "Weakly Correlated Data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Retrieval",
            "Action Recognition",
            "Action Step Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Jun Xu",
            "Tao Mei",
            "Ting Yao",
            "Yong Rui"
        ],
        "dcterms:description": "MSR-VTT is a large video description dataset that bridges video and language, providing a benchmark for video-to-text and text-to-video retrieval tasks.",
        "dcterms:title": "MSR-VTT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Representation Learning"
        ],
        "dcat:keyword": [
            "Video Description",
            "Video Dataset",
            "Language Bridging"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-to-Video Retrieval",
            "Video-to-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Lisa Anne Hendricks",
            "Oliver Wang",
            "Eli Shechtman",
            "Josef Sivic",
            "Trevor Darrell",
            "Bryan Russell"
        ],
        "dcterms:description": "DiDeMo is a dataset for localizing moments in video using natural language, providing a benchmark for temporal grounding tasks.",
        "dcterms:title": "DiDeMo",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Representation Learning"
        ],
        "dcat:keyword": [
            "Moment Localization",
            "Video Dataset",
            "Natural Language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Dima Damen",
            "Hazel Doughty",
            "Giovanni Maria Farinella",
            "Sanja Fidler",
            "Antonino Furnari",
            "Evangelos Kazakos",
            "Davide Moltisanti",
            "Jonathan Munro",
            "Toby Perrett",
            "Will Price"
        ],
        "dcterms:description": "EPIC-KITCHENS is a dataset designed for scaling egocentric vision, containing videos of cooking activities with annotations for various tasks.",
        "dcterms:title": "EPIC-KITCHENS",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Egocentric Vision"
        ],
        "dcat:keyword": [
            "Cooking Videos",
            "Egocentric Dataset",
            "Action Recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Jean-Baptiste Alayrac",
            "Piotr Bojanowski",
            "Nishant Agrawal",
            "Josef Sivic",
            "Ivan Laptev",
            "Simon Lacoste-Julien"
        ],
        "dcterms:description": "YouCook2 is a dataset for unsupervised learning from narrated instruction videos, focusing on cooking tasks with video and text pairs.",
        "dcterms:title": "YouCook2",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Representation Learning"
        ],
        "dcat:keyword": [
            "Cooking Instructions",
            "Video Dataset",
            "Unsupervised Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Dimitri Zhukov",
            "Jean-Baptiste Alayrac",
            "Ramazan Gokberk Cinbis",
            "David Fouhey",
            "Ivan Laptev",
            "Josef Sivic"
        ],
        "dcterms:description": "CrossTask is a dataset for weakly supervised learning from instructional videos, providing a benchmark for action step localization tasks.",
        "dcterms:title": "CrossTask",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Representation Learning"
        ],
        "dcat:keyword": [
            "Instructional Videos",
            "Weak Supervision",
            "Action Localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Step Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Jean-Baptiste Alayrac",
            "Piotr Bojanowski",
            "Nishant Agrawal",
            "Josef Sivic",
            "Ivan Laptev",
            "Simon Lacoste-Julien"
        ],
        "dcterms:description": "HMDB is a dataset for action recognition in videos, providing a benchmark for various action classification tasks.",
        "dcterms:title": "HMDB",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Action Dataset",
            "Video Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Jean-Baptiste Alayrac",
            "Piotr Bojanowski",
            "Nishant Agrawal",
            "Josef Sivic",
            "Ivan Laptev",
            "Simon Lacoste-Julien"
        ],
        "dcterms:description": "UCF is a dataset for action recognition in videos, providing a benchmark for various action classification tasks.",
        "dcterms:title": "UCF",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Action Dataset",
            "Video Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]