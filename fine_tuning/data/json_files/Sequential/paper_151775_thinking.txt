To extract datasets from the research paper titled "Multimodal Utterance-level Affect Analysis using Visual, Audio and Text Features" by Didan Deng et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract highlights the use of the **2018 One-Minute Gradual-Emotion Recognition (OMG-Emotion) challenge**, which suggests that a dataset related to this challenge is likely discussed in the paper.

Next, I will focus on the **Methodology section**, specifically **section II.A (Dataset and Metrics)**, where the authors describe the **OMG-Emotion Behavior Dataset** in detail. They explain how this dataset was constructed from YouTube videos and provide specifics about the number of videos and utterances in the training, validation, and testing sets. This section confirms that the OMG-Emotion dataset is the primary dataset used in their experiments.

I will also check the **References section** to find the full citation for the OMG-Emotion Behavior Dataset. The relevant citation is:

- For the **OMG-Emotion Behavior Dataset**:
  > P. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and S. Wermter. *The omg-emotion behavior dataset*. arXiv preprint arXiv:1803.05434, 2018.

Now that I have identified the dataset and its citation, I will summarize the dataset details:

1. **OMG-Emotion Behavior Dataset**: This dataset is designed for long-term emotion recognition and consists of monologue videos with gradual emotional changes. It includes visual, audio, and transcript information, with annotations for arousal/valence and discrete emotion labels. The dataset is split into training (231 videos), validation (60 videos), and testing (204 videos) sets, totaling 2440, 617, and 2229 utterances respectively.

Finally, I will compile this information into a structured format that clearly outlines the dataset and its citation, ensuring that all details are accurately represented for future reference or processing.