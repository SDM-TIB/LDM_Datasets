To extract datasets from the research paper titled "Multimodal Semantic Attention Network for Video Captioning" by Liang Sun et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. In the abstract, the authors state that they evaluate their algorithm on **two popular public benchmarks: MSVD and MSR-VTT**. This indicates that these datasets are crucial for their research.

Next, I will look into **section 4 (Experiments)**, where the authors provide detailed information about the datasets used. They mention:

1. **MSVD (Microsoft Video Description Dataset)**: This dataset consists of 1,967 short videos. The authors specify that they follow a particular data split for training, validation, and testing.

2. **MSR-VTT (Microsoft Research Video to Text)**: This dataset contains 10,000 video clips across 20 categories. The authors also describe the data split they used for their experiments.

Now, I will check the **References section** to find the full citations for these datasets:

- For **MSVD**, the citation is:
  > David L Chen and William B Dolan. *Collecting highly parallel data for paraphrase evaluation*. In Proceedings of the ACL, 2011.

- For **MSR-VTT**, the citation is:
  > Jun Xu, Tao Mei, Ting Yao, and Yong Rui. *MSR-VTT: A large video description dataset for bridging video and language*. In Proceedings of the CVPR, 2016.

Having identified the datasets and their citations, I will compile this information into a structured format that clearly describes each dataset, ensuring that the full citations are included for reference.

Finally, I will prepare the dataset entries for review or further processing, ensuring that all necessary details are accurately captured.