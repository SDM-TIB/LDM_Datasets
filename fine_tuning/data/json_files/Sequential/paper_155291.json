[
    {
        "dcterms:creator": [
            "Fan Yu",
            "Shiliang Zhang",
            "Yihui Fu",
            "Lei Xie",
            "Siqi Zheng",
            "Zhihao Du",
            "Weilong Huang",
            "Pengcheng Guo",
            "Zhijie Yan"
        ],
        "dcterms:description": "The AliMeeting corpus contains 118.75 hours of speech data collected from 240 meeting audios using an 8-channel microphone array, focusing on speaker diarization and overlapped speech detection.",
        "dcterms:title": "AliMeeting",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Processing",
            "Speaker Diarization",
            "Overlapped Speech Detection"
        ],
        "dcat:keyword": [
            "Multi-channel audio",
            "Speaker diarization",
            "Overlapped speech",
            "Meeting transcription"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speaker Diarization",
            "Overlapped Speech Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Yihui Fu",
            "Luyao Cheng",
            "Shubo Lv",
            "Yukai Jv",
            "Yuxiang Kong",
            "Zhuo Chen",
            "Yanxin Hu",
            "Lei Xie",
            "Jian Wu",
            "Hui Bu"
        ],
        "dcterms:description": "Aishell-4 is an open-source dataset designed for speech enhancement, separation, recognition, and speaker diarization in conference scenarios.",
        "dcterms:title": "Aishell-4",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Enhancement",
            "Speech Recognition",
            "Speaker Diarization"
        ],
        "dcat:keyword": [
            "Conference scenario",
            "Speech enhancement",
            "Speech separation",
            "Speaker recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Enhancement",
            "Speech Recognition",
            "Speaker Diarization"
        ]
    },
    {
        "dcterms:creator": [
            "Yue Fan",
            "JW Kang",
            "LT Li",
            "KC Li",
            "HL Chen",
            "ST Cheng",
            "PY Zhang",
            "ZY Zhou",
            "YQ Cai",
            "Dong Wang"
        ],
        "dcterms:description": "CN-Celeb is a challenging dataset for Chinese speaker recognition, containing a large number of speakers and diverse audio samples.",
        "dcterms:title": "CN-Celeb",
        "dcterms:issued": "2020",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speaker Recognition"
        ],
        "dcat:keyword": [
            "Chinese speakers",
            "Speaker recognition",
            "Challenging dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speaker Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "David Snyder",
            "Guoguo Chen",
            "Daniel Povey"
        ],
        "dcterms:description": "MUSAN is a corpus that includes music, speech, and noise, designed for training and evaluating robust speech recognition systems.",
        "dcterms:title": "MUSAN",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Data Augmentation"
        ],
        "dcat:keyword": [
            "Music corpus",
            "Speech corpus",
            "Noise corpus"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Data Augmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Ko",
            "Vijayaditya Peddinti",
            "Daniel Povey",
            "Michael L Seltzer",
            "Sanjeev Khudanpur"
        ],
        "dcterms:description": "RIRs is a dataset used for studying data augmentation techniques for robust speech recognition, focusing on reverberant speech.",
        "dcterms:title": "RIRs",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Data Augmentation"
        ],
        "dcat:keyword": [
            "Reverberant speech",
            "Data augmentation",
            "Robust speech recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Data Augmentation"
        ]
    }
]