To extract datasets from the research paper titled "Three-Dimensional Lip Motion Network for Text-Independent Speaker Recognition" by Jianrong Wang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reading the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract indicates that the authors present a novel 3D lip motion network and mention the use of a dataset called **LSD-AV**. This is a good starting point.

Next, I will look into the **experiments section**, specifically **section IV (Experiment)**, where the authors describe the dataset in detail. Here, they provide a comprehensive overview of the **LSD-AV dataset**, which includes information about the number of speakers, sentences, and the method of data acquisition using Microsoft Kinect V2. They mention that the dataset consists of 68 speakers and 146 sentences per speaker, collected over 22.4 hours of recording time.

In this section, I will also note the preprocessing methods applied to the dataset, such as coordinate transformation and posture correction, which are important for understanding how the dataset was utilized in their experiments.

After gathering the details about the dataset, I will refer to the **References section** to find the full citation for the LSD-AV dataset. The authors cite the dataset as follows:

> Jianrong Wang, L. Wang, J. Zhang, J. Wei, M. Yu, and R. Yu. *A large-scale depth-based multimodal audio-visual corpus in mandarin*. In Proceedings of the 2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), pages 881â€“885, 2018.

Now that I have the necessary information, I will summarize the dataset details:

1. **LSD-AV Dataset**: This dataset is a large-scale depth-based multimodal audio-visual corpus that includes 3D lip points of 68 speakers, with each speaker uttering 146 sentences. The data was collected using a Microsoft Kinect V2 infrared sensor.

Finally, I will compile this information into a structured format that clearly outlines the dataset and its citation, ensuring that I have accurately captured all relevant details for future reference or processing.