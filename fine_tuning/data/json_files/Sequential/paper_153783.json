[
    {
        "dcterms:creator": [
            "K. M. Hermann",
            "T. Kocisky",
            "E. Grefenstette",
            "L. Espeholt",
            "W. Kay",
            "M. Suleyman",
            "P. Blunsom"
        ],
        "dcterms:description": "The CNNDM dataset contains news articles and the associated highlights (i.e., summaries) collected from the CNN and Daily Mail Online websites, with a total of 287,226 document-summary pairs for training.",
        "dcterms:title": "CNN/DailyMail",
        "dcterms:issued": "2015",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "News articles",
            "Summarization",
            "Document summarization"
        ],
        "dcat:landingPage": "https://github.com/abisee/cnn-dailymail",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Abstractive Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "E. Sandhaus"
        ],
        "dcterms:description": "The NYT dataset is a collection of articles along with multi-sentence summaries written by library scientists, containing 96,834 training examples and 3,452 test examples.",
        "dcterms:title": "New York Times",
        "dcterms:issued": "2008",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Articles",
            "Summarization",
            "Document summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Abstractive Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "X. Zhang",
            "F. Wei",
            "M. Zhou"
        ],
        "dcterms:description": "GIGA-CM is a dataset created for pre-training models, consisting of 6,521,658 unlabeled documents sampled from the English Gigaword dataset and the training documents in CNNDM.",
        "dcterms:title": "GIGA-CM",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Unlabeled documents",
            "Pre-training",
            "Document summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pre-training for Abstractive Summarization"
        ]
    }
]