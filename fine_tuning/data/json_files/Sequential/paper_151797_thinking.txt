To extract the datasets from the research paper titled "To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression" by Yitian Yuan et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors evaluate their method on two public datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on the **experiments section**, where the authors explicitly mention the datasets used for evaluation. In this section, they refer to two datasets:

1. **TACoS (Textually Annotated Cooking Scenes)**: This dataset contains 127 videos depicting cooking tasks, with approximately 17,000 pairs of sentences and video clips. The authors specify that they use 50% of the dataset for training, 25% for validation, and 25% for testing.

2. **ActivityNet Captions**: This dataset includes 20,000 videos with 100,000 descriptions, each with unique start and end times. The authors mention using the public training set for training and the validation set for testing.

To ensure I have the correct citations for these datasets, I will refer to the **References section** of the paper:

- For **TACoS**, the citation is:
  > Regneri, M., Rohrbach, M., Wetzel, D., Thater, S., Schiele, B., & Pinkal, M. (2013). Grounding action descriptions in videos. *Transactions of the Association for Computational Linguistics*, 1, 25â€“36.

- For **ActivityNet Captions**, the citation is:
  > Krishna, R., Hata, K., Ren, F., Fei-Fei, L., & Niebles, J. C. (2017). Dense-captioning events in videos. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing. This will ensure that I have accurately captured the datasets used in the research and their corresponding references.