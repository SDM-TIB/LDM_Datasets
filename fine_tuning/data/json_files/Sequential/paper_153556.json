[
    {
        "dcterms:creator": [
            "W. Wang"
        ],
        "dcterms:description": "The Jigsaw 2018 dataset focuses on the general toxic content detection task and it is comprised of approximately 215,000 annotated comments from Wikipedia talk pages labeled by crowd workers. It provides both a general toxicity label and more fine-grained annotations such as severe toxicity, obscenity, threat, insult, and identity hate.",
        "dcterms:title": "Jigsaw 2018",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Toxic Content Detection"
        ],
        "dcat:keyword": [
            "Toxic comments",
            "Wikipedia",
            "Hate speech",
            "Annotation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxic Content Classification"
        ]
    },
    {
        "dcterms:creator": [
            "W. Wang"
        ],
        "dcterms:description": "The Jigsaw Unintended Bias in Toxicity Classification dataset (Jigsaw 2019) extends the Jigsaw 2018 dataset with 1.8 million comments, each annotated by up to 10 annotators for multiple labels. Jigsaw 2019 contains a field for toxicity which provides the fraction of annotators who labeled the comment as 'toxic'.",
        "dcterms:title": "Jigsaw 2019",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview",
        "dcat:theme": [
            "Toxic Content Detection",
            "Bias in Classification"
        ],
        "dcat:keyword": [
            "Toxic comments",
            "Annotation",
            "Bias",
            "Crowdsourcing"
        ],
        "dcat:landingPage": "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxic Content Classification",
            "Bias Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Waseem",
            "D. Hovy"
        ],
        "dcterms:description": "The OffensEval 2019 dataset consists of 13,240 tweets annotated by crowdworkers. The data contains labels for whether the content is offensive and whether it is targeted, with 33% of the tweets being labeled as offensive.",
        "dcterms:title": "OffensEval 2019",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://competitions.codalab.org/competitions/20011",
        "dcat:theme": [
            "Offensive Language Detection"
        ],
        "dcat:keyword": [
            "Tweets",
            "Offensive language",
            "Annotation",
            "Crowdsourcing"
        ],
        "dcat:landingPage": "https://competitions.codalab.org/competitions/20011",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Offensive Language Classification"
        ]
    }
]