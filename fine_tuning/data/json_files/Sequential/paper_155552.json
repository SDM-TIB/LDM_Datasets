[
    {
        "dcterms:creator": [
            "Olivier Martin",
            "Irene Kotsia",
            "Beno√Æt Macq",
            "Ioannis Pitas"
        ],
        "dcterms:description": "eNTERFACE is an acted emotion dataset containing about 1 hour of utterances with the big-six emotion classes, recorded by 44 speakers from 14 nations.",
        "dcterms:title": "eNTERFACE",
        "dcterms:issued": "2006",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Emotion Recognition",
            "Acted Emotions"
        ],
        "dcat:keyword": [
            "Acted emotions",
            "Emotion dataset",
            "Speech recordings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Steven R. Livingstone",
            "Frank A. Russo"
        ],
        "dcterms:description": "RAVDESS is an acted dataset containing 1.5 hours of speech from 24 speakers, featuring the big-six emotions along with calm and neutral.",
        "dcterms:title": "RAVDESS",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Emotion Recognition",
            "Acted Emotions"
        ],
        "dcat:keyword": [
            "Acted emotions",
            "Speech dataset",
            "Emotional speech"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Carlos Busso",
            "Murtaza Bulut",
            "Chi-Chun Lee",
            "Abe Kazemzadeh",
            "Emily Mower",
            "Samuel Kim",
            "Jeannette N. Chang",
            "Shrikanth Narayanan"
        ],
        "dcterms:description": "IEMOCAP is an elicited emotion dataset containing utterances from 10 speakers over 12 hours, featuring scripted and improvised sessions to elicit emotions.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Emotion Recognition",
            "Elicited Emotions"
        ],
        "dcat:keyword": [
            "Elicited emotions",
            "Emotion dataset",
            "Speech recordings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Amir Zadeh",
            "Paul Pu Liang",
            "Soujanya Poria",
            "Erik Cambria",
            "Louis-Philippe Morency"
        ],
        "dcterms:description": "MOSEI is the largest sentiment and emotion dataset with around 65 hours of data from YouTube videos, annotated for the big-six emotions.",
        "dcterms:title": "MOSEI",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Emotion Recognition",
            "Natural Emotions"
        ],
        "dcat:keyword": [
            "Natural emotions",
            "Emotion dataset",
            "YouTube videos"
        ],
        "dcat:landingPage": "https://github.com/A2Zadeh/CMU-MultimodalSDK/blob/master/mmsdk/mmdatasdk/dataset/standard_datasets/CMU_MOSEI/cmu_mosei_std_folds.py",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    }
]