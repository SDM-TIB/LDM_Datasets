[
    {
        "dcterms:creator": [
            "S. Chaudhuri",
            "J. Roth",
            "D. P. W. Ellis",
            "A. C. Gallagher",
            "L. Kaver",
            "R. Marvin",
            "C. Pantofaru",
            "N. Reale",
            "L. G. Reid",
            "K. W. Wilson",
            "Z. Xi"
        ],
        "dcterms:description": "AVA-Speech is a publicly available dataset of movies densely labeled with speech activity, consisting of 160 segments from movies hosted on YouTube, each 15 minutes in duration, totaling 40 hours of labeled data. The segments are densely labeled for speech activity using labels such as 'NoSpeech', 'CleanSpeech', 'Speech+Music', and 'Speech+Noise'. Each segment is human-labeled by 3 annotators, and the annotations are merged using a frame-level majority vote.",
        "dcterms:title": "AVA-Speech",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Processing",
            "Audio Analysis"
        ],
        "dcat:keyword": [
            "Speech activity detection",
            "Movie dataset",
            "Densely labeled data",
            "Speech and non-speech classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Voice Activity Detection"
        ]
    }
]