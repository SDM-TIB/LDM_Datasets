[
    {
        "dcterms:creator": [],
        "dcterms:description": "The dataset consists of audio speech samples from 363 subjects participating in the Families Overcoming Risks and Building Opportunities for Well Being (FORBOW) research project. Participants are parents who were asked to talk about their children for five minutes without interruption. The dataset includes segments coded for sentiment, objectivity/subjectivity, and emotions.",
        "dcterms:title": "FORBOW",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Audio speech samples",
            "Mental health",
            "Families Overcoming Risks and Building Opportunities for Well Being"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Mental disorder prediction"
        ]
    },
    {
        "dcterms:creator": [
            "S. Mohammad",
            "F. Bravo-Marquez",
            "M. Salameh",
            "S. Kiritchenko"
        ],
        "dcterms:description": "The SemEval-2018 AIT Distant Supervision Corpus (DISC) includes around 100M English tweet ids associated with tweets that contain emotion-related query terms. It is used for emotion recognition tasks.",
        "dcterms:title": "SemEval-2018 AIT Distant Supervision Corpus (DISC)",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Emotion recognition",
            "Tweets",
            "Distant supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Emotion recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Zadeh",
            "P. Liang",
            "S. Poria",
            "E. Cambria",
            "L.-P. Morency"
        ],
        "dcterms:description": "The CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset contains 23,453 annotated video segments from 1,000 distinct speakers and 250 topics, with annotations for Ekman emotions. It is used for emotion recognition and sentiment analysis.",
        "dcterms:title": "CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Multimodal analysis",
            "Emotion recognition",
            "Sentiment analysis",
            "Video segments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion recognition",
            "Sentiment analysis"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The paper mentions using huge auxiliary emotion-labeled text and audio corpora for training emotion-specific embeddings but does not specify a particular dataset.",
        "dcterms:title": "Emotion-labeled text and audio corpora",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Emotion-labeled data",
            "Text corpora",
            "Audio corpora"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]