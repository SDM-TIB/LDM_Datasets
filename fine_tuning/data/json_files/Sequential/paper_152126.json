[
    {
        "dcterms:creator": [
            "M. Pantic",
            "M. Valstar",
            "R. Rademaker",
            "L. Maat"
        ],
        "dcterms:description": "The MMI database contains more than 20 subjects, ranging in age from 19 to 62, with different ethnicities. Subjects display 79 series of facial expressions, six of which are prototypic emotions. Static frames were extracted from each sequence, resulting in 11,500 images.",
        "dcterms:title": "MMI",
        "dcterms:issued": "2005",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Facial Expression Recognition",
            "Emotion Analysis"
        ],
        "dcat:keyword": [
            "Facial expressions",
            "Emotion recognition",
            "Video sequences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Facial Expression Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "P. Lucey",
            "J. F. Cohn",
            "T. Kanade",
            "J. Saragih",
            "Z. Ambadar",
            "I. Matthews"
        ],
        "dcterms:description": "The extended Cohn-Kanade database (CK+) contains 593 videos from 123 subjects, with sequences starting from the neutral state and ending at the apex of one of the six basic expressions. Only 327 sequences contain facial expression labels.",
        "dcterms:title": "CK+",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Facial Expression Recognition",
            "Emotion Analysis"
        ],
        "dcat:keyword": [
            "Facial expressions",
            "Emotion recognition",
            "Video sequences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Facial Expression Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "T. Banziger",
            "K. R. Scherer"
        ],
        "dcterms:description": "The GEMEP-FERA database is a subset of the GEMEP corpus used for the FERA 2011 challenge. It contains 87 image sequences of 7 subjects showing facial expressions of various emotions, annotated with AUs and holistic expressions.",
        "dcterms:title": "GEMEP-FERA",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Facial Expression Recognition",
            "Emotion Analysis"
        ],
        "dcat:keyword": [
            "Facial expressions",
            "Emotion recognition",
            "Video sequences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Facial Expression Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "S. M. Mavadati",
            "M. H. Mahoor",
            "K. Bartlett",
            "P. Trinh",
            "J. F. Cohn"
        ],
        "dcterms:description": "The DISFA database consists of 27 subjects who watched YouTube videos while their spontaneous facial expressions were recorded. It is FACS coded by AU intensity values on a six-point scale.",
        "dcterms:title": "DISFA",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Facial Expression Recognition",
            "Emotion Analysis"
        ],
        "dcat:keyword": [
            "Spontaneous expressions",
            "Facial Action Coding System",
            "Video sequences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Facial Expression Recognition"
        ]
    }
]