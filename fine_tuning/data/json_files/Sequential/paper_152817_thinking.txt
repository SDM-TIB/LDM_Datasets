To extract datasets from the research paper titled "Evaluating Rewards for Question Generation Models" by Tom Hosking and Sebastian Riedel, I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will start by examining the **abstract** and **introduction** sections. The abstract discusses the optimization of question generation models and mentions the use of various objectives, but it does not explicitly mention any datasets. In the introduction, the authors refer to the SQuAD dataset as a source of context-question-answer triples, which indicates that this dataset is relevant to their work.

Next, I will look for a dedicated **datasets section** or any mention of datasets in the **experimental setup** or **results** sections. The paper does not have a specific section labeled "Datasets," but it does mention the SQuAD dataset in the context of question generation. The authors highlight the limitations of using SQuAD due to its single question per context-answer pair, which suggests that they are relying on this dataset for their experiments.

Now, I will check the **References section** to find the full citation for the SQuAD dataset. The relevant citation is:

- For **SQuAD** (Stanford Question Answering Dataset), the citation is:
  > Pranav Rajpurkar, Jian Zhang, Konstantin Liu, and Percy Liang. *SQuAD: 100,000+ Questions for Machine Comprehension of Text*. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 238â€“244, 2016.

Since SQuAD is the only dataset explicitly mentioned in the paper, I will document it accordingly.

Finally, I will compile the dataset entry with the full citation into a structured format that can be used for further processing or review. This ensures that I have accurately captured the relevant dataset information from the paper.