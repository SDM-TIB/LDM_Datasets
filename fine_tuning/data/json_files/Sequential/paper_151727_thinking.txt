To extract datasets from the research paper titled "Revisiting Video Saliency: A Large-scale Benchmark and a New Model" by Wenguan Wang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and related work sections** to identify any datasets mentioned. The abstract indicates that the authors introduce a new benchmark dataset called **DHF1K**, which is a strong lead for dataset extraction.

Next, I will focus on the **introduction section**, where the authors describe the DHF1K dataset in detail. They mention that it consists of 1,000 high-quality video sequences with eye-tracking annotations, which confirms its significance as a dataset for video saliency research.

In the **related work section**, the authors discuss other datasets used for dynamic visual saliency prediction, specifically mentioning **Hollywood-2** and **UCF Sports**. They provide some context about these datasets, which will help in understanding their relevance and characteristics.

I will then look for a dedicated **datasets section** or any subsection that explicitly lists the datasets used in their experiments. In this paper, the authors provide a table summarizing the statistics of various dynamic eye-tracking datasets, including DHF1K, Hollywood-2, and UCF Sports. This table will be crucial for extracting detailed information about each dataset.

For each dataset, I will note the following details:
1. **DHF1K**: 
   - Description: A large-scale dataset of gaze in free-viewing of videos, containing 1,000 videos with over 600,000 frames and per-frame fixation annotations from 17 observers.
   - Citation: 
     > Wenguan Wang, Jianbing Shen, Fang Guo, Ming-Ming Cheng, Ali Borji. *Revisiting Video Saliency: A Large-scale Benchmark and a New Model*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

2. **Hollywood-2**: 
   - Description: Comprises 1,707 videos from 69 Hollywood movies, with human fixation data collected from 19 observers.
   - Citation: 
     > Marszalek, M., Laptev, I., & Schmid, C. *Actions in Context*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.

3. **UCF Sports**: 
   - Description: Contains 150 videos taken from the UCF sports action dataset, covering 9 common sports action classes.
   - Citation: 
     > Rodriguez, M. D., Ahmed, M., & Shah, M. *Action Mach: A Spatio-Temporal Maximum Average Correlation Height Filter for Action Recognition*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.

After gathering all the necessary information, I will compile the dataset entries into a structured format that includes the dataset name, description, and full citation for each dataset. This will ensure that I have a comprehensive overview of the datasets referenced in the paper, ready for further processing or review.