[
    {
        "dcterms:creator": [
            "Daniel Roggen",
            "Alberto Calatroni",
            "Mirco Rossi",
            "Thomas Holleczek",
            "Kilian Förster",
            "Gerhard Tröster",
            "Paul Lukowicz",
            "David Bannach",
            "Gerald Pirkl",
            "Alois Ferscha",
            "Jakob Doppler",
            "Clemens Holzmann",
            "Marc Kurz",
            "Gerald Holl",
            "Ricardo Chavarriaga",
            "Hesam Sagha",
            "Hamidreza Bayati",
            "Marco Creatura",
            "José del R. Millàn"
        ],
        "dcterms:description": "The Opportunity dataset consists of 4 individuals performing a set of activities of daily living. For the gesture recognition challenge of the dataset, there are 18 classes which are to be predicted (open/close door 1 and 2, fridge, dishwasher and drawer 1, 2 and 3, clean table, drink from cup and toggle switch) as well as a null class. The dataset includes 113 feature channels representing individual sensor axes from body-worn accelerometers and inertial measurement units.",
        "dcterms:title": "Opportunity Dataset",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/INSS.2010.5573462",
        "dcat:theme": [
            "Human Activity Recognition",
            "Sensor Data"
        ],
        "dcat:keyword": [
            "Gesture recognition",
            "Activity recognition",
            "Sensor data",
            "Inertial measurement units"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/INSS.2010.5573462",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Philipp M. Scholl",
            "Matthias Wille",
            "Kristof Van Laerhoven"
        ],
        "dcterms:description": "The Wetlab dataset consists of 22 participants performing two DNA extraction experiments within a wetlab environment. Subjects were equipped with a wrist-worn sensing unit capturing 3D acceleration data with a sampling rate of 50Hz. The dataset includes 9 different actions (cutting, inverting, peeling, pestling, pipetting, pouring, stirring, transfer) which, along with a null class, are the target labels to be predicted.",
        "dcterms:title": "Wetlab Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1145/2750858.2807547",
        "dcat:theme": [
            "Human Activity Recognition",
            "Sensor Data"
        ],
        "dcat:keyword": [
            "DNA extraction",
            "Wetlab",
            "3D acceleration",
            "Activity recognition"
        ],
        "dcat:landingPage": "https://doi.org/10.1145/2750858.2807547",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Timo Sztyler",
            "Heiner Stuckenschmidt"
        ],
        "dcterms:description": "The RWHAR dataset contains data from 15 participants performing 8 different activities (walking upstairs, walking downstairs, jumping, lying, standing, sitting, running, walking) as well as a null class. The dataset focuses on 3D acceleration data captured by a wrist-worn sensor at a sampling rate of 50Hz.",
        "dcterms:title": "RealWorld HAR (RWHAR) Dataset",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/PERCOM.2016.7456521",
        "dcat:theme": [
            "Human Activity Recognition",
            "Sensor Data"
        ],
        "dcat:keyword": [
            "Activity recognition",
            "3D acceleration",
            "Wrist-worn sensor"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/PERCOM.2016.7456521",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Jorge-L. Reyes-Ortiz",
            "Luca Oneto",
            "Albert Samà",
            "Xavier Parra",
            "Davide Anguita"
        ],
        "dcterms:description": "The SBHAR dataset consists of 30 participants performing activities of daily living (standing, sitting, lying, walking, walking downstairs, walking upstairs) along with 6 postural transitions (stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, lie-to-stand) and a null class. The dataset uses raw 3D acceleration sensor data sampled at 50Hz.",
        "dcterms:title": "Smartphone-Based Recognition of Human Activities and Postural Transitions (SBHAR) Dataset",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1016/j.neucom.2015.07.085",
        "dcat:theme": [
            "Human Activity Recognition",
            "Sensor Data"
        ],
        "dcat:keyword": [
            "Activity recognition",
            "Postural transitions",
            "3D acceleration"
        ],
        "dcat:landingPage": "https://doi.org/10.1016/j.neucom.2015.07.085",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Allan Stisen",
            "Henrik Blunck",
            "Sourav Bhattacharya",
            "Thor S. Prentow",
            "Mikkel B. Kjærgaard",
            "Anind Dey",
            "Tobias Sonne",
            "Mads M. Jensen"
        ],
        "dcterms:description": "The HHAR dataset contains data from 9 human participants performing activities of daily living. There are 6 activities (biking, sitting, standing, walking, walking upstairs, and walking downstairs) and a null class to be predicted. The dataset uses 3D acceleration data obtained from a wrist-worn sensor unit, sampled at 100Hz.",
        "dcterms:title": "Heterogeneity Activity Recognition (HHAR) Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1145/2809695.2809718",
        "dcat:theme": [
            "Human Activity Recognition",
            "Sensor Data"
        ],
        "dcat:keyword": [
            "Activity recognition",
            "3D acceleration",
            "Wrist-worn sensor"
        ],
        "dcat:landingPage": "https://doi.org/10.1145/2809695.2809718",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity Recognition"
        ]
    }
]