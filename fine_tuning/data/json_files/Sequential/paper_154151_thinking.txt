To extract datasets from the research paper titled "Multi-modal embeddings using multi-task learning for emotion recognition" by Aparna Khare et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reading through the **abstract and introduction** sections. The abstract mentions that the authors evaluate their embeddings on the **CMU-MOSEI dataset**, which indicates that this dataset is significant for their research.

Next, I will examine **section 4 (Experimental setup)**, where the authors discuss the datasets used for training and evaluation. Here, they explicitly mention two datasets:

1. **VoxCeleb2 Dataset**: This dataset is used for training the multi-task transformer models. The authors note that it contains 1.1 million examples and is utilized for extracting text labels via a TDNN ASR model trained on the Librispeech dataset.

2. **CMU-MOSEI Dataset**: This dataset is used for evaluating the performance of the embeddings on the emotion recognition task. The authors describe it as containing 23,453 single-speaker video segments from YouTube, annotated for sentiment and emotion.

Now, I will look at the **References section** to find the full citations for these datasets:

- For the **VoxCeleb2 Dataset**, the citation is:
  > J. S. Chung, A. Nagrani, and A. Zisserman. *Voxceleb2: Deep speaker recognition*. In INTERSPEECH, 2018.

- For the **CMU-MOSEI Dataset**, the citation is:
  > P. Liang, R. Salakhutdinov, and L.-P. Morency. *Computational modeling of human multimodal language: The MOSEI dataset and interpretable dynamic fusion*. 2018.

Having gathered the necessary information, I will now prepare to create structured entries for each dataset, ensuring that I include the full citations as required. This will allow for clear documentation and reference in any subsequent analysis or reporting.