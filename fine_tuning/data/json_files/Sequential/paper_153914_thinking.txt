To extract datasets from the research paper titled "Deep Learning Models for Automatic Summarization" by Pirmin Lemberger, I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reading through the **abstract and introduction** sections. The abstract mentions that the article reviews recent datasets and architectures related to text summarization, which indicates that datasets are likely discussed in detail later in the paper.

Next, I will focus on the **main body of the paper**, particularly sections that discuss datasets. In the **"More and better Data"** section, the author mentions two datasets explicitly:

1. **CNN/Daily Mail Dataset**: This dataset contains 300,000 examples of news articles paired with their summaries. However, the author notes its limitations, which suggests that it may not be the primary focus for future work.

2. **BigPatent Dataset**: This dataset includes 1.3 million patent documents along with their summaries, addressing the limitations of the CNN/Daily Mail dataset.

In the **References section**, I will look for full citations for these datasets:

- For the **BigPatent Dataset**, the citation is:
  > Eva Sharma, Chen Li, Lu Wang. *BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization*. arXiv:1906.03741.

- The **CNN/Daily Mail Dataset** is not cited in the references, but it is widely known in the NLP community. I will use the following citation for it:
  > Abigail See, Peter J. Liu, Christopher D. Manning. *Get To The Point: Summarization with Pointer-Generator Networks*. arXiv:1704.04368.

Now, I will compile the dataset entries, ensuring that I include the full citations as required. This will provide a clear and structured overview of the datasets discussed in the paper, ready for further processing or review.