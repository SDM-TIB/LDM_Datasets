[
    {
        "dcterms:creator": [
            "L. Gao",
            "S. Biderman",
            "S. Black",
            "L. Golding",
            "T. Hoppe",
            "C. Foster",
            "J. Phang",
            "H. He",
            "A. Thite",
            "N. Nabeshima"
        ],
        "dcterms:description": "An 800GB dataset of diverse text for language modeling.",
        "dcterms:title": "Pile",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2101.00027",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Language modeling",
            "Diverse text"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A synthetic corpus generated to preserve unigram frequency distribution found in natural corpora.",
        "dcterms:title": "Zipf corpus",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Synthetic data",
            "Unigram distribution",
            "Natural language structure"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A synthetic corpus generated by sampling word tokens at random, destroying all statistical properties of the language.",
        "dcterms:title": "Uniform corpus",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Synthetic data",
            "Random sampling",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A collection of text data from various sources including news articles and open domain books.",
        "dcterms:title": "Custom corpora (from crawling news articles and open domain books)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Text corpus",
            "News articles",
            "Open domain books"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Synthetic data generated randomly, used for comparison with natural language data.",
        "dcterms:title": "Synthetic randomly generated data",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Synthetic data",
            "Random generation",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A test corpus used for inference on Pythia, consisting of English sentences.",
        "dcterms:title": "English test corpus (used for inference on Pythia)",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Test corpus",
            "Inference",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A collection of datasets including ArXiv, Gutenberg Project, StackExchange, GitHub, and Wikipedia used for training LLaMA.",
        "dcterms:title": "ArXiv, Gutenberg Project, StackExchange, GitHub, Wikipedia (used for LLaMA training)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Training data",
            "Diverse sources",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]