[
    {
        "dcterms:creator": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "dcterms:description": "The WikiText-103 dataset consists of 103M/217K/245K tokens for training, validation, and test sets respectively and has a vocabulary size of about 260K. This dataset is composed of Wikipedia articles and retains punctuation, numbers, and case.",
        "dcterms:title": "WikiText-103",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Language modeling",
            "Tokens"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Mitchell Marcus",
            "Grace Kim",
            "Mary Ann Marcinkiewicz",
            "Robert MacIntyre",
            "Ann Bies",
            "Mark Ferguson",
            "Karen Katz",
            "Britta Schasberger"
        ],
        "dcterms:description": "The Penn Treebank dataset contains about 929K/74K/82K tokens in its train, validation, and test sets respectively. It has a vocabulary size of about 10K.",
        "dcterms:title": "Penn Treebank",
        "dcterms:issued": "1994",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Tokens",
            "Natural Language Processing",
            "Annotation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]