[
    {
        "dcterms:creator": [
            "Andrew M Dai",
            "Christopher Olah",
            "Quoc V Le"
        ],
        "dcterms:description": "A section of The Lord of the Rings dataset is used in this work for training a language model to generate text based on context vectors.",
        "dcterms:title": "The Lord of the Rings dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1507.07998",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Context vectors",
            "Language model"
        ],
        "dcat:landingPage": "https://github.com/wess/iotr",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "Greg S Corrado",
            "Jeff Dean"
        ],
        "dcterms:description": "Word2Vec is used as the embedding model for the word clustering method, preserving semantic similarity and relationships.",
        "dcterms:title": "Word2Vec",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Semantic similarity",
            "Text representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding"
        ]
    },
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe is mentioned as one of the commonly used word embeddings that preserve semantic similarity and relationships.",
        "dcterms:title": "GloVe",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Semantic similarity",
            "Text representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding"
        ]
    },
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Edouard Grave",
            "Piotr Bojanowski",
            "Christian Puhrsch",
            "Armand Joulin"
        ],
        "dcterms:description": "FastText is mentioned as another commonly used word embedding model that captures semantic relationships.",
        "dcterms:title": "FastText",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Semantic similarity",
            "Text representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding"
        ]
    },
    {
        "dcterms:creator": [
            "Dirk Goldhahn",
            "Thomas Eckart",
            "Uwe Quasthoff"
        ],
        "dcterms:description": "Wiki vector space is built from various Wikipedia sources to cover a comprehensive set of vocabulary.",
        "dcterms:title": "Wiki vector space",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Semantic similarity",
            "Text representation"
        ],
        "dcat:landingPage": "http://wortschatz.uni-leipzig.de",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding"
        ]
    }
]