[
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "A dataset that includes both object annotations for the task of object detection and textual descriptions for the task of image captioning, widely used as a benchmark in computer vision.",
        "dcterms:title": "MS-COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object annotations",
            "Textual descriptions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image, Text",
        "mls:task": [
            "Image Captioning",
            "Object Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Bryan A Plummer",
            "Liwei Wang",
            "Chris M Cervantes",
            "Juan C Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "dcterms:description": "A dataset that collects region-to-phrase correspondences for richer image-to-sentence models, enhancing the understanding of visual content in relation to language.",
        "dcterms:title": "Flick30K Entities",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Visual Language Processing"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Region-to-phrase correspondences",
            "Visual content"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image, Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "A dataset that connects language and vision using crowdsourced dense image annotations, providing rich information for various vision-language tasks.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Visual Language Processing"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Dense annotations",
            "Language and vision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image, Text",
        "mls:task": [
            "Image Captioning",
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "C Lawrence Zitnick",
            "Devi Parikh"
        ],
        "dcterms:description": "A dataset that provides abstract scene representations, useful for visually-grounded tasks including sentence interpretation and predicting object dynamics.",
        "dcterms:title": "Clipart Scenes",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Abstraction",
            "Language and Vision"
        ],
        "dcat:keyword": [
            "Clipart dataset",
            "Visual abstraction",
            "Scene representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Language Processing"
        ]
    }
]