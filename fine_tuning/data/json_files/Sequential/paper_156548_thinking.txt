To extract datasets from the research paper titled "On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation" by Di Wu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets mentioned. The abstract provides a brief overview of the study's focus, and the introduction often outlines the datasets used for experiments.

In the **introduction**, the authors mention that they conducted experiments on datasets covering the science and news domains. This indicates that there are specific datasets utilized, but I need to find their names and details.

Next, I will look for a dedicated **section on datasets or evaluation setup**. In this paper, the authors describe two datasets in **section 3.2 (Benchmarks)**:

1. **KP20k**: This dataset is introduced by Meng et al. (2017) and contains 500,000 computer science papers. The authors mention that they train on KP20k and evaluate on the KP20k test set.

2. **KPTimes**: Introduced by Gallina et al. (2019), this dataset is focused on the news domain and contains over 250,000 examples. The authors train on the KPTimes train set and report performance on the union of the KPTimes test set and an out-of-distribution test set called JPTimes.

To ensure I have the correct citations, I will refer to the **References section** of the paper:

- For **KP20k**, the citation is:
  > Meng, Y., Zhao, S., Han, S., He, D., Brusilovsky, P., & Chi, Y. (2017). *Deep keyphrase generation*. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 582–592, Vancouver, Canada. Association for Computational Linguistics.

- For **KPTimes**, the citation is:
  > Gallina, Y., Boudin, F., & Daille, B. (2019). *KPTimes: A large-scale dataset for keyphrase generation on news documents*. In Proceedings of the 12th International Conference on Natural Language Generation, pages 130–135, Tokyo, Japan. Association for Computational Linguistics.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets used in the research and their respective citations.