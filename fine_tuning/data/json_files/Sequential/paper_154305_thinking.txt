To extract datasets from the research paper titled "Video-based Facial Expression Recognition using Graph Convolutional Networks" by Daizong Liu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract indicates that the authors evaluate their method on several datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on the **experiments section** (Section IV) where the authors typically describe the datasets used for their evaluations. In this section, they mention four datasets: **CK+**, **Oulu-CASIA**, **MMI**, and **AFEW 8.0**. I will extract details about each dataset, including their characteristics and the context in which they were used.

1. **CK+ Dataset**: The authors describe it as an extended version of the Cohn-Kanade dataset, which includes 583 image sequences from 123 subjects, with specific expressions labeled. This dataset is crucial for evaluating facial expression recognition methods.

2. **Oulu-CASIA Dataset**: This dataset consists of 6 basic facial expressions from 80 subjects and is divided into three parts based on lighting conditions. It contains 480 sequences, making it a significant dataset for testing the robustness of expression recognition systems.

3. **MMI Dataset**: The MMI dataset includes 213 video sequences from 30 subjects, labeled with 6 basic expressions. This dataset is important for evaluating the performance of the proposed method in recognizing dynamic expressions.

4. **AFEW 8.0 Dataset**: This dataset is mentioned as a challenging "in the wild" dataset, which adds complexity to the evaluation of the proposed method.

After identifying the datasets, I will refer to the **References section** to find the full citations for each dataset:

- For **CK+ Dataset**:
  > Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., & Matthews, I. (2010). The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshop), 94–101.

- For **Oulu-CASIA Dataset**:
  > Zhao, G., Huang, X., Taini, M., Li, S. Z., & Pietikainen, M. (2011). Facial expression recognition from near-infrared videos. Image and Vision Computing, 29(9), 607–619.

- For **MMI Dataset**:
  > Pantic, M., Valstar, M., Rademaker, R., & Maat, L. (2005). Web-based database for facial expression analysis. In IEEE International Conference on Multimedia and Expo, 5–pp.

- For **AFEW 8.0 Dataset**:
  > Dhall, A., Kaur, A., Goecke, R., & Gedeon, T. (2018). Emotiw 2018: Audio-video, student engagement and group-level affect prediction. In Proceedings of the International Conference on Multimodal Interaction, 653–656.

Now that I have gathered all the necessary information about the datasets and their citations, I will compile this information into a structured format for further processing or review.