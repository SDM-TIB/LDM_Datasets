[
    {
        "dcterms:creator": [
            "J. Mao",
            "J. Huang",
            "A. Toshev",
            "O. Camburu",
            "A. L. Yuille",
            "K. Murphy"
        ],
        "dcterms:description": "The Google-Ref dataset is composed of 104,560 expressions referring to 54,822 objects and 26,711 images. These images and ground truth masks are collected from the MS COCO dataset, and referring expressions are annotated from Amazon Mechanical Turk.",
        "dcterms:title": "Google-Ref",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Referring Image Segmentation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Referring expressions",
            "Image segmentation",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Referring Image Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "L. Yu",
            "P. Poirson",
            "S. Yang",
            "A. C. Berg",
            "T. L. Berg"
        ],
        "dcterms:description": "The UNC dataset is based on the MS COCO dataset and contains 19,994 images with 142,209 referring expressions for 50,000 objects. It is gathered by a two-player game where one player annotates the image region according to the expression by the other player interactively.",
        "dcterms:title": "UNC",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Referring Image Segmentation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Referring expressions",
            "Interactive annotation",
            "Image segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Referring Image Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "L. Yu",
            "P. Poirson",
            "S. Yang",
            "A. C. Berg",
            "T. L. Berg"
        ],
        "dcterms:description": "The UNC+ dataset is similar to the UNC dataset with a total of 141,564 expressions for 49,856 objects in 19,992 images. The main difference is that location information is not allowed to refer to the object of interest, relying purely on appearance and context descriptions.",
        "dcterms:title": "UNC+",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Referring Image Segmentation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Referring expressions",
            "Image segmentation",
            "Appearance description"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Referring Image Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Kazemzadeh",
            "V. Ordonez",
            "M. Matten",
            "T. Berg"
        ],
        "dcterms:description": "The Referit dataset is built on the IAPR TC-12 dataset, which includes masks for various objects and stuff. It has 130,525 expressions referring to 96,654 object masks in 19,894 images, collected by a two-player game.",
        "dcterms:title": "Referit",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Referring Image Segmentation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Referring expressions",
            "Image segmentation",
            "Object masks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Referring Image Segmentation"
        ]
    }
]