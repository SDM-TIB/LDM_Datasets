[
    {
        "dcterms:creator": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard Hovy"
        ],
        "dcterms:description": "RACE is a large-scale reading comprehension dataset from examinations, containing over 87,000 English multiple-choice school-level science questions.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Education"
        ],
        "dcat:keyword": [
            "Multiple-choice questions",
            "Reading comprehension",
            "Examinations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Anselmo Peñas",
            "Eduard Hovy",
            "Pamela Forner",
            "Álvaro Rodrigo",
            "Richard Sutcliffe",
            "Corina Forascu",
            "Yassine Benajiba",
            "Petya Osenova"
        ],
        "dcterms:description": "The Bulgarian MRC dataset is designed for machine reading comprehension, providing question-answer pairs for evaluation.",
        "dcterms:title": "Bulgarian MRC dataset",
        "dcterms:issued": "2012",
        "dcterms:language": "Bulgarian",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question answering",
            "Machine reading",
            "Bulgarian language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "TriviaQA is a large-scale distantly supervised challenge dataset for reading comprehension, containing questions and answers sourced from trivia websites.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Trivia"
        ],
        "dcat:keyword": [
            "Distant supervision",
            "Question answering",
            "Reading comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Matthew Richardson",
            "Christopher J.C. Burges",
            "Erin Renshaw"
        ],
        "dcterms:description": "MCTest is a challenge dataset for the open-domain machine comprehension of text, featuring multiple-choice questions based on short stories.",
        "dcterms:title": "MCTest",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Comprehension",
            "Education"
        ],
        "dcat:keyword": [
            "Open-domain",
            "Multiple-choice questions",
            "Reading comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Anselmo Peñas",
            "Eduard Hovy",
            "Pamela Forner",
            "Álvaro Rodrigo",
            "Richard Sutcliffe",
            "Corina Forascu",
            "Yassine Benajiba",
            "Petya Osenova"
        ],
        "dcterms:description": "QA4MRE is a dataset for question answering for machine reading evaluation, providing a framework for evaluating QA systems.",
        "dcterms:title": "QA4MRE",
        "dcterms:issued": "2012",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Machine Reading"
        ],
        "dcat:keyword": [
            "Machine reading",
            "Question answering",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Kai Sun",
            "Dian Yu",
            "Jianshu Chen",
            "Dong Yu",
            "Yejin Choi",
            "Claire Cardie"
        ],
        "dcterms:description": "DREAM is a challenge dataset for dialogue-based reading comprehension, focusing on understanding and answering questions based on dialogues.",
        "dcterms:title": "DREAM",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Systems",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Dialogue-based",
            "Reading comprehension",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue-based Reading Comprehension"
        ]
    }
]