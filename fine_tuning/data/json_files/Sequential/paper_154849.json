[
    {
        "dcterms:creator": [],
        "dcterms:description": "The dataset is constructed from a corpus of Google Sheets publicly shared within the organization, consisting of 46K Google Sheets with formulas, split into 42K for training, 2.3K for validation, and 1.7K for testing. The final dataset includes 770K training samples, 42K for validation, and 34K for testing.",
        "dcterms:title": "Google Sheets Dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Google Sheets",
            "Spreadsheet formulas",
            "Machine learning dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "B. Klimt",
            "Y. Yang"
        ],
        "dcterms:description": "The Enron corpus includes over 17K Excel spreadsheets extracted from the Enron email corpus. The preprocessing was done similarly to the Google Sheets corpus, resulting in a dataset of 178K samples in the training set, 41K samples in the validation set, and 33K samples in the validation set.",
        "dcterms:title": "Enron Corpus",
        "dcterms:issued": "2004",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/SheetJS/enron_xls",
        "dcat:theme": [],
        "dcat:keyword": [
            "Enron",
            "Excel spreadsheets",
            "Machine learning dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]