To extract datasets from the research paper titled "Multimodal Deep Learning for Mental Disorders Prediction from Audio Speech Samples" by Habibeh Naderi et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations accurately.

First, I will read through the **abstract, introduction, and dataset sections** of the paper. The abstract mentions the use of recorded speech samples for predicting mental disorders, which indicates that there are datasets involved. 

Next, I will focus on **section 3 (Dataset)**, where the authors provide detailed information about the dataset used in their research. Here, they describe the dataset as consisting of audio speech samples from 363 subjects participating in the Families Overcoming Risks and Building Opportunities for Well Being (FORBOW) research project. This section includes important details such as the demographics of the participants and the nature of the data collected.

The authors also mention that the dataset includes transcriptions of audio files, which were segmented based on emotional and sentiment changes, resulting in 17,565 segments. This indicates a rich dataset that has been carefully annotated for various emotional and linguistic features.

Now, I will look for any additional datasets mentioned in the paper. In the **methodology section**, the authors refer to auxiliary datasets used for training emotion-specific embeddings. One such dataset is the **SemEval-2018 AIT Distant Supervision Corpus (DISC)**, which is used for emotion recognition in tweets. This dataset is significant as it contributes to the model's ability to understand emotional context.

Another dataset mentioned is the **CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)** dataset, which contains annotated video segments for emotion recognition. This dataset is also crucial for the authors' approach to emotion-specific audio feature extraction.

After identifying these datasets, I will check the **References section** to gather the full citations for each dataset:

1. **FORBOW Dataset**: The citation is not explicitly provided in the paper, but it is essential to note that it is part of the Families Overcoming Risks and Building Opportunities for Well Being research project.

2. **SemEval-2018 AIT Distant Supervision Corpus (DISC)**: The citation is:
   > Mohammad, Saif, Bravo-Marquez, Felipe, Salameh, Mohammad, & Kiritchenko, Svetlana. 2018. *Semeval-2018 task 1: Affect in tweets*. Pages 1–17 of: Proceedings of The 12th International Workshop on Semantic Evaluation.

3. **CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)**: The citation is:
   > Zadeh, Amir, Liang, Paul Pu, Poria, Soujanya, Cambria, Erik, & Morency, Louis-Philippe. 2018. *Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph*. Pages 2236–2246 of: Association for Computational Linguistics (ACL).

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review.