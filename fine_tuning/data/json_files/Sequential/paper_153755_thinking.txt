To extract datasets from the research paper titled "SCATTER: Selective Context Attentional Scene Text Recognizer" by Ron Litman et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** to get an overview of the paper's focus and any mentions of datasets. The abstract indicates that the authors evaluate their method on various benchmarks, which suggests that datasets will be discussed in detail later in the paper.

Next, I will navigate to **section 4.1 (Datasets)**, where the authors explicitly list the datasets used for training and evaluation. This section is crucial as it typically contains the names and descriptions of the datasets.

In section 4.1, the authors mention three synthetic datasets used for training:
1. **MJSynth (MJ)**: A synthetic dataset containing 9 million word box images generated from a lexicon of 90K English words.
2. **SynthText (ST)**: A dataset designed for scene-text detection and recognition, comprising 5.5 million samples, excluding non-alphanumeric characters.
3. **SynthAdd (SA)**: A dataset with 1.2 million word box images, generated to include non-alphanumeric characters.

The authors also describe the evaluation datasets, which include:
- **ICDAR2003**: Contains 867 word-box cropped images for testing.
- **ICDAR2013**: Comprises 848 training and 1,015 testing word-box cropped images.
- **IIIT5K**: Consists of 2,000 training and 3,000 testing images cropped from Google image searches.
- **SVT**: A dataset from Google Street View images with 257 training and 647 testing word-box cropped images.
- **ICDAR2015**: Contains 4,468 training and 2,077 testing word-box cropped images.
- **SVTP**: A dataset with 645 cropped word-box images for testing.
- **CUTE**: Contains 288 cropped word-box images for testing, many of which are curved text images.

After identifying the datasets, I will check the **References section** to find the full citations for each dataset mentioned. This is essential for proper attribution and to provide readers with sources for further exploration.

The full citations I will extract are:
- For **MJSynth (MJ)**:
  > Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. *Synthetic data and artificial neural networks for natural scene text recognition*. arXiv preprint arXiv:1406.2227, 2014.

- For **SynthText (ST)**:
  > Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. *Synthetic data for text localisation in natural images*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2315–2324, 2016.

- For **SynthAdd (SA)**:
  > Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. *Show, attend and read: A simple and strong baseline for irregular text recognition*. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8610–8617, 2019.

- For **ICDAR2003**:
  > Simon M Lucas, Alex Panaretos, Luis Sosa, Anthony Tang, Shirley Wong, and Robert Young. *ICDAR 2003 robust reading competitions*. In Seventh International Conference on Document Analysis and Recognition, pages 682–687. Citeseer, 2003.

- For **ICDAR2013**:
  > Dimosthenis Karatzas, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, and others. *ICDAR 2013 robust reading competition*. In 2013 12th International Conference on Document Analysis and Recognition, pages 1484–1493. IEEE, 2013.

- For **IIIT5K**:
  > Anand Mishra, Karteek Alahari, and CV Jawahar. *Scene text recognition using higher order language priors*. 2012.

- For **SVT**:
  > Kai Wang, Boris Babenko, and Serge Belongie. *End-to-end scene text recognition*. In 2011 International Conference on Computer Vision, pages 1457–1464. IEEE, 2011.

- For **ICDAR2015**:
  > Dimosthenis Karatzas, Lluis Gomez-Bigorda, and others. *ICDAR 2015 competition on robust reading*. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 1156–1160. IEEE, 2015.

- For **SVTP**:
  > Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. *Recognizing text with perspective distortion in natural scenes*. In Proceedings of the IEEE International Conference on Computer Vision, pages 569–576, 2013.

- For **CUTE**:
  > Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan. *A robust arbitrary text detection system for natural scene images*. Expert Systems with Applications, 41(18):8027–8048, 2014.

Now, I will compile this information into a structured format for easy reference and ensure that all dataset citations are included for completeness.