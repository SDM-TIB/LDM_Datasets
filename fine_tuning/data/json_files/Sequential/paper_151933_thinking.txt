To extract datasets from the research paper titled "Characterizing the hyper-parameter space of LSTM language models for mixed context applications" by Victor Akinwande and Sekou L. Remy, I will follow a systematic approach.

First, I will read through the **abstract, introduction, methodology, and results sections** to identify any datasets mentioned. The abstract discusses the application of deep learning models to novel datasets, which suggests that datasets are involved.

In the **introduction**, the authors mention that state-of-the-art models are benchmarked on a limited set of datasets, including Penn Treebank, WikiText, and others. However, I need to look for specific datasets used in their experiments.

Next, I will focus on the **methodology section**, particularly the subsection labeled **A. Datasets**. Here, the authors describe two datasets collected using the Twitter streaming API with geolocation filters for Kenya and Nigeria. They specify:

1. **Dataset 1 (Kenya Corpus)**: This dataset contains code-mixed text with several mixes of English and Swahili, totaling 13,185 words.
2. **Dataset 2 (Nigeria Corpus)**: This dataset consists of English rewritten in a pidgin form, containing 27,855 words.

The authors provide a clear description of the datasets, including their characteristics and the context in which they were collected.

Now, I will check the **References section** to find full citations for the datasets mentioned. However, since these datasets are derived from Twitter data and are not standard datasets with established citations, I will note that they are original datasets created for this research.

Finally, I will summarize the datasets with their descriptions and citations, ensuring to highlight that these datasets are unique to this study and do not have formal citations like established datasets.

After gathering all this information, I will compile the dataset entries into a structured format for further processing or review.