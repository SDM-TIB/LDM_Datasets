[
    {
        "dcterms:creator": [
            "M. P. Marcus",
            "M. A. Marcinkiewicz",
            "B. Santorini"
        ],
        "dcterms:description": "The Penn Tree Bank dataset is made of articles from the Wall Street Journal, contains 929k training tokens and has a vocabulary size of 10k.",
        "dcterms:title": "Penn Tree Bank",
        "dcterms:issued": "1993",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text corpus",
            "Language modeling",
            "Annotated corpus"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "S. Merity",
            "C. Xiong",
            "J. Bradbury",
            "R. Socher"
        ],
        "dcterms:description": "The wikitext2 dataset is derived from Wikipedia articles, contains 2M training tokens and has a vocabulary size of 33k.",
        "dcterms:title": "wikitext2",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text corpus",
            "Wikipedia",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The text8 dataset contains 17M training tokens and has a vocabulary size of 44k words.",
        "dcterms:title": "text8",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The wikitext103 dataset has a training set of size 103M, and a vocabulary size of 267k words.",
        "dcterms:title": "wikitext103",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Paperno",
            "G. Kruszewski",
            "A. Lazaridou",
            "Q. N. Pham",
            "R. Bernardi",
            "S. Pezzelle",
            "M. Baroni",
            "G. Boleda",
            "R. Fern√°ndez"
        ],
        "dcterms:description": "The LAMBADA dataset is a dataset of short passages extracted from novels, where the goal is to predict the last word of the excerpt.",
        "dcterms:title": "LAMBADA",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text corpus",
            "Word prediction",
            "Discourse context"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Prediction"
        ]
    }
]