To extract datasets from the research paper titled "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation" by Mohsen Fayyaz et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental results sections** to identify any datasets mentioned. The abstract states that the proposed method achieves state-of-the-art performance on two datasets, which indicates that datasets are indeed involved.

Next, I will focus on **section 4 (Experimental Results)**, where the authors describe their experiments. Here, they explicitly mention two datasets used for evaluation:

1. **CamVid Dataset**: The authors describe it as a collection of videos with object-class semantic labels, providing ground truth labels that associate each pixel with one of 32 semantic classes. They mention the dataset's partitioning into training, validation, and test images.

2. **NYUDv2 Dataset**: This dataset is described as comprising video sequences from various indoor scenes recorded with an RGB-Depth camera, featuring densely labeled pairs of aligned RGB and depth images.

To confirm the details, I will check the **References section** for full citations of these datasets:

- For the **CamVid Dataset**, the citation is:
  > Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R. *Segmentation and recognition using structure from motion point clouds*. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 44–57, 2008.

- For the **NYUDv2 Dataset**, the citation is:
  > Silberman, N., Hoiem, D., Kohli, P., Fergus, R. *Indoor segmentation and support inference from RGBD images*. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 746–760, 2012.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further use. This ensures that I have accurately captured the datasets referenced in the paper along with their proper citations.