[
    {
        "dcterms:creator": [
            "Leonard Bongard",
            "Lena Held",
            "Ivan Habernal"
        ],
        "dcterms:description": "The dataset comprises three sets: Train Set, Dev Set, and Test Set, containing 666, 84, and 98 data points, respectively. Each entry includes fields such as Question, Answer, Explanation, Label, Analysis, and Complete-Analysis.",
        "dcterms:title": "SemEval-2024 Task 5 Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Legal Argument Reasoning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Legal Texts",
            "Binary Classification",
            "Question Answering",
            "Civil Procedure"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Legal Argument Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Ilias Chalkidis",
            "Manos Fergadiotis",
            "Prodromos Malakasiotis",
            "Nikolaos Aletras",
            "Ion Androutsopoulos"
        ],
        "dcterms:description": "LEGAL-BERT is a transformer model fine-tuned on legal data to improve performance on legal NLP tasks.",
        "dcterms:title": "Legal-BERT",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Legal NLP",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Legal Texts",
            "Transformer Model",
            "Fine-tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Legal Text Classification",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "Greg Corrado",
            "Jeffrey Dean"
        ],
        "dcterms:description": "Word2Vec is a model for generating word embeddings that capture semantic meanings of words based on their context.",
        "dcterms:title": "Word2Vec",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Word Embeddings",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Word Representation",
            "Semantic Similarity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Similarity",
            "Word Representation"
        ]
    },
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe is a model for generating word embeddings that captures global statistical information of words in a corpus.",
        "dcterms:title": "GloVe",
        "dcterms:issued": "2014",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Word Embeddings",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Global Vectors",
            "Word Representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Similarity",
            "Word Representation"
        ]
    },
    {
        "dcterms:creator": [
            "Pengcheng He",
            "Xiaodong Liu",
            "Jianfeng Gao",
            "Wei Chen"
        ],
        "dcterms:description": "Deberta is a transformer model that enhances BERT with disentangled attention mechanisms.",
        "dcterms:title": "Deberta",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Transformer Models"
        ],
        "dcat:keyword": [
            "Disentangled Attention",
            "Word Representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Question Answering"
        ]
    }
]