[
    {
        "dcterms:creator": [
            "T. Yu",
            "D. Quillen",
            "Z. He",
            "R. Julian",
            "K. Hausman",
            "C. Finn",
            "S. Levine"
        ],
        "dcterms:description": "A benchmark for multi-task and meta reinforcement learning, consisting of various robotic manipulation tasks designed to evaluate the performance of RL algorithms.",
        "dcterms:title": "MetaWorld Benchmark",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Robotics"
        ],
        "dcat:keyword": [
            "Multi-task learning",
            "Meta reinforcement learning",
            "Robotic manipulation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Robotic manipulation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Fu",
            "A. Kumar",
            "O. Nachum",
            "G. Tucker",
            "S. Levine"
        ],
        "dcterms:description": "Datasets for deep data-driven reinforcement learning, providing a variety of environments and tasks for evaluating RL algorithms.",
        "dcterms:title": "D4RL Benchmark",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2004.07219",
        "dcat:theme": [
            "Reinforcement Learning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Deep reinforcement learning",
            "Datasets",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "T. Haarnoja",
            "A. Zhou",
            "P. Abbeel",
            "S. Levine"
        ],
        "dcterms:description": "A reinforcement learning algorithm that utilizes a maximum entropy framework to learn policies in an off-policy manner.",
        "dcterms:title": "SAC Model",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Soft Actor-Critic",
            "Maximum entropy",
            "Off-policy learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy optimization"
        ]
    },
    {
        "dcterms:creator": [
            "C. Kim",
            "J. Park",
            "J. Shin",
            "H. Lee",
            "P. Abbeel",
            "K. Lee"
        ],
        "dcterms:description": "A model that uses transformers to learn human preferences for reinforcement learning tasks.",
        "dcterms:title": "Preference Transformer",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Preference Learning"
        ],
        "dcat:keyword": [
            "Transformers",
            "Human preferences",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Preference modeling"
        ]
    },
    {
        "dcterms:creator": [
            "I. Kostrikov",
            "A. Nair",
            "S. Levine"
        ],
        "dcterms:description": "An offline reinforcement learning algorithm that employs implicit Q-learning to optimize policies from historical data.",
        "dcterms:title": "P-IQL",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Offline Learning"
        ],
        "dcat:keyword": [
            "Implicit Q-learning",
            "Offline reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy optimization"
        ]
    },
    {
        "dcterms:creator": [
            "G. An",
            "J. Lee",
            "X. Zuo",
            "N. Kosaka",
            "K.-M. Kim",
            "H. O. Song"
        ],
        "dcterms:description": "A method for optimizing policies based on direct preferences without the need for reward modeling.",
        "dcterms:title": "BC Regularization",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Preference Learning"
        ],
        "dcat:keyword": [
            "Behavior cloning",
            "Preference-based optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy optimization"
        ]
    },
    {
        "dcterms:creator": [
            "C. Kim",
            "J. Park",
            "J. Shin",
            "H. Lee",
            "P. Abbeel",
            "K. Lee"
        ],
        "dcterms:description": "Results from experiments using real human preferences on the D4RL benchmark, demonstrating the effectiveness of the Preference Transformer.",
        "dcterms:title": "D4RL Real Human Preference Results",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Human preferences",
            "Reinforcement learning",
            "D4RL"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Preference modeling"
        ]
    }
]