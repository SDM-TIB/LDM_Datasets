[
    {
        "dcterms:creator": [
            "Richard Socher",
            "Alex Perelygin",
            "Jean Wu",
            "Jason Chuang",
            "Christopher D. Manning",
            "A. Ng",
            "Christopher Potts"
        ],
        "dcterms:description": "The SST-5 dataset is used for evaluating the performance of models on sentiment analysis tasks, specifically focusing on the sensitivity of causal language models to the order of in-context examples.",
        "dcterms:title": "SST-5",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sentiment Analysis"
        ],
        "dcat:keyword": [
            "Sentiment",
            "Text Classification",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Xiaodong Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "The MMLU dataset is employed to assess the performance of models in a massive multitask language understanding context, particularly focusing on the robustness of models against variations in demonstration order.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multitask Learning",
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Richard Socher",
            "Alex Perelygin",
            "Jean Wu",
            "Jason Chuang",
            "Christopher D. Manning",
            "A. Ng",
            "Christopher Potts"
        ],
        "dcterms:description": "The SST-2 dataset is utilized for evaluating sentiment analysis models, particularly in the context of understanding the impact of in-context example order on model performance.",
        "dcterms:title": "SST-2",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sentiment Analysis"
        ],
        "dcat:keyword": [
            "Sentiment",
            "Text Classification",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Ido Dagan",
            "Oren Glickman",
            "Bernardo Magnini"
        ],
        "dcterms:description": "The RTE dataset is used for evaluating textual entailment tasks, focusing on the robustness of models against variations in the order of in-context examples.",
        "dcterms:title": "RTE",
        "dcterms:issued": "2005",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Textual Entailment"
        ],
        "dcat:keyword": [
            "Entailment",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Textual Entailment"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "The QNLI dataset is part of the GLUE benchmark and is used to evaluate natural language understanding tasks, particularly in the context of in-context learning and the order of demonstration examples.",
        "dcterms:title": "QNLI",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Understanding"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]