[
    {
        "dcterms:creator": [
            "Chaitanya Ahuja",
            "Dong Won Lee",
            "Ryo Ishii",
            "Louis-Philippe Morency"
        ],
        "dcterms:description": "The PATS dataset consists of transcribed poses with aligned audios and text transcriptions, containing around 84,000 clips from 25 speakers with a mean length of 10.7 seconds, totaling 251 hours.",
        "dcterms:title": "PATS dataset",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Gesture Recognition",
            "Speech Processing"
        ],
        "dcat:keyword": [
            "Co-speech gestures",
            "Audio alignment",
            "Video clips"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Gesture Generation",
            "Speech-to-Video Synthesis"
        ]
    },
    {
        "dcterms:creator": [
            "Xian Liu",
            "Qianyi Wu",
            "Hang Zhou",
            "Yuanqi Du",
            "Wayne Wu",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "dcterms:description": "ANGIE is a dataset for audio-driven co-speech gesture video generation, utilizing an unsupervised feature to model body motion and leveraging a VQ-VAE to quantize common patterns.",
        "dcterms:title": "ANGIE",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Gesture Generation",
            "Audio-Visual Processing"
        ],
        "dcat:keyword": [
            "Co-speech gestures",
            "Video generation",
            "Audio-driven synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Gesture Video Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Ruilong Li",
            "Shan Yang",
            "David A Ross",
            "Angjoo Kanazawa"
        ],
        "dcterms:description": "AIST++ is a dataset for music conditioned 3D dance generation, providing a rich set of dance videos with corresponding music tracks.",
        "dcterms:title": "AIST++",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dance Generation",
            "Music Synchronization"
        ],
        "dcat:keyword": [
            "3D dance",
            "Music-driven generation",
            "Dance videos"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Dance Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Will Kay",
            "Joao Carreira",
            "Karen Simonyan",
            "Brian Zhang",
            "Chloe Hillier",
            "Sudheendra Vijayanarasimhan",
            "Fabio Viola",
            "Tim Green",
            "Trevor Back",
            "Paul Natsev"
        ],
        "dcterms:description": "Kinetics-400 is a large-scale dataset for human action recognition, containing thousands of video clips across various action categories.",
        "dcterms:title": "Kinetics-400",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Video Analysis"
        ],
        "dcat:keyword": [
            "Human actions",
            "Video dataset",
            "Action classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Haiyang Liu",
            "Zihao Zhu",
            "Naoya Iwamoto",
            "Yichen Peng",
            "Zhengqing Li",
            "You Zhou",
            "Elif Bozkurt",
            "Bo Zheng"
        ],
        "dcterms:description": "The BEAT dataset is a large-scale semantic and emotional multi-modal dataset designed for conversational gesture synthesis.",
        "dcterms:title": "BEAT dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Gesture Synthesis",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Multi-modal dataset",
            "Conversational gestures",
            "Emotional gestures"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Gesture Synthesis"
        ]
    }
]