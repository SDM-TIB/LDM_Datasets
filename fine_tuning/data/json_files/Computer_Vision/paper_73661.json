[
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric Xing"
        ],
        "dcterms:description": "MT-Bench is a multi-turn question set designed for evaluating language models across various tasks including writing, roleplay, extraction, reasoning, math, coding, and knowledge in STEM and humanities/social sciences.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multi-turn questions",
            "Evaluation",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of language models"
        ]
    },
    {
        "dcterms:creator": [
            "Wei-Lin Chiang",
            "Zhuohan Li",
            "Zi Lin",
            "Ying Sheng",
            "Zhanghao Wu",
            "Hao Zhang",
            "Lianmin Zheng",
            "Siyuan Zhuang",
            "Yonghao Zhuang",
            "Joseph E. Gonzalez",
            "Ion Stoica",
            "Eric P. Xing"
        ],
        "dcterms:description": "Vicuna-Bench is a single-turn question set aimed at evaluating language models on tasks such as writing, roleplay, generic queries, Fermi problems, counterfactuals, coding, math, and knowledge.",
        "dcterms:title": "Vicuna-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Single-turn questions",
            "Evaluation",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of language models"
        ]
    },
    {
        "dcterms:creator": [
            "Edward Beeching",
            "Cl√©mentine Fourrier",
            "Nathan Habib",
            "Sheon Han",
            "Nathan Lambert",
            "Nazneen Rajani",
            "Omar Sanseviero",
            "Lewis Tunstall",
            "Thomas Wolf"
        ],
        "dcterms:description": "The Open-LLM Leaderboard includes various datasets for evaluating language models on tasks such as commonsense reasoning, multi-task language understanding, human falsehood mimicry, and math problem solving.",
        "dcterms:title": "Open-LLM Leaderboard",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Multi-task evaluation",
            "Language models"
        ],
        "dcat:landingPage": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of language models"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan"
        ],
        "dcterms:description": "HH-RLHF is a dataset used for training language models to be helpful and harmless through reinforcement learning from human feedback.",
        "dcterms:title": "HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reinforcement learning",
            "Human feedback",
            "Language models"
        ],
        "dcat:landingPage": "arXiv:2204.05862",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Training language models"
        ]
    },
    {
        "dcterms:creator": [
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Guanming Yao",
            "Wei Zhu",
            "Yuan Ni",
            "Guotong Xie",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "dcterms:description": "Ultra-feedback is a dataset designed to enhance language models by providing high-quality feedback.",
        "dcterms:title": "Ultra-feedback",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.01377",
        "dcat:theme": [
            "Natural Language Processing",
            "Feedback Mechanisms"
        ],
        "dcat:keyword": [
            "High-quality feedback",
            "Language models",
            "Training"
        ],
        "dcat:landingPage": "arXiv:2310.01377",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Training language models"
        ]
    },
    {
        "dcterms:creator": [
            "Mohammad Gheshlaghi Azar",
            "Zhaohan Daniel Guo",
            "Bilal Piot",
            "Remi Munos",
            "Mark Rowland",
            "Michal Valko",
            "Daniele Calandriello"
        ],
        "dcterms:description": "Ultrafeedback-binarized-preferences is a dataset that provides a theoretical framework for understanding learning from human preferences.",
        "dcterms:title": "Ultrafeedback-binarized-preferences",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Preference Learning"
        ],
        "dcat:keyword": [
            "Human preferences",
            "Learning",
            "Theoretical framework"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Understanding human preferences"
        ]
    }
]