[
    {
        "dcterms:creator": [
            "J. Xu",
            "T. Mei",
            "T. Yao",
            "Y. Rui"
        ],
        "dcterms:description": "MSR-VTT benchmark contains 10K videos from YouTube with 5 captions per video, focusing on short videos averaging 15 seconds.",
        "dcterms:title": "MSR-VTT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Video Description"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Video captions",
            "YouTube videos"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-to-Video Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "M. Bain",
            "A. Nagrani",
            "A. Brown",
            "A. Zisserman"
        ],
        "dcterms:description": "Condensed Movies Dataset (CMD) is a long-form text-video dataset consisting of 34K videos of movie scenes with an average duration of 132 seconds and corresponding high-level semantic textual descriptions.",
        "dcterms:title": "Condensed Movies Dataset (CMD)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Movie Scene Understanding"
        ],
        "dcat:keyword": [
            "Movie dataset",
            "Video scenes",
            "Semantic descriptions"
        ],
        "dcat:landingPage": "https://www.robots.ox.ac.uk/~vgg/research/condensed-movies/challenge.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-to-Video Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "R. Krishna",
            "K. Hata",
            "F. Ren",
            "L. Fei-Fei",
            "J. Carlos Niebles",
            "A. Farhadi"
        ],
        "dcterms:description": "ActivityNet Captions consists of 20K videos from YouTube focused primarily on actions, annotated with 100K sentences, capturing a diverse range of events and actions.",
        "dcterms:title": "ActivityNet Captions",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Action dataset",
            "Video captions",
            "YouTube videos"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-to-Video Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "G. Sigurdsson",
            "G. Varol",
            "X. Wang",
            "A. Farhadi",
            "I. Laptev",
            "A. Gupta"
        ],
        "dcterms:description": "Charades is a video classification dataset consisting of daily activities with an average duration of 30 seconds, allowing for multilabel and multiclass classification.",
        "dcterms:title": "Charades",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Classification",
            "Activity Recognition"
        ],
        "dcat:keyword": [
            "Activity dataset",
            "Video classification",
            "Daily activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Classification"
        ]
    }
]