[
    {
        "dcterms:creator": [
            "Hao Tan",
            "Mohit Bansal"
        ],
        "dcterms:description": "A dataset used for learning cross-modality encoder representations from transformers, focusing on vision and language tasks.",
        "dcterms:title": "LXMERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Cross-modality",
            "Encoder representations",
            "Transformers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Masked Language Modeling",
            "Vision-Language Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Piyush Sharma",
            "Nan Ding",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "dcterms:description": "A cleaned, hypernymed dataset for automatic image captioning, providing image alt-text.",
        "dcterms:title": "Conceptual Captions (CC)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Image alt-text",
            "Automatic captioning",
            "Hypernymed dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Vicente Ordonez",
            "Girish Kulkarni",
            "Tamara L. Berg"
        ],
        "dcterms:description": "A dataset containing 1 million captioned photographs for describing images.",
        "dcterms:title": "SBU Captions",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Description",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Captioned photographs",
            "Image description",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "A dataset of common objects in context, providing images with multiple annotations.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Detection",
            "Image Segmentation"
        ],
        "dcat:keyword": [
            "Common objects",
            "Image annotations",
            "Computer Vision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Object Detection",
            "Image Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "A dataset connecting language and vision using crowdsourced dense image annotations.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Vision-Language"
        ],
        "dcat:keyword": [
            "Dense annotations",
            "Image understanding",
            "Crowdsourced data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image Annotation",
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "A dataset for visual question answering that emphasizes the role of image understanding.",
        "dcterms:title": "VQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Question answering",
            "Image understanding",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yuke Zhu",
            "Oliver Groth",
            "Michael S. Bernstein",
            "Li Fei-Fei"
        ],
        "dcterms:description": "A dataset for grounded question answering in images, focusing on spatial and object relationships.",
        "dcterms:title": "VG-QA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Image Understanding"
        ],
        "dcat:keyword": [
            "Grounded question answering",
            "Spatial relationships",
            "Object relationships"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Drew A. Hudson",
            "Christopher D. Manning"
        ],
        "dcterms:description": "A new dataset for real-world visual reasoning and compositional question answering.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Compositional reasoning",
            "Visual reasoning",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Marc Brysbaert",
            "Amy Beth Warriner",
            "Victor Kuperman"
        ],
        "dcterms:description": "A dataset providing concreteness ratings for 40 thousand generally known English word lemmas.",
        "dcterms:title": "Lexical Concreteness Dataset",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Linguistics",
            "Psycholinguistics"
        ],
        "dcat:keyword": [
            "Concreteness ratings",
            "Word lemmas",
            "Language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]