[
    {
        "dcterms:creator": [
            "P. Auer",
            "N. Cesa-Bianchi",
            "P. Fischer"
        ],
        "dcterms:description": "The UCB algorithm is a popular method for addressing the multi-armed bandit problem, focusing on balancing exploration and exploitation without prior knowledge of arm statistics.",
        "dcterms:title": "UCB Algorithm",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-armed Bandit",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "UCB",
            "Multi-armed Bandit",
            "Exploration-Exploitation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [
            "A. Garivier",
            "O. Cappe"
        ],
        "dcterms:description": "The KL-UCB algorithm extends the UCB algorithm by incorporating Kullback-Leibler divergence to improve performance in bounded stochastic bandits.",
        "dcterms:title": "KL-UCB Algorithm",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-armed Bandit",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "KL-UCB",
            "Kullback-Leibler Divergence",
            "Multi-armed Bandit"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [
            "S. Agrawal",
            "N. Goyal"
        ],
        "dcterms:description": "Thompson Sampling (TS) is a Bayesian approach to the multi-armed bandit problem that uses probability distributions to model the uncertainty of arm rewards.",
        "dcterms:title": "Thompson Sampling (TS) Algorithm",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-armed Bandit",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Thompson Sampling",
            "Bayesian Methods",
            "Multi-armed Bandit"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [
            "S. Agrawal",
            "N. Goyal"
        ],
        "dcterms:description": "The Bernoulli distribution is used in the context of multi-armed bandits to model binary outcomes, where each arm can yield a reward of either success or failure.",
        "dcterms:title": "Bernoulli Distribution",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Probability Distribution",
            "Multi-armed Bandit"
        ],
        "dcat:keyword": [
            "Bernoulli",
            "Binary Outcomes",
            "Multi-armed Bandit"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [
            "S. Agrawal",
            "N. Goyal"
        ],
        "dcterms:description": "The Gaussian distribution is utilized in multi-armed bandit algorithms to model continuous outcomes, allowing for a wider range of reward structures.",
        "dcterms:title": "Gaussian Distribution",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Probability Distribution",
            "Multi-armed Bandit"
        ],
        "dcat:keyword": [
            "Gaussian",
            "Continuous Outcomes",
            "Multi-armed Bandit"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [
            "L. Besson",
            "E. Kaufmann",
            "C. Moy"
        ],
        "dcterms:description": "The Velcro MAB architecture aggregates multiple multi-armed bandit algorithms to optimize resource allocation in opportunistic spectrum access scenarios.",
        "dcterms:title": "Velcro MAB Architecture",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-armed Bandit",
            "Wireless Communications"
        ],
        "dcat:keyword": [
            "Velcro MAB",
            "Spectrum Access",
            "Multi-armed Bandit"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [
            "S. V. S. Santosh",
            "S. J. Darak"
        ],
        "dcterms:description": "The SBTS algorithm is a synthesizable version of Thompson Sampling designed for efficient implementation on hardware platforms.",
        "dcterms:title": "SBTS Algorithm",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-armed Bandit",
            "Hardware Implementation"
        ],
        "dcat:keyword": [
            "SBTS",
            "Thompson Sampling",
            "Hardware Implementation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The RI-MAB algorithm is a reconﬁgurable and intelligent architecture for multi-armed bandit algorithms that adapts to varying arm distributions.",
        "dcterms:title": "RI-MAB Algorithm",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/Sai-Santosh-99/TS-Aggregator",
        "dcat:theme": [
            "Multi-armed Bandit",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "RI-MAB",
            "Reconﬁgurable Architecture",
            "Multi-armed Bandit"
        ],
        "dcat:landingPage": "https://github.com/Sai-Santosh-99/TS-Aggregator",
        "dcterms:hasVersion": "",
        "dcterms:format": ""
    }
]