[
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "Marie-Anne Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar"
        ],
        "dcterms:description": "LLaMA2 is an open foundation and fine-tuned chat model that serves as a base for training large language models.",
        "dcterms:title": "LLaMA2",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Model",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Foundation model",
            "Chat model",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Chatbot development"
        ]
    },
    {
        "dcterms:creator": [
            "Guilherme Penedo",
            "Quentin Malartic",
            "Daniel Hesslow",
            "Ruxandra Cojocaru",
            "Alessandro Cappelli",
            "Hamza Alobeidli",
            "Baptiste Pannier",
            "Ebtesam Almazrouei",
            "Julien Launay"
        ],
        "dcterms:description": "RefinedWeb is a dataset that outperforms curated corpora with web data, specifically designed for training language models.",
        "dcterms:title": "RefinedWeb",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Web Data",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "Web dataset",
            "Language model training",
            "Curated corpora"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Yudong Li",
            "Yuqing Zhang",
            "Zhe Zhao",
            "Linlin Shen",
            "Weijie Liu",
            "Weiquan Mao",
            "Hui Zhang"
        ],
        "dcterms:description": "CSL is a large-scale Chinese scientific literature dataset used for pre-training language models.",
        "dcterms:title": "CSL",
        "dcterms:issued": "2022",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Scientific Literature",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "Chinese dataset",
            "Scientific literature",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Scientific text understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yan Gong",
            "Yiping Peng",
            "Qiang Niu",
            "Baochang Ma",
            "Yunjie Ji",
            "Yong Deng",
            "Xiangang Li"
        ],
        "dcterms:description": "BELLE is a large language model engine designed to enhance conversational AI capabilities.",
        "dcterms:title": "BELLE",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "https://github.com/LianjiaTech/BELLE",
        "dcat:theme": [
            "Conversational AI",
            "Language Model"
        ],
        "dcat:keyword": [
            "Language model",
            "Conversational AI",
            "Instruction data"
        ],
        "dcat:landingPage": "https://github.com/LianjiaTech/BELLE",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational AI",
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Ning Ding",
            "Yulin Chen",
            "Bokai Xu",
            "Yujia Qin",
            "Zhi Zheng",
            "Shengding Hu"
        ],
        "dcterms:description": "UltraChat is a dataset that enhances chat language models by scaling high-quality instructional conversations.",
        "dcterms:title": "UltraChat",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instructional Data",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Instructional conversations",
            "Chat model training",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational AI",
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Jason Wei",
            "Maarten Bosma",
            "Vincent Zhao",
            "Kelvin Guu",
            "Adams Wei Yu"
        ],
        "dcterms:description": "FLAN is a dataset designed for fine-tuning language models to improve their zero-shot learning capabilities.",
        "dcterms:title": "FLAN",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fine-tuning",
            "Language Model"
        ],
        "dcat:keyword": [
            "Fine-tuning",
            "Zero-shot learning",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Zero-shot learning"
        ]
    },
    {
        "dcterms:creator": [
            "Ge Zhang",
            "Yemin Shi",
            "Ruibo Liu",
            "Ruibin Yuan",
            "Yizhi Li",
            "Siwei Dong",
            "Yu Shu",
            "Zhaoqun Li",
            "Zekun Wang"
        ],
        "dcterms:description": "COIG is a dataset for Chinese open instruction generalist models, providing a preliminary release for research.",
        "dcterms:title": "COIG",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Data",
            "Language Model"
        ],
        "dcat:keyword": [
            "Chinese dataset",
            "Instruction following",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following",
            "Language modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "MMLU is a benchmark for measuring massive multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmark",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Benchmarking"
        ]
    },
    {
        "dcterms:creator": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su"
        ],
        "dcterms:description": "C-Eval is a multi-level, multi-discipline Chinese evaluation suite for foundation models.",
        "dcterms:title": "C-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Language Model"
        ],
        "dcat:keyword": [
            "Chinese evaluation",
            "Foundation models",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Haonan Li",
            "Yixuan Zhang",
            "Fajri Koto",
            "Yifei Yang",
            "Hai Zhao",
            "Yeyun Gong",
            "Nan Duan",
            "Timothy Baldwin"
        ],
        "dcterms:description": "CMMLU is a benchmark for measuring massive multitask language understanding specifically in Chinese.",
        "dcterms:title": "CMMLU",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmark",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Chinese benchmark",
            "Multitask learning",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Benchmarking"
        ]
    },
    {
        "dcterms:creator": [
            "Xiaotian Zhang",
            "Chunyang Li",
            "Yi Zong",
            "Zhengyu Ying",
            "Liang He",
            "Xipeng Qiu"
        ],
        "dcterms:description": "GAOKAO is a benchmark evaluating the performance of large language models on the Chinese college entrance examination.",
        "dcterms:title": "GAOKAO",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Language Model"
        ],
        "dcat:keyword": [
            "Chinese college entrance exam",
            "Benchmarking",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Liang Xu",
            "Anqi Li",
            "Lei Zhu",
            "Hang Xue",
            "Changtai Zhu",
            "Kangkang Zhao",
            "Haonan He",
            "Xuanwei Zhang",
            "Qiyue Kang",
            "Zhenzhong Lan"
        ],
        "dcterms:description": "SuperCLUE is a comprehensive Chinese large language model benchmark for evaluating model performance.",
        "dcterms:title": "SuperCLUE",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmark",
            "Language Model"
        ],
        "dcat:keyword": [
            "Chinese benchmark",
            "Model evaluation",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Miquel Esplà-Gomis",
            "Mikel L Forcada",
            "Gema Ramírez-Sánchez",
            "Hieu Hoang"
        ],
        "dcterms:description": "ParaCrawl v9 is a web-scale parallel corpus for the languages of the EU, useful for training multilingual models.",
        "dcterms:title": "ParaCrawl v9",
        "dcterms:issued": "2019",
        "dcterms:language": "Multilingual",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Parallel Corpus",
            "Multilingual Training"
        ],
        "dcat:keyword": [
            "Parallel corpus",
            "Web-scale data",
            "Multilingual training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Translation"
        ]
    },
    {
        "dcterms:creator": [
            "Holger Schwenk",
            "Vishrav Chaudhary",
            "Shuo Sun",
            "Hongyu Gong",
            "Francisco Guzmán"
        ],
        "dcterms:description": "WikiMatrix is a dataset that mines parallel sentences in multiple languages from Wikipedia, facilitating multilingual training.",
        "dcterms:title": "WikiMatrix",
        "dcterms:issued": "2021",
        "dcterms:language": "Multilingual",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Parallel Corpus",
            "Multilingual Training"
        ],
        "dcat:keyword": [
            "Parallel sentences",
            "Wikipedia data",
            "Multilingual training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Translation"
        ]
    },
    {
        "dcterms:creator": [
            "Liang Xu",
            "Xuanwei Zhang",
            "Qianqian Dong"
        ],
        "dcterms:description": "CLUECorpus is a large-scale Chinese corpus designed for pre-training language models.",
        "dcterms:title": "CLUECorpus",
        "dcterms:issued": "2020",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Chinese Corpus",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "Chinese dataset",
            "Language model training",
            "Pre-training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Pre-training"
        ]
    }
]