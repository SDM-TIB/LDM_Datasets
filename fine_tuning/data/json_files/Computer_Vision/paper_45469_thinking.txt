To extract datasets from the research paper titled "EDNet: Efficient Disparity Estimation with Cost Volume Combination and Attention-based Spatial Residual" by Songyan Zhang et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract states that extensive experiments were conducted on the **Scene Flow** and **KITTI** datasets, indicating that these datasets are crucial for the research.

Next, I will look for specific details about these datasets in the **experiments section**. In this section, the authors describe the **Scene Flow dataset** as consisting of 39,824 pairs of synthetic stereo RGB images, with 35,454 pairs for training and 4,370 pairs for testing. They also mention the **KITTI dataset**, which includes real scenes with a full resolution of 1242×375, and that the ground truth is generated by lidar, providing sparse ground truth data.

I will then check the **References section** to find the full citations for these datasets:

1. For the **Scene Flow dataset**, the citation is:
   > Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., & Brox, T. (2016). A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 4040–4048.

2. For the **KITTI dataset**, the citation is:
   > Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? The KITTI vision benchmark suite. In *2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 3354–3361.

Now that I have identified the datasets and their full citations, I will compile this information into a structured format for further processing or review.