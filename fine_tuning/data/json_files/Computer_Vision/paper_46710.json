[
    {
        "dcterms:creator": [
            "Kamel A",
            "Sheng B",
            "Yang P",
            "Li P",
            "Shen R",
            "Feng DD"
        ],
        "dcterms:description": "The MSRAction3D dataset is used for action recognition and contains 557 depth video sequences and 557 skeleton sequences for 20 actions captured by Kinect sensor. The actions include high arm wave, horizontal arm wave, hammer, hand catch, forward punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, side boxing, bend, forward kick, side kick, jogging, tennis swing, tennis serve, golf swing, and pickup and throw, performed by 10 subjects.",
        "dcterms:title": "MSRAction3D",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/TSMC.2018.2850149",
        "dcat:theme": [
            "Human Action Recognition",
            "Depth Video Analysis"
        ],
        "dcat:keyword": [
            "Depth video sequences",
            "Action recognition",
            "Kinect sensor",
            "Human actions"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/TSMC.2018.2850149",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Chen C",
            "Jafari R",
            "Kehtarnavaz N"
        ],
        "dcterms:description": "UTD-MHAD is a multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor. It consists of 861 samples of 8 subjects performing 27 actions, each action performed 4 times.",
        "dcterms:title": "UTD-MHAD",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "doi:10.1109/icip.2015.7350781",
        "dcat:theme": [
            "Human Action Recognition",
            "Multimodal Data"
        ],
        "dcat:keyword": [
            "Depth camera",
            "Inertial sensor",
            "Human actions",
            "Multimodal dataset"
        ],
        "dcat:landingPage": "doi:10.1109/icip.2015.7350781",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Bulbul M F",
            "Islam S",
            "Ali H"
        ],
        "dcterms:description": "The DHA dataset is organized with 483 depth video sequences for 23 actions, each sample performed by 2 or 3 times by 21 subjects. It includes actions such as bend, jack, jump, pjump, run, side, skip, walk, and various hand and leg movements.",
        "dcterms:title": "DHA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1007/s11042-019-7365-2",
        "dcat:theme": [
            "Human Action Recognition",
            "Depth Video Analysis"
        ],
        "dcat:keyword": [
            "Depth video sequences",
            "Human actions",
            "Action recognition"
        ],
        "dcat:landingPage": "https://doi.org/10.1007/s11042-019-7365-2",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]