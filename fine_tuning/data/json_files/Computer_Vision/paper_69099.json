[
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Robin Jia",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset containing questions based on a set of Wikipedia articles, including unanswerable questions to test the ability of models to recognize when they cannot answer a question.",
        "dcterms:title": "SQuAD 2.0",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1806.03822",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Christopher Clark",
            "Kenton Lee",
            "Ming-Wei Chang",
            "Tom Kwiatkowski",
            "Michael Collins",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A dataset designed for evaluating models on natural yes/no questions, focusing on the difficulty of such questions.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1905.10044",
        "dcat:theme": [
            "Yes/No Question Answering"
        ],
        "dcat:keyword": [
            "Natural Language Processing",
            "Question Answering",
            "Machine Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Yes/No Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Todor Mihaylov",
            "Peter Clark",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "dcterms:description": "A dataset for open book question answering that requires reasoning and knowledge beyond the text provided.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1809.02789",
        "dcat:theme": [
            "Open Book Question Answering"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Commonsense Reasoning",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Open Book Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset that tests reasoning about physical commonsense in natural language, requiring models to understand physical interactions.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Physical Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense Reasoning",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Physical Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of models to complete sentences in a coherent and contextually appropriate manner.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1905.07830",
        "dcat:theme": [
            "Sentence Completion"
        ],
        "dcat:keyword": [
            "Natural Language Processing",
            "Sentence Completion"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence Completion"
        ]
    },
    {
        "dcterms:creator": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "An adversarial dataset designed to challenge models on the Winograd Schema Challenge at scale.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Coreference Resolution"
        ],
        "dcat:keyword": [
            "Natural Language Processing",
            "Coreference Resolution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Coreference Resolution"
        ]
    }
]