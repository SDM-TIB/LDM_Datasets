To extract datasets from the research paper titled "PointConvFormer: Revenge of the Point-based Convolution" by Wenxuan Wu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** of the paper. In the abstract, the authors mention that they experiment on multiple datasets, which indicates that there are datasets to extract.

Next, I will focus on the **experiments section** (Section 4) where the authors detail their evaluation methods. Here, they explicitly mention the datasets used for their experiments:

1. **ScanNet**: This dataset is described as a large-scale indoor scene dataset. The authors mention that they use the official split with 1,201 scenes for training and 312 for validation.

2. **SemanticKitti**: This dataset is noted as a large-scale street view point cloud dataset built upon the KITTI Vision Odometry Benchmark. It consists of 43,552 point cloud scans sampled from 22 driving scene sequences.

3. **FlyingThings3D**: This dataset is used for training in the context of scene flow estimation. It is a synthetic dataset designed for evaluating scene flow estimation algorithms.

4. **KITTI Scene Flow 2015**: This dataset is also used for scene flow estimation and is part of the KITTI dataset suite, which is widely used in autonomous driving research.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset:

- For **ScanNet**, the citation is:
  > Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. *ScanNet: Richly-annotated 3D reconstructions of indoor scenes*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

- For **SemanticKitti**, the citation is:
  > Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. *SemanticKITTI: A dataset for semantic scene understanding of lidar sequences*. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9297–9307, 2019.

- For **FlyingThings3D**, the citation is:
  > Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. *A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4040–4048, 2016.

- For **KITTI Scene Flow 2015**, the citation is:
  > Moritz Menze, Christian Heipke, and Andreas Geiger. *Object scene flow for autonomous vehicles*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3061–3070, 2015.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review.