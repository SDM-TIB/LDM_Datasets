To extract datasets from the research paper titled "Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos" by Yitian Yuan et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will start by reading the **abstract** and **introduction** sections. The abstract mentions that the authors conducted extensive experiments on three public datasets, which indicates that there are specific datasets to look for. The introduction further elaborates on the task of temporal sentence grounding in videos and hints at the datasets used for evaluation.

Next, I will navigate to **section 4.1 (Datasets)**, where the authors typically provide detailed information about the datasets used in their experiments. In this section, I will look for the names of the datasets, their descriptions, and any relevant statistics or characteristics that the authors provide.

Upon reviewing section 4.1, I find that the authors mention three datasets:

1. **TACoS**: This dataset contains videos depicting human cooking activities. It is specifically designed for the task of temporal sentence grounding.

2. **Charades-STA**: This dataset focuses on more complex human activities in daily life and is also used for the temporal sentence grounding task.

3. **ActivityNet Captions**: This dataset includes a variety of activities and is used for evaluating the effectiveness of the proposed model in grounding sentences in video segments.

Next, I will check the **References section** to find the full citations for these datasets. This is crucial for proper attribution and to ensure that the datasets can be easily located by others.

The full citations I find are:

- For **TACoS**:
  > Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. *Grounding action descriptions in videos*. Transactions of the Association for Computational Linguistics, 1:25–36, 2013.

- For **Charades-STA**:
  > Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. *Tall: Temporal activity localization via language query*. In Proceedings of the IEEE International Conference on Computer Vision, pages 5267–5275, 2017.

- For **ActivityNet Captions**:
  > Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. *Dense-captioning events in videos*. In Proceedings of the IEEE International Conference on Computer Vision, pages 706–715, 2017.

Now that I have identified the datasets and their citations, I will compile this information into a structured format that clearly outlines each dataset along with its citation. This will ensure that the datasets are properly documented and can be referenced in future work.