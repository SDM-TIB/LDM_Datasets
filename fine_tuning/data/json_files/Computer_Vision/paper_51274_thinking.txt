To extract datasets from the research paper titled "VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation" by Su Ho Han et al., I will follow a systematic approach.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract states that the authors achieved state-of-the-art accuracy on the **YouTube-VIS 2019 and 2021 datasets**, which indicates that these datasets are crucial for their research.

Next, I will look into the **experiments section** to confirm the datasets used for evaluation. In this section, the authors explicitly mention that they evaluated their method on the **YouTube-VIS 2019 and 2021 datasets**. This reinforces the importance of these datasets in their experiments.

I will then check the **references section** to find the full citations for these datasets. The citations for the datasets are as follows:

- For **YouTube-VIS 2019**, the citation is:
  > Linjie Yang, Yuchen Fan, and Ning Xu. *Video instance segmentation*. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.

- For **YouTube-VIS 2021**, the citation is:
  > Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. *End-to-end video instance segmentation with transformers*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8741â€“8750, June 2021.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets used in the research and their corresponding citations.