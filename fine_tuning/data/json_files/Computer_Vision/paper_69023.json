[
    {
        "dcterms:creator": [],
        "dcterms:description": "A large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotional and style labels.",
        "dcterms:title": "M2F-D (Media2Face Dataset)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Facial Animation",
            "Emotion Recognition",
            "Style Adaptation"
        ],
        "dcat:keyword": [
            "Facial Animation",
            "Co-speech",
            "3D Dataset",
            "Emotional Labels",
            "Style Labels"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Animation",
        "mls:task": [
            "Facial Animation Generation",
            "Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "K. Wang",
            "Q. Wu",
            "L. Song",
            "Z. Yang",
            "W. Wu",
            "C. Qian",
            "R. He",
            "Y. Qiao",
            "C. C. Loy"
        ],
        "dcterms:description": "A large-scale audio-visual dataset for emotional talking-face generation, comprising talking-face videos of 60 actors and actresses expressing 8 different emotions at 3 varying intensity levels.",
        "dcterms:title": "MEAD",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Emotion Recognition",
            "Facial Animation"
        ],
        "dcat:keyword": [
            "Emotional Talking-Face",
            "Audio-Visual Dataset",
            "Facial Animation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion Recognition",
            "Facial Animation Generation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Cao",
            "D. G. Cooper",
            "M. K. Keutmann",
            "R. C. Gur",
            "A. Nenkova",
            "R. Verma"
        ],
        "dcterms:description": "A crowd-sourced emotional multimodal actors dataset comprising 7,442 distinct clips featuring 91 actors who delivered 12 sentences expressing 6 different emotions at 4 different intensity levels.",
        "dcterms:title": "CREMA-D",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Multimodal Dataset"
        ],
        "dcat:keyword": [
            "Emotional Clips",
            "Multimodal Dataset",
            "Actor Performance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion Recognition",
            "Facial Animation Generation"
        ]
    },
    {
        "dcterms:creator": [
            "S. R. Livingstone",
            "F. A. Russo"
        ],
        "dcterms:description": "The Ryerson audio-visual database of emotional speech and song, consisting of videos by 24 professional actors who vocalize speeches and songs, with a total of 7 and 5 emotions respectively.",
        "dcterms:title": "RAVDESS",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Emotion Recognition",
            "Speech Emotion Recognition"
        ],
        "dcat:keyword": [
            "Emotional Speech",
            "Audio-Visual Dataset",
            "Facial Expressions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion Recognition",
            "Facial Animation Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Zhang",
            "L. Li",
            "Y. Ding",
            "C. Fan"
        ],
        "dcterms:description": "A high-resolution audio-visual dataset for flow-guided one-shot talking face generation.",
        "dcterms:title": "HDTF",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Facial Animation",
            "Audio-Visual Generation"
        ],
        "dcat:keyword": [
            "High-Resolution Dataset",
            "Talking Face Generation",
            "Audio-Visual Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Facial Animation Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. F. Montesinos",
            "V. S. Kadandale",
            "G. Haro"
        ],
        "dcterms:description": "A dataset for audio-visual singing voice separation, containing solo singing videos.",
        "dcterms:title": "Acappella",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Separation",
            "Singing Voice Separation"
        ],
        "dcat:keyword": [
            "Singing Voice",
            "Audio-Visual Dataset",
            "Voice Separation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Audio-Visual Separation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Cudeiro",
            "T. Bolkart",
            "C. Laidlaw",
            "A. Ranjan",
            "M. J. Black"
        ],
        "dcterms:description": "A dataset for capturing, learning, and synthesizing 3D speaking styles.",
        "dcterms:title": "VOCASET",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Facial Animation",
            "Speech Synthesis"
        ],
        "dcat:keyword": [
            "3D Speaking Styles",
            "Audio-Visual Dataset",
            "Facial Animation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Animation",
        "mls:task": [
            "Facial Animation Generation"
        ]
    },
    {
        "dcterms:creator": [
            "G. Fanelli",
            "J. Gall",
            "H. Romsdorfer",
            "T. Weise",
            "L. Van Gool"
        ],
        "dcterms:description": "A 3-D audio-visual corpus of affective communication.",
        "dcterms:title": "BIWI",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Affective Communication",
            "Audio-Visual Dataset"
        ],
        "dcat:keyword": [
            "Affective Communication",
            "3D Dataset",
            "Facial Expressions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion Recognition",
            "Facial Animation Generation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset consisting of a range of motion data for training facial animation models.",
        "dcterms:title": "Range of Motion (RoM)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Facial Animation",
            "3D Modeling"
        ],
        "dcat:keyword": [
            "Facial Motion",
            "3D Dataset",
            "Animation Training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]