[
    {
        "dcterms:creator": [
            "S. Zhao",
            "F. Rudzicz"
        ],
        "dcterms:description": "KARA ONE dataset combines 3 modalities (EEG, face tracking, and audio) during imagined and vocalized phonemic and single-word prompts. Each participant produced 132 trials consisting of 4 successive states: a rest state, a stimulus state, an imagined speech state, and a speaking state. The EEG data were collected using a 64-channel Neuroscan Quick-cap, following the 10-20 system.",
        "dcterms:title": "KARA ONE",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "EEG-Speech Modeling",
            "Multimodal Data"
        ],
        "dcat:keyword": [
            "EEG",
            "Speech Recognition",
            "Imagined Speech",
            "Phonemes",
            "Multimodal Fusion"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "EEG, Audio",
        "mls:task": [
            "Speech Classification",
            "Speech Recovery"
        ]
    }
]