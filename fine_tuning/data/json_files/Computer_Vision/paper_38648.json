[
    {
        "dcterms:creator": [
            "Drew A. Hudson",
            "Christopher D. Manning"
        ],
        "dcterms:description": "A new dataset for compositional question answering over real-world images, focusing on strong language priors and visual relationships.",
        "dcterms:title": "GQA Dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1902.09506",
        "dcat:theme": [
            "Visual Question Answering",
            "Image Understanding"
        ],
        "dcat:keyword": [
            "Compositional reasoning",
            "Visual relationships",
            "Real-world images"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Stanislaw Antol",
            "Margaret Mitchell",
            "C. Lawrence Zitnick",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "A popular visual question answering benchmark that measures the ability to answer questions based on visual content, with a focus on language priors.",
        "dcterms:title": "VQA 2.0 Dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Visual question answering",
            "Benchmark",
            "Language priors"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma",
            "Michael Bernstein",
            "Li Fei-Fei"
        ],
        "dcterms:description": "A dataset that connects language and vision using crowdsourced dense image annotations, facilitating visual relationship representation learning.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Relationship Detection",
            "Image Annotation"
        ],
        "dcat:keyword": [
            "Dense annotations",
            "Visual relationships",
            "Language and vision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]