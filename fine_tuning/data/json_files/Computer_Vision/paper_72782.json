[
    {
        "dcterms:creator": [
            "Xun Long Ng",
            "Kian Eng Ong",
            "Qichen Zheng",
            "Yun Ni",
            "Si Yong Yeo",
            "Jun Liu"
        ],
        "dcterms:description": "Animal Kingdom provides frame-level natural language descriptions of wildlife behavior, including various wildlife species sourced from YouTube, with a total of 50 hours of video across 4301 long video sequences and 18,744 annotated sentences.",
        "dcterms:title": "Animal Kingdom",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Animal Behavior",
            "Video Analysis"
        ],
        "dcat:keyword": [
            "Wildlife",
            "Natural Language Descriptions",
            "Video Dataset",
            "Temporal Grounding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Grounding",
            "Behavior Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Jiyang Gao",
            "Chen Sun",
            "Zhenheng Yang",
            "Ram Nevatia"
        ],
        "dcterms:description": "Charades-STA is a dataset designed for temporal activity localization via language queries, containing various activities annotated in video clips.",
        "dcterms:title": "Charades-STA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Recognition",
            "Video Analysis"
        ],
        "dcat:keyword": [
            "Temporal Activity Localization",
            "Language Queries",
            "Video Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Kenji Hata",
            "Frederic Ren",
            "Li Fei-Fei",
            "Juan Carlos Niebles"
        ],
        "dcterms:description": "ActivityNet Captions is a dataset that provides dense captioning of events in videos, allowing for detailed analysis of activities over time.",
        "dcterms:title": "ActivityNet Captions",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Event Detection",
            "Video Analysis"
        ],
        "dcat:keyword": [
            "Dense Captioning",
            "Video Dataset",
            "Event Localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Event Localization",
            "Captioning"
        ]
    }
]