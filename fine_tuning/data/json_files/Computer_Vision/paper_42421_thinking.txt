To extract datasets from the research paper titled "3DSSD: Point-based 3D Single Stage Object Detector" by Zetong Yang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. In the abstract, the authors mention evaluating their method on the **KITTI dataset** and the **nuScenes dataset**, which indicates that these are the datasets used for their experiments.

Next, I will look for more detailed descriptions of these datasets in the **experiments section**. In section 4.1, the authors provide information about the **KITTI dataset**, stating it contains 7,481 training images/point clouds and 7,518 test images/point clouds, specifically focusing on the class "Car." They also mention the evaluation metric used (average precision) and the IoU threshold for classifying instances.

In section 4.2, the authors describe the **nuScenes dataset**, noting that it contains 1,000 scenes with 1.4 million 3D objects across 10 different classes. They highlight the complexity of this dataset and the specific challenges it presents for 3D object detection.

Now, I will check the **References section** to gather the full citations for these datasets:

1. For the **KITTI dataset**, the citation is:
   > Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel I. J. Urtasun. *Vision meets robotics: The KITTI dataset*. Robotics Research, 2013.

2. For the **nuScenes dataset**, the citation is:
   > Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. *nuScenes: A multimodal dataset for autonomous driving*. arXiv preprint arXiv:1903.11027, 2019.

With this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a clear and comprehensive overview of the datasets used in the research.