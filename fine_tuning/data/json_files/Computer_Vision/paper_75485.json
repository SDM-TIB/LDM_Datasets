[
    {
        "dcterms:creator": [
            "C. D. Kim",
            "B. Kim",
            "H. Lee",
            "G. Kim"
        ],
        "dcterms:description": "The AudioCap Evaluation Set is used to generate unique audio clips from captions, serving as text prompts for evaluating audio generation quality and efficiency.",
        "dcterms:title": "AudioCap Evaluation Set",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Generation",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Audio synthesis",
            "Text-to-audio",
            "Evaluation metrics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio generation evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Kong",
            "W. Ping",
            "J. Huang",
            "K. Zhao",
            "B. Catanzaro"
        ],
        "dcterms:description": "DiffWave is a versatile diffusion model for audio synthesis, showcasing the potential of diffusion models in generating high-quality audio.",
        "dcterms:title": "DiffWave",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Synthesis",
            "Diffusion Models"
        ],
        "dcat:keyword": [
            "Audio generation",
            "Diffusion model",
            "Synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio synthesis"
        ]
    },
    {
        "dcterms:creator": [
            "M. Jeong",
            "H. Kim",
            "S. J. Cheon",
            "B. J. Choi",
            "N. S. Kim"
        ],
        "dcterms:description": "Diff-TTS is a denoising diffusion model specifically designed for text-to-speech applications, demonstrating the effectiveness of diffusion models in generating speech.",
        "dcterms:title": "Diff-TTS",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Speech",
            "Diffusion Models"
        ],
        "dcat:keyword": [
            "Speech synthesis",
            "Diffusion model",
            "Text-to-speech"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Text-to-speech synthesis"
        ]
    },
    {
        "dcterms:creator": [
            "D. Yang",
            "J. Yu",
            "H. Wang",
            "W. Wang",
            "C. Weng",
            "Y. Zou",
            "D. Yu"
        ],
        "dcterms:description": "Diffsound is a discrete diffusion model for text-to-sound generation, showcasing advancements in generating sound from textual descriptions.",
        "dcterms:title": "Diffsound",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Sound",
            "Diffusion Models"
        ],
        "dcat:keyword": [
            "Sound generation",
            "Diffusion model",
            "Text-to-sound"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Text-to-sound generation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "Z. Chen",
            "Y. Yuan",
            "X. Mei",
            "X. Liu",
            "D. Mandic",
            "W. Wang",
            "M. D. Plumbley"
        ],
        "dcterms:description": "AudioLDM is a model for text-to-audio generation using latent diffusion models, enhancing the quality of audio synthesis from textual inputs.",
        "dcterms:title": "AudioLDM",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Audio",
            "Diffusion Models"
        ],
        "dcat:keyword": [
            "Audio generation",
            "Latent diffusion",
            "Text-to-audio"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Text-to-audio generation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "Q. Tian",
            "Y. Yuan",
            "X. Liu",
            "X. Mei",
            "Q. Kong",
            "Y. Wang",
            "W. Wang",
            "Y. Wang",
            "M. D. Plumbley"
        ],
        "dcterms:description": "AudioLDM2 is an improved version of AudioLDM, focusing on holistic audio generation with self-supervised pretraining techniques.",
        "dcterms:title": "AudioLDM2",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Audio",
            "Diffusion Models"
        ],
        "dcat:keyword": [
            "Audio generation",
            "Self-supervised learning",
            "Text-to-audio"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Text-to-audio generation"
        ]
    }
]