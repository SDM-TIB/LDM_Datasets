To extract datasets from the research paper titled "Towards Deeper and Better Multi-view Feature Fusion for 3D Semantic Segmentation" by Chaolong Yang et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets mentioned. The abstract states that the framework achieves state-of-the-art results on **ScanNetv2** and **NYUv2** datasets, indicating that these datasets are crucial for the experiments.

Next, I will look for specific details about these datasets in the **experiments section**. In section 4.1, the authors describe the **ScanNetv2 dataset**, noting that it includes 2.5 million RGB-D frames across 706 different scenes, with a specific split for training and validation. They also mention the **NYUv2 dataset**, which contains 1,449 densely labeled pairs of aligned RGB and depth images, with a training and testing split.

I will then check the **References section** to find the full citations for these datasets:

1. For **ScanNetv2**, the citation is:
   > Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., & Nießner, M. (2017). *ScanNet: Richly-annotated 3D reconstructions of indoor scenes*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5828–5839.

2. For **NYUv2**, the citation is:
   > Silberman, N., Hoiem, D., Kohli, P., & Fergus, R. (2012). *Indoor segmentation and support inference from RGBD images*. In European Conference on Computer Vision (ECCV), pp. 746–760.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is accurately described and that the full citations are included. This will provide a comprehensive overview of the datasets used in the research paper.