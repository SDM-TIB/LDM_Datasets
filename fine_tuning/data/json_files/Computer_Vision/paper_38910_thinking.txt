To extract datasets from the research paper titled "PointCloud Saliency Maps" by Tianhang Zheng et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reviewing the **abstract** and **introduction** sections of the paper. The abstract mentions the evaluation of their method on several state-of-the-art point-cloud recognition models, which suggests that datasets are likely involved. I will look for any specific dataset names mentioned.

Next, I will examine the **experiments section** (section 5) closely, as this is typically where authors describe the datasets used for their evaluations. In this section, the authors explicitly mention two datasets:

1. **3D MNIST**: This dataset consists of 6000 raw 3D point clouds generated from 2D MNIST images, with 5000 used for training and 1000 for testing. Each point cloud contains approximately 20,000 points.

2. **ModelNet40**: This dataset contains 12,311 meshed CAD models across 40 categories, with 9,843 models used for training and 2,468 for testing.

I will also check the **References section** to find the full citations for these datasets. The citations are crucial for proper attribution and to guide readers to the original sources.

For **3D MNIST**, the citation is:
> Daavoo. *3D MNIST Dataset*. Available at: https://www.kaggle.com/daavoo/3d-mnist/version/13.

For **ModelNet40**, the citation is:
> Wu, Zhirong, et al. *3D ShapeNets: A Deep Representation for Volumetric Shapes*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912â€“1920, 2015. Available at: http://modelnet.cs.princeton.edu/.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is described accurately and that the full citations are included for reference. This structured approach will help ensure that I do not miss any important details regarding the datasets used in the research.