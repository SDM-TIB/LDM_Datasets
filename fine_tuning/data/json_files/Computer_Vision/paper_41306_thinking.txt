To extract datasets from the research paper titled "STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits" by Uttaran Bhattacharya et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract** to see if any datasets are mentioned. The abstract states that the authors present a novel classifier network and mention the use of "hundreds of annotated real-world gait videos" and "thousands of annotated synthetic gaits." This indicates that there are datasets involved, but I need to find their specific names and details.

Next, I will examine the **introduction** section, where the authors discuss the importance of emotion recognition from gaits and mention the challenges due to the lack of annotated gait data. They also highlight the creation of a new dataset, which is a good lead for further investigation.

I will then look into **section 5.2 (Dataset: Emotion-Gait)**, where the authors provide a detailed description of the dataset they created. They mention that the Emotion-Gait (E-Gait) dataset consists of 2,177 real gaits and 1,000 synthetic gaits, totaling 3,177 gaits across four emotion classes (angry, neutral, happy, sad). They also describe how the real gaits were collected and labeled by domain experts, and how the synthetic gaits were generated.

To ensure I have the correct citations, I will check the **References section** for any papers that describe the datasets used. The authors mention that 1,835 gaits were taken from the Edinburgh Locomotion MOCAP Database, which I will need to cite properly.

Now, I will compile the information I have gathered:

1. **E-Gait Dataset**: This dataset consists of 2,177 real-world gaits and 1,000 synthetic gaits, annotated with emotion labels. The dataset was created to facilitate emotion recognition from gaits.

2. **Edinburgh Locomotion MOCAP Database**: This database provided additional gaits that were labeled with emotions by domain experts.

Next, I will retrieve the full citations for these datasets:

- For the **E-Gait Dataset**, since it is a novel dataset introduced in this paper, I will cite it as follows:
  > Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, Dinesh Manocha. *STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits*. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.

- For the **Edinburgh Locomotion MOCAP Database**, the citation is:
  > I. Habibie, D. Holden, J. Schwarz, J. Yearsley, and T. Komura. *A recurrent variational autoencoder for human motion synthesis*. In BMVC, 2017.

After gathering all this information, I will prepare the dataset entries for review or further processing, ensuring that each dataset is accurately described and properly cited.