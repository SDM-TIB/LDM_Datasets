To extract datasets from the research paper titled "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction" by Guanglei Yang et al., I will follow a systematic approach to ensure that I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract states that the proposed method achieves state-of-the-art performance on three challenging datasets, which indicates that datasets are indeed involved.

Next, I will focus on **section 4.1 (Datasets)**, where the authors explicitly list the datasets used for evaluation. Here, they mention three datasets:

1. **NYU Dataset**: This dataset is used for both depth estimation and surface normal estimation tasks. It consists of 120K RGB-Depth pairs with a resolution of 480×640 pixels, acquired from 464 indoor scenes. The authors follow a standard train/test split, using 249 scenes for training and 215 scenes for testing.

2. **KITTI Dataset**: This is a large-scale outdoor dataset created for various autonomous driving tasks. The authors use it to evaluate depth estimation performance, specifically utilizing 22,600 frames from 32 scenes for training and 697 frames from the remaining 29 scenes for testing.

3. **ScanNet Dataset**: This dataset is described as a large RGB-D dataset for 3D scene understanding. It is divided into 189,916 samples for training and 20,942 for testing, with file lists provided in the original paper.

After identifying the datasets, I will check the **References section** to find the full citations for each dataset:

- For the **NYU Dataset**, the citation is:
  > Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. *Indoor segmentation and support inference from RGBD images*. In Proceedings of the European Conference on Computer Vision (ECCV), 2012.

- For the **KITTI Dataset**, the citation is:
  > Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. *Vision meets robotics: The KITTI dataset*. In The International Journal of Robotics Research (IJRR), 2013.

- For the **ScanNet Dataset**, the citation is:
  > Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. *ScanNet: Richly-annotated 3D reconstructions of indoor scenes*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

Now that I have gathered all the necessary information, I will compile the dataset entries, ensuring that each entry includes the dataset name, description, and full citation. This structured approach will help in accurately documenting the datasets used in the research paper.