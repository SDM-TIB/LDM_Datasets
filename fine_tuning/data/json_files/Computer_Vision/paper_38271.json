[
    {
        "dcterms:creator": [
            "M. Arjovsky",
            "S. Chintala",
            "L. Bottou"
        ],
        "dcterms:description": "Wasserstein Generative Adversarial Networks (WGAN) is a framework for training generative adversarial networks using the Wasserstein distance, which provides a more stable training process compared to traditional GANs.",
        "dcterms:title": "Wasserstein Generative Adversarial Networks (WGAN)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Models",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "WGAN",
            "Generative Adversarial Networks",
            "Wasserstein Distance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Arora",
            "R. Ge",
            "Y. Liang",
            "T. Ma",
            "Y. Zhang"
        ],
        "dcterms:description": "Neural Net Distance is a metric used to measure the distance between two distributions based on neural networks, providing a way to evaluate the performance of generative adversarial networks.",
        "dcterms:title": "Neural Net Distance",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Models",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Neural Net Distance",
            "Generative Adversarial Networks",
            "Distance Metric"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "G. Dziugaite",
            "D. Roy",
            "Z. Ghahramani"
        ],
        "dcterms:description": "Maximum Mean Discrepancy (MMD) is a statistical test used to measure the distance between two probability distributions, often employed in the context of generative models to assess the quality of generated samples.",
        "dcterms:title": "Maximum Mean Discrepancy (MMD)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistical Tests",
            "Generative Models"
        ],
        "dcat:keyword": [
            "Maximum Mean Discrepancy",
            "Statistical Distance",
            "Generative Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Y. Li",
            "K. Swersky",
            "R. Zemel"
        ],
        "dcterms:description": "Generative Moment Matching Networks provide a framework for generative modeling that matches moments of the generated distribution to those of the target distribution, enhancing the quality of generated samples.",
        "dcterms:title": "Generative Moment Matching Networks",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Models",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Moment Matching",
            "Generative Models",
            "Generative Moment Matching Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Müller"
        ],
        "dcterms:description": "Integral Probability Metrics provide a framework for measuring the distance between probability distributions, which is crucial for various applications in machine learning, particularly in generative models.",
        "dcterms:title": "Integral Probability Metrics",
        "dcterms:issued": "1997",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Probability Theory",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Integral Probability Metrics",
            "Distance Metrics",
            "Probability Distributions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Golowich",
            "A. Rakhlin",
            "O. Shamir"
        ],
        "dcterms:description": "Rademacher Complexity is a measure of the capacity of a class of functions, which is used to derive generalization bounds in machine learning, particularly in the context of neural networks.",
        "dcterms:title": "Rademacher Complexity",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Learning Theory",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Rademacher Complexity",
            "Generalization Bounds",
            "Neural Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Liang"
        ],
        "dcterms:description": "The Generalization Error of GANs refers to the ability of generative adversarial networks to learn the underlying distribution of the data, which is crucial for their effectiveness in generating realistic samples.",
        "dcterms:title": "Generalization Error of GANs",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Models",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Generalization Error",
            "Generative Adversarial Networks",
            "Learning Theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Muandet",
            "K. Fukumizu",
            "F. Dinuzzo",
            "B. Schölkopf"
        ],
        "dcterms:description": "Support Measure Machines are a framework for learning from distributions, which can be applied in various machine learning tasks, particularly in generative modeling.",
        "dcterms:title": "Support Measure Machines",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Generative Models"
        ],
        "dcat:keyword": [
            "Support Measure Machines",
            "Learning from Distributions",
            "Generative Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Zou",
            "Y. Liang",
            "H. V. Poor"
        ],
        "dcterms:description": "Anomaly Detection refers to the identification of rare items, events, or observations that raise suspicions by differing significantly from the majority of the data, which is crucial in various applications.",
        "dcterms:title": "Anomaly Detection",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Anomaly Detection",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Anomaly Detection",
            "Outlier Detection",
            "Machine Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]