[
    {
        "dcterms:creator": [
            "C. Guo",
            "S. Zou",
            "X. Zuo",
            "S. Wang",
            "W. Ji",
            "X. Li",
            "L. Cheng"
        ],
        "dcterms:description": "The HumanML3D dataset consists of about 28k motions, each with 3 or 4 captions, used for generating diverse and natural 3D motions from text.",
        "dcterms:title": "HumanML3D",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human Motion Generation",
            "3D Motion Synthesis"
        ],
        "dcat:keyword": [
            "3D motions",
            "text-to-motion",
            "motion generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Motion Data",
        "mls:task": [
            "Text-to-Motion Generation"
        ]
    },
    {
        "dcterms:creator": [
            "R. Li",
            "S. Yang",
            "D. A. Ross",
            "A. Kanazawa"
        ],
        "dcterms:description": "AIST++ dataset encompasses 1408 distinct dance motion sequences, spanning various dance genres and providing detailed descriptions for each sequence.",
        "dcterms:title": "AIST++",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dance Motion Generation",
            "Music Conditioned Generation"
        ],
        "dcat:keyword": [
            "dance sequences",
            "music-to-dance",
            "dance genres"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Motion Data",
        "mls:task": [
            "Music-to-Dance Generation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "Z. Zhu",
            "N. Iwamoto",
            "Y. Peng",
            "Z. Li",
            "Y. Zhou",
            "E. Bozkurt",
            "B. Zheng"
        ],
        "dcterms:description": "BEAT is a large-scale semantic and emotional multi-modal dataset designed for conversational gesture synthesis, providing motion sequences aligned with speech.",
        "dcterms:title": "BEAT",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Gesture Synthesis",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "conversational gestures",
            "multi-modal dataset",
            "speech alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Motion Data",
        "mls:task": [
            "Speech-to-Gesture Generation"
        ]
    }
]