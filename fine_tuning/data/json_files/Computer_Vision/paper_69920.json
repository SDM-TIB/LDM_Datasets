[
    {
        "dcterms:creator": [
            "N. Krishnaswamy",
            "W. Pickard",
            "B. Cates",
            "N. Blanchard",
            "J. Pustejovsky"
        ],
        "dcterms:description": "The VoxWorld platform for interactive agents takes in linguistic input and operationalizes it in the 3D world by converting the object motions described therein into movements within the environment.",
        "dcterms:title": "VoxWorld",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Interaction",
            "Simulation"
        ],
        "dcat:keyword": [
            "Interactive agents",
            "3D simulation",
            "Linguistic input"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Simulation",
        "mls:task": [
            "Object manipulation",
            "Physical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "J. Li",
            "D. Li",
            "C. Xiong",
            "S. Hoi"
        ],
        "dcterms:description": "BLIP is a vision-language model trained with sophisticated cross-modal attention to identify cases relevant to object physical properties.",
        "dcterms:title": "BLIP",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Understanding",
            "Cross-Modal Attention"
        ],
        "dcat:keyword": [
            "Vision-language model",
            "Cross-modal attention",
            "Object properties"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Object recognition",
            "Physical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "C. Li",
            "Q. Wu",
            "Y. J. Lee"
        ],
        "dcterms:description": "LLaVA integrates CLIP ViT-L/14 with a LLaMA-based generative language model to perform visual instruction tuning.",
        "dcterms:title": "LLaVA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Instruction Tuning",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Visual instruction",
            "Generative model",
            "Multimodal integration"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Visual reasoning",
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "D. Driess",
            "F. Xia",
            "M. S. Sajjadi",
            "C. Lynch",
            "A. Chowdhery",
            "B. Ichter",
            "A. Wahid",
            "J. Tompson",
            "Q. Vuong",
            "T. Yu"
        ],
        "dcterms:description": "PALM-E is an embodied multimodal language model that addresses physical reasoning tasks.",
        "dcterms:title": "PALM-E",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Embodied AI",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Embodied language model",
            "Physical reasoning",
            "Multimodal interaction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Physical reasoning",
            "Object manipulation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Ahn",
            "A. Brohan",
            "N. Brown",
            "Y. Chebotar",
            "O. Cortes",
            "B. David",
            "C. Finn",
            "C. Fu",
            "K. Gopalakrishnan",
            "K. Hausman"
        ],
        "dcterms:description": "Say-Can grounds language in robotic affordances, allowing robots to perform tasks based on language instructions.",
        "dcterms:title": "Say-Can",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Language Grounding"
        ],
        "dcat:keyword": [
            "Robotic affordances",
            "Language grounding",
            "Task execution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Robotic manipulation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "A. Barbu",
            "D. Mayo",
            "J. Alverio",
            "W. Luo",
            "C. Wang",
            "D. Gutfreund",
            "J. Tenenbaum",
            "B. Katz"
        ],
        "dcterms:description": "ObjectNet is a large-scale bias-controlled dataset designed to push the limits of object recognition models.",
        "dcterms:title": "ObjectNet",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Recognition",
            "Bias Control"
        ],
        "dcat:keyword": [
            "Object recognition",
            "Bias control",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Dataset",
        "mls:task": [
            "Object recognition",
            "Bias evaluation"
        ]
    }
]