[
    {
        "dcterms:creator": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard Hovy"
        ],
        "dcterms:description": "RACE is a reading comprehension dataset constructed from real-world examinations, featuring multiple-choice questions based on passages that require multi-sentence and compositional reasoning.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1704.04683",
        "dcat:theme": [
            "Machine Comprehension",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Multiple choice",
            "Examinations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multiple Choice Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Matthew Dunn",
            "Levent Sagun",
            "Mike Higgins",
            "Ugur Guney",
            "Volkan Cirik",
            "Kyunghyun Cho"
        ],
        "dcterms:description": "SearchQA is a question-answering dataset that emulates a real-world QA system, requiring models to extract passages from search engine results and answer questions based on those snippets.",
        "dcterms:title": "SearchQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1704.05179",
        "dcat:theme": [
            "Question Answering",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Question answering",
            "Search engine",
            "Contextual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Span Prediction",
            "Multiple Choice Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tomáš Kočiský",
            "Jonathan Schwarz",
            "Phil Blunsom",
            "Chris Dyer",
            "Karl Moritz Hermann",
            "Gábor Melis",
            "Edward Grefenstette"
        ],
        "dcterms:description": "NarrativeQA is a benchmark for story-based reading comprehension, where answers are handwritten by human annotators, requiring models to understand and reason over narrative texts.",
        "dcterms:title": "NarrativeQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1712.07040",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Understanding"
        ],
        "dcat:keyword": [
            "Narrative understanding",
            "Human-annotated answers",
            "Story comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Span Prediction",
            "Multiple Choice Question Answering"
        ]
    }
]