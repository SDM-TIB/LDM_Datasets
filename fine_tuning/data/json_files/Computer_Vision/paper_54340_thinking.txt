To extract datasets from the research paper titled "Generative Modelling with Inverse Heat Dissipation" by Severi Rissanen et al., I will follow a systematic approach to identify and document the datasets mentioned in the text.

First, I will read through the **abstract, introduction, methods, and experiments sections** of the paper. The abstract provides a brief overview of the research and may mention datasets used for evaluation. In the introduction, the authors often discuss the context of their work, which may include references to specific datasets.

Next, I will focus on the **experiments section**, as this is where the authors typically describe the datasets used for their experiments. I will look for any explicit mentions of datasets, including their names and any relevant details about their characteristics or sizes.

In the **experiments section**, the authors mention several datasets used for evaluation:

1. **MNIST**: A well-known dataset of handwritten digits, consisting of 60,000 training images and 10,000 test images, each of size 28x28 pixels.
   
2. **CIFAR-10**: A dataset containing 60,000 32x32 color images in 10 classes, with 6,000 images per class. It is commonly used for image classification tasks.

3. **AFHQ**: A dataset for animal face generation, consisting of 15,000 images across three categories (cats, dogs, and wildlife), with images of size 64x64 and 256x256 pixels.

4. **FFHQ**: The Flickr-Faces-HQ dataset, which contains 70,000 high-quality images of human faces, with a resolution of 1024x1024 pixels.

5. **LSUN Churches**: A dataset containing images of churches, with a resolution of 128x128 pixels, used for various image generation tasks.

After identifying these datasets, I will check the **References section** of the paper to find full citations for each dataset. This is crucial for proper attribution and to provide readers with the necessary information to access the datasets.

The full citations for the datasets are as follows:

- **MNIST**: 
  > Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. *Gradient-based learning applied to document recognition*. Proceedings of the IEEE, 86(11):2278–2324, 1998.

- **CIFAR-10**: 
  > Alex Krizhevsky. *Learning Multiple Layers of Features from Tiny Images*. MSc thesis, University of Toronto, Canada, 2009.

- **AFHQ**: 
  > Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. *StarGAN v2: Diverse image synthesis for multiple domains*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8188–8197, 2020.

- **FFHQ**: 
  > Tero Karras, Samuli Laine, and Timo Aila. *A style-based generator architecture for generative adversarial networks*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410, 2019.

- **LSUN Churches**: 
  > Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. *LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop*. arXiv preprint arXiv:1506.03365, 2015.

Finally, I will compile the dataset information, including their names, descriptions, and citations, into a structured format for easy reference and further processing.