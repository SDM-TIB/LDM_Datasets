To extract datasets from the research paper titled "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering" by Yanlin Feng et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reading through the **abstract and introduction** sections. The abstract mentions that the authors empirically show the effectiveness of their method on **CommonsenseQA** and **OpenbookQA** datasets. This indicates that these datasets are likely to be important for the research.

Next, I will look for specific sections that discuss datasets in detail. In this case, the **experiments section (Section 5)** is crucial as it typically contains information about the datasets used for evaluation. Here, the authors explicitly mention the two datasets they evaluate their models on:

1. **CommonsenseQA**: This dataset is described as requiring various commonsense reasoning skills and is created with entities from ConceptNet. It is designed to probe latent compositional relations between entities.

2. **OpenbookQA**: This dataset provides elementary science questions along with an open book of science facts, probing general common sense beyond the provided facts.

Now, I will check the **References section** to find the full citations for these datasets. The citations are as follows:

- For **CommonsenseQA**, the citation is:
  > Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. *CommonsenseQA: A question answering challenge targeting commonsense knowledge*. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis, Minnesota. Association for Computational Linguistics, 2019.

- For **OpenbookQA**, the citation is:
  > Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. *Can a suit of armor conduct electricity? A new dataset for open book question answering*. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381–2391, Brussels, Belgium. Association for Computational Linguistics, 2018.

After gathering this information, I will summarize the datasets and their citations clearly, ensuring that I have accurately captured the relevant details for each dataset mentioned in the paper. This will allow for proper referencing and further exploration of the datasets used in the research.