[
    {
        "dcterms:creator": [
            "Kristen Grauman",
            "Andrew Westbury",
            "Eugene Byrne",
            "Zachary Chavis",
            "Antonino Furnari",
            "Rohit Girdhar",
            "Jackson Hamburger",
            "Hao Jiang",
            "Miao Liu",
            "Xingyu Liu"
        ],
        "dcterms:description": "Ego4D is a large-scale first-person video dataset that consists of approximately 3000 hours of egocentric videos, designed for tasks such as Moment Query (MQ) and Natural Language Query (NLQ).",
        "dcterms:title": "Ego4D",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Action Detection",
            "Moment Retrieval"
        ],
        "dcat:keyword": [
            "Egocentric video",
            "Temporal action detection",
            "Moment retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Action Detection",
            "Moment Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Gunnar A Sigurdsson",
            "GÃ¼l Varol",
            "Xiaolong Wang",
            "Ali Farhadi",
            "Ivan Laptev",
            "Abhinav Gupta"
        ],
        "dcterms:description": "Charades is a large-scale dataset consisting of 9.8k videos with dense action annotations, aimed at action detection tasks.",
        "dcterms:title": "Charades",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Action Detection"
        ],
        "dcat:keyword": [
            "Action detection",
            "Video dataset",
            "Dense annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Action Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Jiyang Gao",
            "Chen Sun",
            "Zhenheng Yang",
            "Ram Nevatia"
        ],
        "dcterms:description": "Charades-STA is an extension of Charades that introduces natural language descriptions as annotations for action detection.",
        "dcterms:title": "Charades-STA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Action Detection",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Action detection",
            "Natural language descriptions",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Moment Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Fabian Caba Heilbron",
            "Victor Escorcia",
            "Bernard Ghanem",
            "Juan Carlos Niebles"
        ],
        "dcterms:description": "ActivityNet (ANet) is a large-scale video benchmark for human activity understanding, containing a diverse set of action categories.",
        "dcterms:title": "ActivityNet (ANet)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Action Detection"
        ],
        "dcat:keyword": [
            "Activity recognition",
            "Video dataset",
            "Human activity understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Action Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Kenji Hata",
            "Frederic Ren",
            "Li Fei-Fei",
            "Juan Carlos Niebles"
        ],
        "dcterms:description": "ActivityNet-Caption (ANet-Caption) is built upon ActivityNet, providing dense captions for events in videos.",
        "dcterms:title": "ActivityNet-Caption (ANet-Caption)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Action Detection",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Dense captioning",
            "Video dataset",
            "Event description"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Moment Retrieval"
        ]
    }
]