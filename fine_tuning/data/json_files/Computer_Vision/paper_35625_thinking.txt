To extract datasets from the research paper titled "Pointwise Convolutional Neural Networks" by Binh-Son Hua et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by examining the **abstract** and **introduction** sections. The abstract mentions that the paper presents a convolutional neural network for semantic segmentation and object recognition with 3D point clouds, which suggests that datasets related to these tasks may be discussed.

Next, I will look into **section 4 (Evaluations)**, where the authors describe their experiments. This section is likely to contain detailed information about the datasets used for semantic segmentation and object recognition. 

In the evaluations, I find that the authors mention two datasets for semantic segmentation:

1. **S3DIS Dataset**: This dataset is used for semantic scene segmentation and contains 13 categories of indoor scene objects. The authors state that each point has 9 attributes, including XYZ coordinates and RGB color.

2. **SceneNN Dataset**: This dataset is annotated with 40 categories defined by the NYU v2 dataset and is used for testing semantic segmentation on more complex indoor scenes.

For object recognition, the authors mention two additional datasets:

3. **ModelNet40 Dataset**: A CAD model dataset consisting of 40 categories, which serves as a standard benchmark for object recognition.

4. **ObjectNN Dataset**: This dataset is derived from RGB-D scene reconstruction mixed with CAD models, aimed at studying 3D object retrieval.

Now, I will check the **References section** to find the full citations for each of these datasets:

- For the **S3DIS Dataset**, the citation is:
  > Armeni, I., Sener, O., Zamir, A. R., Jiang, H., Brilakis, I., Fischer, M., & Savarese, S. (2016). 3D semantic parsing of large-scale indoor spaces. In CVPR.

- For the **SceneNN Dataset**, the citation is:
  > Hua, B.-S., Pham, Q.-H., Nguyen, D. T., Tran, M.-K., Yu, L.-F., & Yeung, S.-K. (2016). SceneNN: A scene meshes dataset with annotations. In International Conference on 3D Vision (3DV). http://www.scenenn.net.

- For the **ModelNet40 Dataset**, the citation is:
  > Wu, Z., Song, S., Khosla, A., Tang, X., & Xiao, J. (2015). 3D ShapeNets: A deep representation for volumetric shapes. In CVPR.

- For the **ObjectNN Dataset**, the citation is:
  > Hua, B.-S., Truong, Q.-T., Tran, M.-K., Pham, Q.-H., Kanezaki, A., Lee, T., Chiang, H., Hsu, W., Li, B., Lu, Y., et al. (2017). SHREC17: RGB-D to CAD retrieval with ObjectNN dataset.

After gathering all this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research paper.