To extract datasets from the research paper titled "KPConv: Flexible and Deformable Convolution for Point Clouds" by Hugues Thomas et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract** and **introduction** sections to get an overview of the datasets referenced. The abstract mentions that the KPConv networks outperform state-of-the-art methods on several datasets, indicating that datasets will be discussed in detail later in the paper.

Next, I will focus on the **experiments section**, particularly **section 4**, which is likely to contain detailed information about the datasets used for evaluation. In **section 4.1**, the authors state that they evaluate their networks on two common model datasets: **ModelNet40** and **ShapeNetPart**. This is a clear indication that these datasets are significant for their experiments.

In **section 4.2**, the authors mention using four additional datasets for 3D scene segmentation: **Scannet**, **S3DIS**, **Semantic3D**, and **Paris-Lille-3D**. This section provides a comprehensive list of datasets utilized in their experiments.

Now, I will check the **References section** to find the full citations for each dataset mentioned. The citations are crucial for proper attribution and to provide context for each dataset.

1. **ModelNet40**: The citation is:
   > Wu, Zhirong, et al. "3D ShapeNets: A Deep Representation for Volumetric Shapes." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1912–1920, 2015.

2. **ShapeNetPart**: The citation is:
   > Yi, Li, et al. "A Scalable Active Framework for Region Annotation in 3D Shape Collections." ACM Transactions on Graphics (TOG), 35(6):210, 2016.

3. **Scannet**: The citation is:
   > Dai, Angela, et al. "ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5828–5839, 2017.

4. **S3DIS**: The citation is:
   > Armeni, Iro, et al. "3D Semantic Parsing of Large-Scale Indoor Spaces." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1534–1543, 2016.

5. **Semantic3D**: The citation is:
   > Hackel, Timo, et al. "Semantic3D.net: A New Large-Scale Point Cloud Classification Benchmark." arXiv preprint arXiv:1704.03847, 2017.

6. **Paris-Lille-3D**: The citation is:
   > Roynard, Xavier, et al. "Paris-Lille-3D: A Large and High-Quality Ground-Truth Urban Point Cloud Dataset for Automatic Segmentation and Classification." The International Journal of Robotics Research, 37(6):545–557, 2018.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a clear and comprehensive overview of the datasets used in the research paper.