To extract datasets from the research paper titled "Monocular Depth Estimation using Diffusion Models" by Saurabh Saxena et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** to identify any datasets mentioned. The abstract indicates that the authors achieved state-of-the-art performance on the indoor NYU dataset and near state-of-the-art results on the outdoor KITTI dataset. This suggests that these datasets are crucial to their research.

Next, I will examine the **introduction** section for any additional context or mentions of datasets. The introduction discusses the challenges of training data availability and quality, but it primarily reiterates the importance of the NYU and KITTI datasets.

I will then focus on **section 4.1 (Datasets)**, where the authors explicitly list the datasets used for their experiments. Here, they mention several datasets:

1. **ScanNet**: A dataset of 2.5 million images captured using a Kinect v1-like sensor, providing depth maps and RGB images.
2. **SceneNet RGB-D**: A synthetic dataset of 5 million images generated by rendering ShapeNet objects in various scenes.
3. **NYU Depth V2**: A commonly used dataset for evaluating indoor depth prediction models, providing aligned image and depth maps.
4. **Waymo Open Dataset**: A large-scale driving dataset consisting of about 200,000 frames with RGB images and LiDAR maps.
5. **KITTI**: An outdoor driving dataset that provides RGB images and LiDAR scans.

In the **experiments section**, the authors confirm that they used the NYU and KITTI datasets for evaluation, reinforcing their significance in the study.

Next, I will check the **References section** to gather full citations for each dataset mentioned:

- For **ScanNet**, the citation is:
  > Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., and Nießner, M. *ScanNet: Richly-annotated 3D reconstructions of indoor scenes*. arXiv:1702.04405, 2017.

- For **SceneNet RGB-D**, the citation is:
  > McCormac, J., Handa, A., Leutenegger, S., and Davison, A. J. *SceneNet RGB-D: 5M photorealistic images of synthetic indoor trajectories with ground truth*. arXiv:1612.05079, 2016.

- For **NYU Depth V2**, the citation is:
  > Silberman, N., Hoiem, D., Kohli, P., and Fergus, R. *Indoor segmentation and support inference from RGBD images*. In European Conference on Computer Vision (ECCV), pp. 746–760. Springer, 2012.

- For **Waymo Open Dataset**, the citation is:
  > Sun, P., Kretzschmar, H., Dotiwalla, X., et al. *Scalability in perception for autonomous driving: Waymo open dataset*. In CVPR, 2020.

- For **KITTI**, the citation is:
  > Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. *Vision meets robotics: The KITTI dataset*. The International Journal of Robotics Research, 32(11):1231–1237, 2013.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured all relevant datasets and their full citations from the paper.