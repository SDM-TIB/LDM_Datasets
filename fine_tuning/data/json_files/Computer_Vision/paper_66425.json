[
    {
        "dcterms:creator": [
            "Xiaotian Han",
            "Quanzeng You",
            "Yongfei Liu",
            "Wentao Chen",
            "Huangjie Zheng",
            "Khalil Mrini",
            "Xudong Lin",
            "Yiqi Wang",
            "Bohan Zhai",
            "Jianbo Yuan",
            "Heng Wang",
            "Hongxia Yang"
        ],
        "dcterms:description": "InfiMM-Eval is a manually curated benchmark dataset designed specifically for evaluating multi-modal large language models (MLLMs) on complex reasoning tasks, including deductive, abductive, and analogical reasoning. It includes detailed reasoning steps for each question to assess the reasoning capabilities of MLLMs.",
        "dcterms:title": "InfiMM-Eval",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://infimm.github.io/InfiMM-Eval/",
        "dcat:theme": [
            "Multi-modal Reasoning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multi-modal Large Language Models",
            "Reasoning",
            "Benchmark",
            "Open-ended Questions"
        ],
        "dcat:landingPage": "https://infimm.github.io/InfiMM-Eval/",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Complex Reasoning Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Weihao Yu",
            "Zhengyuan Yang",
            "Linjie Li",
            "Jianfeng Wang",
            "Kevin Lin",
            "Zicheng Liu",
            "Xinchao Wang",
            "Lijuan Wang"
        ],
        "dcterms:description": "MM-Vet is a dataset designed to evaluate large multimodal models for their integrated capabilities, focusing on various multimodal tasks.",
        "dcterms:title": "MM-Vet",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Evaluation"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Evaluation",
            "Integrated Capabilities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multimodal Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Tomohiro Sawada",
            "Daniel Paleka",
            "Alexander Havrilla",
            "Pranav Tadepalli",
            "Paula Vidas",
            "Alexander Kranias",
            "John J. Nay",
            "Kshitij Gupta",
            "Aran Komatsuzaki"
        ],
        "dcterms:description": "ARB is an advanced reasoning benchmark designed for large language models, focusing on evaluating their reasoning capabilities.",
        "dcterms:title": "ARB",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reasoning Benchmark"
        ],
        "dcat:keyword": [
            "Reasoning",
            "Benchmark",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reasoning Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "ARC is the AI2 Reasoning Challenge, a benchmark for evaluating question answering capabilities in AI systems.",
        "dcterms:title": "ARC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1803.05457",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reasoning",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Vineet Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano",
            "Christopher Hesse",
            "John Schulman"
        ],
        "dcterms:description": "GSM8k is a dataset for training models to solve math word problems, providing a benchmark for evaluating mathematical reasoning.",
        "dcterms:title": "GSM8k",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematical Reasoning"
        ],
        "dcat:keyword": [
            "Math Problems",
            "Benchmark",
            "Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Chaoyou Fu",
            "Peixian Chen",
            "Yunhang Shen",
            "Yulei Qin",
            "Mengdan Zhang",
            "Xu Lin",
            "Zhenyu Qiu",
            "Wei Lin",
            "Jinrui Yang",
            "Xiawu Zheng"
        ],
        "dcterms:description": "MME is a comprehensive evaluation benchmark for multimodal large language models, assessing various capabilities.",
        "dcterms:title": "MME",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.13394",
        "dcat:theme": [
            "Multimodal Evaluation"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Evaluation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multimodal Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yuan Liu",
            "Haodong Duan",
            "Yuanhan Zhang",
            "Bo Li",
            "Songyang Zhang",
            "Wangbo Zhao",
            "Yike Yuan",
            "Jiaqi Wang",
            "Conghui He",
            "Ziwei Liu"
        ],
        "dcterms:description": "MMBench is a benchmark designed to evaluate the performance of multimodal models across various tasks.",
        "dcterms:title": "MMBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.06281",
        "dcat:theme": [
            "Multimodal Evaluation"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multimodal Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "dcterms:description": "SeedBench is a benchmark for evaluating multimodal large language models with a focus on generative comprehension.",
        "dcterms:title": "SeedBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.16125",
        "dcat:theme": [
            "Multimodal Evaluation"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Evaluation",
            "Generative Comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Generative Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Pan Lu",
            "Hritik Bansal",
            "Tony Xia",
            "Jiacheng Liu",
            "Chunyuan Li",
            "Hannaneh Hajishirzi",
            "Hao Cheng",
            "Kai-Wei Chang",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "dcterms:description": "MathVista is a dataset for evaluating the mathematical reasoning capabilities of foundation models in visual contexts.",
        "dcterms:title": "MathVista",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.02255",
        "dcat:theme": [
            "Mathematical Reasoning"
        ],
        "dcat:keyword": [
            "Mathematics",
            "Visual Reasoning",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Shuai Bai",
            "Shusheng Yang",
            "Jinze Bai",
            "Peng Wang",
            "Xingxuan Zhang",
            "Junyang Lin",
            "Xinggang Wang",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "dcterms:description": "TouchStone is a benchmark for evaluating vision-language models using language models as evaluators.",
        "dcterms:title": "TouchStone",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Evaluation"
        ],
        "dcat:keyword": [
            "Vision-Language Models",
            "Evaluation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Vision-Language Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bitton",
            "Hritik Bansal",
            "Jack Hessel",
            "Rulin Shao",
            "Wanrong Zhu",
            "Anas Awadalla",
            "Josh Gardner",
            "Rohan Taori",
            "Ludwig Schimdt"
        ],
        "dcterms:description": "VisIT-Bench is a benchmark for evaluating vision-language instruction following inspired by real-world use cases.",
        "dcterms:title": "VisIT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Evaluation"
        ],
        "dcat:keyword": [
            "Vision-Language Models",
            "Evaluation",
            "Instruction Following"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Vision-Language Evaluation"
        ]
    }
]