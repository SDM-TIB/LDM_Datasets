To extract datasets from the research paper titled "Fake face detection via adaptive manipulation traces extraction network" by Zhiqing Guo et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental results sections** to identify any datasets mentioned. The abstract provides a brief overview of the problem and the proposed solution, but it may not detail specific datasets. The introduction often discusses the context and may reference datasets used in previous works.

Next, I will focus on **section IV (Experimental Results and Analysis)**, where the authors describe their experiments. Here, they mention the construction of a **hybrid fake face (HFF) dataset** that includes real and fake face images. This section is crucial as it typically contains detailed descriptions of the datasets used in the experiments.

In the description of the HFF dataset, the authors specify that it contains real face images from three sources:
1. **CelebA**: A dataset of celebrity images.
2. **CelebA-HQ**: A high-resolution version of the CelebA dataset.
3. **FaceForensics**: A dataset containing video frames of faces.

Additionally, the authors mention that fake face images are generated using several techniques:
- **PGGAN**: A generative model for identity manipulation.
- **StyleGAN**: Another generative model for identity manipulation.
- **Face2Face**: A CG-based expression manipulation technique.
- **Glow**: A generative model for expression manipulation.
- **StarGAN**: A generative model for attribute transfer.

I will also check the **References section** to find full citations for these datasets. The citations are essential for proper attribution and to allow others to access the datasets.

Here are the citations I will extract:
- For **CelebA**:
  > Liu, Z., Luo, P., Wang, X., & Tang, X. (2015). Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 3730-3738.

- For **CelebA-HQ**:
  > Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2018). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.

- For **FaceForensics**:
  > Rössler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., & Nießner, M. (2018). FaceForensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179.

- For **PGGAN**:
  > Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2018). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.

- For **StyleGAN**:
  > Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4401-4410.

- For **Face2Face**:
  > Thies, J., Zollhöfer, M., Stamminger, M., Theobalt, C., & Nießner, M. (2016). Face2face: Real-time face capture and reenactment of RGB videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2387-2395.

- For **Glow**:
  > Kingma, D. P., & Dhariwal, P. (2018). Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, 10, 215-224.

- For **StarGAN**:
  > Choi, Y., Choi, M., Kim, J.-W., Ha, J.-W., Kim, S., & Choo, J. (2018). StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 8789-8797.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research paper.