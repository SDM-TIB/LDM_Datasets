To extract datasets from the research paper titled "Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind AI Generated Image Quality Assessment" by Jun Fu et al., I will follow a systematic approach.

First, I will read through the **abstract and introduction** to identify any mentions of datasets. The abstract indicates that the authors conducted experiments on two public AGIQA datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will look for a dedicated **section on datasets** or any relevant sections that discuss the experimental setup. In this case, the **Experiments section** (Section III) provides information about the datasets used. The authors mention two datasets:

1. **AGIQA-3K**: This dataset contains 2,982 AI-generated images (AGIs) produced by various generative models, including Glide, Stable Diffusion, and others. Each image is accompanied by perceptual quality and alignment scores annotated by subjects.

2. **AIGCIQA-2023**: This dataset consists of 2,400 AGIs generated by models like Glide and Stable Diffusion. Notably, user input text prompts are not available in this dataset.

Now, I will check the **References section** to find the full citations for these datasets. The citations are as follows:

- For **AGIQA-3K**, the citation is:
  > C. Li, Z. Zhang, H. Wu, W. Sun, X. Min, X. Liu, G. Zhai, and W. Lin. *AGIQA-3K: An open database for AI-generated image quality assessment*. arXiv preprint arXiv:2306.04717, 2023.

- For **AIGCIQA-2023**, the citation is:
  > J. Wang, H. Duan, J. Liu, S. Chen, X. Min, and G. Zhai. *AIGCIQA-2023: A large-scale image quality assessment database for AI generated images: from the perspectives of quality, authenticity and correspondence*. arXiv preprint arXiv:2307.00211, 2023.

With these citations in hand, I will summarize the datasets and their citations clearly.

Finally, I will compile the dataset entries into a structured format for further processing or review.