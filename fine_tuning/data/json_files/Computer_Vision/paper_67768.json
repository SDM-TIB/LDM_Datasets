[
    {
        "dcterms:creator": [
            "Matthew Loper",
            "Naureen Mahmood",
            "Javier Romero",
            "Gerard Pons-Moll",
            "Michael J Black"
        ],
        "dcterms:description": "SMPL-X is a parametric human model that integrates head and hand models, allowing for the extraction of body parts and localization of camera poses for rendering.",
        "dcterms:title": "SMPL-X",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Modeling",
            "Human Shape Representation"
        ],
        "dcat:keyword": [
            "Parametric model",
            "Human shape",
            "3D avatar"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Human Shape Representation"
        ]
    },
    {
        "dcterms:creator": [
            "Tianchang Shen",
            "Jun Gao",
            "Kangxue Yin",
            "Ming-Yu Liu",
            "Sanja Fidler"
        ],
        "dcterms:description": "DMTet is a hybrid representation for high-resolution 3D shape synthesis, combining a tetrahedral grid with an implicit signed distance function.",
        "dcterms:title": "DMTet",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Shape Synthesis",
            "Computer Graphics"
        ],
        "dcat:keyword": [
            "Hybrid representation",
            "3D shape synthesis",
            "Tetrahedral grid"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "3D Shape Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jong Wook Kim",
            "Chris Hallacy",
            "Aditya Ramesh",
            "Gabriel Goh",
            "Sandhini Agarwal",
            "Girish Sastry",
            "Amanda Askell",
            "Pamela Mishkin",
            "Jack Clark"
        ],
        "dcterms:description": "CLIP is a model that learns transferable visual representations from natural language supervision, enabling various applications in computer vision.",
        "dcterms:title": "CLIP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual representation",
            "Natural language supervision",
            "Transfer learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Classification",
            "Text-to-Image Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Thiemo Alldieck",
            "Hongyi Xu",
            "Cristian Sminchisescu"
        ],
        "dcterms:description": "imGHUM is an implicit generative model of 3D human shape and articulated pose, providing a framework for generating human avatars.",
        "dcterms:title": "imGHUM",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Human Modeling",
            "Pose Estimation"
        ],
        "dcat:keyword": [
            "Implicit model",
            "3D human shape",
            "Articulated pose"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Human Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "Yukang Cao",
            "Yan-Pei Cao",
            "Kai Han",
            "Ying Shan",
            "Kwan-Yee K Wong"
        ],
        "dcterms:description": "DreamAvatar is a framework for text-and-shape guided 3D human avatar generation via diffusion models, enabling the creation of diverse avatars from textual descriptions.",
        "dcterms:title": "DreamAvatar",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "Diffusion models",
            "3D avatar generation",
            "Text-guided generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Fangzhou Hong",
            "Mingyuan Zhang",
            "Liang Pan",
            "Zhongang Cai",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "dcterms:description": "AvatarCLIP is a zero-shot text-driven generation and animation framework for 3D avatars, allowing for the creation of avatars based on textual prompts.",
        "dcterms:title": "AvatarCLIP",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "Zero-shot generation",
            "Text-driven animation",
            "3D avatars"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Generation",
            "Animation"
        ]
    },
    {
        "dcterms:creator": [
            "Shuo Huang",
            "Zongxin Yang",
            "Liangting Li",
            "Yi Yang",
            "Jia Jia"
        ],
        "dcterms:description": "AvatarFusion is a framework for zero-shot generation of clothing-decoupled 3D avatars using 2D diffusion models, enabling the creation of avatars with separate clothing and body representations.",
        "dcterms:title": "AvatarFusion",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "Zero-shot generation",
            "Clothing decoupling",
            "2D diffusion models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Nikos Kolotouros",
            "Thiemo Alldieck",
            "Andrei Zanfir",
            "Eduard Gabriel Bazavan",
            "Mihai Fieraru",
            "Cristian Sminchisescu"
        ],
        "dcterms:description": "DreamHuman is a framework for generating animatable 3D avatars from text, focusing on creating avatars that can be animated based on textual descriptions.",
        "dcterms:title": "DreamHuman",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "Animatable avatars",
            "Text-guided generation",
            "3D avatars"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Generation",
            "Animation"
        ]
    },
    {
        "dcterms:creator": [
            "Rui Chen",
            "Yongwei Chen",
            "Ningxin Jiao",
            "Kui Jia"
        ],
        "dcterms:description": "Fantasia3D is a framework that disentangles geometry and appearance for high-quality text-to-3D content creation, allowing for the generation of detailed 3D models from text prompts.",
        "dcterms:title": "Fantasia3D",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Content Creation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "Geometry and appearance disentanglement",
            "Text-to-3D generation",
            "High-quality models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "3D Content Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Tingting Liao",
            "Hongwei Yi",
            "Yuliang Xiu",
            "Jiaxaing Tang",
            "Justus Thies",
            "Michael J Black"
        ],
        "dcterms:description": "TADA is a framework for generating animatable digital avatars from text, focusing on creating avatars that can be animated based on textual descriptions.",
        "dcterms:title": "TADA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "Animatable avatars",
            "Text-guided generation",
            "3D avatars"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Generation",
            "Animation"
        ]
    }
]