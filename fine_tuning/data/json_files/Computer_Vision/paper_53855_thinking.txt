To extract datasets from the research paper titled "Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data" by Sungwon Kim et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors trained their model on "large-scale untranscribed datasets," which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on the **experiments section**, particularly the part where the authors discuss the datasets used for training and evaluation. In this section, they mention several datasets:

1. **LibriSpeech**: A multi-speaker ASR dataset of approximately 1,000 hours.
2. **LibriTTS**: A large-scale multi-speaker TTS dataset consisting of approximately 585 hours of labeled speech spoken by 2,456 speakers.
3. **Libri-Light**: A dataset composed of 60K hours of unlabeled speech and a small amount of labeled speech (10 hours, 1 hour, 10 minutes) obtained from audiobooks.
4. **VCTK**: A paired dataset of approximately 44 hours uttered by 110 speakers.
5. **LJSpeech**: A female speaker dataset consisting of 13,100 audio clips.

I will then verify the details of these datasets in the **background or methodology sections** to ensure I have accurate descriptions and confirm their relevance to the study.

After gathering the dataset names and descriptions, I will look at the **References section** to find the full citations for each dataset. The citations are crucial for proper attribution and will include the authors, title, publication venue, and year.

The full citations I will extract are:

- For **LibriSpeech**:
  > Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. *Librispeech: An ASR corpus based on public domain audio books*. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964.

- For **LibriTTS**:
  > Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. *LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech*. In Proc. Interspeech 2019, pages 1526–1530, 2019. doi: 10.21437/Interspeech.2019-2441.

- For **Libri-Light**:
  > J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. *Libri-light: A benchmark for ASR with limited or no supervision*. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669–7673, 2020. https://github.com/facebookresearch/libri-light.

- For **VCTK**:
  > Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. *CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)*. 2019.

- For **LJSpeech**:
  > Keith Ito. *The LJ Speech Dataset*. https://keithito.com/LJ-Speech-Dataset/, 2017.

Finally, I will compile the dataset entries with their descriptions and citations into a structured format for easy reference and further processing.