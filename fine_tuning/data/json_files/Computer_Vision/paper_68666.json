[
    {
        "dcterms:creator": [
            "Alex Wang",
            "Yada Pruksachatkun",
            "Nikita Nangia",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A benchmark of diverse NLP tasks and their expert-written instructions, enabling rigorous benchmarking of the more practical settings for the CL of LLMs. Specifically, in the types of dialogue generation, information extraction, question answering, summarization, and sentiment analysis, we select three tasks for each type, forming a sequence comprising a total of 15 tasks to evaluate various methods.",
        "dcterms:title": "SuperNI Benchmark",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "NLP tasks",
            "dialogue generation",
            "information extraction",
            "question answering",
            "summarization",
            "sentiment analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Generation",
            "Information Extraction",
            "Question Answering",
            "Summarization",
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Anastasia Razdaibiedina",
            "Yuning Mao",
            "Rui Hou",
            "Madiam Khabsa",
            "Mike Lewis",
            "Amjad Almahairi"
        ],
        "dcterms:description": "A continual learning benchmark of 15 classification datasets. Following Razdaibiedina et al. (2023); Wang et al. (2023a), we select 1,000 random samples for training each task and hold out 500 samples per class for validation and testing.",
        "dcterms:title": "Long Sequence Benchmark",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Continual Learning"
        ],
        "dcat:keyword": [
            "classification datasets",
            "continual learning",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Classification"
        ]
    }
]