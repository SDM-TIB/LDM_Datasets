[
    {
        "dcterms:creator": [
            "O. Martin",
            "I. Kotsia",
            "B. M. Macq",
            "I. Pitas"
        ],
        "dcterms:description": "The eNTERFACE’05 corpus is a publicly-available multi-modal corpus of elicited emotional utterances, containing audio, facial expression, and text data. It includes 42 subjects from various countries, each expressing emotions based on short stories designed to elicit specific emotions.",
        "dcterms:title": "eNTERFACE’05 corpus",
        "dcterms:issued": "2006",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Audio-Visual Data"
        ],
        "dcat:keyword": [
            "Emotion corpus",
            "Speech emotion recognition",
            "Multi-modal data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "D. Snyder",
            "G. Chen",
            "D. Povey"
        ],
        "dcterms:description": "The MUSAN corpus consists of three portions: music, speech, and noise. It is used for data augmentation in speech emotion recognition tasks, specifically utilizing the noise portion to mix with clean emotional utterances.",
        "dcterms:title": "MUSAN corpus",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1510.08484v1",
        "dcat:theme": [
            "Audio Processing",
            "Noise Data"
        ],
        "dcat:keyword": [
            "Noise corpus",
            "Data augmentation",
            "Speech recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Data Augmentation"
        ]
    },
    {
        "dcterms:creator": [
            "B. W. Schuller",
            "S. Steidl",
            "A. Batliner",
            "A. Vinciarelli",
            "K. R. Scherer",
            "F. Ringeval",
            "M. Chetouani",
            "F. Weninger",
            "F. Eyben",
            "E. Marchi",
            "M. Mortillaro",
            "H. Salamin",
            "A. Polychroniou",
            "F. Valente",
            "S. Kim"
        ],
        "dcterms:description": "The INTERSPEECH challenges feature sets include a variety of hand-crafted features used for speech emotion recognition and computational paralinguistics challenges, providing a benchmark for evaluating emotion recognition systems.",
        "dcterms:title": "INTERSPEECH challenges feature sets",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Processing",
            "Emotion Recognition"
        ],
        "dcat:keyword": [
            "Feature sets",
            "Speech emotion recognition",
            "Paralinguistics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    }
]