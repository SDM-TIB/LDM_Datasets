[
    {
        "dcterms:creator": [
            "Max Bain",
            "Arsha Nagrani",
            "GÃ¼l Varol",
            "Andrew Zisserman"
        ],
        "dcterms:description": "WebVid2M contains about 2.5M image-text pairs harvested from the web, with most pairs having visual and language well-aligned captions.",
        "dcterms:title": "WebVid2M",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Retrieval"
        ],
        "dcat:keyword": [
            "Image-Text Pairs",
            "Web Data",
            "Video-Text Alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Piyush Sharma",
            "Nan Ding",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "dcterms:description": "Google Conceptual Captions (CC3M) contains about 3.3M image-text pairs, with raw text descriptions gleaned from HTML attributes associated with the images, exhibiting large diversity in style.",
        "dcterms:title": "Google Conceptual Captions (CC3M)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image-Text Retrieval"
        ],
        "dcat:keyword": [
            "Image-Text Pairs",
            "Diversity",
            "HTML Attributes"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Jun Xu",
            "Tao Mei",
            "Ting Yao",
            "Yong Rui"
        ],
        "dcterms:description": "MSR-VTT provides 10K video clips with about 200K video-text pairs, serving as a popular benchmark for video-text retrieval.",
        "dcterms:title": "MSR-VTT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Retrieval"
        ],
        "dcat:keyword": [
            "Video Clips",
            "Video-Text Pairs",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Lisa Anne Hendricks",
            "Oliver Wang",
            "Eli Shechtman",
            "Josef Sivic",
            "Trevor Darrell",
            "Bryan Russell"
        ],
        "dcterms:description": "DiDeMo consists of 10K videos, each labeled with multiple sentences, leading to a total of 40K sentences, used for moment localization in videos.",
        "dcterms:title": "DiDeMo",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Localization"
        ],
        "dcat:keyword": [
            "Video Sentences",
            "Moment Localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Moment Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Niket Tandon",
            "Bernt Schiele"
        ],
        "dcterms:description": "LSMDC-2016 consists of 128K clips for movie description, providing a dataset for evaluating video description models.",
        "dcterms:title": "LSMDC",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Description"
        ],
        "dcat:keyword": [
            "Movie Clips",
            "Video Description"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Description"
        ]
    },
    {
        "dcterms:creator": [
            "David Chen",
            "William B Dolan"
        ],
        "dcterms:description": "MSVD is a smaller benchmark consisting of 1,970 videos, each described by 40 English sentences, used for evaluating video captioning.",
        "dcterms:title": "MSVD",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Captioning"
        ],
        "dcat:keyword": [
            "Video Sentences",
            "Video Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Video Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Antoine Miech",
            "Dimitri Zhukov",
            "Jean-Baptiste Alayrac",
            "Makarand Tapaswi",
            "Ivan Laptev",
            "Josef Sivic"
        ],
        "dcterms:description": "HowTo100M is a large-scale dataset containing narrated video clips, used for learning text-video embeddings.",
        "dcterms:title": "HowTo100M",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Narrated Videos",
            "Text-Video Embedding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-Video Embedding"
        ]
    }
]