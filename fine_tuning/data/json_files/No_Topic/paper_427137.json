[
    {
        "dcterms:creator": [
            "Haotian Liu",
            "Chunyuan Li",
            "Yuheng Li",
            "Yong Jae Lee"
        ],
        "dcterms:description": "A comprehensive mixture of image-text paired data for pre-training and instruction-following tasks, enhancing general vision-language understanding.",
        "dcterms:title": "LLaVA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Understanding",
            "Image-Text Pairing"
        ],
        "dcat:keyword": [
            "Visual Instruction Tuning",
            "Multimodal Learning",
            "Image-Text Data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Visual Question Answering",
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Lin Chen",
            "Jinsong Li",
            "Xiao wen Dong",
            "Pan Zhang",
            "Conghui He",
            "Jiaqi Wang",
            "Feng Zhao",
            "Dahua Lin"
        ],
        "dcterms:description": "A dataset that improves large multimodal models with better captions, consisting of a mixture of images and text.",
        "dcterms:title": "ShareGPT4V",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Understanding",
            "Image-Text Pairing"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Image Captioning",
            "Visual Instruction Tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Visual Question Answering",
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Xinlei Chen",
            "Hao Fang",
            "Tsung-Yi Lin",
            "Ramakrishna Vedantam",
            "Saurabh Gupta",
            "Piotr Doll√°r",
            "C. Lawrence Zitnick"
        ],
        "dcterms:description": "A dataset for image captioning that includes a large number of images with corresponding captions, facilitating evaluation and data collection.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image Annotations",
            "Caption Generation",
            "Visual Recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "D. A. Hudson",
            "C. D. Manning"
        ],
        "dcterms:description": "A dataset designed for real-world visual reasoning and compositional question answering, providing a diverse set of questions and images.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Compositional Questions",
            "Visual Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Anand Mishra",
            "Shashank Shekhar",
            "Ajeet Kumar Singh",
            "Anirban Chakraborty"
        ],
        "dcterms:description": "A dataset for visual question answering that focuses on reading text in images, enabling models to answer questions based on textual information present in images.",
        "dcterms:title": "OCR-VQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Text Recognition"
        ],
        "dcat:keyword": [
            "Text in Images",
            "Visual Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Singh",
            "V. Natarajan",
            "M. Shah",
            "Y. Jiang",
            "X. Chen",
            "D. Batra",
            "D. Parikh",
            "M. Rohrbach"
        ],
        "dcterms:description": "A dataset aimed at developing visual question answering models that can read text in images, enhancing the model's ability to understand visual content.",
        "dcterms:title": "TextVQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Text Recognition"
        ],
        "dcat:keyword": [
            "Text in Images",
            "Visual Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A. Shamma",
            "Michael S. Bernstein",
            "Li Fei-Fei"
        ],
        "dcterms:description": "A dataset that connects language and vision using crowdsourced dense image annotations, providing a rich source of visual information.",
        "dcterms:title": "VisualGenome",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Visual Understanding"
        ],
        "dcat:keyword": [
            "Dense Annotations",
            "Visual Recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Image Annotation"
        ]
    },
    {
        "dcterms:creator": [
            "Sahar Kazemzadeh",
            "Vicente Ordonez",
            "Mark Matten",
            "Tamara Berg"
        ],
        "dcterms:description": "A dataset for referring to objects in photographs of natural scenes, facilitating the study of reference resolution in visual contexts.",
        "dcterms:title": "RefCOCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reference Resolution",
            "Visual Understanding"
        ],
        "dcat:keyword": [
            "Object Reference",
            "Natural Scenes"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Object Reference"
        ]
    },
    {
        "dcterms:creator": [
            "Harsh Agrawal",
            "Karan Desai",
            "Yufei Wang",
            "Xinlei Chen",
            "Rishabh Jain",
            "Mark Johnson",
            "Dhruv Batra",
            "Devi Parikh",
            "Stefan Lee",
            "Peter Anderson"
        ],
        "dcterms:description": "A dataset for novel object captioning at scale, focusing on generating captions for images containing novel objects.",
        "dcterms:title": "Nocaps",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Novel Object Recognition"
        ],
        "dcat:keyword": [
            "Novel Objects",
            "Image Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Young, P.",
            "Lai, A.",
            "Hodosh, M.",
            "Hockenmaier, J."
        ],
        "dcterms:description": "A dataset consisting of image descriptions that facilitate semantic inference over event descriptions.",
        "dcterms:title": "Flickr30K",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Semantic Inference"
        ],
        "dcat:keyword": [
            "Image Descriptions",
            "Event Descriptions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Image Captioning"
        ]
    }
]