[
    {
        "dcterms:creator": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Hongchang Lyu",
            "Jiankai Tang",
            "Zhidian Huang",
            "Zhengxiao Du",
            "Xiao Liu",
            "Aohan Zeng",
            "Lei Hou"
        ],
        "dcterms:description": "A bilingual, multitask benchmark for long context understanding, designed to test the capabilities of language models in handling extended documents and complex information sequences.",
        "dcterms:title": "LongBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2308.14508",
        "dcat:theme": [
            "Natural Language Processing",
            "Long Context Understanding"
        ],
        "dcat:keyword": [
            "benchmark",
            "long context",
            "language models",
            "multitask evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tomáš Koˇcisk`y",
            "Jonathan Schwarz",
            "Phil Blunsom",
            "Chris Dyer",
            "Karl Moritz Hermann",
            "Gábor Melis",
            "Edward Grefenstette"
        ],
        "dcterms:description": "A dataset for reading comprehension that poses narrative questions based on stories, designed to evaluate the ability of models to understand and answer questions based on a single document.",
        "dcterms:title": "Single-Document QA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "reading comprehension",
            "narrative questions",
            "single document"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Zhilin Yang",
            "Peng Qi",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "William Cohen",
            "Ruslan Salakhutdinov",
            "Christopher D Manning"
        ],
        "dcterms:description": "A dataset for multi-hop question answering that requires models to retrieve and reason over multiple documents to answer questions, emphasizing diverse and explainable answers.",
        "dcterms:title": "Multi-Document QA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "multi-hop",
            "question answering",
            "explainable answers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alexander Richard Fabbri",
            "Irene Li",
            "Tianwei She",
            "Suyi Li",
            "Dragomir Radev"
        ],
        "dcterms:description": "A large-scale dataset for multi-document summarization that includes diverse news articles, designed to evaluate models' ability to generate coherent summaries from multiple sources.",
        "dcterms:title": "Summarization",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "10.18653/v1/P19-1102",
        "dcat:theme": [
            "Summarization"
        ],
        "dcat:keyword": [
            "multi-document",
            "summarization",
            "news articles"
        ],
        "dcat:landingPage": "https://aclanthology.org/P19-1102",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large-scale dataset for reading comprehension that includes trivia questions and answers, designed for evaluating models' ability to answer questions based on diverse contexts.",
        "dcterms:title": "Few-shot Learning",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "trivia questions",
            "reading comprehension",
            "few-shot learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Daya Guo",
            "Canwen Xu",
            "Nan Duan",
            "Jian Yin",
            "Julian McAuley"
        ],
        "dcterms:description": "A dataset designed for evaluating code completion tasks, focusing on the ability of models to generate code snippets based on given contexts.",
        "dcterms:title": "Synthetic Task",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.14893",
        "dcat:theme": [
            "Code Completion"
        ],
        "dcat:keyword": [
            "code completion",
            "synthetic tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Tianyang Liu",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "dcterms:description": "A benchmark for evaluating repository-level code auto-completion systems, focusing on the ability of models to suggest code completions based on repository context.",
        "dcterms:title": "Code Completion",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Completion"
        ],
        "dcat:keyword": [
            "code auto-completion",
            "repository-level"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    }
]