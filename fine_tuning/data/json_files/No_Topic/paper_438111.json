[
    {
        "dcterms:creator": [
            "Haotian Liu",
            "Chunyuan Li",
            "Yuheng Li",
            "Yong Jae Lee"
        ],
        "dcterms:description": "A dataset used for visual instruction tuning, providing prompts for self-generated responses in the SIMA framework.",
        "dcterms:title": "LLaVA’s visual instruction tuning dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.03744",
        "dcat:theme": [
            "Visual Instruction Tuning",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Visual instruction",
            "Response generation",
            "Self-improvement"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Response Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Dollár",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "A dataset used for evaluating object detection and segmentation tasks, providing a validation set for hallucination benchmarks.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object detection",
            "Segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection",
            "Image Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Anna Rohrbach",
            "Lisa Anne Hendricks",
            "Kaylee Burns",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "dcterms:description": "A dataset for evaluating object hallucination in image captioning tasks.",
        "dcterms:title": "CHAIR",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1809.02156",
        "dcat:theme": [
            "Image Captioning",
            "Object Hallucination"
        ],
        "dcat:keyword": [
            "Image captioning",
            "Hallucination evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image Captioning",
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Xiyao Wang",
            "Yuhang Zhou",
            "Xiaoyu Liu",
            "Hongjin Lu",
            "Yuancheng Xu",
            "Feihong He",
            "Jaehong Yoon",
            "Taixi Lu",
            "Gedas Bertasius",
            "Mohit Bansal"
        ],
        "dcterms:description": "A comprehensive benchmark for multimodal large language model reasoning over image sequences.",
        "dcterms:title": "Mementos",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2401.10529",
        "dcat:theme": [
            "Multimodal Reasoning",
            "Image Sequences"
        ],
        "dcat:keyword": [
            "Multimodal reasoning",
            "Image sequences",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multimodal Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Haotian Liu",
            "Chunyuan Li",
            "Qingyang Wu",
            "Yong Jae Lee"
        ],
        "dcterms:description": "A dataset for visual instruction tuning, enhancing the performance of large vision-language models.",
        "dcterms:title": "LLaVA in the Wild",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Instruction Tuning",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Visual instruction",
            "Large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Response Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Pan Lu",
            "Swaroop Mishra",
            "Tanglin Xia",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Song-Chun Zhu",
            "Oyvind Tafjord",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "dcterms:description": "A dataset designed for multimodal reasoning in science question answering.",
        "dcterms:title": "ScienceQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Science Question Answering",
            "Multimodal Reasoning"
        ],
        "dcat:keyword": [
            "Science questions",
            "Multimodal reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering",
            "Multimodal Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Amanpreet Singh",
            "Vivek Natarajan",
            "Meet Shah",
            "Yu Jiang",
            "Xinlei Chen",
            "Dhruv Batra",
            "Devi Parikh",
            "Marcus Rohrbach"
        ],
        "dcterms:description": "A dataset that benchmarks visual reasoning based on text in images.",
        "dcterms:title": "TextVQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Text in Images"
        ],
        "dcat:keyword": [
            "Text VQA",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Chaoyou Fu",
            "Peixian Chen",
            "Yunhang Shen",
            "Yulei Qin",
            "Mengdan Zhang",
            "Xu Lin",
            "Jinrui Yang",
            "Xiawu Zheng",
            "Ke Li",
            "Xing Sun",
            "Yunsheng Wu",
            "Rongrong Ji"
        ],
        "dcterms:description": "A comprehensive evaluation benchmark for multimodal large language models.",
        "dcterms:title": "MME",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Large Language Models"
        ],
        "dcat:keyword": [
            "Evaluation benchmark",
            "Multimodal models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yuan Liu",
            "Haodong Duan",
            "Yuanhan Zhang",
            "Bo Li",
            "Songyang Zhang",
            "Wangbo Zhao",
            "Yike Yuan",
            "Jiaqi Wang",
            "Conghui He",
            "Ziwei Liu"
        ],
        "dcterms:description": "A benchmark to evaluate multimodal models across various tasks.",
        "dcterms:title": "MMBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.06281",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multimodal models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Weihao Yu",
            "Zhengyuan Yang",
            "Linjie Li",
            "Jianfeng Wang",
            "Kevin Lin",
            "Zicheng Liu",
            "Xinchao Wang",
            "Lijuan Wang"
        ],
        "dcterms:description": "A benchmark for evaluating large multimodal models for integrated capabilities.",
        "dcterms:title": "MM-Vet",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2308.02490",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Model Capabilities"
        ],
        "dcat:keyword": [
            "Evaluation",
            "Multimodal models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "dcterms:description": "A benchmark for assessing multimodal large language models with generative comprehension.",
        "dcterms:title": "SEEDBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.16125",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Generative Comprehension"
        ],
        "dcat:keyword": [
            "Evaluation benchmark",
            "Generative comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Danna Gurari",
            "Qing Li",
            "Abigale J Stangl",
            "Anhong Guo",
            "Chi Lin",
            "Kristen Grauman",
            "Jiebo Luo",
            "Jeffrey P Bigham"
        ],
        "dcterms:description": "A dataset for answering visual questions from blind people.",
        "dcterms:title": "VizWiz",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Accessibility"
        ],
        "dcat:keyword": [
            "Visual questions",
            "Blind accessibility"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]