[
    {
        "dcterms:creator": [
            "Xinlei Chen",
            "Hao Fang",
            "Tsung-Yi Lin",
            "Ramakrishna Vedantam",
            "Saurabh Gupta",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "The MS COCO dataset comprises 123,287 images for training, validation, and testing, with five human-annotated captions for each image.",
        "dcterms:title": "MS COCO dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1504.00325",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Image annotations",
            "Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Visual Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "Visual Genome is a general-purpose dataset meant to connect vision and language, containing 108,077 images with 50 region descriptions and 35 objects per image on average.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Vision and Language"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Dense annotations",
            "Region descriptions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Grounding",
            "Image Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Bryan A. Plummer",
            "Liwei Wang",
            "Christopher M. Cervantes",
            "Juan C. Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "dcterms:description": "Flickr30k Entities extends the original Flickr30k dataset with manually annotated bounding boxes grounding each noun phrase in the caption.",
        "dcterms:title": "Flickr30k Entities",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Bounding boxes",
            "Noun phrase grounding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Visual Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "Jordi Pont-Tuset",
            "Jasper Uijlings",
            "Soravit Changpinyo",
            "Radu Soricut",
            "Vittorio Ferrari"
        ],
        "dcterms:description": "Localized Narratives is a multimodal dataset for visual grounding based on the natural human task of describing images while simultaneously pointing at the regions being described.",
        "dcterms:title": "Localized Narratives",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Vision and Language"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Mouse traces",
            "Dense annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Grounding",
            "Image Description"
        ]
    },
    {
        "dcterms:creator": [
            "Sahar Kazemzadeh",
            "Vicente Ordonez",
            "Mark Matten",
            "Tamara Berg"
        ],
        "dcterms:description": "RefCOCO is a dataset for referring expressions comprehension and segmentation, built on top of the MS COCO dataset.",
        "dcterms:title": "RefCOCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Referring expressions",
            "Image segmentation",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Referring Expression Comprehension",
            "Visual Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "Junhua Mao",
            "Jonathan Huang",
            "Alexander Toshev",
            "Oana Camburu",
            "Alan L. Yuille",
            "Kevin Murphy"
        ],
        "dcterms:description": "RefCOCOg is a dataset for referring expressions comprehension and segmentation, containing longer descriptions including both appearance and location attributes of the target instances.",
        "dcterms:title": "RefCOCOg",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Referring expressions",
            "Image segmentation",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Referring Expression Comprehension",
            "Visual Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "Bolei Zhou",
            "Hang Zhao",
            "Xavier Puig",
            "Sanja Fidler",
            "Adela Barriuso",
            "Antonio Torralba"
        ],
        "dcterms:description": "ADE20K is a dataset for scene parsing that includes a wide variety of scenes and objects, providing pixel-level annotations.",
        "dcterms:title": "ADE20K",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Scene Parsing"
        ],
        "dcat:keyword": [
            "Scene dataset",
            "Pixel-level annotations",
            "Semantic segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Scene Parsing",
            "Visual Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "Alina Kuznetsova",
            "Hassan Rom",
            "Neil Alldrin",
            "Jasper Uijlings",
            "Ivan Krasin",
            "Jordi Pont-Tuset",
            "Shahab Kamali",
            "Stefan Popov",
            "Matteo Malloci",
            "Alexander Kolesnikov"
        ],
        "dcterms:description": "Open Images is a large-scale dataset with image-level labels, object bounding boxes, and visual relationships.",
        "dcterms:title": "Open Images",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Annotation"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object detection",
            "Visual relationships"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection",
            "Visual Grounding"
        ]
    }
]