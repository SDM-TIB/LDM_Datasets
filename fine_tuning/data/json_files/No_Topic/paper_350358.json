[
    {
        "dcterms:creator": [
            "F. De La Torre",
            "J. Hodgins",
            "A. Bargteil",
            "X. Martin",
            "J. Macey",
            "A. Collado",
            "P. Beltran"
        ],
        "dcterms:description": "The EgoProceL dataset consists of 62 hours of egocentric videos captured by 130 subjects performing 16 tasks, providing a clear view of the actions involved in various procedures.",
        "dcterms:title": "EgoProceL",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://sid2697.github.io/egoprocel/",
        "dcat:theme": [
            "Procedure Learning",
            "Egocentric Video Analysis"
        ],
        "dcat:keyword": [
            "Egocentric videos",
            "Procedure learning",
            "Task performance",
            "Key-step annotations"
        ],
        "dcat:landingPage": "http://cvit.iiit.ac.in/research/projects/cvit-projects/egoprocel",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Procedure Learning",
            "Action Recognition",
            "Understanding Hand-Object Interaction"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Li",
            "M. Liu",
            "J. M. Rehg"
        ],
        "dcterms:description": "EGTEA Gaze+ is a dataset that focuses on joint learning of gaze and actions in first-person videos, providing dense activity annotations for various tasks.",
        "dcterms:title": "EGTEA Gaze+",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Gaze Estimation"
        ],
        "dcat:keyword": [
            "First-person video",
            "Gaze tracking",
            "Activity recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Gaze and Action Learning"
        ]
    },
    {
        "dcterms:creator": [
            "F. Ragusa",
            "A. Furnari",
            "S. Livatino",
            "G. M. Farinella"
        ],
        "dcterms:description": "The MECCANO dataset is designed to understand human-object interactions from egocentric videos, focusing on tasks that involve assembling and manipulating objects.",
        "dcterms:title": "MECCANO",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human-Object Interaction",
            "Egocentric Video Analysis"
        ],
        "dcat:keyword": [
            "Human-object interaction",
            "Egocentric videos",
            "Assembly tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Understanding Human-Object Interactions"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Jang",
            "B. Sullivan",
            "C. Ludwig",
            "I. Gilchrist",
            "D. Damen",
            "W. Mayol-Cuevas"
        ],
        "dcterms:description": "EPIC-Tents is an egocentric video dataset specifically created for the task of camping tent assembly, providing a rich set of videos for procedure learning.",
        "dcterms:title": "EPIC-Tents",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Procedure Learning",
            "Egocentric Video Analysis"
        ],
        "dcat:keyword": [
            "Camping tent assembly",
            "Egocentric videos",
            "Instructional videos"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Procedure Learning"
        ]
    },
    {
        "dcterms:creator": [
            "E. Elhamifar",
            "D. Huynh"
        ],
        "dcterms:description": "ProceL is a dataset for unsupervised procedure learning, focusing on instructional videos that demonstrate various tasks.",
        "dcterms:title": "ProceL",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Procedure Learning",
            "Instructional Videos"
        ],
        "dcat:keyword": [
            "Unsupervised learning",
            "Instructional videos",
            "Procedure learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Procedure Learning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Zhukov",
            "J. B. Alayrac",
            "R. G. Cinbis",
            "D. Fouhey",
            "I. Laptev",
            "J. Sivic"
        ],
        "dcterms:description": "CrossTask is a dataset that enables weakly supervised learning from instructional videos across different tasks, focusing on the relationships between actions.",
        "dcterms:title": "CrossTask",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Weakly Supervised Learning",
            "Instructional Videos"
        ],
        "dcat:keyword": [
            "Weakly supervised learning",
            "Instructional videos",
            "Cross-task learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Weakly Supervised Learning"
        ]
    },
    {
        "dcterms:creator": [
            "H. Kuehne",
            "A. B. Arslan",
            "T. Serre"
        ],
        "dcterms:description": "The Breakfast dataset is designed to recover the syntax and semantics of goal-directed human activities, providing a rich set of videos for action recognition.",
        "dcterms:title": "Breakfast",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Human Activities"
        ],
        "dcat:keyword": [
            "Goal-directed activities",
            "Action recognition",
            "Human activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "J. B. Alayrac",
            "P. Bojanowski",
            "N. Agrawal",
            "I. Laptev",
            "J. Sivic",
            "S. Lacoste-Julien"
        ],
        "dcterms:description": "The Inria dataset focuses on unsupervised learning from narrated instruction videos, providing a unique perspective on learning from visual demonstrations.",
        "dcterms:title": "Inria",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Unsupervised Learning",
            "Instructional Videos"
        ],
        "dcat:keyword": [
            "Unsupervised learning",
            "Narrated videos",
            "Instructional videos"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Unsupervised Learning"
        ]
    }
]