To extract datasets from the research paper titled "CIAO! A Contrastive Adaptation Mechanism for Non-Universal Facial Expression Recognition" by Pablo Barros and Alessandra Sciutti, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets mentioned. The abstract indicates that the authors evaluated their method on **six different datasets** with unique affective representations, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on the **experiments section**, particularly where the authors describe their evaluation setup. In this section, they explicitly mention the datasets used for their experiments, which are:

1. **AffectNet**: This dataset contains over 500,000 manually annotated images, each labeled with one of eight affective states and continuous arousal and valence values.

2. **FER (Facial Expression Recognition)** and **FER+**: These datasets contain around 31,000 gray-scaled face images. FER uses a categorical labeling scheme, while FER+ introduces a new labeling scheme with multiple tags per image.

3. **JAFFE (Japanese Female Facial Expression)**: This dataset consists of 210 images of 10 Japanese actresses performing various facial expressions, with each actress providing three images per expression.

4. **EmoReact**: This dataset includes 1,200 short videos of children reacting to various stimuli, annotated for the presence of eight affective states.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset. The citations I need to extract are:

- For **AffectNet**:
  > Mollahosseini, A., Hasani, B., & Mahoor, M. H. (2017). *AffectNet: A database for facial expression, valence, and arousal computing in the wild*. IEEE Transactions on Affective Computing, 10(1), 18-31.

- For **FER**:
  > Barsoum, E., Zhang, C., Ferrer, C. C., & Zhang, Z. (2016). *Training deep networks for facial expression recognition with crowd-sourced label distribution*. In Proceedings of the 18th ACM International Conference on Multimodal Interaction (pp. 279-283).

- For **JAFFE**:
  > Lyons, M. J., Kamachi, M., & Gyoba, J. (2020). *Coding facial expressions with Gabor wavelets*. arXiv preprint arXiv:2009.05938.

- For **EmoReact**:
  > Nojavanasghari, B., Baltru≈°aitis, T., Hughes, C. E., & Morency, L.-P. (2016). *EmoReact: A multimodal approach and dataset for recognizing emotional responses in children*. In Proceedings of the 18th ACM International Conference on Multimodal Interaction (pp. 137-144).

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review.