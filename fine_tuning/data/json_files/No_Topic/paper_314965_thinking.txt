To extract datasets from the research paper titled "VOXCELEB ENRICHMENT FOR AGE AND GENDER RECOGNITION" by Khaled Hechmi et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract** and **introduction** sections to get an overview of the datasets referenced. The abstract mentions the VoxCeleb datasets, which indicates that they are central to the research. The introduction further elaborates on the VoxCeleb corpora, suggesting that they are used for speaker recognition and related tasks.

Next, I will focus on **section 2 (VoxCeleb Metadata)**, where the authors provide detailed descriptions of the datasets. Here, I will find specific mentions of the two main datasets:

1. **VoxCeleb 1**: This dataset consists of 153,516 utterances from 1,251 unique speakers, extracted from 22,496 different YouTube videos. It is crucial to note the number of speakers and utterances, as well as the source of the data.

2. **VoxCeleb 2**: This dataset is larger, containing 6,112 celebrities, 150,480 videos, and over 1 million recordings. It also includes metadata such as gender and nationality information for each speaker.

I will also check the **References section** to find the full citations for these datasets. The citations are essential for proper attribution and to guide readers to the original sources.

The citations for the VoxCeleb datasets are as follows:

- For **VoxCeleb 1**, the citation is:
  > A. Nagrani, J. S. Chung, and A. Zisserman. "Voxceleb: a large-scale speaker identification dataset." In INTERSPEECH, 2017.

- For **VoxCeleb 2**, the citation is:
  > J. S. Chung, A. Nagrani, and A. Zisserman. "Voxceleb2: Deep speaker recognition." In INTERSPEECH, 2018.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is clearly described and properly cited. This will allow for a structured output that can be used for further analysis or integration into other systems.