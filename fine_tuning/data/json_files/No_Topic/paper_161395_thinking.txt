To extract datasets from the research paper titled "Depth Pooling Based Large-scale 3D Action Recognition with Convolutional Neural Networks" by Pichao Wang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental sections** of the paper. The abstract mentions that the proposed method achieved state-of-the-art results on three large datasets, which indicates that there are datasets to extract.

Next, I will focus on the **experiments section** where the authors typically describe the datasets used for their evaluations. In this case, I will look for a section that explicitly lists the datasets, often labeled as "Datasets" or "Experimental Setup."

Upon reviewing the paper, I find that the authors mention three datasets:

1. **ChaLearn LAP IsoGD Dataset**: This dataset is derived from the ChaLearn Gesture Dataset (CGD) and includes 47,933 RGB-D depth sequences representing 249 gestures performed by 21 different individuals.

2. **ChaLearn LAP ConGD Dataset**: Also derived from the ChaLearn Gesture Dataset, this dataset contains 47,933 RGB-D gesture instances in 22,535 RGB-D gesture videos, with the same number of gestures and subjects as the IsoGD dataset.

3. **NTU RGB+D Dataset**: This dataset is noted as the largest action recognition dataset, containing over 56,000 sequences and 4 million frames, with 60 actions performed by 40 subjects.

Next, I will check the **References section** of the paper to find the full citations for these datasets. The citations are crucial for proper attribution and to allow others to access the datasets.

The citations I find are:

- For the **ChaLearn LAP IsoGD Dataset**:
  > I. Guyon, V. Athitsos, P. Jangyodsuk, and H. J. Escalante, “The ChaLearn Gesture Dataset (CGD 2011),” *Machine Vision and Applications*, vol. 25, no. 8, pp. 1929–1951, 2014.

- For the **ChaLearn LAP ConGD Dataset**:
  > I. Guyon, V. Athitsos, P. Jangyodsuk, and H. J. Escalante, “Chalearn looking at people rgb-d isolated and continuous datasets for gesture recognition,” in *Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, 2016, pp. 1–9.

- For the **NTU RGB+D Dataset**:
  > A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “NTU RGB+D: A large scale dataset for 3D human activity analysis,” in *Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2016.

Now that I have identified the datasets and their full citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured all necessary details for each dataset mentioned in the paper.