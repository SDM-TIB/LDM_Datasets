[
    {
        "dcterms:creator": [
            "Pan Lu",
            "Swaroop Mishra",
            "Tanglin Xia",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Song-Chun Zhu",
            "Oyvind Tafjord",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "dcterms:description": "ScienceQA is a large-scale multimodal dataset for science question answering, covering various domains, including 3 subjects, 26 topics, 127 categories, and 379 skills. It consists of text-only and text-image examples in three splits: train, val, and test.",
        "dcterms:title": "ScienceQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Science questions",
            "Multimodal dataset",
            "Text-image pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "Alphaca-52k contains 52k text-only instruction-following data generated by GPT-3.5.",
        "dcterms:title": "Alphaca-52k",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Instruction Following",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction data",
            "Text-only dataset",
            "GPT-3.5"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Haotian Liu",
            "Chunyuan Li",
            "Qingyang Wu",
            "Yong Jae Lee"
        ],
        "dcterms:description": "LLaVA-158k is a large-scale text-image instruction-following dataset, where the answers are automatically generated by GPT-4.",
        "dcterms:title": "LLaVA-158k",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Following",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Text-image pairs",
            "Instruction data",
            "GPT-4"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Instruction Following"
        ]
    }
]