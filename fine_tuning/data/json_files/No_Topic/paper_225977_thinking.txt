To extract the datasets mentioned in the research paper titled "Let’s Ask Again: Refine Network for Automatic Question Generation" by Preksha Nema et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental sections** of the paper. The abstract mentions that the authors evaluate their method on three datasets: **SQuAD, HOTPOT-QA, and DROP**. This indicates that these datasets are crucial for understanding the experiments conducted.

Next, I will look for a dedicated **section on datasets** or any mention of datasets in the **experiments section**. In the **Experimental Details section**, the authors provide descriptions of the datasets used:

1. **SQuAD (Stanford Question Answering Dataset)**: This dataset contains 100,000 question-answer pairs derived from 536 Wikipedia articles, where the answers are spans in the passages. The authors mention using both sentence-level and passage-level splits for their experiments.

2. **HOTPOT-QA**: This is a multi-document, multi-hop question answering dataset that includes supporting facts that lead to the answers. The authors note that they concatenate these supporting facts to form the passage for their experiments.

3. **DROP (Discrete Reasoning Over Paragraphs)**: This dataset consists of 96,000 questions requiring discrete reasoning, such as addition or counting, to arrive at the answers. The authors specify that they use a portion of the original training data for validation.

Now, I will check the **References section** to find the full citations for these datasets:

- For **SQuAD**, the citation is:
  > Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. *SQuAD: 100,000+ Questions for Machine Comprehension of Text*. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2383–2392, 2016.

- For **HOTPOT-QA**, the citation is:
  > Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. *HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering*. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.

- For **DROP**, the citation is:
  > Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. *DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs*. In Proceedings of NAACL, 2019.

After gathering all this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research paper, ready for further processing or review.