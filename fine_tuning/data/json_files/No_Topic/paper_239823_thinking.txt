To extract datasets from the research paper titled "Word2Vec: Optimal hyper-parameters and their impact on NLP downstream tasks" by Tosin P. Adewumi et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** to identify any datasets mentioned. The abstract discusses the evaluation of various hyper-parameter combinations and mentions intrinsic and extrinsic evaluations, which suggests that datasets are involved.

Next, I will examine the **introduction** section for any references to specific datasets. The introduction provides context about the Word2Vec model and its applications, but I need to look for explicit mentions of datasets.

Moving on to **section 3 (Materials and methods)**, I will focus on the subsection titled **3.1 Datasets**. Here, the authors list the datasets used for their experiments:

1. **2019 English Wiki News Abstract**: This dataset is approximately 15MB in size and is used for training word embeddings.
2. **2019 English Simple Wiki Articles**: This dataset is about 711MB and is also used for training word embeddings.
3. **Billion Word Dataset**: This dataset is significantly larger at 3.9GB and is used for training word embeddings.
4. **IMDb Dataset**: This dataset consists of movie reviews and is used for sentiment analysis, containing 25,000 sentences with balanced positive and negative sentiments.
5. **Groningen Meaning Bank (GMB)**: This dataset contains 47,959 sentence samples and is used for named entity recognition (NER).

In the same section, the authors also mention two evaluation datasets:

- **Google analogy test set**: Used for intrinsic evaluations.
- **WordSimilarity-353**: Another dataset used for intrinsic evaluations.

Now, I will check the **References section** to find the full citations for each dataset mentioned:

- For the **2019 English Wiki News Abstract**, the citation is:
  > Wikipedia. Wiki news abstract. 2019.

- For the **2019 English Simple Wiki Articles**, the citation is:
  > Wikipedia. Simple wiki articles. 2019.

- For the **Billion Word Dataset**, the citation is:
  > Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. *One billion word benchmark for measuring progress in statistical language modeling*. Technical report, Google, 2013.

- For the **IMDb Dataset**, the citation is:
  > Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. *Learning word vectors for sentiment analysis*. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142–150. Association for Computational Linguistics, 2011.

- For the **Groningen Meaning Bank (GMB)**, the citation is:
  > Johan Bos, Valerio Basile, Kilian Evang, Noortje J Venhuizen, and Johannes Bjerva. *The groningen meaning bank*. In Handbook of linguistic annotation, pages 463–496. Springer, 2017.

- For the **Google analogy test set**, the citation is:
  > Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. *Efficient estimation of word representations in vector space*. arXiv preprint arXiv:1301.3781, 2013.

- For the **WordSimilarity-353**, the citation is:
  > Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. *Placing search in context: The concept revisited*. ACM Transactions on information systems, 20(1):116–131, 2002.

Now that I have gathered all the necessary information and citations, I will compile the dataset entries into a structured format for further processing or review.