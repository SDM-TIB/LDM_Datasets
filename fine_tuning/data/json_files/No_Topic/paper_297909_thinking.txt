To extract datasets from the research paper titled "T-EMDE: Sketching-based global similarity for cross-modal retrieval" by Mikolaj Wieczorek et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract provides a brief overview of the research, and while it may not explicitly list datasets, it sets the context for the experiments.

Next, I will focus on **section 4 (Experiments)**, where the authors typically describe the datasets used for evaluation. In this section, they mention using two datasets: **MSCOCO** and **Flickr30K**. I will look for detailed descriptions of these datasets, including their characteristics and how they were utilized in the experiments.

1. **MSCOCO Dataset**: The authors describe it as a large-scale object detection, segmentation, and captioning dataset containing 123,287 photos, each annotated with 5 captions. They specify that they used a popular cross-modal retrieval split known as the "Karpathy" split, which divides the dataset into training, validation, and testing sets.

2. **Flickr30K Dataset**: This dataset consists of 31,783 images, each with 5 corresponding captions. The authors note that the captions in Flickr30K are generally longer and more detailed compared to those in MSCOCO.

After identifying the datasets, I will refer to the **References section** to find the full citations for these datasets. The citations are crucial for proper attribution and to allow others to access the datasets.

- For the **MSCOCO dataset**, the citation is:
  > Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. *Microsoft COCO: Common Objects in Context*. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages 740–755, 2014. https://doi.org/10.1007/978-3-319-10602-1_48

- For the **Flickr30K dataset**, the citation is:
  > Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. *From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions*. Trans. Assoc. Comput. Linguistics 2, pages 67–78, 2014. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/229

Now that I have gathered the necessary information about the datasets and their citations, I will compile this into a structured format for further processing or review.