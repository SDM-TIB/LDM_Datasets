[
    {
        "dcterms:creator": [
            "A. N. Burnetas",
            "M. N. Katehakis"
        ],
        "dcterms:description": "The UCB-MDP policy is a classical upper confidence bound policy for Markov decision processes that estimates the values of available actions based on available data, inflated by a confidence interval derived from the Kullback-Leibler divergence.",
        "dcterms:title": "UCB-MDP policy",
        "dcterms:issued": "1997",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Markov Decision Processes"
        ],
        "dcat:keyword": [
            "UCB policy",
            "Markov decision processes",
            "Upper confidence bound"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sequential decision making"
        ]
    },
    {
        "dcterms:creator": [
            "J. Honda",
            "A. Takemura"
        ],
        "dcterms:description": "The DMED algorithm is an asymptotically optimal bandit algorithm for bounded support models, which estimates the optimal rates at which actions should be taken in the context of the multi-armed bandit problem.",
        "dcterms:title": "DMED algorithm",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-armed Bandit Problem",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Asymptotically optimal",
            "Bandit algorithm",
            "Bounded support"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Action selection",
            "Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Abbeel",
            "A. Y. Ng"
        ],
        "dcterms:description": "Thompson Sampling (PS-MDP) is a method based on posterior sampling for reinforcement learning, where estimates for unknown parameters are generated randomly according to the posterior distribution based on current data.",
        "dcterms:title": "Thompson Sampling (PS-MDP)",
        "dcterms:issued": "2004",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Bayesian Methods"
        ],
        "dcat:keyword": [
            "Thompson Sampling",
            "Posterior sampling",
            "Probability matching"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sequential decision making",
            "Parameter estimation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Auer",
            "N. Cesa-Bianchi",
            "P. Fischer"
        ],
        "dcterms:description": "The Kullback-Leibler divergence is a measure used in the analysis of the multi-armed bandit problem, providing a way to quantify the difference between two probability distributions.",
        "dcterms:title": "Kullback-Leibler divergence",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistics",
            "Information Theory"
        ],
        "dcat:keyword": [
            "Divergence",
            "Probability distributions",
            "Statistical analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Sutton",
            "A. Barto"
        ],
        "dcterms:description": "Reinforcement Learning (RL) techniques encompass a variety of methods designed for sequential decision making, particularly in environments with unknown parameters.",
        "dcterms:title": "Reinforcement Learning (RL) techniques",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "dcat:keyword": [
            "Reinforcement Learning",
            "Sequential decision making",
            "Learning algorithms"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Decision making",
            "Learning"
        ]
    },
    {
        "dcterms:creator": [
            "J. Gittins"
        ],
        "dcterms:description": "The Multi-armed Bandit Problem is a classic problem in probability theory and decision theory, focusing on the trade-off between exploration and exploitation in sequential decision making.",
        "dcterms:title": "Multi-armed Bandit Problem",
        "dcterms:issued": "1979",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Decision Theory",
            "Probability Theory"
        ],
        "dcat:keyword": [
            "Bandit problem",
            "Exploration",
            "Exploitation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Decision making",
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "W. Cowan",
            "M. N. Katehakis"
        ],
        "dcterms:description": "Asymptotically Optimal policies are policies that achieve the lower bound of logarithmic asymptotic growth of regret in reinforcement learning contexts.",
        "dcterms:title": "Asymptotically Optimal policies",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "10.1017/S0269964818000529",
        "dcat:theme": [
            "Reinforcement Learning",
            "Optimal Control"
        ],
        "dcat:keyword": [
            "Optimal policies",
            "Asymptotic analysis",
            "Regret minimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy optimization"
        ]
    },
    {
        "dcterms:creator": [
            "P. Auer",
            "N. Cesa-Bianchi",
            "P. Fischer"
        ],
        "dcterms:description": "The Optimal Policy Under Known Parameters refers to a scenario in Markov decision processes where all elements of the model are known, allowing for the computation of optimal policies.",
        "dcterms:title": "Optimal Policy Under Known Parameters",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Markov Decision Processes",
            "Optimal Control"
        ],
        "dcat:keyword": [
            "Optimal policy",
            "Complete information",
            "Markov processes"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy computation"
        ]
    }
]