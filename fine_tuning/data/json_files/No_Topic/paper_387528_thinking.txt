To extract datasets from the research paper titled "Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals" by Jinhan Wang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental details sections** to identify any datasets mentioned. The abstract indicates that the authors achieved an F1-score of 0.7349 on the **DAIC-WoZ dataset**, which suggests that this dataset is central to their research.

Next, I will examine **section 3 (Experimental Details)**, particularly **subsection 3.1 (Dataset and Input Features)**, where the authors provide a detailed description of the dataset used. Here, they mention the **DAIC-WoZ database**, describing it as a collection of audio-visual interviews featuring 189 participants who underwent psychological distress evaluations. This section confirms that the DAIC-WoZ dataset is indeed the primary dataset used in their experiments.

In this subsection, the authors specify that the dataset contains 107 speakers for training and 35 for evaluation, and they detail the preprocessing steps taken with the audio data. This information is crucial for understanding the dataset's structure and application.

Now, I will look at the **References section** to find the full citation for the DAIC-WoZ dataset. The citation provided in the paper is:

- For the **DAIC-WoZ dataset**, the citation is:
  > Valstar, M., Gratch, J., Ringeval, F., et al. *AVEC 2016: Depression, mood, and emotion recognition workshop and challenge*. In Proceedings of the 6th international workshop on AVEC, 2016, pp. 3â€“10.

Since this is the only dataset explicitly mentioned in the paper, I will compile the information into a structured format.

Finally, I will ensure that the dataset entry includes all relevant details, such as the dataset name, description, and citation, ready for further processing or review.