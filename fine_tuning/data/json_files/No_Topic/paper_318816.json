[
    {
        "dcterms:creator": [
            "C. Shi",
            "C. Shen"
        ],
        "dcterms:description": "A framework for federated multi-armed bandits that allows multiple heterogeneous agents to learn local models while the principal learns a global model.",
        "dcterms:title": "Federated MAB (FMAB)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Federated Learning",
            "Multi-Armed Bandits",
            "Heterogeneous Agents"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation",
            "Best Arm Identification"
        ]
    },
    {
        "dcterms:creator": [
            "P. Auer",
            "N. Cesa-Bianchi",
            "P. Fischer"
        ],
        "dcterms:description": "A theoretical analysis of the multi-armed bandit problem providing finite-time bounds.",
        "dcterms:title": "α-UCB Algorithm",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "UCB",
            "Multi-Armed Bandits",
            "Theoretical Analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation",
            "Regret Minimization"
        ]
    },
    {
        "dcterms:creator": [
            "S. Agrawal",
            "N. Goyal"
        ],
        "dcterms:description": "An analysis of Thompson Sampling for the multi-armed bandit problem.",
        "dcterms:title": "Thompson Sampling",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Thompson Sampling",
            "Multi-Armed Bandits",
            "Bayesian Methods"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation",
            "Regret Minimization"
        ]
    },
    {
        "dcterms:creator": [
            "A. Garivier",
            "O. Cappé"
        ],
        "dcterms:description": "The KL-UCB algorithm for bounded stochastic bandits and its applications.",
        "dcterms:title": "KL-UCB Algorithm",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "KL-UCB",
            "Multi-Armed Bandits",
            "Theoretical Analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation",
            "Regret Minimization"
        ]
    },
    {
        "dcterms:creator": [
            "S. Magureanu",
            "R. Combes",
            "A. Proutiere"
        ],
        "dcterms:description": "Lipschitz bandits and their regret lower bounds and optimal algorithms.",
        "dcterms:title": "Lipschitz Bandits",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Lipschitz Bandits",
            "Regret Lower Bound",
            "Optimal Algorithms"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation",
            "Regret Minimization"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Mansour",
            "A. Slivkins",
            "V. Syrgkanis"
        ],
        "dcterms:description": "Bayesian incentive-compatible bandit exploration.",
        "dcterms:title": "Bayesian Incentive-Compatible Bandit Exploration",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Bayesian Methods",
            "Incentive-Compatible",
            "Multi-Armed Bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation",
            "Regret Minimization"
        ]
    },
    {
        "dcterms:creator": [
            "K. Jamieson",
            "R. Nowak"
        ],
        "dcterms:description": "Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting.",
        "dcterms:title": "Best Arm Identification Algorithms",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Best Arm Identification",
            "Multi-Armed Bandits",
            "Fixed Confidence"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Best Arm Identification"
        ]
    },
    {
        "dcterms:creator": [
            "B. Chen",
            "P. Frazier",
            "D. Kempe"
        ],
        "dcterms:description": "Incentivizing exploration by heterogeneous users.",
        "dcterms:title": "Incentivizing Exploration with Heterogeneous Users",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Incentivizing Exploration",
            "Heterogeneous Users",
            "Multi-Armed Bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration-Exploitation"
        ]
    }
]