[
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "A benchmark dataset for question answering research that includes a diverse set of questions and answers derived from real user queries.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question answering",
            "Benchmark dataset",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large-scale dataset designed for reading comprehension, containing questions and answers derived from trivia.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Trivia questions",
            "Reading comprehension",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "The development dataset for Natural Questions, used for evaluating question answering systems.",
        "dcterms:title": "NQ dev dataset",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Development dataset",
            "Question answering",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "Contexts retrieved from the Natural Questions and TriviaQA datasets, used for evaluating question answering systems.",
        "dcterms:title": "DPR retrieved contexts from NQ/TriviaQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Retrieved contexts",
            "Question answering",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]