To extract datasets from the research paper titled "Managing sparsity, time, and quality of inference in topic models" by Khoat Than and Tu Bao Ho, I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract, introduction, and experimental sections** of the paper. The abstract provides a brief overview of the research and may mention datasets used for experiments. The introduction often elaborates on the context and significance of the datasets, while the experimental sections typically detail the datasets used for validation or testing.

Next, I will specifically look for any sections that explicitly mention datasets. In this paper, the **Empirical Evaluation (Section 4)** is likely to contain information about the datasets used for experiments. I will focus on any tables or figures that summarize the datasets, as they often provide key details such as the names of the datasets, their sizes, and other relevant characteristics.

Upon reviewing Section 4, I find a table (Table 1) that lists several datasets used in the experiments. The datasets mentioned include:

1. **AP (Associated Press)**: This dataset has a training size of 2021 documents and a testing size of 10473 documents, with 225 terms and 0 classes.
2. **KOS (Knowledge Organization System)**: This dataset has a training size of 3087 documents and a testing size of 6906 documents, with 343 terms and 0 classes.
3. **NIPS (Neural Information Processing Systems)**: This dataset has a training size of 1350 documents and a testing size of 12419 documents, with 150 terms and 0 classes.
4. **Grolier**: This dataset has a training size of 23044 documents and a testing size of 15276 documents, with 6718 terms and 0 classes.
5. **Enron**: This dataset has a training size of 35875 documents and a testing size of 28102 documents, with 3986 terms and 20 classes.
6. **20Newsgroups**: This dataset has a training size of 15935 documents and a testing size of 62061 documents, with 3993 terms and 2 classes.
7. **Emailspam**: This dataset has a training size of 3461 documents and a testing size of 38729 documents, with 866 terms and 0 classes.

Now, I will check the **References section** to find full citations for these datasets. However, the paper does not provide explicit citations for the datasets used, which is common in empirical research where datasets are well-known or publicly available.

For the datasets mentioned, I can provide general references based on common knowledge:

- **AP (Associated Press)**: The dataset is often used in text classification tasks and can be referenced as:
  > Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research*, 3, 993-1022.

- **KOS**: This dataset is typically derived from knowledge organization systems and may not have a specific citation.

- **NIPS**: The NIPS dataset is commonly used in machine learning research and can be referenced as:
  > Blei, D. M., & Lafferty, J. D. (2007). A correlated topic model of science. *The Annals of Applied Statistics*, 1(1), 17-35.

- **Grolier**: This dataset is often used in educational contexts and may not have a specific citation.

- **Enron**: The Enron email dataset is widely used and can be referenced as:
  > Klimt, B., & Yang, Y. (2004). The Enron Corpus: A New Dataset for Email Classification Research. *Proceedings of the 15th European Conference on Machine Learning (ECML)*.

- **20Newsgroups**: This dataset is well-known in text classification and can be referenced as:
  > Lang, K. (1995). Newsweeder: Learning to filter netnews. *Proceedings of the 1995 International Conference on Machine Learning*.

- **Emailspam**: This dataset is commonly used for spam detection and may not have a specific citation.

After gathering this information, I will compile the dataset entries, ensuring to include the relevant citations where applicable. This will provide a comprehensive overview of the datasets used in the research paper.