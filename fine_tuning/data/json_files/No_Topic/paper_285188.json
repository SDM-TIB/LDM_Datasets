[
    {
        "dcterms:creator": [
            "G. Watson"
        ],
        "dcterms:description": "The Watson-Nadaraya estimator implements a data-dependent regression model, which is foundational in the study of attention mechanisms.",
        "dcterms:title": "Watson-Nadaraya estimator",
        "dcterms:issued": "1964",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistics",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Regression",
            "Attention",
            "Data-dependent model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Graves",
            "G. Wayne",
            "I. Danihelka"
        ],
        "dcterms:description": "Neural Turing Machines are a type of neural network that can read from and write to an external memory matrix, enhancing their ability to learn complex tasks.",
        "dcterms:title": "Neural Turing Machines",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1410.5401",
        "dcat:theme": [
            "Neural Networks",
            "Memory Augmentation"
        ],
        "dcat:keyword": [
            "Neural Networks",
            "Memory",
            "Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Devlin",
            "M.-W. Chang",
            "K. Lee",
            "K. Toutanova"
        ],
        "dcterms:description": "BERT is a pre-trained deep learning model designed for natural language understanding, utilizing bidirectional transformers to capture context from both directions.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1810.04805",
        "dcat:theme": [
            "Natural Language Processing",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Transformers",
            "Language Understanding",
            "Pre-training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Question Answering",
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "M. Dehghani",
            "S. Gouws",
            "O. Vinyals",
            "J. Uszkoreit",
            "Ł. Kaiser"
        ],
        "dcterms:description": "Universal Transformers extend the original transformer architecture by allowing for dynamic computation steps, enhancing their ability to model complex sequences.",
        "dcterms:title": "Universal Transformers",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1807.03819",
        "dcat:theme": [
            "Natural Language Processing",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Transformers",
            "Dynamic Computation",
            "Sequence Modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Lee",
            "Y. Lee",
            "J. Kim",
            "A. R. Kosiorek",
            "S. Choi",
            "Y. W. Teh"
        ],
        "dcterms:description": "Set Transformer is a framework designed for attention-based permutation-invariant neural networks, enabling effective processing of sets of varying sizes.",
        "dcterms:title": "Set Transformer",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1810.00825",
        "dcat:theme": [
            "Machine Learning",
            "Neural Networks"
        ],
        "dcat:keyword": [
            "Attention",
            "Permutation Invariance",
            "Set Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "B. Baker",
            "I. Kanitscheider",
            "T. Markov",
            "Y. Wu",
            "G. Powell",
            "B. McGrew",
            "I. Mordatch"
        ],
        "dcterms:description": "Multi-Agent Autocurricula explores how multiple agents can learn to use tools through interactions and competition, leading to emergent behaviors.",
        "dcterms:title": "Multi-Agent Autocurricula",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1909.07528",
        "dcat:theme": [
            "Reinforcement Learning",
            "Multi-Agent Systems"
        ],
        "dcat:keyword": [
            "Emergent Behavior",
            "Tool Use",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Berger"
        ],
        "dcterms:description": "Error-Correcting Output Codes is a method for improving the performance of multi-class classifiers by transforming them into multiple binary classifiers.",
        "dcterms:title": "Error-Correcting Output Codes",
        "dcterms:issued": "1999",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Text Classification"
        ],
        "dcat:keyword": [
            "Error Correction",
            "Multi-class Classification",
            "Binary Classifiers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Kassner",
            "H. Schütze"
        ],
        "dcterms:description": "Negated Sentences dataset explores how pretrained language models handle negation, revealing insights into their robustness and prediction consistency.",
        "dcterms:title": "Negated Sentences",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Negation",
            "Language Models",
            "Robustness"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]