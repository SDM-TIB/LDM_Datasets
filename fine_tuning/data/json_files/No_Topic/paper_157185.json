[
    {
        "dcterms:creator": [
            "C. Busso",
            "M. Bulut",
            "C. Lee",
            "A. Kazemzadeh",
            "E. Mower",
            "S. Kim",
            "J. Chang",
            "S. Lee",
            "S. Narayanan"
        ],
        "dcterms:description": "The IEMOCAP database comprises 12 hours of audiovisual data along with motion capture recordings of face and text transcriptions. It includes 151 conversations labeled as either improvised (spontaneous) or scripted, with multiple annotators providing emotion labels such as joy, sadness, anger, frustration, and excitement.",
        "dcterms:title": "IEMOCAP database",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Speech Processing"
        ],
        "dcat:keyword": [
            "Emotion recognition",
            "Spontaneous speech",
            "Audiovisual data",
            "Motion capture",
            "Speech emotion dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video",
        "mls:task": [
            "Emotion Recognition",
            "Spontaneity Detection"
        ]
    }
]