[
    {
        "dcterms:creator": [
            "Daniel M Ziegler",
            "Nisan Stiennon",
            "Jeffrey Wu",
            "Tom B Brown",
            "Alec Radford",
            "Dario Amodei",
            "Paul Christiano",
            "Geoffrey Irving"
        ],
        "dcterms:description": "A dataset used for fine-tuning language models based on human preferences, specifically for summarization tasks.",
        "dcterms:title": "Reddit TL;DR",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1909.08593",
        "dcat:theme": [
            "Natural Language Processing",
            "Summarization"
        ],
        "dcat:keyword": [
            "Reddit",
            "Summarization",
            "Language Models",
            "Human Preferences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan"
        ],
        "dcterms:description": "A dataset designed to train models to be helpful and harmless through reinforcement learning from human feedback.",
        "dcterms:title": "Anthropic Helpfulness",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Natural Language Processing",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Helpfulness",
            "Reinforcement Learning",
            "Human Feedback",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Alignment",
            "Human Feedback Training"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan"
        ],
        "dcterms:description": "A dataset focused on identifying and mitigating harmful responses generated by language models.",
        "dcterms:title": "Harmfulness",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Natural Language Processing",
            "Safety in AI"
        ],
        "dcat:keyword": [
            "Harmfulness",
            "Safety",
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Safety",
            "Harmfulness Detection"
        ]
    }
]