[
    {
        "dcterms:creator": [
            "T.-Y. Lin",
            "M. Maire",
            "S. J. Belongie",
            "J. Hays",
            "P. Perona",
            "D. Ramanan",
            "P. Doll√°r",
            "C. L. Zitnick"
        ],
        "dcterms:description": "The COCO dataset contains images with annotations for object detection, segmentation, and captioning, providing a rich context for visual understanding.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Object detection",
            "Image segmentation",
            "Image captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "B. A. Plummer",
            "L. Wang",
            "C. M. Cervantes",
            "J. C. Caicedo",
            "J. Hockenmaier",
            "S. Lazebnik"
        ],
        "dcterms:description": "Flickr30k contains images with corresponding textual descriptions, enabling the study of image-to-sentence models and region-to-phrase correspondences.",
        "dcterms:title": "Flickr30k",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "10.1109/ICCV.2015.303",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image description",
            "Region-to-phrase correspondence"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "S. Antol",
            "A. Agrawal",
            "J. Lu",
            "M. Mitchell",
            "D. Batra",
            "C. L. Zitnick",
            "D. Parikh"
        ],
        "dcterms:description": "VQAv2 is a dataset for visual question answering that includes images and corresponding questions, designed to evaluate models' understanding of visual content.",
        "dcterms:title": "VQAv2",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Visual question answering",
            "Image understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Singh",
            "V. Natarjan",
            "M. Shah",
            "Y. Jiang",
            "X. Chen",
            "D. Parikh",
            "M. Rohrbach"
        ],
        "dcterms:description": "TextVQA is a dataset that focuses on visual question answering tasks that require reading text within images, enhancing the capabilities of VQA models.",
        "dcterms:title": "TextVQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Text reading",
            "Visual question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "K. Marino",
            "M. Rastegari",
            "A. Farhadi",
            "R. Mottaghi"
        ],
        "dcterms:description": "OK-VQA is a visual question answering benchmark that requires external knowledge to answer questions, pushing the boundaries of VQA capabilities.",
        "dcterms:title": "OK-VQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "External knowledge",
            "Visual question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Gurari",
            "Q. Li",
            "A. J. Stangl",
            "A. Guo",
            "C. Lin",
            "K. Grauman",
            "J. Luo",
            "J. P. Bigham"
        ],
        "dcterms:description": "VizWiz is a dataset designed to answer visual questions posed by blind individuals, focusing on accessibility and inclusivity in visual question answering.",
        "dcterms:title": "VizWiz",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Accessibility",
            "Visual question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]