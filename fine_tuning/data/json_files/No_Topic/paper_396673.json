[
    {
        "dcterms:creator": [
            "Mattia Soldan",
            "Alejandro Pardo",
            "Juan León Alcázar",
            "Fabian Caba",
            "Chen Zhao",
            "Silvio Giancola",
            "Bernard Ghanem"
        ],
        "dcterms:description": "A scalable dataset for language grounding in videos from movie audio descriptions, containing a diverse set of movies making more than 1200 hours of content.",
        "dcterms:title": "Movie Audio Description (MAD)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Movie dataset",
            "Audio descriptions",
            "Language grounding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video, Audio, Text",
        "mls:task": [
            "Language grounding",
            "Multimodal reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Qingqiu Huang",
            "Yu Xiong",
            "Anyi Rao",
            "Jiaze Wang",
            "Dahua Lin"
        ],
        "dcterms:description": "A holistic dataset for movie understanding, containing 1,100 full movies and 60K trailers with annotations for various tasks including scene segmentation, character recognition, and genre prediction.",
        "dcterms:title": "MovieNet",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Movie Understanding",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Movie dataset",
            "Scene segmentation",
            "Character recognition",
            "Genre prediction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Scene segmentation",
            "Character recognition",
            "Genre prediction"
        ]
    },
    {
        "dcterms:creator": [
            "Chao-Yuan Wu",
            "Philipp Krahenbuhl"
        ],
        "dcterms:description": "A benchmark for long-form video understanding containing a total of 9 tasks related to content understanding, metadata prediction, and user engagement.",
        "dcterms:title": "LVU Benchmark",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Long-form Video Understanding",
            "Benchmark"
        ],
        "dcat:keyword": [
            "Video understanding",
            "Content understanding",
            "Metadata prediction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Content understanding",
            "Metadata prediction",
            "User engagement"
        ]
    },
    {
        "dcterms:creator": [
            "Xiaolong Liu",
            "Yao Hu",
            "Song Bai",
            "Fei Ding",
            "Xiang Bai",
            "Philip HS Torr"
        ],
        "dcterms:description": "A benchmark for multi-shot temporal event localization, containing 3,697 videos collected from more than 1000 drama episodes.",
        "dcterms:title": "MUSES Benchmark",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Event Localization",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Event localization",
            "Video dataset",
            "Temporal events"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Temporal event localization"
        ]
    },
    {
        "dcterms:creator": [
            "Dawit Mureja Argaw",
            "Fabian Caba Heilbron",
            "Joon-Young Lee",
            "Markus Woodson",
            "In So Kweon"
        ],
        "dcterms:description": "A dataset and benchmark suite for AI-assisted video editing, focusing on the anatomy of video editing.",
        "dcterms:title": "AVE Benchmark",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Editing",
            "Benchmark"
        ],
        "dcat:keyword": [
            "Video editing",
            "Editing patterns",
            "Cinematography"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Video editing",
            "Editing pattern prediction"
        ]
    },
    {
        "dcterms:creator": [
            "Max Bain",
            "Arsha Nagrani",
            "Andrew Brown",
            "Andrew Zisserman"
        ],
        "dcterms:description": "A dataset for story-based retrieval with contextual embeddings, containing a total of 33,976 scenes and their corresponding textual descriptions.",
        "dcterms:title": "Condensed Movies (CDM)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Story Retrieval",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Scene retrieval",
            "Textual descriptions",
            "Contextual embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Scene description retrieval"
        ]
    }
]