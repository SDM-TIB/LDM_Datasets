[
    {
        "dcterms:creator": [
            "Fabian Caba Heilbron",
            "Victor Escorcia",
            "Bernard Ghanem",
            "Juan Carlos Niebles"
        ],
        "dcterms:description": "A large-scale video benchmark for human activity understanding, providing human captions for specific video scenes depicting useful actions in long videos.",
        "dcterms:title": "ActivityNet",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Analysis",
            "Human Activity Recognition"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Human activity",
            "Action recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Jun Xu",
            "Tao Mei",
            "Ting Yao",
            "Yong Rui"
        ],
        "dcterms:description": "A large video description dataset for bridging video and language, consisting of videos with corresponding textual descriptions.",
        "dcterms:title": "MSR-VTT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Description",
            "Video and Language"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Video description",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Xin Wang",
            "Jiawei Wu",
            "Junkun Chen",
            "Lei Li",
            "Yuan-Fang Wang",
            "William Yang Wang"
        ],
        "dcterms:description": "A large-scale, high-quality multilingual dataset for video-and-language research, facilitating the study of video understanding across different languages.",
        "dcterms:title": "VaTeX",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multilingual Video Understanding",
            "Video and Language"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Multilingual",
            "Video understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Captioning",
            "Cross-lingual Video Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Gemini Team",
            "Rohan Anil",
            "Sebastian Borgeaud",
            "Yonghui Wu",
            "Jean-Baptiste Alayrac",
            "Jiahui Yu",
            "Radu Soricut",
            "Johan Schalkwyk",
            "Andrew M Dai",
            "Anja Hauth"
        ],
        "dcterms:description": "A family of highly capable multimodal models designed for various tasks, including video generation and understanding.",
        "dcterms:title": "Gemini-Pro-Vision-1.0",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2312.11805",
        "dcat:theme": [
            "Multimodal Models",
            "Video Generation"
        ],
        "dcat:keyword": [
            "Multimodal",
            "Video generation",
            "AI models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "1.0",
        "dcterms:format": "Model",
        "mls:task": [
            "Video Generation",
            "Multimodal Understanding"
        ]
    }
]