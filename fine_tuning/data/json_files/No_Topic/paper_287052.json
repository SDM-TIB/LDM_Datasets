[
    {
        "dcterms:creator": [
            "Marc Lanctot",
            "Edward Lockhart",
            "Jean-Baptiste Lespiau",
            "Vinicius Zambaldi",
            "Satyaki Upadhyay",
            "Julien Pérolat",
            "Sriram Srinivasan",
            "Finbarr Timbers",
            "Karl Tuyls",
            "Shayegan Omidshaﬁei",
            "Daniel Hennes",
            "Dustin Morrill",
            "Paul Muller",
            "Timo Ewalds",
            "Ryan Faulkner",
            "János Kramár",
            "Bart De Vylder",
            "Brennan Saeta",
            "James Bradbury",
            "David Ding",
            "Sebastian Borgeaud",
            "Matthew Lai",
            "Julian Schrittwieser",
            "Thomas Anthony",
            "Edward Hughes",
            "Ivo Danihelka",
            "Jonah Ryan-Davis"
        ],
        "dcterms:description": "OpenSpiel is a framework for reinforcement learning in games, providing a collection of environments and algorithm implementations for studying RL in various game settings.",
        "dcterms:title": "OpenSpiel",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1908.09453",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game Theory"
        ],
        "dcat:keyword": [
            "Reinforcement Learning",
            "Games",
            "Multi-Agent Systems"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "H.W. Kuhn",
            "A.W. Tucker"
        ],
        "dcterms:description": "Kuhn is a simple poker variant used for testing algorithms in the OpenSpiel framework.",
        "dcterms:title": "Kuhn",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory"
        ],
        "dcat:keyword": [
            "Poker",
            "Game Theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Kevin Waugh",
            "Dustin Morrill",
            "J. Andrew Bagnell",
            "Michael Bowling"
        ],
        "dcterms:description": "Leduc is a more complex poker variant used for testing algorithms in the OpenSpiel framework.",
        "dcterms:title": "Leduc",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory"
        ],
        "dcat:keyword": [
            "Poker",
            "Game Theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Edward Lockhart",
            "Marc Lanctot",
            "Julien Pérolat",
            "Jean-Baptiste Lespiau",
            "Dustin Morrill",
            "Finbarr Timbers",
            "Karl Tuyls"
        ],
        "dcterms:description": "Heads up No-Limit Texas Holdem is a complex poker variant used for testing algorithms in the OpenSpiel framework.",
        "dcterms:title": "Heads up No-Limit Texas Holdem",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1903.05614",
        "dcat:theme": [
            "Game Theory"
        ],
        "dcat:keyword": [
            "Poker",
            "Game Theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Noam Brown",
            "Adam Lerer",
            "Sam Gross",
            "Tuomas Sandholm"
        ],
        "dcterms:description": "DeepCFR is an algorithm that uses deep learning for counterfactual regret minimization in games.",
        "dcterms:title": "DeepCFR",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game Theory"
        ],
        "dcat:keyword": [
            "Counterfactual Regret Minimization",
            "Deep Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Johannes Heinrich",
            "David Silver"
        ],
        "dcterms:description": "Neural Fictitious Self Play (NFSP) is an algorithm that combines deep reinforcement learning with self-play in imperfect-information games.",
        "dcterms:title": "Neural Fictitious Self Play (NFSP)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1603.01121",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game Theory"
        ],
        "dcat:keyword": [
            "Self-Play",
            "Deep Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Marc Lanctot",
            "Vinicius Zambaldi",
            "Audrunas Gruslys",
            "Angeliki Lazaridou",
            "Karl Tuyls",
            "Julien Perolat",
            "David Silver",
            "Thore Graepel"
        ],
        "dcterms:description": "Policy Space Response Oracles (PSRO) is a framework for multi-agent reinforcement learning that generalizes fictitious play.",
        "dcterms:title": "Policy Space Response Oracles (PSRO)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game Theory"
        ],
        "dcat:keyword": [
            "Multi-Agent Systems",
            "Game Theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Sriram Srinivasan",
            "Marc Lanctot",
            "Vinícius Flores Zambaldi",
            "Julien Pérolat",
            "Karl Tuyls",
            "Rémi Munos",
            "Michael Bowling"
        ],
        "dcterms:description": "Policy Gradient methods are a family of algorithms utilizing the policy gradient theorem from reinforcement learning.",
        "dcterms:title": "Policy Gradient",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1810.09026",
        "dcat:theme": [
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Policy Gradient",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Daniel Hennes",
            "Dustin Morrill",
            "Shayegan Omidshaﬁei",
            "Remi Munos",
            "Julien Perolat",
            "Marc Lanctot",
            "Audrunas Gruslys",
            "Jean-Baptiste Lespiau",
            "Paavo Parmas",
            "Edgar Duenez-Guzman",
            "Karl Tuyls"
        ],
        "dcterms:description": "Neural Replicator Dynamics (NeuRD) is a policy gradient algorithm that improves empirical performance through modifications of vanilla softmax policy gradient.",
        "dcterms:title": "Neural Replicator Dynamics (NeuRD)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Policy Gradient",
            "Game Theory"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Lisha Li",
            "Kevin G. Jamieson",
            "Giulia DeSalvo",
            "Afshin Rostamizadeh",
            "Ameet Talwalkar"
        ],
        "dcterms:description": "Hyperband is a bandit-based hyperparameter search algorithm that utilizes adaptive resource allocation and early stopping criteria.",
        "dcterms:title": "Hyperband",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1603.06560",
        "dcat:theme": [
            "Hyperparameter Optimization"
        ],
        "dcat:keyword": [
            "Hyperparameter Tuning",
            "Optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Audr¯unas Gruslys",
            "Marc Lanctot",
            "Remi Munos",
            "Finbarr Timbers",
            "Martin Schmid",
            "Pérolat Julien",
            "Dustin Morrill",
            "Vinicius Zambaldi",
            "Jean-Baptiste Lespiau",
            "John Schultz",
            "Mohammad Azar",
            "Michael Bowling",
            "Karl Tuyls"
        ],
        "dcterms:description": "Advantage Regret Matching Actor Critic (ARMAC) is an algorithm that improves performance in multi-agent reinforcement learning.",
        "dcterms:title": "Advantage Regret Matching Actor Critic (ARMAC)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Actor-Critic",
            "Multi-Agent Systems"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Paul Muller",
            "Shayegan Omidshaﬁei",
            "Mark Rowland",
            "Karl Tuyls",
            "Julien Pérolat",
            "Siqi Liu",
            "Daniel Hennes",
            "Luke Marris",
            "Marc Lanctot",
            "Edward Hughes",
            "Zhe Wang",
            "Guy Lever",
            "Nicolas Heess",
            "Thore Graepel",
            "Rémi Munos"
        ],
        "dcterms:description": "Generalized Training Approach for Multiagent Learning is a framework for training multi-agent systems.",
        "dcterms:title": "Generalized Training Approach for Multiagent Learning",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1909.12823",
        "dcat:theme": [
            "Multi-Agent Systems",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Multi-Agent Learning",
            "Training Framework"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]