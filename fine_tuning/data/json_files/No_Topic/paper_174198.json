[
    {
        "dcterms:creator": [
            "J. M. Zelle",
            "R. J. Mooney"
        ],
        "dcterms:description": "GEOQUERY contains 880 questions and database queries about US geography. The utterances are compositional, but the language is simple and vocabulary size small. The majority of questions include at most one entity.",
        "dcterms:title": "GEOQUERY",
        "dcterms:issued": "1996",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Parsing",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Geography",
            "Database Queries",
            "Natural Language Questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Parsing",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "S. Reddy",
            "J. Blitzer",
            "J. Hockenmaier",
            "M. Steedman"
        ],
        "dcterms:description": "SPADES contains 93,319 questions derived from CLUEWEB09 sentences. The questions were created by randomly removing an entity, thus producing sentence-denotation pairs. The sentences include two or more entities and are not very compositional.",
        "dcterms:title": "SPADES",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Parsing",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question Generation",
            "Sentence-Denotation Pairs",
            "Large-Scale Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Parsing",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Berant",
            "A. Chou",
            "R. Frostig",
            "P. Liang"
        ],
        "dcterms:description": "WEBQUESTIONS contains 5,810 question-answer pairs based on Freebase. The questions are real questions asked by people on the Web and are not very compositional.",
        "dcterms:title": "WEBQUESTIONS",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Parsing",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Freebase",
            "Natural Language Questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Parsing",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Su",
            "H. Sun",
            "B. Sadler",
            "M. Srivatsa",
            "I. Gur",
            "Z. Yan",
            "X. Yan"
        ],
        "dcterms:description": "GRAPHQUESTIONS contains 5,166 question-answer pairs created by showing Freebase graph queries to Amazon Mechanical Turk workers and asking them to paraphrase them into natural language.",
        "dcterms:title": "GRAPHQUESTIONS",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Parsing",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question Paraphrasing",
            "Freebase",
            "Natural Language Questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Parsing",
            "Question Answering"
        ]
    }
]