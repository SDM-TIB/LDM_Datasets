To extract datasets from the research paper titled "FDDH: Fast Discriminative Discrete Hashing for Large-Scale Cross-Modal Retrieval" by Xin Liu et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract, introduction, and experimental sections** of the paper. The abstract provides a brief overview of the research and often mentions the datasets used for evaluation. In this case, the authors discuss their method's performance on various benchmarks, which suggests that datasets are involved.

Next, I will focus on the **experimental section** (Section IV), where the authors typically describe the datasets used for their experiments. Here, they mention three public benchmarks: **PASCAL-VOC-2007**, **MIRFlickr**, and **NUS-WIDE**. I will note down the details provided for each dataset, including any specific characteristics or configurations mentioned.

1. **PASCAL-VOC-2007**: The authors state that this dataset is divided into train, validation, and test subsets, containing 5000 and 4919 pairs, respectively. This dataset is commonly used for object detection and segmentation tasks.

2. **MIRFlickr**: The authors mention that they used 20,015 image-text pairs, with a query set of 2000 instances. This dataset is known for its diverse multimedia content and is often used for cross-modal retrieval tasks.

3. **NUS-WIDE**: The authors describe this dataset as containing 186,577 annotated instances from the top 10 most frequent concepts, with a query set of 1866 instances. This dataset is widely used for evaluating cross-modal retrieval methods.

After identifying the datasets, I will look into the **References section** to find the full citations for these datasets. The citations are crucial for proper attribution and to allow others to access the datasets used in the research.

- For **PASCAL-VOC-2007**, the citation is:
  > M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. "The pascal visual object classes (voc) challenge." *International Journal of Computer Vision*, vol. 88, no. 2, pp. 303–338, 2010.

- For **MIRFlickr**, the citation is:
  > M. J. Huiskes, B. Thomee, and M. S. Lew. "New trends and ideas in visual concept detection: The mir flickr retrieval evaluation initiative." In *Proc. International Conference on Multimedia Information Retrieval*, 2010, pp. 527–536.

- For **NUS-WIDE**, the citation is:
  > T. S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. "Nus-wide: A real-world web image database from national university of singapore." In *Proc. ACM International Conference on Image and Video Retrieval*, 2009, pp. 1–9.

Now that I have gathered the necessary information about the datasets and their citations, I will compile this information into a structured format for further use or analysis.