[
    {
        "dcterms:creator": [
            "R. Cowie",
            "E. Douglas-Cowie",
            "S. Savvidou*",
            "E. McMahon",
            "M. Sawey",
            "M. Schröder"
        ],
        "dcterms:description": "FEELTRACE is a dimensional model where the most intense emotions define a circle. It is designed for observers to describe perceived emotional state by moving a pointer to the appropriate point in the model.",
        "dcterms:title": "FEELTRACE",
        "dcterms:issued": "2000",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Affective Computing"
        ],
        "dcat:keyword": [
            "Emotion tracking",
            "Dimensional model",
            "Real-time emotion"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Ghosh",
            "M. Chollet",
            "E. Laksana",
            "L.-P. Morency",
            "S. Scherer"
        ],
        "dcterms:description": "Affect-LM is a neural language model designed for customizable affective text generation, allowing for the generation of text that conveys specific emotional states.",
        "dcterms:title": "Affect-LM",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv1704.06851",
        "dcat:theme": [
            "Natural Language Processing",
            "Affective Computing"
        ],
        "dcat:keyword": [
            "Affective text generation",
            "Neural language model",
            "Customizable emotions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation"
        ]
    },
    {
        "dcterms:creator": [
            "G. Cunha Sergio",
            "M. Lee"
        ],
        "dcterms:description": "Scene2Wav is a deep convolutional sequence-to-conditional SampleRNN designed for emotional scene musicalization, transforming visual scenes into corresponding audio.",
        "dcterms:title": "Scene2Wav",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Generation",
            "Affective Computing"
        ],
        "dcat:keyword": [
            "Audio generation",
            "Emotional music",
            "Scene analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Music Synthesis"
        ]
    },
    {
        "dcterms:creator": [
            "M. Scirea",
            "J. Togelius",
            "P. Eklund",
            "S. Risi"
        ],
        "dcterms:description": "MetaCompose is a system for affective evolutionary music composition that utilizes evolutionary algorithms to create music that conveys specific emotions.",
        "dcterms:title": "MetaCompose",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Generation",
            "Evolutionary Algorithms"
        ],
        "dcat:keyword": [
            "Affective music composition",
            "Evolutionary music",
            "Emotion expression"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Music Composition"
        ]
    },
    {
        "dcterms:creator": [
            "J. Thies",
            "M. Zollhöfer",
            "M. Stamminger",
            "C. Theobalt",
            "M. Nießner"
        ],
        "dcterms:description": "GANimation is a generative adversarial network that enables anatomically-aware facial animation from a single image, allowing for the generation of diverse facial expressions.",
        "dcterms:title": "GANimation",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Facial Animation"
        ],
        "dcat:keyword": [
            "Facial expression synthesis",
            "Generative adversarial networks",
            "Image processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Facial Animation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Ohtani",
            "Y. Nasu",
            "M. Morita",
            "M. Akamine"
        ],
        "dcterms:description": "The Emotion Additive Model (EAM) is a statistical model for emotional transplant in speech synthesis, allowing for the modification of speech to convey different emotions.",
        "dcterms:title": "Emotion Additive Model (EAM)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Synthesis",
            "Affective Computing"
        ],
        "dcat:keyword": [
            "Emotion synthesis",
            "Speech modification",
            "Statistical modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Speech Synthesis"
        ]
    },
    {
        "dcterms:creator": [
            "D. M. McNair",
            "M. Lorr",
            "L. F. Droppleman"
        ],
        "dcterms:description": "The Profile of Mood States (POMS) is a psychological assessment tool that evaluates six basic moods: anger, confusion, depression, fatigue, tension, and vigor.",
        "dcterms:title": "Profile of Mood States (POMS)",
        "dcterms:issued": "1992",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Psychological Assessment",
            "Mood Evaluation"
        ],
        "dcat:keyword": [
            "Mood states",
            "Psychological evaluation",
            "Emotional assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Sadiq",
            "E. Erzin"
        ],
        "dcterms:description": "The Emotion Recognition Dataset is used for emotion-dependent domain adaptation in speech-driven affective facial feature synthesis, focusing on the synthesis of facial expressions based on emotional speech.",
        "dcterms:title": "Emotion Recognition Dataset",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Facial Expression Synthesis"
        ],
        "dcat:keyword": [
            "Emotion recognition",
            "Facial features",
            "Speech synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Emotion Recognition"
        ]
    }
]