[
    {
        "dcterms:creator": [
            "Bart van Oort",
            "Luís Cruz",
            "Babak Loni",
            "Arie van Deursen"
        ],
        "dcterms:description": "An open-source static analysis tool designed to evaluate the software quality of Python ML projects by performing static analysis on the project’s source code, data, and configuration of supporting tools.",
        "dcterms:title": "mllint",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/bvobart/mllint",
        "dcat:theme": [
            "Software Quality",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "static analysis",
            "project smells",
            "Python",
            "ML projects"
        ],
        "dcat:landingPage": "https://github.com/bvobart/mllint",
        "dcterms:hasVersion": "",
        "dcterms:format": "Software",
        "mls:task": [
            "Static Analysis",
            "Quality Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Eric Breck",
            "Shanqing Cai",
            "Eric Nielsen",
            "M. Salib",
            "D. Sculley"
        ],
        "dcterms:description": "A rubric with 28 specific tests and monitoring needs, along with a scoring system to determine the production-readiness of ML systems.",
        "dcterms:title": "The ML Test Score",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Production Readiness"
        ],
        "dcat:keyword": [
            "production readiness",
            "testing",
            "monitoring"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Production Readiness Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Bart van Oort",
            "Luís Cruz",
            "Maurício Aniche",
            "Arie van Deursen"
        ],
        "dcterms:description": "A study on the prevalence of code smells in machine learning projects, highlighting issues related to code quality in ML applications.",
        "dcterms:title": "Code Smells in Machine Learning Projects",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/WAIN52551.2021.00011",
        "dcat:theme": [
            "Software Quality",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "code smells",
            "machine learning",
            "software engineering"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/WAIN52551.2021.00011",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Quality Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Serban",
            "Koen van der Blom",
            "Holger Hoos",
            "Joost Visser"
        ],
        "dcterms:description": "A collection of best practices for engineering trustworthy machine learning applications.",
        "dcterms:title": "SE4ML",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://se-ml.github.io/practices/",
        "dcat:theme": [
            "Software Engineering",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "best practices",
            "machine learning",
            "software engineering"
        ],
        "dcat:landingPage": "https://se-ml.github.io/practices/",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Best Practices Implementation"
        ]
    }
]