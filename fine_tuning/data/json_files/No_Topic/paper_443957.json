[
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A multi-task benchmark and analysis platform for natural language understanding, consisting of various tasks designed to evaluate the performance of NLP models.",
        "dcterms:title": "GLUE Benchmark",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "NLP tasks",
            "benchmark",
            "multi-task learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Classification",
            "Natural Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark designed to measure massive multitask language understanding across various subjects, assessing the general and STEM knowledge of models.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Knowledge Assessment"
        ],
        "dcat:keyword": [
            "multitask learning",
            "knowledge evaluation",
            "STEM subjects"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Knowledge Assessment",
            "Multiple Choice Questions"
        ]
    },
    {
        "dcterms:creator": [
            "Stephanie Lin",
            "Jacob Hilton",
            "Owain Evans"
        ],
        "dcterms:description": "A benchmark that measures how models mimic human falsehoods, assessing the truthfulness of language models across various categories.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Truthfulness Assessment"
        ],
        "dcat:keyword": [
            "truthfulness",
            "language models",
            "falsehoods"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Truthfulness Evaluation",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alicia Parrish",
            "Angelica Chen",
            "Nikita Nangia",
            "Vishakh Padmakumar",
            "Jason Phang",
            "Jana Thompson",
            "Phu Mon Htut",
            "Samuel Bowman"
        ],
        "dcterms:description": "A hand-built bias benchmark for question answering, designed to evaluate the presence of social biases in language models.",
        "dcterms:title": "BBQ",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Bias Assessment"
        ],
        "dcat:keyword": [
            "bias",
            "question answering",
            "social dimensions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Evaluation",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Ramesh Nallapati",
            "Bowen Zhou",
            "Caglar Gulcehre",
            "Bing Xiang"
        ],
        "dcterms:description": "A dataset used for abstractive text summarization, requiring models to generate summaries of news articles.",
        "dcterms:title": "CNN-DailyMail",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1602.06023",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Summarization"
        ],
        "dcat:keyword": [
            "text summarization",
            "news articles",
            "abstractive summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for the task of question answering, part of the GLUE benchmark, assessing the ability to determine if a question can be answered based on a given context.",
        "dcterms:title": "QNLI",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "question answering",
            "natural language inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for assessing grammatical acceptability, part of the GLUE benchmark.",
        "dcterms:title": "COLA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Linguistic Acceptability"
        ],
        "dcat:keyword": [
            "grammaticality",
            "linguistic acceptability"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Linguistic Acceptability Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for semantic textual similarity, part of the GLUE benchmark, used to evaluate the similarity between two sentences.",
        "dcterms:title": "STS-B",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Semantic Similarity"
        ],
        "dcat:keyword": [
            "semantic similarity",
            "sentence similarity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Similarity Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for paraphrase identification, part of the GLUE benchmark, assessing whether two sentences are paraphrases of each other.",
        "dcterms:title": "QQP",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Paraphrase Identification"
        ],
        "dcat:keyword": [
            "paraphrase identification",
            "textual similarity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Paraphrase Identification"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for sentence pair similarity, part of the GLUE benchmark, used to evaluate whether two sentences have the same meaning.",
        "dcterms:title": "MRPC",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Sentence Pair Similarity"
        ],
        "dcat:keyword": [
            "sentence similarity",
            "textual entailment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence Pair Similarity Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for natural language inference, part of the GLUE benchmark, assessing the ability to determine entailment between sentence pairs.",
        "dcterms:title": "MNLI",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "natural language inference",
            "entailment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "A dataset for recognizing textual entailment, part of the GLUE benchmark, used to evaluate the entailment relationship between sentence pairs.",
        "dcterms:title": "RTE",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Textual Entailment"
        ],
        "dcat:keyword": [
            "textual entailment",
            "natural language inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Textual Entailment Recognition"
        ]
    }
]