[
    {
        "dcterms:creator": [
            "A. H. Anderson",
            "M. Bader",
            "E. G. Bard",
            "E. Boyle",
            "G. Doherty",
            "S. Garrod",
            "R. Weinert"
        ],
        "dcterms:description": "The Maptask Corpus is a dataset used for studying dialogue and conversation, consisting of task-oriented dialogues where participants collaborate to complete a map task.",
        "dcterms:title": "Maptask Corpus",
        "dcterms:issued": "1991",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Analysis",
            "Conversational Research"
        ],
        "dcat:keyword": [
            "Task-oriented dialogue",
            "Conversation",
            "Dialogue corpus"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue analysis",
            "Conversational research"
        ]
    },
    {
        "dcterms:creator": [
            "J. J. Godfrey",
            "E. C. Holliman",
            "J. McDaniel"
        ],
        "dcterms:description": "The Switchboard Corpus is a large telephone speech corpus designed for research and development in speech recognition and understanding.",
        "dcterms:title": "Switchboard Corpus",
        "dcterms:issued": "1992",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Conversational Analysis"
        ],
        "dcat:keyword": [
            "Telephone speech",
            "Speech corpus",
            "Conversation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech recognition",
            "Conversational analysis"
        ]
    },
    {
        "dcterms:creator": [
            "S. Poria",
            "D. Hazarika",
            "N. Majumder",
            "G. Naik",
            "E. Cambria",
            "R. Mihalcea"
        ],
        "dcterms:description": "MELD is a multimodal dataset for emotion recognition in conversations, containing transcripts, audio, and visual data from multi-party dialogues.",
        "dcterms:title": "MELD",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1810.02508",
        "dcat:theme": [
            "Emotion Recognition",
            "Multimodal Analysis"
        ],
        "dcat:keyword": [
            "Emotion recognition",
            "Multimodal dataset",
            "Conversation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Audio, Video",
        "mls:task": [
            "Emotion recognition",
            "Conversational analysis"
        ]
    },
    {
        "dcterms:creator": [
            "N. Barros",
            "E. Churamani",
            "H. Lakomkin",
            "A. Sequeira",
            "S. Sutherland",
            "S. Wermter"
        ],
        "dcterms:description": "The OMG-Empathy dataset is designed for studying emotional behavior in conversations, providing data for emotion recognition tasks.",
        "dcterms:title": "OMG-Empathy dataset",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Behavior Analysis"
        ],
        "dcat:keyword": [
            "Emotion behavior",
            "Conversation",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Audio",
        "mls:task": [
            "Emotion recognition",
            "Behavior analysis"
        ]
    },
    {
        "dcterms:creator": [
            "S. R. Livingstone",
            "F. A. Russo"
        ],
        "dcterms:description": "The RAVDESS is a dynamic, multimodal dataset of emotional speech and song, featuring facial and vocal expressions in North American English.",
        "dcterms:title": "Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "10.1371/journal.pone.0196391",
        "dcat:theme": [
            "Emotion Recognition",
            "Multimodal Analysis"
        ],
        "dcat:keyword": [
            "Emotional speech",
            "Facial expressions",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video",
        "mls:task": [
            "Emotion recognition",
            "Facial expression analysis"
        ]
    },
    {
        "dcterms:creator": [
            "A. Mollahosseini",
            "B. Hasani",
            "M. H. Mahoor"
        ],
        "dcterms:description": "AffectNet is a database for facial expression, valence, and arousal computing in the wild, providing images labeled with emotions.",
        "dcterms:title": "AffectNet",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Facial Expression Analysis"
        ],
        "dcat:keyword": [
            "Facial expression",
            "Emotion recognition",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Facial expression recognition",
            "Emotion analysis"
        ]
    }
]