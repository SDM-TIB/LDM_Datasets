[
    {
        "dcterms:creator": [
            "B. Mildenhall",
            "P. P. Srinivasan",
            "M. Tancik",
            "J. T. Barron",
            "R. Ramamoorthi",
            "R. Ng"
        ],
        "dcterms:description": "NeRF represents scenes as neural radiance fields for view synthesis, allowing for the generation of 3D scenes from 2D images.",
        "dcterms:title": "NeRF (Neural Radiance Fields)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Scene Representation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Neural Radiance Fields",
            "3D Generation",
            "View Synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "3D Scene Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. T. Barron",
            "B. Mildenhall",
            "D. Verbin",
            "P. P. Srinivasan",
            "P. Hedman"
        ],
        "dcterms:description": "Mip-NeRF 360 is an unbounded anti-aliased neural radiance field that improves visual quality and minimizes artifacts in 3D scene rendering.",
        "dcterms:title": "Mip-NeRF 360",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Scene Representation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Anti-Aliasing",
            "Neural Radiance Fields",
            "3D Rendering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "3D Scene Generation"
        ]
    },
    {
        "dcterms:creator": [
            "A. Radford",
            "J. W. Kim",
            "C. Hallacy",
            "A. Ramesh",
            "G. Goh",
            "S. Agarwal",
            "G. Sastry",
            "A. Askell",
            "P. Mishkin",
            "J. Clark",
            "G. Krueger",
            "I. Sutskever"
        ],
        "dcterms:description": "CLIP is a model that learns transferable visual models from natural language supervision, enabling various tasks in computer vision.",
        "dcterms:title": "CLIP (Contrastive Languageâ€“Image Pretraining)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Contrastive Learning",
            "Visual Models",
            "Natural Language Supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image Classification",
            "Zero-Shot Learning"
        ]
    },
    {
        "dcterms:creator": [
            "C. Saharia",
            "W. Chan",
            "S. Saxena",
            "L. Li",
            "J. Whang",
            "E. L. Denton",
            "K. Ghasemipour",
            "R. Gontijo Lopes",
            "A. Karagol Ayan",
            "T. Salimans"
        ],
        "dcterms:description": "Imagen is a photorealistic text-to-image diffusion model that incorporates deep language understanding for generating images from text prompts.",
        "dcterms:title": "Imagen",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Image Generation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Diffusion Models",
            "Text-to-Image",
            "Photorealism"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image Generation"
        ]
    },
    {
        "dcterms:creator": [
            "B. Poole",
            "A. Jain",
            "J. T. Barron",
            "B. Mildenhall"
        ],
        "dcterms:description": "DreamFusion is a method for text-to-3D generation using 2D diffusion models, enabling the creation of 3D objects from text prompts.",
        "dcterms:title": "DreamFusion",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2209.14988",
        "dcat:theme": [
            "Text-to-3D Generation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Text-to-3D",
            "Diffusion Models",
            "3D Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "3D Object Generation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Cohen-Bar",
            "E. Richardson",
            "G. Metzer",
            "R. Giryes",
            "D. Cohen-Or"
        ],
        "dcterms:description": "Set-the-scene is a dataset for generating controllable NeRF scenes through global-local training, allowing for better scene composition.",
        "dcterms:title": "Set-the-scene",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Scene Generation",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Controllable Scenes",
            "NeRF",
            "Scene Composition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Scene Generation"
        ]
    }
]