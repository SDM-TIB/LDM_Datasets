[
    {
        "dcterms:creator": [
            "A. Hallak",
            "D. Di Castro",
            "S. Mannor"
        ],
        "dcterms:description": "Contextual Markov Decision Processes (CMDPs) model the interaction of an agent with an environment where the context influences the decision-making process. The optimal policy for a given context corresponds to the optimal policy of the associated MDP.",
        "dcterms:title": "Contextual MDPs (CMDPs)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1502.02259",
        "dcat:theme": [
            "Reinforcement Learning",
            "Decision Making"
        ],
        "dcat:keyword": [
            "Contextual MDP",
            "Reinforcement Learning",
            "Decision Processes"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "N. Jiang",
            "A. Krishnamurthy",
            "A. Agarwal",
            "J. Langford",
            "R. E. Schapire"
        ],
        "dcterms:description": "Contextual Decision Processes (CDP) are a framework for learning in environments where decisions are made based on contextual information, with a focus on low Bellman rank scenarios.",
        "dcterms:title": "Contextual Decision Processes (CDP)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Decision Making"
        ],
        "dcat:keyword": [
            "Contextual Decision Processes",
            "PAC Learning",
            "Bellman Rank"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Lattimore",
            "C. Szepesv√°ri"
        ],
        "dcterms:description": "The Contextual Multi-Armed Bandit (CMAB) framework extends the multi-armed bandit problem by incorporating context, allowing for more informed decision-making based on additional information.",
        "dcterms:title": "Contextual Multi-Armed Bandit (CMAB)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bandit Algorithms",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Multi-Armed Bandit",
            "Contextual Learning",
            "Decision Making"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "O. Levy",
            "A. Cohen",
            "A. B. Cassel",
            "Y. Mansour"
        ],
        "dcterms:description": "Adversarial Contextual MDPs extend the CMDP framework to adversarial settings, focusing on efficient regret minimization using online function approximation.",
        "dcterms:title": "Adversarial Contextual MDPs",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Adversarial Learning",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Adversarial Learning",
            "Contextual MDP",
            "Regret Minimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Foster",
            "A. Rakhlin"
        ],
        "dcterms:description": "This dataset involves contextual bandits that utilize regression oracles to optimize decision-making processes beyond traditional upper confidence bounds.",
        "dcterms:title": "Contextual Bandits with Regression Oracles",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Decision Making"
        ],
        "dcat:keyword": [
            "Contextual Bandits",
            "Regression Oracles",
            "Decision Making"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Modi",
            "A. Tewari"
        ],
        "dcterms:description": "The Generalized Linear Model (GLM) dataset is used for exploring no-regret exploration strategies in contextual reinforcement learning.",
        "dcterms:title": "Generalized Linear Model (GLM)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Statistical Learning"
        ],
        "dcat:keyword": [
            "Generalized Linear Model",
            "Contextual Learning",
            "No-Regret Exploration"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Foster",
            "A. Krishnamurthy"
        ],
        "dcterms:description": "The Estimation to Decision (E2D) meta algorithm is designed for efficient contextual bandit learning, focusing on prediction and allocation tasks.",
        "dcterms:title": "Estimation to Decision (E2D) meta algorithm",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Decision Making"
        ],
        "dcat:keyword": [
            "Meta Algorithm",
            "Contextual Bandits",
            "Decision Making"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]