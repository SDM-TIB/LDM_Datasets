[
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "GPT-4 is a large language model developed by OpenAI, designed for a wide range of applications including text generation, summarization, and more.",
        "dcterms:title": "GPT-4",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://openai.com/gpt-4",
        "dcat:theme": [],
        "dcat:keyword": [
            "Large Language Model",
            "Text Generation"
        ],
        "dcat:landingPage": "https://openai.com/gpt-4",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "Marie-Anne Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar"
        ],
        "dcterms:description": "LLaMA is an open and efficient foundation language model designed for various natural language processing tasks.",
        "dcterms:title": "LLaMA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [],
        "dcat:keyword": [
            "Foundation Model",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "MosaicML"
        ],
        "dcterms:description": "MPT-StoryWriter is a model designed for open-source, commercially usable large language models, focusing on high-quality text generation.",
        "dcterms:title": "MPT-StoryWriter",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.mosaicml.com/blog/mpt-7b",
        "dcat:theme": [],
        "dcat:keyword": [
            "Text Generation",
            "Open Source"
        ],
        "dcat:landingPage": "https://www.mosaicml.com/blog/mpt-7b",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "Marie-Anne Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar"
        ],
        "dcterms:description": "Llama-2 is an extension of the LLaMA model, providing enhanced capabilities for various natural language processing tasks.",
        "dcterms:title": "Llama-2",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [],
        "dcat:keyword": [
            "Foundation Model",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "HuggingFace"
        ],
        "dcterms:description": "HuggingFace TGI is a framework for text generation inference, enabling efficient serving of large language models.",
        "dcterms:title": "HuggingFace TGI",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/text-generation-inference",
        "dcat:theme": [],
        "dcat:keyword": [
            "Text Generation",
            "Inference Framework"
        ],
        "dcat:landingPage": "https://huggingface.co/text-generation-inference",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "NVIDIA"
        ],
        "dcterms:description": "NVIDIA TensorRT-LLM is a toolbox for optimizing large language model inference using TensorRT.",
        "dcterms:title": "NVIDIA TensorRT-LLM",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/NVIDIA/TensorRT-LLM",
        "dcat:theme": [],
        "dcat:keyword": [
            "Inference Optimization",
            "Large Language Models"
        ],
        "dcat:landingPage": "https://github.com/NVIDIA/TensorRT-LLM",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Amey Agrawal",
            "Ashish Panwar",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Bhargav S Gulavani",
            "Ramachandran Ramjee"
        ],
        "dcterms:description": "Sarathi is a system designed for efficient LLM inference by optimizing the decoding process with chunked prefills.",
        "dcterms:title": "Sarathi",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2308.16369",
        "dcat:theme": [],
        "dcat:keyword": [
            "Efficient Inference",
            "LLM Optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tri Dao",
            "Daniel Y. Fu",
            "Stefano Ermon",
            "Atri Rudra",
            "Christopher Ré"
        ],
        "dcterms:description": "FlashAttention is a memory-efficient attention mechanism designed to improve the performance of transformer models.",
        "dcterms:title": "FlashAttention",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Attention Mechanism",
            "Transformer Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "NVIDIA"
        ],
        "dcterms:description": "FasterTransformer is a library designed to accelerate transformer model inference.",
        "dcterms:title": "FasterTransformer",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/NVIDIA/FasterTransformer",
        "dcat:theme": [],
        "dcat:keyword": [
            "Inference Acceleration",
            "Transformer Models"
        ],
        "dcat:landingPage": "https://github.com/NVIDIA/FasterTransformer",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]