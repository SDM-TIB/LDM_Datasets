[
    {
        "dcterms:creator": [
            "Carlos Busso",
            "Murtaza Bulut",
            "Chi-Chun Lee",
            "Abe Kazemzadeh",
            "Emily Mower",
            "Samuel Kim",
            "Jeannette N Chang",
            "Sungbok Lee",
            "Shrikanth S Narayanan"
        ],
        "dcterms:description": "The IEMOCAP dataset contains 12 hours of emotional speech performed by 10 actors from the Drama Department of University of Southern California. The performances are divided into improvised and scripted parts, with utterances labeled with 9 types of emotion including anger, happiness, excitement, sadness, frustration, fear, surprise, other, and neutral state.",
        "dcterms:title": "Interactive Emotional Dyadic Motion Capture (IEMOCAP)",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Emotion Recognition",
            "Emotion Analysis"
        ],
        "dcat:keyword": [
            "Emotional speech",
            "Emotion labels",
            "Actor performances",
            "Speech dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    }
]