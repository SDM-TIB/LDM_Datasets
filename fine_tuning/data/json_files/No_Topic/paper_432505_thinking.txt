To extract datasets from the research paper titled "Equivariant Spatio-Temporal Self-Supervision for LiDAR Object Detection" by Deepti Hegde et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset mentioned.

First, I will read through the **abstract, introduction, and experiments sections** of the paper. The abstract provides a brief overview of the study and may mention datasets used for evaluation. In the introduction, the authors often discuss the context of their work, which may include references to specific datasets.

Next, I will focus on **section 4 (Experiments)**, where the authors typically describe the datasets used for their experiments. In this section, they mention two datasets:

1. **KITTI-360**: This dataset consists of 100k LiDAR scenes from 11 sequences captured in urban environments. It is used for pre-training the model.

2. **SemanticKITTI**: This dataset contains 22 sequences and roughly 48k scans, also used for pre-training. The authors specify that they removed the validation sequences for their experiments.

To confirm the details, I will check the **References section** for the full citations of these datasets:

- For **KITTI-360**, the citation is:
  > Liao, Y., Xie, J., Geiger, A. (2022). *KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2D and 3D*. Pattern Analysis and Machine Intelligence (PAMI).

- For **SemanticKITTI**, the citation is:
  > Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J. (2019). *SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences*. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).

After gathering this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will allow for a clear understanding of the datasets used in the research and their significance in the context of the study.