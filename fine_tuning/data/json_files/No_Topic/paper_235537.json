[
    {
        "dcterms:creator": [
            "H. W. LÃ¶llmann",
            "C. Evers",
            "A. Schmidt",
            "H. Mellmann",
            "H. Barfuss",
            "P. A. Naylor",
            "W. Kellermann"
        ],
        "dcterms:description": "The LOCATA corpus provides recordings from four microphone arrays in static and dynamic scenarios, completely annotated with the ground-truth positions and orientations for all sources and sensors, hand-labelled voice activity information, and close-talking microphone signals as reference.",
        "dcterms:title": "LOCATA Corpus",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.5281/zenodo.3630470",
        "dcat:theme": [
            "Acoustic Signal Processing",
            "Source Localization",
            "Source Tracking"
        ],
        "dcat:keyword": [
            "Acoustic events",
            "Localization",
            "Tracking",
            "Microphone arrays",
            "Dynamic scenarios"
        ],
        "dcat:landingPage": "https://doi.org/10.5281/zenodo.3630470",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Source Localization",
            "Source Tracking"
        ]
    },
    {
        "dcterms:creator": [
            "J. K. Nielsen",
            "J. R. Jensen",
            "S. H. Jensen",
            "M. G. Christensen"
        ],
        "dcterms:description": "The SMARD dataset provides audio recordings and the corresponding ground-truth positional information obtained from multiple microphone arrays and loudspeakers in a low-reverberant room. Only a static single-source scenario is considered.",
        "dcterms:title": "SMARD Dataset",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "http://www.smard.es.aau.dk/",
        "dcat:theme": [
            "Audio Processing",
            "Source Localization"
        ],
        "dcat:keyword": [
            "Audio recordings",
            "Ground-truth positional information",
            "Static sources"
        ],
        "dcat:landingPage": "http://www.smard.es.aau.dk/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Source Localization"
        ]
    },
    {
        "dcterms:creator": [
            "M. Ravanelli",
            "L. Cristoforetti",
            "R. Gretter",
            "M. Pellin",
            "A. Sosi",
            "M. Omologo"
        ],
        "dcterms:description": "The DIRHA corpus provides multichannel recordings for various static source-sensor scenarios in three realistic acoustic enclosures.",
        "dcterms:title": "DIRHA Corpus",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Acoustic Signal Processing"
        ],
        "dcat:keyword": [
            "Multichannel recordings",
            "Static sources",
            "Acoustic enclosures"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "G. Lathoud",
            "J.-M. Odobez",
            "D. Gatica-Perez"
        ],
        "dcterms:description": "AV16.3 is an audio-visual corpus for speaker localization and tracking, designed for machine learning applications.",
        "dcterms:title": "AV16.3 Dataset",
        "dcterms:issued": "2005",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Processing",
            "Speaker Localization"
        ],
        "dcat:keyword": [
            "Audio-visual corpus",
            "Speaker tracking",
            "Localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio-Visual",
        "mls:task": [
            "Speaker Localization",
            "Speaker Tracking"
        ]
    },
    {
        "dcterms:creator": [
            "X. Alameda-Pineda",
            "J. Sanchez-Riera",
            "J. Wienke",
            "V. Franc",
            "J. Cech",
            "K. Kulkarni",
            "A. Deleforge",
            "R. P. Horaud"
        ],
        "dcterms:description": "RAVEL is an annotated corpus for training robots with audiovisual abilities, providing recordings from a rotating robot head.",
        "dcterms:title": "RAVEL Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Audio-Visual Processing"
        ],
        "dcat:keyword": [
            "Annotated corpus",
            "Robots",
            "Audiovisual abilities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio-Visual",
        "mls:task": [
            "Training Robots"
        ]
    },
    {
        "dcterms:creator": [
            "A. Deleforge",
            "R. Horaud"
        ],
        "dcterms:description": "The CAMIL dataset focuses on learning the direction of a sound source using head motions and spectral features.",
        "dcterms:title": "CAMIL Dataset",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "https://hal.inria.fr/inria-00564708",
        "dcat:theme": [
            "Machine Learning",
            "Sound Localization"
        ],
        "dcat:keyword": [
            "Sound source direction",
            "Head motions",
            "Spectral features"
        ],
        "dcat:landingPage": "https://hal.inria.fr/inria-00564708",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Sound Localization"
        ]
    },
    {
        "dcterms:creator": [
            "R. Stiefelhagen",
            "K. Bernardin",
            "R. Bowers",
            "J. Garofolo",
            "D. Mostefa",
            "P. Soundararajan"
        ],
        "dcterms:description": "The CLEAR challenge dataset provides audio-visual recordings from seminars and meetings involving moving talkers.",
        "dcterms:title": "CLEAR Challenge Dataset",
        "dcterms:issued": "2007",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Processing",
            "Speaker Tracking"
        ],
        "dcat:keyword": [
            "Audio-visual recordings",
            "Moving talkers",
            "Localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio-Visual",
        "mls:task": [
            "Speaker Tracking"
        ]
    },
    {
        "dcterms:creator": [
            "M. Strauss",
            "P. Mordel",
            "V. Miguet",
            "A. Deleforge"
        ],
        "dcterms:description": "DREGON is a dataset and methods for UAV-embedded sound source localization, focusing on recordings from static loudspeakers.",
        "dcterms:title": "DREGON Dataset",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "UAV Technology",
            "Sound Localization"
        ],
        "dcat:keyword": [
            "UAV",
            "Sound source localization",
            "Static loudspeakers"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Sound Localization"
        ]
    }
]