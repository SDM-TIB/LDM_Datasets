[
    {
        "dcterms:creator": [
            "Timnit Gebru",
            "Jamie Morgenstern",
            "Briana Vecchione",
            "Jennifer Wortman Vaughan",
            "Hanna Wallach",
            "Hal Daumé III",
            "Kate Crawford"
        ],
        "dcterms:description": "The AI Safety Benchmark v0.5 is designed to assess the safety risks of AI systems that use language models, focusing on a single use case of an adult chatting with a general-purpose assistant in English. It includes a taxonomy of hazard categories and tests for various safety risks.",
        "dcterms:title": "AI Safety Benchmark v0.5",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Safety",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "AI safety",
            "language models",
            "benchmark",
            "hazard categories"
        ],
        "dcat:landingPage": "https://mlcommons.org/aisafety",
        "dcterms:hasVersion": "v0.5",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation",
            "Risk assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Percy Liang",
            "Rishi Bommasani",
            "Tony Lee",
            "Dimitris Tsipras",
            "Dilara Soylu",
            "Michihiro Yasunaga",
            "Yian Zhang",
            "Deepak Narayanan",
            "Yuhuai Wu",
            "Ananya Kumar",
            "Benjamin Newman",
            "Binhang Yuan",
            "Bobby Yan",
            "Ce Zhang",
            "Christian Cosgrove",
            "Christopher D. Manning",
            "Christopher Ré",
            "Diana Acosta-Navas",
            "Drew A. Hudson",
            "Eric Zelikman",
            "Esin Durmus",
            "Faisal Ladhak",
            "Frieda Rong",
            "Hongyu Ren",
            "Huaxiu Yao",
            "Jue Wang",
            "Keshav Santhanam",
            "Laurel Orr",
            "Lucia Zheng",
            "Mert Yuksekgonul",
            "Mirac Suzgun",
            "Nathan Kim",
            "Neel Guha",
            "Niladri Chatterji",
            "Omar Khattab",
            "Peter Henderson",
            "Qian Huang",
            "Ryan Chi",
            "Sang Michael Xie",
            "Shibani Santurkar",
            "Surya Ganguli",
            "Tatsunori Hashimoto",
            "Thomas Icard",
            "Tianyi Zhang",
            "Vishrav Chaudhary",
            "William Wang",
            "Xuechen Li",
            "Yifan Mai",
            "Yuhui Zhang",
            "Yuta Koreeda"
        ],
        "dcterms:description": "HELM is a holistic evaluation framework for language models that assesses their performance across various tasks and dimensions, including safety.",
        "dcterms:title": "HELM",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "language models",
            "evaluation framework",
            "safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Safety assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Rohan Anil",
            "et al."
        ],
        "dcterms:description": "BIG-bench is a benchmark that quantifies and extrapolates the capabilities of language models beyond traditional evaluation methods.",
        "dcterms:title": "BIG-bench",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Benchmarking",
            "Language Models"
        ],
        "dcat:keyword": [
            "benchmark",
            "language models",
            "evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model benchmarking"
        ]
    },
    {
        "dcterms:creator": [
            "Alicia Parrish",
            "Angelica Chen",
            "Nikita Nangia",
            "Vishakh Padmakumar",
            "Jason Phang",
            "Jana Thompson",
            "Phu Mon Htut",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "BBQ is a hand-built bias benchmark for question answering that evaluates the biases present in language models.",
        "dcterms:title": "BBQ",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2110.08193",
        "dcat:theme": [
            "Bias Evaluation",
            "Question Answering"
        ],
        "dcat:keyword": [
            "bias benchmark",
            "question answering",
            "language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Debora Nozza",
            "Federico Bianchi",
            "Dirk Hovy"
        ],
        "dcterms:description": "HONEST measures hurtful sentence completion in language models, focusing on the potential harm caused by their outputs.",
        "dcterms:title": "HONEST",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Safety",
            "Language Models"
        ],
        "dcat:keyword": [
            "hurtful content",
            "language models",
            "safety evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Samuel Gehman",
            "Suchin Gururangan",
            "Maarten Sap",
            "Yejin Choi",
            "Noah A Smith"
        ],
        "dcterms:description": "RealToxicityPrompts evaluates neural toxic degeneration in language models by assessing their responses to prompts that may elicit toxic content.",
        "dcterms:title": "RealToxicityPrompts",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2009.11462",
        "dcat:theme": [
            "Toxicity Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "toxicity",
            "language models",
            "evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxicity evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Zhexin Zhang",
            "Leqi Lei",
            "Lindong Wu",
            "Rui Sun",
            "Yongkang Huang",
            "Chong Long",
            "Xiao Liu",
            "Xuanyu Lei",
            "Jie Tang",
            "Minlie Huang"
        ],
        "dcterms:description": "SafetyBench evaluates the safety of large language models with multiple choice questions, focusing on their ability to avoid unsafe responses.",
        "dcterms:title": "SafetyBench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Safety",
            "Language Models"
        ],
        "dcat:keyword": [
            "safety evaluation",
            "language models",
            "multiple choice"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Lichao Sun",
            "Yue Huang",
            "Haoran Wang",
            "Siyuan Wu",
            "Qihui Zhang",
            "Yuan Li",
            "Chujie Gao",
            "Yixin Huang",
            "Wenhan Lyu",
            "Yixuan Zhang",
            "Xiner Li",
            "Zhengliang Liu",
            "Yixin Liu",
            "Yijue Wang",
            "Zhikun Zhang",
            "Bertie Vidgen",
            "Bhavya Kailkhura",
            "Caiming Xiong",
            "Chaowei Xiao",
            "Chunyuan Li",
            "Eric Xing",
            "Furong Huang",
            "Hao Liu",
            "Heng Ji",
            "Hongyi Wang",
            "Huan Zhang",
            "Huaxiu Yao",
            "Manolis Kellis",
            "Marinka Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Neil Zhenqiang Gong",
            "Philip S. Yu",
            "Pin-Yu Chen",
            "Quanquan Gu",
            "Ran Xu",
            "Rex Ying",
            "Shuiwang Ji",
            "Suman Jana",
            "Tianlong Chen",
            "Tianming Liu",
            "Tianyi Zhou",
            "William Wang",
            "Xiang Li",
            "Xiangliang Zhang",
            "Xiao Wang",
            "Xing Xie",
            "Xun Chen",
            "Xuyu Wang",
            "Yan Liu",
            "Yanfang Ye",
            "Yue Zhao"
        ],
        "dcterms:description": "TrustLLM assesses the trustworthiness of large language models, focusing on their reliability and safety in various applications.",
        "dcterms:title": "TrustLLM",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Trustworthiness",
            "Language Models"
        ],
        "dcat:keyword": [
            "trustworthiness",
            "language models",
            "safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Trust evaluation"
        ]
    }
]