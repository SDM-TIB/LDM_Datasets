[
    {
        "dcterms:creator": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "dcterms:description": "This benchmark dataset consists of approximately 200 toxic questions generated using the text-davinci-002 model. The questions cover six different categories of adjectives – racist, stereotypical, sexist, illegal, toxic, and harmful.",
        "dcterms:title": "DangerousQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Toxicity Evaluation",
            "Question Generation"
        ],
        "dcat:keyword": [
            "Toxic questions",
            "Bias",
            "Safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Andy Zou",
            "Long Phan",
            "Sarah Chen",
            "James Campbell",
            "Phillip Guo",
            "Richard Ren",
            "Alexander Pan",
            "Xuwang Yin",
            "Mantas Mazeika",
            "Ann-Kathrin Dombrowski",
            "Shashwat Goel",
            "Nathaniel Li",
            "Michael J. Byun",
            "Zifan Wang",
            "Alex Mallen",
            "Steven Basart",
            "Sanmi Koyejo",
            "Dawn Song",
            "Matt Fredrikson",
            "J. Zico Kolter",
            "Dan Hendrycks"
        ],
        "dcterms:description": "This benchmark dataset consists of 500 harmful instructions encompassing various behaviors such as profanity, graphic depictions, threats, misinformation, discrimination, cyber-crime, dangerous and illegal activities.",
        "dcterms:title": "AdvBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.01405",
        "dcat:theme": [
            "Instruction Evaluation",
            "Harmful Behavior Assessment"
        ],
        "dcat:keyword": [
            "Harmful instructions",
            "Safety",
            "Ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Rima Hazra",
            "Sayan Layek",
            "Somnath Banerjee",
            "Animesh Mukherjee"
        ],
        "dcterms:description": "This dataset consists of 330 harmful instructions across 11 prohibited categories for evaluating the harmfulness of language models.",
        "dcterms:title": "HEx-PHI",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "abs/2401.10647",
        "dcat:theme": [
            "Harmful Instruction Evaluation"
        ],
        "dcat:keyword": [
            "Harmful instructions",
            "Safety",
            "Ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "dcterms:description": "This dataset consists of 388 unethical questions covering various topics such as hate speech and discrimination, fake news and propaganda, cruelty and violence, conspiracy theories and paranoia, controlling the thoughts and emotions of learners, and advanced technology to create weapons.",
        "dcterms:title": "NicheHazardQA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Unethical Question Evaluation"
        ],
        "dcat:keyword": [
            "Unethical questions",
            "Safety",
            "Ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Somnath Banerjee",
            "Sayan Layek",
            "Rima Hazra",
            "Animesh Mukherjee"
        ],
        "dcterms:description": "This dataset consists of approximately 1850 harmful instructions across 7 technology-oriented and influenced topics for evaluating the harmfulness of language models.",
        "dcterms:title": "TechHazardQA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Technology Safety Evaluation"
        ],
        "dcat:keyword": [
            "Harmful instructions",
            "Technology",
            "Safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "dcterms:description": "To further evaluate our safety strategy SAFEINFER across a broad spectrum of potential hazards, we developed HarmEval, a dataset based on prohibited scenarios listed in OpenAI and Meta’s usage policies. HarmEval categorizes risks into 11 main categories, resulting in approximately ∼550 crafted harmful queries.",
        "dcterms:title": "HarmEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.03693",
        "dcat:theme": [
            "Harmful Query Evaluation"
        ],
        "dcat:keyword": [
            "Harmful queries",
            "Safety",
            "Ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    }
]