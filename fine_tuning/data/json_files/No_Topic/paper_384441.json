[
    {
        "dcterms:creator": [
            "R. Ardila",
            "M. Branson",
            "K. Davis",
            "M. Henretty",
            "M. Kohler",
            "J. Meyer",
            "R. Morais",
            "L. Saunders",
            "F. M. Tyers",
            "G. Weber"
        ],
        "dcterms:description": "A massively-multilingual speech corpus used for training voice conversion models, particularly to enhance robustness against variations in accent and background noise.",
        "dcterms:title": "CommonVoice",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1912.06670",
        "dcat:theme": [
            "Speech Processing",
            "Multilingual Data"
        ],
        "dcat:keyword": [
            "Speech corpus",
            "Voice conversion",
            "Multilingual",
            "Accent variation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Voice Conversion"
        ]
    },
    {
        "dcterms:creator": [
            "K. Qian",
            "Y. Zhang",
            "S. Chang",
            "X. Yang",
            "M. Hasegawa-Johnson"
        ],
        "dcterms:description": "A voice conversion model that allows zero-shot voice style transfer using only autoencoder loss, utilized in the context of adapting voice conversion models.",
        "dcterms:title": "AutoVC",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Voice Conversion",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Voice style transfer",
            "Autoencoder",
            "Zero-shot learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Voice Conversion"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Y. Lin",
            "C.-M. Chien",
            "J.-H. Lin",
            "H.-y. Lee",
            "L.-s. Lee"
        ],
        "dcterms:description": "An end-to-end voice conversion model that extracts and fuses fine-grained voice fragments with attention mechanisms, used for improving voice conversion performance.",
        "dcterms:title": "FragmentVC",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Voice Conversion",
            "Signal Processing"
        ],
        "dcat:keyword": [
            "Voice conversion",
            "Attention mechanism",
            "Fine-grained voice fragments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Voice Conversion"
        ]
    },
    {
        "dcterms:creator": [
            "A. Baevski",
            "Y. Zhou",
            "A. Mohamed",
            "M. Auli"
        ],
        "dcterms:description": "A framework for self-supervised learning of speech representations, used as a pretrained encoder in voice conversion models to enhance their performance.",
        "dcterms:title": "Wav2Vec",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Processing",
            "Self-supervised Learning"
        ],
        "dcat:keyword": [
            "Speech representation",
            "Self-supervised learning",
            "Pretrained model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Representation Learning"
        ]
    }
]