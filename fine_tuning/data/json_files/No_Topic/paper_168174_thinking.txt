To extract datasets from the research paper titled "Semantic Document Distance Measures and Unsupervised Document Revision Detection" by Xiaofeng Zhu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by examining the **abstract** and **introduction** sections of the paper. The abstract mentions that the authors demonstrate their system using **Wikipedia revision dumps** and **simulated datasets**. This indicates that these are the datasets I need to focus on.

Next, I will look into **section 5 (Numerical Experiments)**, where the authors provide detailed descriptions of the datasets used for their experiments. Here, they explicitly mention:

1. **Wikipedia Revision Dumps**: This dataset consists of revision edits from Wikipedia, processed to contain 62,234 documents with 46,354 revisions. The authors note that they pre-processed the dumps using the JWPL Revision Machine and filtered out short documents to reduce noise.

2. **Simulated Data Sets**: The authors describe a dataset generated through a simulation process that mimics real-world document revisions. They explain how the dataset was created, including the number of documents and the types of revisions made.

Now, I will check the **References section** to find the full citations for these datasets:

- For the **Wikipedia Revision Dumps**, the citation is:
  > Jure Leskovec, Daniel P Huttenlocher, and Jon M Kleinberg. *Governance in social media: A case study of the Wikipedia promotion process*. In International Conference on Weblogs and Social Media, 2010.

- For the **simulated datasets**, since they are generated by the authors themselves, there may not be a formal citation. However, I can note that the methodology for generating these datasets is described in the paper itself.

After gathering this information, I will summarize the datasets as follows:

1. **Wikipedia Revision Dumps**: A dataset containing 62,234 documents and 46,354 revisions, processed from Wikipedia's revision history.
2. **Simulated Data Sets**: A dataset created through a simulation process to mimic document revisions, with details on the number of documents and types of revisions.

Finally, I will compile this information into a structured format for further processing or review, ensuring that the full citations are included for the datasets mentioned in the paper.