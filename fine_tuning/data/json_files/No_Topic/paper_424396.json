[
    {
        "dcterms:creator": [
            "Jiaming Ji",
            "Mickel Liu",
            "Juntao Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Ce Bian",
            "Boyuan Chen",
            "Ruiyang Sun",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "dcterms:description": "A dataset aimed at fostering research on safety alignment in large language models (LLMs), uniquely separating annotations of helpfulness and harmlessness for question-answering pairs. It contains safety meta-labels for 333,963 QA pairs and 361,903 pairs of expert comparison data for helpfulness and harmlessness metrics.",
        "dcterms:title": "BEAVERTAILS",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://sites.google.com/view/pku-beavertails",
        "dcat:theme": [
            "Safety Alignment",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Large Language Models",
            "Safety",
            "Human-Preference Dataset",
            "Question-Answering",
            "Content Moderation"
        ],
        "dcat:landingPage": "https://sites.google.com/view/pku-beavertails",
        "dcterms:hasVersion": "BEAVERTAILS-330k",
        "dcterms:format": "Text",
        "mls:task": [
            "Content Moderation",
            "Reinforcement Learning from Human Feedback"
        ]
    },
    {
        "dcterms:creator": [
            "Jing Xu",
            "Da Ju",
            "Margaret Li",
            "Y-Lan Boureau",
            "Jason Weston",
            "Emily Dinan"
        ],
        "dcterms:description": "A dialogue dataset where annotators attempt to elicit unsafe behaviors from chat-bots using unsafe utterances, with all utterances annotated as offensive or safe.",
        "dcterms:title": "BAD",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Systems",
            "Safety in AI"
        ],
        "dcat:keyword": [
            "Dialogue Dataset",
            "Safety",
            "Adversarial"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Samuel Gehman",
            "Suchin Gururangan",
            "Maarten Sap",
            "Yejin Choi",
            "Noah A Smith"
        ],
        "dcterms:description": "A dataset consisting of 100k sentences annotated with toxicity scores from the Perspective API, used to evaluate neural toxic degeneration in language models.",
        "dcterms:title": "REALTOXICITYPROMPTS",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Toxicity Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Toxicity",
            "Language Models",
            "Evaluation Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxicity Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Kawin Ethayarajh",
            "Heidi Zhang",
            "Yizhong Wang",
            "Dan Jurafsky"
        ],
        "dcterms:description": "A dataset consisting of 385k collective human preferences regarding the helpfulness of responses to questions and instructions across 18 different subject areas.",
        "dcterms:title": "SHP",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human Preferences",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Human Preference Dataset",
            "Helpfulness",
            "Question-Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Stephanie Lin",
            "Jacob Hilton",
            "Owain Evans"
        ],
        "dcterms:description": "An evaluation dataset consisting of 817 human-written questions aimed at evaluating the trustfulness in the responses generated by language models.",
        "dcterms:title": "TRUSTFULQA",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Trust Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Trustfulness",
            "Evaluation Dataset",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Trust Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Alicia Parrish",
            "Angelica Chen",
            "Nikita Nangia",
            "Vishakh Padmakumar",
            "Jason Phang",
            "Jana Thompson",
            "Phu Mon Htut",
            "Samuel R Bowman"
        ],
        "dcterms:description": "A hand-built bias benchmark for question answering, annotated with labels encompassing nine categories of social bias encountered in English QA tasks.",
        "dcterms:title": "BBQ",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2110.08193",
        "dcat:theme": [
            "Bias Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Bias",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Deep Ganguli",
            "Liane Lovitt",
            "Jackson Kernion",
            "Amanda Askell",
            "Yuntao Bai",
            "Saurav Kadavath",
            "Ben Mann",
            "Ethan Perez",
            "Nicholas Schiefer",
            "Kamal Ndousse"
        ],
        "dcterms:description": "A dataset focused on red teaming language models to reduce harms, detailing methods, scaling behaviors, and lessons learned.",
        "dcterms:title": "HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2209.07858",
        "dcat:theme": [
            "Safety Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Red Teaming",
            "Safety",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Google Jigsaw"
        ],
        "dcterms:description": "An automated service that provides an array of scores along 8 dimensions for a given text input, aimed at evaluating toxicity.",
        "dcterms:title": "PERSPECTIVE API",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "https://www.perspectiveapi.com/",
        "dcat:theme": [
            "Toxicity Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Toxicity",
            "API",
            "Content Moderation"
        ],
        "dcat:landingPage": "https://www.perspectiveapi.com/",
        "dcterms:hasVersion": "",
        "dcterms:format": "API",
        "mls:task": [
            "Content Moderation"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "An API that flags a given input as harmful if the score of any harm category exceeds a pre-defined probability threshold.",
        "dcterms:title": "OpenAI Moderation API",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://platform.openai.com/docs/guides/moderation/overview",
        "dcat:theme": [
            "Content Moderation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Moderation",
            "API",
            "Content Safety"
        ],
        "dcat:landingPage": "https://platform.openai.com/docs/guides/moderation/overview",
        "dcterms:hasVersion": "",
        "dcterms:format": "API",
        "mls:task": [
            "Content Moderation"
        ]
    }
]