To extract datasets from the research paper titled "An Entity-Driven Framework for Abstractive Summarization" by Eva Sharma et al., I will follow a systematic approach to ensure that I capture all relevant datasets and their citations accurately.

First, I will read through the **abstract** and **introduction** sections to identify any mentions of datasets. The abstract states that the authors evaluated their model on popular news summarization datasets, which indicates that datasets are involved.

Next, I will look for specific sections that typically contain dataset information, such as **section 3 (Experimental Setups)**. In this section, the authors mention two datasets used for evaluation:

1. **New York Times (NYT) Dataset**: The authors refer to the NYT dataset as a popular summarization dataset and provide a citation for it. They mention that they followed specific preprocessing steps to obtain training, validation, and test samples.

2. **CNN/Daily Mail (CNN/DM) Dataset**: Similarly, the authors mention the CNN/DM dataset as another popular summarization dataset and provide details about the preprocessing steps they followed for this dataset as well.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset:

- For the **New York Times (NYT) Dataset**, the citation is:
  > Evan Sandhaus. *The New York Times Annotated Corpus*. Linguistic Data Consortium, Philadelphia, 2008.

- For the **CNN/Daily Mail (CNN/DM) Dataset**, the citation is:
  > Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. *Teaching Machines to Read and Comprehend*. In Advances in Neural Information Processing Systems, pages 1693â€“1701, 2015.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further use or processing. This ensures that I have accurately captured the datasets mentioned in the paper along with their proper citations.