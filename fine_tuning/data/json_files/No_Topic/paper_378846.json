[
    {
        "dcterms:creator": [
            "Alexander R. Fabbri",
            "Wojciech Kryściński",
            "Bryan McCann",
            "Caiming Xiong",
            "Richard Socher",
            "Dragomir Radev"
        ],
        "dcterms:description": "A collection of human annotations for model-generated summaries on 100 CNN/DailyMail news articles over 4 dimensions: coherence, fluency, consistency, and relevance.",
        "dcterms:title": "SummEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Summarization",
            "Evaluation",
            "Human Annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Shikib Mehri",
            "Maxine Eskenazi"
        ],
        "dcterms:description": "A dataset containing fine-grained human judgments for 124 conversations, evaluating aspects such as coherence, error recovery, consistency, diversity, topic depth, likeability, understanding, flexibility, informativeness, inquisitiveness, and overall performance.",
        "dcterms:title": "FED",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Evaluation",
            "Human Judgments"
        ],
        "dcat:keyword": [
            "Dialogue",
            "Evaluation",
            "Human Judgments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Response Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jian Guan",
            "Zhexin Zhang",
            "Zhuoer Feng",
            "Zitao Liu",
            "Wenbiao Ding",
            "Xiaoxi Mao",
            "Changjie Fan",
            "Minlie Huang"
        ],
        "dcterms:description": "A benchmark for evaluating open-ended story generation metrics, containing 200 story beginnings and 5 corresponding machine-generated storylines for each beginning, manually annotated in terms of overall quality.",
        "dcterms:title": "OpenMEVA-ROC",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Story Generation",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Story Generation",
            "Evaluation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Story Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Wei Xu",
            "Alan Ritter",
            "Chris Callison-Burch",
            "William B. Dolan",
            "Yangfeng Ji"
        ],
        "dcterms:description": "A dataset containing 761 input sentences, each with an average of 9.41 paraphrase candidates, used for evaluating paraphrase generation.",
        "dcterms:title": "Twitter-Para",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Paraphrase Generation",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Paraphrase",
            "Twitter",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Paraphrase Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Wei Xu",
            "Chris Callison-Burch",
            "Bill Dolan"
        ],
        "dcterms:description": "An extended version of the Twitter-Para dataset, which includes additional paraphrase candidates for 20% of the input sentences.",
        "dcterms:title": "Twitter (Extend)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Paraphrase Generation",
            "Evaluation Metrics"
        ],
        "dcat:keyword": [
            "Paraphrase",
            "Twitter",
            "Extended Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Paraphrase Generation"
        ]
    }
]