To extract datasets from the research paper titled "Unsupervised Learning of Important Objects from First-Person Videos" by Gedas Bertasius et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by examining the **abstract** and **introduction** sections of the paper. The abstract mentions that the authors demonstrate their method's success on two different important object datasets, which indicates that datasets are indeed present in the paper.

Next, I will look for a dedicated **datasets section** or any mention of datasets in the **experiments** section. In this paper, the authors specifically mention using two datasets for their experiments: 

1. **First-Person Important Object RGBD Dataset**: This dataset is referenced multiple times throughout the paper, particularly in the experimental results section. It is noted that it contains per-pixel important object annotations.

2. **GTEA Gaze+ Dataset**: This dataset is also mentioned as being used for evaluation, capturing activities of cooking different meals with less variation in scenes.

Now, I will check the **References section** to find the full citations for these datasets:

- For the **First-Person Important Object RGBD Dataset**, the citation is:
  > Gedas Bertasius, Hyun Soo Park, Stella X. Yu, and Jianbo Shi. *First-person action-object detection with egonet*. In Proceedings of Robotics: Science and Systems, July 2017.

- For the **GTEA Gaze+ Dataset**, the citation is:
  > Yin Li, Zhefan Ye, and James M. Rehg. *Delving into egocentric actions*. In CVPR, 2015.

Having gathered the dataset names and their citations, I will ensure that I accurately represent this information in the required format for downstream processing.

Finally, I will compile the dataset entries, ensuring that each dataset is clearly described along with its full citation, ready for review or further analysis.