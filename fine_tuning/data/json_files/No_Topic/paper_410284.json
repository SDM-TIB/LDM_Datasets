[
    {
        "dcterms:creator": [
            "Y. Wang",
            "K. Yeganeh",
            "M. Swaroop"
        ],
        "dcterms:description": "A dataset generated for knowledge question-answering (QA) tasks in urban renewal, created using self-instruct methods based on urban renewal scientific literature.",
        "dcterms:title": "Urban Renewal QA Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Urban Renewal",
            "Question Answering"
        ],
        "dcat:keyword": [
            "QA dataset",
            "Urban renewal",
            "Self-instruct"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Knowledge Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Du",
            "Y. Qian",
            "X. Liu"
        ],
        "dcterms:description": "A general language model pretraining dataset that incorporates autoregressive blank infilling techniques.",
        "dcterms:title": "ChatGLM",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "10.48550/arXiv.2103.10360",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language model",
            "Pretraining",
            "Autoregressive"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2103.10360",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "X. Li",
            "P. Liang"
        ],
        "dcterms:description": "A fine-tuning method that optimizes continuous prompts for generation tasks.",
        "dcterms:title": "Prefix Tuning",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "10.48550/arXiv.2101.00190",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Fine-tuning"
        ],
        "dcat:keyword": [
            "Fine-tuning",
            "Prompt optimization"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2101.00190",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation"
        ]
    },
    {
        "dcterms:creator": [
            "E. J. Hu",
            "Y. Shen",
            "P. Wallis"
        ],
        "dcterms:description": "A low-rank adaptation method for fine-tuning large language models efficiently.",
        "dcterms:title": "LoRA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "10.48550/arXiv.2106.09685",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Fine-tuning"
        ],
        "dcat:keyword": [
            "Low-rank adaptation",
            "Efficient fine-tuning"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2106.09685",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Fine-tuning"
        ]
    }
]