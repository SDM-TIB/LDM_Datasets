To extract datasets from the research paper titled "MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model" by Han Fu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by examining the **abstract** and **introduction** sections of the paper. The abstract mentions that the authors conducted experiments on the **Recipe1M dataset**, which indicates that this dataset is crucial for their research. The introduction further emphasizes the importance of this dataset in the context of cross-modal retrieval tasks.

Next, I will look for a dedicated **datasets section** or any mention of datasets in the **experiments section**. In this case, the authors explicitly state in section 4.1 that they conducted experiments on the **Recipe1M dataset**. They describe it as a large-scale collection that includes cooking instructions along with food images, consisting of over 1 million textual recipes and around 900,000 images. They also mention the number of matching pairs used for training, validation, and testing.

Now, I will check the **References section** to find the full citation for the Recipe1M dataset. The relevant citation is:

- For **Recipe1M dataset**:
  > Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Oflı, Ingmar Weber, and Antonio Torralba. *Learning cross-modal embeddings for cooking recipes and food images*. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3068–3076, 2017.

Since the paper primarily focuses on the Recipe1M dataset, I will ensure that this dataset is thoroughly documented in my extraction.

Finally, I will compile the extracted information into a structured format, ensuring that the dataset entry includes the name, description, and full citation. This will provide a comprehensive overview of the dataset used in the research paper.