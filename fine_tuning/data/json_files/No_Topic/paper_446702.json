[
    {
        "dcterms:creator": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "dcterms:description": "HaluBench is a comprehensive hallucination evaluation benchmark consisting of 15k samples sourced from various real-world domains, designed to evaluate the performance of LLMs in detecting hallucinations.",
        "dcterms:title": "HaluBench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://huggingface.co/datasets/PatronusAI/HaluBench",
        "dcat:theme": [
            "Natural Language Processing",
            "Hallucination Detection"
        ],
        "dcat:keyword": [
            "hallucination evaluation",
            "benchmark",
            "language models",
            "question answering"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/PatronusAI/HaluBench",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Hallucination Detection",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Timo MÃ¶ller",
            "Anthony Reina",
            "Raghavan Jayakumar",
            "Malte Pietsch"
        ],
        "dcterms:description": "CovidQA is a question answering dataset for COVID-19, consisting of question-answer pairs annotated by biomedical experts on scientific articles related to COVID-19.",
        "dcterms:title": "CovidQA",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Biomedical",
            "Question Answering"
        ],
        "dcat:keyword": [
            "COVID-19",
            "question answering",
            "biomedical dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Qiao Jin",
            "Bhuwan Dhingra",
            "Zhengping Liu",
            "William W Cohen",
            "Xinghua Lu"
        ],
        "dcterms:description": "PubMedQA is a biomedical question answering dataset collected from PubMed abstracts, designed to answer research questions with yes/no/maybe responses.",
        "dcterms:title": "PubMedQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Biomedical",
            "Question Answering"
        ],
        "dcat:keyword": [
            "biomedical",
            "question answering",
            "PubMed"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Dheeru Dua",
            "Yizhong Wang",
            "Pradeep Dasigi",
            "Gabriel Stanovsky",
            "Sameer Singh",
            "Matt Gardner"
        ],
        "dcterms:description": "DROP is an English reading comprehension benchmark that assesses reasoning ability over the content of paragraphs, requiring discrete reasoning over paragraphs.",
        "dcterms:title": "DROP",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "reading comprehension",
            "benchmark",
            "reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Pranab Islam",
            "Anand Kannappan",
            "Douwe Kiela",
            "Rebecca Qian",
            "Nino Scherrer",
            "Bertie Vidgen"
        ],
        "dcterms:description": "FinanceBench is a new benchmark for financial question answering, consisting of questions, contexts, and answers over financial documents.",
        "dcterms:title": "FinanceBench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2311.11944",
        "dcat:theme": [
            "Finance",
            "Question Answering"
        ],
        "dcat:keyword": [
            "financial question answering",
            "benchmark",
            "finance"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2311.11944",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yuanhao Wu",
            "Juno Zhu",
            "Siliang Xu",
            "Kashun Shum",
            "Cheng Niu",
            "Randy Zhong",
            "Juntong Song",
            "Tong Zhang"
        ],
        "dcterms:description": "RAGTruth is a hallucination corpus for developing trustworthy retrieval-augmented language models, containing word-level hallucination annotations on LLM generated text.",
        "dcterms:title": "RAGTruth",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2401.00396",
        "dcat:theme": [
            "Natural Language Processing",
            "Hallucination Detection"
        ],
        "dcat:keyword": [
            "hallucination corpus",
            "retrieval-augmented generation",
            "language models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2401.00396",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Hallucination Detection"
        ]
    }
]