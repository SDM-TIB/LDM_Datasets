[
    {
        "dcterms:creator": [],
        "dcterms:description": "The MuJoCo Ant robot control environments are used to evaluate the proposed method for learning diverse skills without external rewards.",
        "dcterms:title": "MuJoCo Ant robot control environments",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Robotic control",
            "Reinforcement learning",
            "Unsupervised learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Hansen",
            "W. Dabney",
            "A. Barreto",
            "D. Warde-Farley",
            "T. V. de Wiele",
            "V. Mnih"
        ],
        "dcterms:description": "Variational Intrinsic Successor Features (VISR) is a method that learns continuous skills associated with rewards, allowing for potentially infinite skill learning.",
        "dcterms:title": "VISR (Variational Intrinsic Successor Features)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://openreview.net/forum?id=BJeAHkrYDS",
        "dcat:theme": [],
        "dcat:keyword": [
            "Skill learning",
            "Reinforcement learning",
            "Continuous skills"
        ],
        "dcat:landingPage": "https://openreview.net/forum?id=BJeAHkrYDS",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "B. Eysenbach",
            "A. Gupta",
            "J. Ibarz",
            "S. Levine"
        ],
        "dcterms:description": "Diversity is All You Need (DIAYN) is a method for learning skills without a reward function, focusing on skill diversity.",
        "dcterms:title": "DIAYN (Diversity is All You Need)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1802.06070",
        "dcat:theme": [],
        "dcat:keyword": [
            "Skill learning",
            "Unsupervised learning",
            "Diversity"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1802.06070",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Haarnoja",
            "A. Zhou",
            "K. Hartikainen",
            "G. Tucker",
            "S. Ha",
            "J. Tan",
            "V. Kumar",
            "H. Zhu",
            "A. Gupta",
            "P. Abbeel"
        ],
        "dcterms:description": "Soft Actor-Critic (SAC) is a reinforcement learning algorithm that is sample-efficient and used for learning policies in continuous action spaces.",
        "dcterms:title": "SAC (Soft Actor-Critic)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1812.05905",
        "dcat:theme": [],
        "dcat:keyword": [
            "Reinforcement learning",
            "Continuous actions",
            "Sample efficiency"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1812.05905",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Sharma",
            "S. Gu",
            "S. Levine",
            "V. Kumar",
            "K. Hausman"
        ],
        "dcterms:description": "Diversity-Aware Skill Discovery (DADS) is a method for unsupervised discovery of skills that takes dynamics into account.",
        "dcterms:title": "DADS (Diversity-Aware Skill Discovery)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1907.01657",
        "dcat:theme": [],
        "dcat:keyword": [
            "Skill discovery",
            "Unsupervised learning",
            "Diversity"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1907.01657",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Andrychowicz",
            "F. Wolski",
            "A. Ray",
            "J. Schneider",
            "R. Fong",
            "P. Welinder",
            "B. McGrew",
            "J. Tobin",
            "O. P. Abbeel",
            "W. Zaremba"
        ],
        "dcterms:description": "Hindsight Experience Replay (HER) is a technique that allows agents to learn from past experiences by reinterpreting them with different goals.",
        "dcterms:title": "HER (Hindsight Experience Replay)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Experience replay",
            "Reinforcement learning",
            "Goal-oriented learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Kumar",
            "Y. Tsvetkov"
        ],
        "dcterms:description": "The von Mises-Fisher distribution (vMF) is used in training sequence-to-sequence models with continuous outputs.",
        "dcterms:title": "vMF (von Mises-Fisher distribution)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1812.04616",
        "dcat:theme": [],
        "dcat:keyword": [
            "Probability distribution",
            "Machine learning",
            "Continuous outputs"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1812.04616",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]