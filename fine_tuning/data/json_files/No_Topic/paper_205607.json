[
    {
        "dcterms:creator": [
            "J. Barker",
            "R. Marxer",
            "E. Vincent",
            "S. Watanabe"
        ],
        "dcterms:description": "The ChiME3 corpus is used for audio-visual speech processing, specifically for modeling human performance in noisy environments by integrating audio and visual cues.",
        "dcterms:title": "ChiME3 corpus",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Recognition",
            "Audio-Visual Processing"
        ],
        "dcat:keyword": [
            "Speech separation",
            "Speech recognition",
            "Audio-visual cues",
            "Noisy environments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Enhancement",
            "Speech Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "M. Cooke",
            "J. Barker",
            "S. Cunningham",
            "X. Shao"
        ],
        "dcterms:description": "The Grid videos dataset is an audio-visual corpus designed for speech perception and automatic speech recognition, providing a controlled environment for studying the integration of visual and auditory information.",
        "dcterms:title": "Grid videos",
        "dcterms:issued": "2006",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Perception",
            "Automatic Speech Recognition"
        ],
        "dcat:keyword": [
            "Audio-visual corpus",
            "Speech perception",
            "Automatic speech recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Speech Recognition",
            "Speech Perception"
        ]
    }
]