[
    {
        "dcterms:creator": [],
        "dcterms:description": "A large-scale SC dataset made up of 289,148 ESL SC questions from real-world standardized English examinations.",
        "dcterms:title": "SC-Ques",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/ai4ed/SC-Ques",
        "dcat:theme": [],
        "dcat:keyword": [
            "Sentence Completion",
            "ESL",
            "Language Learning",
            "Educational Technology"
        ],
        "dcat:landingPage": "https://github.com/ai4ed/SC-Ques",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "G. Zweig",
            "C. J. Burges"
        ],
        "dcterms:description": "The Microsoft Research Sentence Completion Challenge dataset consisting of 1,040 sentences with four impostor sentences for each original sentence.",
        "dcterms:title": "MSR SC dataset",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Sentence Completion",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "M. Chen",
            "M. Dâ€™Arcy",
            "A. Liu",
            "J. Fernandez",
            "D. Downey"
        ],
        "dcterms:description": "CODAH is an adversarially-authored question answering dataset for common sense, involving multiple choice SC questions.",
        "dcterms:title": "CODAH",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Common Sense",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "H. Park",
            "J. Park"
        ],
        "dcterms:description": "TOPIK dataset consists of 1823 cloze-style questions with multiple-choice questions from the Test of Proficiency in Korean.",
        "dcterms:title": "TOPIK",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Language Assessment",
            "Cloze Test"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "D. Nozza",
            "F. Bianchi",
            "D. Hovy"
        ],
        "dcterms:description": "Hurtful SC dataset is a manually labeled dataset measuring hurtful sentence completion in language models, available in six languages.",
        "dcterms:title": "Hurtful SC dataset",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Hurtful Language",
            "Sentence Completion"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    }
]