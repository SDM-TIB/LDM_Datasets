[
    {
        "dcterms:creator": [
            "D. A. Hudson",
            "C. D. Manning"
        ],
        "dcterms:description": "The GQA dataset is a collection developed by the Stanford University Manning group, focusing on real-world visual reasoning and question answering. It aims to address language bias issues present in previous VQA datasets by employing a more intricate question generation approach, emphasizing reasoning capabilities and compositional behavior of VQA models.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Visual reasoning",
            "Question answering",
            "Scene graph",
            "Compositional behavior"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "J. Johnson",
            "B. Hariharan",
            "L. Van Der Maaten",
            "L. Fei-Fei",
            "C. Lawrence Zitnick",
            "R. Girshick"
        ],
        "dcterms:description": "The CLEVR dataset is a synthetic diagnostic dataset widely employed for training and testing various visual-textual reasoning capabilities in VQA or VQA grounding task models. It consists of abstracted stereometric geometry and objects, requiring models to rely on visual reasoning to perform various VQA tasks.",
        "dcterms:title": "CLEVR",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Synthetic dataset",
            "Visual reasoning",
            "Question answering",
            "Geometry"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "C. Chen",
            "S. Anjum",
            "D. Gurari"
        ],
        "dcterms:description": "The VizWiz-Answering-Grounding dataset is introduced to facilitate visual question answering tasks for individuals with visual impairments. It collects data directly from real users with visual impairments, ensuring a more accurate reflection of their genuine needs and providing ground truth grounding information for objects referred to in the question text.",
        "dcterms:title": "VizWiz-Answering-Grounding",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Accessibility"
        ],
        "dcat:keyword": [
            "Visual impairments",
            "Grounding",
            "Real-world data",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Grounding"
        ]
    }
]