[
    {
        "dcterms:creator": [
            "P. Frazier",
            "D. Kempe",
            "J. Kleinberg",
            "R. Kleinberg"
        ],
        "dcterms:description": "The KCMAB dataset is used to study the Known-Compensation Multi-Arm Bandit problem, where a system controller incentivizes players to explore different arms by providing compensation.",
        "dcterms:title": "KCMAB (Known-Compensation Multi-Arm Bandit)",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Incentivized Learning"
        ],
        "dcat:keyword": [
            "Compensation",
            "Exploration",
            "Multi-Armed Bandit",
            "Regret Minimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Regret Minimization",
            "Exploration Strategy"
        ]
    },
    {
        "dcterms:creator": [
            "T. L. Lai",
            "H. Robbins"
        ],
        "dcterms:description": "The Bernoulli Bandits dataset is used to analyze adaptive allocation rules that are asymptotically efficient.",
        "dcterms:title": "Bernoulli Bandits",
        "dcterms:issued": "1985",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Adaptive Allocation"
        ],
        "dcat:keyword": [
            "Bernoulli Distribution",
            "Adaptive Allocation",
            "Asymptotic Efficiency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Adaptive Allocation"
        ]
    },
    {
        "dcterms:creator": [
            "D. A. Berry",
            "B. Fristedt"
        ],
        "dcterms:description": "The Multi-armed Bandit (MAB) dataset is foundational for studying the sequential allocation of experiments.",
        "dcterms:title": "Multi-armed Bandit (MAB)",
        "dcterms:issued": "1985",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Sequential Decision Making"
        ],
        "dcat:keyword": [
            "Sequential Allocation",
            "Experiments",
            "Decision Making"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sequential Decision Making"
        ]
    },
    {
        "dcterms:creator": [
            "W. R. Thompson"
        ],
        "dcterms:description": "The Thompson Sampling dataset is used to study the likelihood that one unknown probability exceeds another based on evidence from samples.",
        "dcterms:title": "Thompson Sampling",
        "dcterms:issued": "1933",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Bayesian Methods"
        ],
        "dcat:keyword": [
            "Thompson Sampling",
            "Bayesian Inference",
            "Probability"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Bayesian Decision Making"
        ]
    },
    {
        "dcterms:creator": [
            "P. Auer",
            "N. Cesa-Bianchi",
            "P. Fischer"
        ],
        "dcterms:description": "The UCB (Upper Confidence Bound) dataset is used to analyze the finite-time performance of the multi-armed bandit problem.",
        "dcterms:title": "UCB (Upper Confidence Bound)",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Confidence Bounds"
        ],
        "dcat:keyword": [
            "Upper Confidence Bound",
            "Finite-Time Analysis",
            "Multi-Armed Bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Regret Minimization"
        ]
    },
    {
        "dcterms:creator": [
            "C. J. C. H. Watkins"
        ],
        "dcterms:description": "The ε-greedy dataset is used to study learning from delayed rewards in reinforcement learning.",
        "dcterms:title": "ε-greedy",
        "dcterms:issued": "1989",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Exploration Strategies"
        ],
        "dcat:keyword": [
            "ε-greedy",
            "Delayed Rewards",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration Strategy"
        ]
    },
    {
        "dcterms:creator": [
            "P. Auer",
            "N. Cesa-Bianchi",
            "Y. Freund",
            "R. E. Schapire"
        ],
        "dcterms:description": "The Contextual Bandits dataset is used to analyze the non-stochastic multi-armed bandit problem where context influences decision making.",
        "dcterms:title": "Contextual Bandits",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Contextual Decision Making"
        ],
        "dcat:keyword": [
            "Contextual Bandits",
            "Decision Making",
            "Contextual Information"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Contextual Decision Making"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Papanastasiou",
            "K. Bimpikis",
            "N. Savva"
        ],
        "dcterms:description": "The Crowdsourcing Exploration dataset is used to study the dynamics of exploration in crowdsourcing environments.",
        "dcterms:title": "Crowdsourcing Exploration",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Crowdsourcing",
            "Exploration Strategies"
        ],
        "dcat:keyword": [
            "Crowdsourcing",
            "Exploration",
            "Incentives"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration Strategy"
        ]
    },
    {
        "dcterms:creator": [
            "R. Combes",
            "C. Jiang",
            "R. Srikant"
        ],
        "dcterms:description": "The Budgeted MAB dataset is used to analyze multi-armed bandit problems with budget constraints.",
        "dcterms:title": "Budgeted MAB",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Armed Bandits",
            "Budget Constraints"
        ],
        "dcat:keyword": [
            "Budgeted Bandits",
            "Regret Minimization",
            "Multi-Armed Bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Budget-Constrained Decision Making"
        ]
    }
]