[
    {
        "dcterms:creator": [
            "Rishi Bommasani",
            "Kevin Klyman",
            "Daniel Zhang",
            "Percy Liang"
        ],
        "dcterms:description": "Foundation models are trained on broad data using self-supervision at scale and can be adapted to a wide range of downstream tasks.",
        "dcterms:title": "Foundation Models",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Foundation models",
            "Self-supervised learning",
            "Transfer learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Scott Reed",
            "Konrad Zolna",
            "Emilio Parisotto",
            "Sergio Gómez Colmenarejo",
            "Alexander Novikov",
            "Gabriel Barthmaron",
            "Mai Giménez",
            "Yury Sulsky",
            "Jackie Kay",
            "Jost Tobias Springenberg",
            "Tom Eccles",
            "Jake Bruce",
            "Ali Razavi",
            "Ashley Edwards",
            "Nicolas Heess",
            "Yutian Chen",
            "Raia Hadsell",
            "Oriol Vinyals",
            "Mahyar Bordbar",
            "Nando de Freitas"
        ],
        "dcterms:description": "Gato is a generalist agent capable of performing multiple tasks across different modalities.",
        "dcterms:title": "Gato",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Artificial Intelligence",
            "Multi-task Learning"
        ],
        "dcat:keyword": [
            "Generalist agent",
            "Multi-task learning",
            "AI agent"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT is a model designed for pre-training deep bidirectional transformers for language understanding.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Transformers",
            "Language model",
            "Bidirectional"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text classification",
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "E.A.M. van Dis",
            "J. Bollen",
            "W. Zuidema",
            "R. van Rooij",
            "C.L. Bockting"
        ],
        "dcterms:description": "ChatGPT is a conversational AI model that has been prioritized for research in various domains.",
        "dcterms:title": "ChatGPT",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Artificial Intelligence",
            "Conversational Agents"
        ],
        "dcat:keyword": [
            "Conversational AI",
            "Language model",
            "Chatbot"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Conversational interaction",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Nichol",
            "Aditya Ramesh",
            "Pamela Mishkin",
            "Prafulla Dariwal",
            "Joanne Jang",
            "Mark Chen"
        ],
        "dcterms:description": "DALL-E is a model that generates images from textual descriptions, showcasing capabilities in image synthesis.",
        "dcterms:title": "DALL-E",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Generative Models"
        ],
        "dcat:keyword": [
            "Image generation",
            "Text-to-image",
            "Generative AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image synthesis",
            "Visual content generation"
        ]
    },
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "Marie-Anne Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar",
            "Aurelien Rodriguez",
            "Armand Joulin",
            "Edouard Grave",
            "Guillaume Lample"
        ],
        "dcterms:description": "LLaMA is an open and efficient foundation language model designed for various language tasks.",
        "dcterms:title": "LLAMA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Foundation model",
            "Language understanding",
            "Open-source"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "Robin Rombach",
            "Andreas Blattmann",
            "Dominik Lorenz",
            "Patrick Esser",
            "Björn Ommer"
        ],
        "dcterms:description": "Stable Diffusion is a model for high-resolution image synthesis using latent diffusion techniques.",
        "dcterms:title": "Stable Diffusion",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Generative Models"
        ],
        "dcat:keyword": [
            "Image synthesis",
            "Diffusion models",
            "Latent space"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image generation",
            "Visual content creation"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "GPT-4 is a state-of-the-art language model that demonstrates advanced capabilities in text generation and understanding.",
        "dcterms:title": "GPT-4",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Large language model",
            "Text generation",
            "AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Language understanding"
        ]
    }
]