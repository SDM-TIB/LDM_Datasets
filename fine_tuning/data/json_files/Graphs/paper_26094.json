[
    {
        "dcterms:creator": [
            "S. C. Liew",
            "C. Kai",
            "H. C. Leung",
            "P. Wong"
        ],
        "dcterms:description": "The BoE throughput corresponds to a value that allows us to adopt shortcuts in performance evaluation and bypass complicated stochastic analysis.",
        "dcterms:title": "BoE throughput",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Wireless Networks",
            "Throughput Analysis"
        ],
        "dcat:keyword": [
            "Throughput",
            "CSMA",
            "Wireless Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Monderer",
            "L. S. Shapley"
        ],
        "dcterms:description": "SAP is a method for collecting training data based on potential game theory that determines the action to reach the Nash equilibrium wherein the payoff function is the highest.",
        "dcterms:title": "SAP (as a method for data collection)",
        "dcterms:issued": "1996",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Data Collection"
        ],
        "dcat:keyword": [
            "Potential Games",
            "Nash Equilibrium",
            "Data Collection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Yamamoto"
        ],
        "dcterms:description": "SAP is a method for collecting training data based on potential game theory that determines the action to reach the Nash equilibrium wherein the payoff function is the highest.",
        "dcterms:title": "SAP (as a method for data collection)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Data Collection"
        ],
        "dcat:keyword": [
            "Potential Games",
            "Nash Equilibrium",
            "Data Collection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "H. Van Hasselt",
            "A. Guez",
            "D. Silver"
        ],
        "dcterms:description": "DDQN is a deep reinforcement learning method that avoids overestimations by using two networks: a main network to select actions and a target network to evaluate those actions.",
        "dcterms:title": "DDQN (as a method)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Deep Reinforcement Learning",
            "Double Q-Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. S. Sutton",
            "A. G. Barto"
        ],
        "dcterms:description": "ǫ-greedy is a method for collecting training data that randomly selects an action with probability ǫ and selects the greedy action with probability 1 - ǫ.",
        "dcterms:title": "ǫ-greedy (as a method)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Exploration",
            "Exploitation",
            "Action Selection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Z. Wang",
            "T. Schaul",
            "M. Hessel",
            "H. Van Hasselt",
            "M. Lanctot",
            "N. De Freitas"
        ],
        "dcterms:description": "Dueling Network is a method that can learn which states are valuable without learning the effect of each action for each state, using two streams to estimate state-value and advantages.",
        "dcterms:title": "Dueling Network (as a method)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "State-Value Estimation",
            "Advantage Estimation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Schaul",
            "J. Quan",
            "I. Antonoglou",
            "D. Silver"
        ],
        "dcterms:description": "Prioritized Experience Replay is a method that allocates priority to all samples based on the TD errors, allowing the agent to learn from more important experiences.",
        "dcterms:title": "Prioritized Experience Replay (as a method)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Experience Replay",
            "TD Error",
            "Sample Prioritization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]