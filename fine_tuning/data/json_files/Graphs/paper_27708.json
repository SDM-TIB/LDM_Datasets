[
    {
        "dcterms:creator": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BERT is a large language model that uses deep bidirectional transformers for language understanding, with 340 million parameters.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/n19-1423",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Transformers",
            "Bidirectional"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/n19-1423",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jeff Wu",
            "Rewon Child",
            "David Luan",
            "Dario Amodei",
            "Ilya Sutskever"
        ],
        "dcterms:description": "GPT-2 is a large language model that is capable of generating coherent and contextually relevant text based on a given prompt, with 1.5 billion parameters.",
        "dcterms:title": "GPT-2",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Text generation",
            "Unsupervised learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "GPT-3 is a state-of-the-art language model developed by OpenAI, known for its ability to generate human-like text and perform various language tasks with 175 billion parameters.",
        "dcterms:title": "GPT-3",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://lambdalabs.com/blog/demystifying-gpt-3/",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Text generation",
            "Few-shot learning"
        ],
        "dcat:landingPage": "https://lambdalabs.com/blog/demystifying-gpt-3/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "NVIDIA BERT is a version of the BERT model optimized for performance on NVIDIA GPUs, providing implementations and examples for deep learning applications.",
        "dcterms:title": "NVIDIA BERT",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/NVIDIA/DeepLearningExamples",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "BERT",
            "NVIDIA",
            "Deep Learning"
        ],
        "dcat:landingPage": "https://github.com/NVIDIA/DeepLearningExamples",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    }
]