[
    {
        "dcterms:creator": [
            "Zhenfang Chen",
            "Lin Ma",
            "Wenhan Luo",
            "Kwan-Yee K Wong"
        ],
        "dcterms:description": "VID-Sentence is a dataset that includes 7,654 trimmed videos with annotated language captions, re-labeled based on the ImageNet video object detection dataset. It contains 6,582 instance pairs for training and 536 pairs for testing.",
        "dcterms:title": "VID-Sentence",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1906.02549",
        "dcat:theme": [
            "Video Understanding",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Language captions",
            "Referring expression comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Referring Expression Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Zhenyang Li",
            "Ran Tao",
            "Efstratios Gavves",
            "Cees GM Snoek",
            "Arnold WM Smeulders"
        ],
        "dcterms:description": "Lingual OTB99 is a dataset built based on the popular object tracking dataset OTB100, containing 100 videos. It is split into 51 instances for training and 48 instances for testing.",
        "dcterms:title": "Lingual OTB99",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Tracking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Object tracking",
            "Natural language specification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Object Tracking"
        ]
    },
    {
        "dcterms:creator": [
            "Licheng Yu",
            "Patrick Poirson",
            "Shan Yang",
            "Alexander C Berg",
            "Tamara L Berg"
        ],
        "dcterms:description": "RefCOCO consists of 142,210 referring expressions for 50,000 objects in 19,994 images, with an average length of 3.5 words per expression. It is split into train, validation, testA, and testB sets.",
        "dcterms:title": "RefCOCO",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Understanding",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Referring expressions",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Referring Expression Comprehension"
        ]
    }
]