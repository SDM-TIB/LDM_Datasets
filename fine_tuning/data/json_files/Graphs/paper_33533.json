[
    {
        "dcterms:creator": [
            "Y. Bai",
            "A. Jones",
            "K. Ndousse"
        ],
        "dcterms:description": "A dataset used for training a reward model to align language models with human preferences, containing preference strength information.",
        "dcterms:title": "Anthropic’s HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Reward Model",
            "Human Preferences",
            "Language Models"
        ],
        "dcat:landingPage": "https://github.com/OpenLMLab/MOSS-RLHF",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reward Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "N. Stiennon",
            "L. Ouyang",
            "J. Wu"
        ],
        "dcterms:description": "A dataset used for training models to summarize text based on human feedback.",
        "dcterms:title": "OpenAI’s summarization",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.01325",
        "dcat:theme": [
            "Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Summarization",
            "Human Feedback",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "M. Völske",
            "M. Potthast",
            "S. Syed"
        ],
        "dcterms:description": "A dataset mined from Reddit to learn automatic summarization.",
        "dcterms:title": "Reddit TL;DR",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Summarization",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reddit",
            "Summarization",
            "Automatic Summarization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Köpf",
            "Y. Kilcher",
            "D. von Rütte"
        ],
        "dcterms:description": "A dataset of conversations aimed at democratizing large language model alignment.",
        "dcterms:title": "Oasst1",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.07327",
        "dcat:theme": [
            "Conversational AI",
            "Language Model Alignment"
        ],
        "dcat:keyword": [
            "Conversations",
            "Language Models",
            "AI Alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "S. Casper",
            "X. Davies",
            "C. Shi"
        ],
        "dcterms:description": "A dataset addressing open problems and limitations in reinforcement learning from human feedback.",
        "dcterms:title": "PKU-SafeRLHF",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.15217",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Safety",
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety in AI"
        ]
    }
]