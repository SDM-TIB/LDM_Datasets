To extract datasets from the research paper titled "Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training" by Dezhao Luo et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experimental sections** of the paper to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on two benchmark datasets for video moment retrieval, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on **section 4.1 (Experimental Settings)**, where the authors describe the datasets used for their experiments. Here, they mention:

1. **Charades-STA Dataset**: This dataset is adapted from the original Charades dataset for video moment retrieval tasks. It contains 6,670 videos and 16,124 queries, with specific training and testing splits.

2. **ActivityNet-Captions Dataset**: This dataset is used for dense video captioning and consists of 19,811 videos with 71,957 queries. It is noted for its diverse content and includes specific training and validation splits.

In the **References section**, I will look for full citations for these datasets to ensure proper attribution. The citations are as follows:

- For **Charades-STA Dataset**:
  > Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. *Tall: Temporal activity localization via language query*. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5267–5275, 2017.

- For **ActivityNet-Captions Dataset**:
  > Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. *Activitynet: A large-scale video benchmark for human activity understanding*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 961–970, 2015.

Now that I have identified the datasets and their citations, I will summarize the findings in a structured manner, ensuring that each dataset is clearly described along with its full citation for proper referencing.