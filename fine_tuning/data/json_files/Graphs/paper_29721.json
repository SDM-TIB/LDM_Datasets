[
    {
        "dcterms:creator": [
            "T. Y. Lin",
            "M. Maire",
            "S. Belongie",
            "J. Hays",
            "P. Perona",
            "D. Ramanan",
            "P. Dollár",
            "C. L. Zitnick"
        ],
        "dcterms:description": "The Microsoft Common Objects in Context dataset is a medium-scale object detection dataset with about 900k bounding box annotations for 80 object categories.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Detection"
        ],
        "dcat:keyword": [
            "Bounding boxes",
            "Object categories",
            "Image annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection"
        ]
    },
    {
        "dcterms:creator": [
            "B. A. Plummer",
            "L. Wang",
            "C. M. Cervantes",
            "J. C. Caicedo",
            "J. Hockenmaier",
            "S. Lazebnik"
        ],
        "dcterms:description": "Flickr30k entities dataset provides region-to-phrase correspondences for richer image-to-sentence models.",
        "dcterms:title": "Flickr30k-entities",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Segmentation",
            "Phrase Grounding"
        ],
        "dcat:keyword": [
            "Region annotations",
            "Phrase correspondences",
            "Image-to-sentence models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Phrase Grounding"
        ]
    },
    {
        "dcterms:creator": [
            "L. H. Li",
            "P. Zhang",
            "H. Zhang",
            "J. Yang",
            "C. Li",
            "Y. Zhong",
            "L. Wang",
            "L. Yuan",
            "L. Zhang",
            "J. N. Hwang"
        ],
        "dcterms:description": "ODinW is a benchmark for object detection in the wild, utilizing multiple datasets to evaluate performance.",
        "dcterms:title": "ODinW",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Detection"
        ],
        "dcat:keyword": [
            "Object detection",
            "Benchmark",
            "Wild datasets"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection"
        ]
    },
    {
        "dcterms:creator": [
            "A. Gupta",
            "P. Dollar",
            "R. Girshick"
        ],
        "dcterms:description": "LVIS is a dataset for large vocabulary instance segmentation, featuring a long-tail distribution of object categories.",
        "dcterms:title": "LVIS",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instance Segmentation"
        ],
        "dcat:keyword": [
            "Instance segmentation",
            "Long-tail distribution",
            "Object categories"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Instance Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "C. Wu",
            "Z. Lin",
            "S. Cohen",
            "T. Bui",
            "S. Maji"
        ],
        "dcterms:description": "PhraseCut is a dataset for language-based image segmentation in the wild, focusing on segmenting images based on natural language phrases.",
        "dcterms:title": "PhraseCut",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Segmentation"
        ],
        "dcat:keyword": [
            "Language-based segmentation",
            "Image segmentation",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Goyal",
            "T. Khot",
            "D. Summers-Stay",
            "D. Batra",
            "D. Parikh"
        ],
        "dcterms:description": "VQA requires the model to predict an answer given an image and a question, facilitating visual question answering tasks.",
        "dcterms:title": "VQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Visual understanding",
            "Image and question pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "X. Chen",
            "H. Fang",
            "T. Y. Lin",
            "R. Vedantam",
            "S. Gupta",
            "P. Dollár",
            "C. L. Zitnick"
        ],
        "dcterms:description": "COCO Captioning dataset aims to generate natural language descriptions for images, providing a benchmark for image captioning tasks.",
        "dcterms:title": "COCO Captioning",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image descriptions",
            "Natural language generation",
            "Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "S. Shao",
            "Z. Li",
            "T. Zhang",
            "C. Peng",
            "G. Yu",
            "X. Zhang",
            "J. Li",
            "J. Sun"
        ],
        "dcterms:description": "Objects365 is a large-scale, high-quality dataset for object detection, providing a diverse set of images and annotations.",
        "dcterms:title": "Objects365",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Detection"
        ],
        "dcat:keyword": [
            "Large-scale dataset",
            "Object detection",
            "High-quality annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection"
        ]
    }
]