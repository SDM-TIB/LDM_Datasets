To extract datasets from the research paper titled "Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding" by Shentong Mo et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract provides a brief overview of the research, and the introduction typically outlines the context and significance of the work, which may include references to datasets.

In the **introduction**, the authors discuss the challenges of query-based video grounding and mention the need for datasets that support this research. However, I need to look for specific dataset names and details.

Next, I will focus on **section 4.1 (Datasets and Evaluation Metrics)**, where the authors explicitly mention two datasets used in their experiments:

1. **Charades-STA Dataset**: This dataset is built on the Charades dataset and contains 6,672 videos of indoor activities, with 16,128 query-video pairs. It specifies that there are 12,408 pairs for training and 3,720 for testing, with an average video duration of 29.76 seconds.

2. **ActivityNet-Caption Dataset**: This dataset consists of 20,000 videos and 100,000 queries, with 37,421 query-video pairs for training and 34,536 for testing. The average duration of the videos is 1 minute and 50 seconds, and each video has an average of 3.65 annotated moments.

To ensure I have the correct citations for these datasets, I will refer to the **References section** of the paper:

- For the **Charades-STA Dataset**, the citation is:
  > Sigurdsson, G. A., Varol, G., Wang, X., Farhadi, A., Laptev, I., & Gupta, A. (2016). Hollywood in homes: Crowdsourcing data collection for activity understanding. In *European Conference on Computer Vision (ECCV)*, pages 510–526.

- For the **ActivityNet-Caption Dataset**, the citation is:
  > Krishna, R., Hata, K., Ren, F., Fei-Fei, L., & Niebles, J. C. (2017). Dense-captioning events in videos. In *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, pages 706–715.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further use or processing.