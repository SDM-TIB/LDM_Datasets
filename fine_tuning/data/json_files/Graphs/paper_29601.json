[
    {
        "dcterms:creator": [
            "T. Kwiatkowski",
            "J. Palomaki",
            "O. Redfield",
            "M. Collins",
            "A. Parikh",
            "C. Alberti",
            "D. Epstein",
            "I. Polosukhin",
            "J. Devlin",
            "K. Lee",
            "K. Toutanova",
            "L. Jones",
            "M. Kelcey",
            "M.-W. Chang",
            "A. M. Dai",
            "J. Uszkoreit",
            "Q. Le",
            "S. Petrov"
        ],
        "dcterms:description": "A benchmark for question answering research that includes a diverse set of questions and answers derived from real user queries.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "A large scale distantly supervised challenge dataset for reading comprehension, containing questions and answers from Wikipedia.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Distant Supervision",
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Berant",
            "A. Chou",
            "R. Frostig",
            "P. Liang"
        ],
        "dcterms:description": "A dataset for semantic parsing on Freebase from question-answer pairs, designed to evaluate the ability to understand and generate structured data.",
        "dcterms:title": "WebQuestions",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Semantic Parsing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Semantic Parsing",
            "Question Answering",
            "Freebase"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Parsing",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "P. Baudiš",
            "J. Šedivý"
        ],
        "dcterms:description": "A dataset modeling the question answering task, used for evaluating systems in cross-language evaluation.",
        "dcterms:title": "CuratedTrec",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Cross-Language Evaluation"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Cross-Language Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "P. Rajpurkar",
            "J. Zhang",
            "K. Lopyrev",
            "P. Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text, designed to evaluate reading comprehension systems.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Machine Comprehension",
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "C. Sciavolino",
            "Z. Zhong",
            "J. Lee",
            "D. Chen"
        ],
        "dcterms:description": "A dataset that challenges dense retrievers with simple entity-centric questions, aimed at evaluating retrieval performance.",
        "dcterms:title": "EntityQuestions",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Entity-Centric Questions",
            "Dense Retrieval",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]