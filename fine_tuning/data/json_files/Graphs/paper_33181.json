[
    {
        "dcterms:creator": [
            "Jinxing Zhou",
            "Jianyuan Wang",
            "Jiayi Zhang",
            "Weixuan Sun",
            "Jing Zhang",
            "Stan Birchfield",
            "Dan Guo",
            "Lingpeng Kong",
            "Meng Wang",
            "Yiran Zhong"
        ],
        "dcterms:description": "AVSBench is a benchmark dataset for audio-visual segmentation that includes pixel-level annotations for each 5-second video, indicating the pixels of sounding objects for the corresponding audio.",
        "dcterms:title": "AVSBench",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Segmentation"
        ],
        "dcat:keyword": [
            "Audio-visual segmentation",
            "Pixel-level annotations",
            "Sound localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Audio-visual segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Arda Senocak",
            "Tae-Hyun Oh",
            "Junsik Kim",
            "Ming-Hsuan Yang",
            "In So Kweon"
        ],
        "dcterms:description": "Attention10k is a dataset designed for learning to localize sound sources in visual scenes, utilizing a two-stream architecture based on an attention mechanism.",
        "dcterms:title": "Attention10k",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sound Localization"
        ],
        "dcat:keyword": [
            "Sound source localization",
            "Attention mechanism",
            "Visual scenes"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sound localization"
        ]
    },
    {
        "dcterms:creator": [
            "Shentong Mo",
            "Pedro Morgado"
        ],
        "dcterms:description": "EZVSL is a dataset that focuses on localizing visual sounds using a multiple-instance contrastive learning objective to learn the alignment across regions with the most corresponding audio.",
        "dcterms:title": "EZVSL",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Sound Localization"
        ],
        "dcat:keyword": [
            "Visual sounds",
            "Contrastive learning",
            "Sound localization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual sound localization"
        ]
    },
    {
        "dcterms:creator": [
            "Jinheng Xie",
            "Jianfeng Xiang",
            "Junliang Chen",
            "Xianxu Hou",
            "Xiaodong Zhao",
            "Linlin Shen"
        ],
        "dcterms:description": "C2AM is a dataset that leverages contrastive learning of class-agnostic activation maps for weakly supervised object localization and semantic segmentation.",
        "dcterms:title": "C2AM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Weakly Supervised Learning"
        ],
        "dcat:keyword": [
            "Weakly supervised localization",
            "Semantic segmentation",
            "Contrastive learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Object localization",
            "Semantic segmentation"
        ]
    }
]