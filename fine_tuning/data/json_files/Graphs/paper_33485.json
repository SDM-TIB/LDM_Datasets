[
    {
        "dcterms:creator": [
            "J. Ferguson",
            "M. Gardner",
            "H. Hajishirzi",
            "T. Khot",
            "P. Dasigi"
        ],
        "dcterms:description": "The IIRC dataset contains incomplete information reading comprehension questions, which are used to evaluate the performance of models in handling questions with missing context.",
        "dcterms:title": "IIRC",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Incomplete information",
            "Reading comprehension",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Nguyen",
            "M. Rosenberg",
            "X. Song",
            "J. Gao",
            "S. Tiwary",
            "R. Majumder",
            "L. Deng"
        ],
        "dcterms:description": "MS MARCO is a human-generated machine reading comprehension dataset designed for training and evaluating models on open-domain question answering tasks.",
        "dcterms:title": "MSMARCO",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Reading Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Human-generated",
            "Machine reading comprehension",
            "Open-domain QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Kwiatkowski",
            "J. Palomaki",
            "O. Redfield",
            "M. Collins",
            "A. Parikh",
            "C. Alberti",
            "D. Epstein",
            "I. Polosukhin",
            "J. Devlin",
            "K. Lee"
        ],
        "dcterms:description": "The Natural Questions dataset is a benchmark for question answering research, containing real user questions and corresponding answers from Wikipedia.",
        "dcterms:title": "Natural Question (NQ)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Question answering",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. S. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "Trivia QA is a large-scale dataset for reading comprehension that is distantly supervised, providing a challenge for models to answer trivia questions.",
        "dcterms:title": "Trivia QA (TQ)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Trivia questions",
            "Distant supervision",
            "Reading comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Nguyen",
            "M. Rosenberg",
            "X. Song",
            "J. Gao",
            "S. Tiwary",
            "R. Majumder",
            "L. Deng"
        ],
        "dcterms:description": "MSMARCO Passage Ranking is a subset of the MS MARCO dataset focused on passage ranking tasks for question answering.",
        "dcterms:title": "MSMARCO Passage Ranking (MS-pas)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Passage Ranking",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Passage ranking",
            "Open-domain QA",
            "Machine reading comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Passage Ranking"
        ]
    }
]