[
    {
        "dcterms:creator": [
            "C. Schuhmann",
            "R. Vencu",
            "R. Beaumont",
            "R. Kaczmarczyk",
            "C. Mullis",
            "A. Katta",
            "T. Coombes",
            "J. Jitsev",
            "A. Komatsuzaki"
        ],
        "dcterms:description": "A large dataset containing 400 million image-text pairs filtered using CLIP.",
        "dcterms:title": "LAION-2B",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2111.02114",
        "dcat:theme": [
            "Image-Text Data"
        ],
        "dcat:keyword": [
            "Image-Text Pairs",
            "CLIP",
            "Large-scale Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "S.Y. Gadre",
            "G. Ilharco",
            "A. Fang",
            "J. Hayase",
            "G. Smyrnis",
            "T. Nguyen",
            "R. Marten",
            "M. Wortsman",
            "D. Ghosh",
            "J. Zhang"
        ],
        "dcterms:description": "A dataset aimed at exploring the next generation of multimodal datasets.",
        "dcterms:title": "DataComp-1B",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.14108",
        "dcat:theme": [
            "Multimodal Data"
        ],
        "dcat:keyword": [
            "Multimodal Dataset",
            "Data Exploration"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "K. Grauman",
            "A. Westbury",
            "E. Byrne",
            "Z. Chavis",
            "A. Furnari",
            "R. Girdhar",
            "J. Hamburger",
            "H. Jiang",
            "M. Liu",
            "X. Liu"
        ],
        "dcterms:description": "A dataset containing 3,000 hours of egocentric video for various tasks.",
        "dcterms:title": "Ego4D",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Data"
        ],
        "dcat:keyword": [
            "Egocentric Video",
            "Video Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "T.Y. Lin",
            "P. Dollár",
            "R. Girshick",
            "K. He",
            "B. Hariharan",
            "S. Belongie"
        ],
        "dcterms:description": "A dataset for image captioning that includes a large number of images with corresponding captions.",
        "dcterms:title": "COCO Captions",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1504.00325",
        "dcat:theme": [
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image Captions",
            "Image Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "B.A. Plummer",
            "L. Wang",
            "C.M. Cervantes",
            "J.C. Caicedo",
            "J. Hockenmaier",
            "S. Lazebnik"
        ],
        "dcterms:description": "A dataset that collects region-to-phrase correspondences for richer image-to-sentence models.",
        "dcterms:title": "Flickr30K",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image-Text Data"
        ],
        "dcat:keyword": [
            "Image Dataset",
            "Region-Phrase Correspondences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "A. Radford",
            "J.W. Kim",
            "C. Hallacy",
            "A. Ramesh",
            "G. Goh",
            "S. Agarwal",
            "G. Sastry",
            "A. Askell",
            "P. Mishkin",
            "J. Clark",
            "G. Krueger",
            "I. Sutskever"
        ],
        "dcterms:description": "A dataset used for training visual models from natural language supervision.",
        "dcterms:title": "OpenAI CLIP’s WIT",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Data"
        ],
        "dcat:keyword": [
            "Visual Models",
            "Natural Language Supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "C. Jia",
            "Y. Yang",
            "Y. Xia",
            "Y.T. Chen",
            "Z. Parekh",
            "H. Pham",
            "Q.V. Le",
            "Y. Sung",
            "Z. Li",
            "T. Duerig"
        ],
        "dcterms:description": "A dataset that scales up visual and vision-language representation learning with noisy text supervision.",
        "dcterms:title": "ALIGN",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Data"
        ],
        "dcat:keyword": [
            "Visual Representation",
            "Noisy Text Supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Du",
            "F. Wei",
            "Z. Zhang",
            "M. Shi",
            "Y. Gao",
            "G. Li"
        ],
        "dcterms:description": "A dataset for learning to prompt for open-vocabulary object detection with vision-language models.",
        "dcterms:title": "CLIP-400M",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Detection"
        ],
        "dcat:keyword": [
            "Open-Vocabulary Detection",
            "Vision-Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Open-Vocabulary Detection"
        ]
    }
]