To extract datasets from the research paper titled "An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models" by Yusheng Liao et al., I will follow a systematic approach to identify and document the datasets mentioned in the text.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets that are explicitly mentioned or described. The abstract indicates that the paper proposes a benchmark and mentions "three constructed test sets," which suggests that datasets are involved.

Next, I will focus on the **methodology section**, particularly **section 3 (Proposed Framework)**, where the authors describe the construction of the datasets. Here, they mention three specific datasets:

1. **MedQA**: This dataset is derived from the English test set of the Medical Licensing Examinations of the United States (USMLE). It consists of multiple-choice questions reformulated to include patient information.

2. **QMax**: This dataset includes materials purchased from USMLE-RX resources, covering various medical categories. The authors note that it contains a total of 11 categories, such as cardiology and gastroenterology.

3. **Sample Exam**: This dataset is sourced from USMLE practice materials and includes sample exam questions. The authors mention that they filtered out duplicated questions in the test set.

In the **experiments section**, the authors provide statistical information about these datasets, confirming their use in the evaluation of the proposed framework.

Now, I will consult the **References section** to retrieve full citations for these datasets. The citations for the datasets are as follows:

- For **MedQA**, the citation is:
  > Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David A. Sontag. *Large language models are few-shot clinical information extractors*. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1998–2022. Association for Computational Linguistics.

- For **QMax**, the citation is:
  > Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. *Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models*. PLOS Digital Health, 2(2):1–12, 2023.

- For **Sample Exam**, the citation is:
  > The dataset is sourced from USMLE practice materials as referenced in Kung et al. (2023).

With this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research paper, ready for further processing or review.