To extract datasets from the research paper titled "Attention Driven Fusion for Multi-Modal Emotion Recognition" by Darshana Priyasad et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reviewing the **abstract** and **introduction** sections. The abstract mentions that the proposed system is evaluated on the **IEMOCAP dataset**, which indicates that this dataset is crucial for the experiments conducted in the paper.

Next, I will look into the **experiments section** (Section 3) to gather more details about the dataset. In this section, the authors describe the **IEMOCAP dataset** in detail, including its composition and the emotions it encompasses. They mention that the dataset includes five sessions of utterances from 10 unique speakers, annotated with four basic emotions: "anger," "happiness," "neutral," and "sadness." They also provide specific counts for each emotion category, which is important for understanding the dataset's structure.

Now, I will check the **references section** to find the full citation for the IEMOCAP dataset. The relevant citation is:

- **IEMOCAP Dataset**:
  > Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J. N., Lee, S., & Narayanan, S. S. (2008). *IEMOCAP: Interactive emotional dyadic motion capture database*. Language Resources and Evaluation, 42(4), 335.

Since the paper primarily focuses on the IEMOCAP dataset for its experiments, I will ensure to highlight this dataset in my final output.

After gathering all the necessary information, I will compile the dataset entry, ensuring that the full citation is included and that the description accurately reflects the details provided in the paper. This will ensure that the dataset extraction is thorough and well-documented.