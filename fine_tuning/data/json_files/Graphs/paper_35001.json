[
    {
        "dcterms:creator": [
            "M. Defferrard",
            "K. Benzi",
            "P. Vandergheynst",
            "X. Bresson"
        ],
        "dcterms:description": "The Free Music Archive (FMA) dataset is used as a self-supervised dataset, specifically the medium subset containing 25000 clips of 30 seconds of audio.",
        "dcterms:title": "The Free Music Archive (FMA)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Analysis",
            "Self-Supervised Learning"
        ],
        "dcat:keyword": [
            "Audio dataset",
            "Self-supervised learning",
            "Music clips"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Music Information Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "E. Law",
            "K. West",
            "M. I. Mandel",
            "M. Bay",
            "J. S. Downie"
        ],
        "dcterms:description": "MagnaTagATune (MTAT) is used as labeled data for automatic tagging and evaluation of general music understanding.",
        "dcterms:title": "MagnaTagATune (MTAT)",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Tagging",
            "Music Understanding"
        ],
        "dcat:keyword": [
            "Labeled dataset",
            "Music tagging",
            "Evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Automatic Tagging",
            "Music Information Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "D. Bogdanov",
            "M. Won",
            "P. Tovstogan"
        ],
        "dcterms:description": "MTG-Jamendo is used as another tagging dataset, including various tags such as genre, mood/theme, and instrument.",
        "dcterms:title": "MTG-Jamendo",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Tagging"
        ],
        "dcat:keyword": [
            "Tagging dataset",
            "Music genre",
            "Mood",
            "Instrument"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Automatic Tagging"
        ]
    },
    {
        "dcterms:creator": [
            "J. Engel",
            "C. Resnick",
            "A. Roberts",
            "S. Dieleman",
            "M. Norouzi",
            "D. Eck",
            "K. Simonyan"
        ],
        "dcterms:description": "NSynth is utilized for pitch and instrument classification of short snippets.",
        "dcterms:title": "NSynth",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Classification"
        ],
        "dcat:keyword": [
            "Pitch classification",
            "Instrument classification",
            "Audio dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Pitch Classification",
            "Instrument Classification"
        ]
    },
    {
        "dcterms:creator": [
            "R. M. Bittner",
            "J. Salamon",
            "M. Tierney",
            "M. Mauch",
            "C. Cannam",
            "J. P. Bello"
        ],
        "dcterms:description": "MedleyDB is used for instrument classification with longer audio clips than NSynth.",
        "dcterms:title": "MedleyDB",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Classification"
        ],
        "dcat:keyword": [
            "Multi-track dataset",
            "Instrument classification",
            "Audio dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Instrument Classification"
        ]
    },
    {
        "dcterms:creator": [
            "P. Knees",
            "Á. Faraldo Pérez",
            "H. Boyer",
            "R. Vogl",
            "S. Böck",
            "F. Hörschläger",
            "M. Le Goff"
        ],
        "dcterms:description": "Giantsteps is used as a key classification dataset.",
        "dcterms:title": "Giantsteps",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Classification"
        ],
        "dcat:keyword": [
            "Key classification",
            "Electronic dance music",
            "Audio dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Key Classification"
        ]
    },
    {
        "dcterms:creator": [
            "G. Tzanetakis",
            "P. Cook"
        ],
        "dcterms:description": "GTZAN is used for genre classification, specifically the fault-filtered version.",
        "dcterms:title": "GTZAN",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Classification"
        ],
        "dcat:keyword": [
            "Genre classification",
            "Audio dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Genre Classification"
        ]
    },
    {
        "dcterms:creator": [
            "J. Wilkins",
            "P. Seetharaman",
            "A. Wahl",
            "B. Pardo"
        ],
        "dcterms:description": "VocalSet is used for singer identification and technique classification.",
        "dcterms:title": "VocalSet",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Classification"
        ],
        "dcat:keyword": [
            "Singing voice dataset",
            "Singer identification",
            "Technique classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Singer Identification",
            "Technique Classification"
        ]
    },
    {
        "dcterms:creator": [
            "M. Soleymani",
            "M. N. Caro",
            "E. M. Schmidt",
            "C.-Y. Sha",
            "Y.-H. Yang"
        ],
        "dcterms:description": "EmoMusic is used for regressing Arousal and Valence as a downstream evaluation task.",
        "dcterms:title": "EmoMusic",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotional Analysis"
        ],
        "dcat:keyword": [
            "Emotional analysis",
            "Music dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Emotion Regression"
        ]
    }
]