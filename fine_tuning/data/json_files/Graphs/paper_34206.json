[
    {
        "dcterms:creator": [
            "J. Liu",
            "A. Shahroudy",
            "M. Perez",
            "G. Wang",
            "L. Duan",
            "A. C. Kot"
        ],
        "dcterms:description": "The NTU RGB+D dataset serves as a prominent benchmark for skeleton-based human action recognition. It comprises 56,880 skeletal action sequences, each performed by either one or two individuals, captured by three Microsoft Kinect-V2 depth sensors from varying horizontal perspectives.",
        "dcterms:title": "NTU RGB+D",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/CVPR.2016.115",
        "dcat:theme": [
            "Computer Vision",
            "Human Action Recognition"
        ],
        "dcat:keyword": [
            "Skeleton data",
            "Action recognition",
            "Kinect"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/CVPR.2016.115",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "J. Liu",
            "A. Shahroudy",
            "M. Perez",
            "G. Wang",
            "L. Duan",
            "A. C. Kot"
        ],
        "dcterms:description": "The NTU RGB+D 120 dataset extends the NTU RGB+D dataset by including an additional 57,367 skeleton sequences across 60 supplementary action classes, making it the largest available dataset with 3D joint annotations for human action recognition.",
        "dcterms:title": "NTU RGB+D 120",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/TPAMI.2019.2916873",
        "dcat:theme": [
            "Computer Vision",
            "Human Action Recognition"
        ],
        "dcat:keyword": [
            "Skeleton data",
            "Action recognition",
            "3D joint annotations"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/TPAMI.2019.2916873",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "J. Wang",
            "X. Nie",
            "Y. Xia",
            "Y. Wu",
            "S. C. Zhu"
        ],
        "dcterms:description": "The Northwestern-UCLA dataset is captured by three Kinect sensors from various viewpoints and comprises 1,494 video sequences across 10 action categories.",
        "dcterms:title": "Northwestern-UCLA",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Human Action Recognition"
        ],
        "dcat:keyword": [
            "Skeleton data",
            "Action recognition",
            "Kinect"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Action Recognition"
        ]
    }
]