To extract the datasets mentioned in the research paper titled "Facial Expression Restoration Based on Improved Graph Convolutional Networks" by Zhilei Liu et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** of the paper. The abstract mentions that the proposed method is evaluated on two benchmarks, which indicates that datasets are involved. 

Next, I will focus on **section 4.1 (Datasets and Settings)**, where the authors explicitly list the datasets used for their experiments. Here, they mention:

1. **BP4D Dataset**: This dataset is described as containing 100,760 frames for training and 45,809 frames for testing, with 12 AUs labeled for occurrence or absence. 

2. **DISFA Dataset**: This dataset consists of 87,209 frames for training and 43,605 frames for testing, with 8 AUs labeled similarly.

I will also verify the details provided in the **experiments section** to confirm that these datasets were indeed used for evaluation, which they are.

Now, I will check the **References section** to find the full citations for these datasets:

- For the **BP4D Dataset**, the citation is:
  > Mavadati, S.M., Mahoor, M.H., Bartlett, K., Trinh, P., Cohn, J.F. (2013). *DISFA: A spontaneous facial action intensity database*. IEEE Transactions on Affective Computing, 4(2), 151–160.

- For the **DISFA Dataset**, the citation is:
  > Zhang, X., Yin, L., Cohn, J.F., Canavan, S., Reale, M., Horowitz, A., Liu, P. (2013). *A high-resolution spontaneous 3D dynamic facial expression database*. In Proceedings of the 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), 1–6.

With this information, I will compile the dataset entries, ensuring that each dataset is accurately described and includes its full citation. This will provide a comprehensive overview of the datasets used in the research paper.