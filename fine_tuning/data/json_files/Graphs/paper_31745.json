[
    {
        "dcterms:creator": [
            "M. Abadi",
            "A. Agarwal",
            "P. Barham",
            "E. Brevdo",
            "Z. Chen",
            "C. Citro",
            "G. S. Corrado",
            "A. Davis",
            "J. Dean",
            "M. Devin",
            "S. Ghemawat",
            "I. Goodfellow",
            "A. Harp",
            "G. Irving",
            "M. Isard",
            "R. Jozefowicz",
            "Y. Jia",
            "L. Kaiser",
            "M. Kudlur",
            "J. Levenberg",
            "D. Mané",
            "M. Schuster",
            "R. Monga",
            "S. Moore",
            "D. Murray",
            "C. Olah",
            "J. Shlens",
            "B. Steiner",
            "I. Sutskever",
            "K. Talwar",
            "P. Tucker",
            "V. Vanhoucke",
            "V. Vasudevan",
            "F. Viégas",
            "O. Vinyals",
            "P. Warden",
            "M. Wattenberg",
            "M. Wicke",
            "Y. Yu",
            "X. Zheng"
        ],
        "dcterms:description": "TensorFlow is a large-scale machine learning framework that allows for the construction and training of deep learning models using a computation graph representation.",
        "dcterms:title": "TensorFlow",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tensorflow/tensorflow",
        "dcat:theme": [
            "Machine Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Machine learning framework",
            "Deep learning",
            "Computation graph"
        ],
        "dcat:landingPage": "https://github.com/tensorflow/tensorflow",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Paszke",
            "S. Gross",
            "F. Massa",
            "A. Lerer",
            "J. Bradbury",
            "G. Chanan",
            "T. Killeen",
            "Z. Lin",
            "N. Gimelshein",
            "L. Antiga",
            "A. Desmaison",
            "A. Kopf",
            "E. Yang",
            "Z. DeVito",
            "M. Raison",
            "A. Tejani",
            "S. Chilamkurthy",
            "B. Steiner",
            "L. Fang",
            "J. Bai",
            "S. Chintala"
        ],
        "dcterms:description": "PyTorch is an imperative style, high-performance deep learning library that provides tools for building and training neural networks.",
        "dcterms:title": "PyTorch",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Deep learning library",
            "Neural networks",
            "High-performance computing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Chen",
            "M. Li",
            "Y. Li",
            "M. Lin",
            "N. Wang",
            "M. Wang",
            "T. Xiao",
            "B. Xu",
            "C. Zhang",
            "Z. Zhang"
        ],
        "dcterms:description": "MXNet is a flexible and efficient machine learning library designed for heterogeneous distributed systems.",
        "dcterms:title": "MXNet",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Machine learning library",
            "Distributed systems",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Bai",
            "F. Lu",
            "K. Zhang"
        ],
        "dcterms:description": "ONNX is an open format for representing machine learning models, allowing models to be transferred between different frameworks.",
        "dcterms:title": "ONNX",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/onnx/onnx",
        "dcat:theme": [
            "Machine Learning",
            "Model Interoperability"
        ],
        "dcat:keyword": [
            "Model format",
            "Machine learning interoperability"
        ],
        "dcat:landingPage": "https://github.com/onnx/onnx",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Ramesh",
            "M. Pavlov",
            "G. Goh",
            "S. Gray",
            "C. Voss",
            "A. Radford",
            "M. Chen",
            "I. Sutskever"
        ],
        "dcterms:description": "DALL-E is a model capable of generating images from textual descriptions, showcasing zero-shot text-to-image generation capabilities.",
        "dcterms:title": "DALL-E",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2102.12092",
        "dcat:theme": [
            "Computer Vision",
            "Generative Models"
        ],
        "dcat:keyword": [
            "Image generation",
            "Text-to-image",
            "Generative model"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2102.12092",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Devlin",
            "M. Chang",
            "K. Lee",
            "K. Toutanova"
        ],
        "dcterms:description": "BERT is a model for pre-training deep bidirectional transformers for language understanding, achieving state-of-the-art results on various NLP tasks.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/n19-1423",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Language model",
            "Transformers",
            "Natural language understanding"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/n19-1423",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. He",
            "X. Zhang",
            "S. Ren",
            "J. Sun"
        ],
        "dcterms:description": "ResNet-18 is a deep residual learning framework for image recognition, known for its skip connections that help mitigate the vanishing gradient problem.",
        "dcterms:title": "ResNet-18",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/CVPR.2016.90",
        "dcat:theme": [
            "Computer Vision",
            "Image Recognition"
        ],
        "dcat:keyword": [
            "Residual networks",
            "Image classification",
            "Deep learning"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/CVPR.2016.90",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "C. Szegedy",
            "V. Vanhoucke",
            "S. Ioffe",
            "J. Shlens",
            "Z. Wojna"
        ],
        "dcterms:description": "InceptionV3 is a convolutional neural network architecture that improves upon previous versions by using factorized convolutions and auxiliary classifiers.",
        "dcterms:title": "InceptionV3",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/CVPR.2016.308",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Convolutional neural network",
            "Image classification",
            "Deep learning"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/CVPR.2016.308",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "F. N. Iandola",
            "S. Han",
            "M. W. Moskewicz",
            "K. Ashraf",
            "W. J. Dally",
            "K. S. Keutzer"
        ],
        "dcterms:description": "SqueezeNet is a small deep learning model that achieves AlexNet-level accuracy with significantly fewer parameters, making it suitable for resource-constrained environments.",
        "dcterms:title": "SqueezeNet",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Model Compression"
        ],
        "dcat:keyword": [
            "Small model",
            "Deep learning",
            "Model efficiency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Nayak"
        ],
        "dcterms:description": "ViT (Vision Transformer) is a model that applies transformer architecture to image classification tasks, demonstrating the effectiveness of self-attention mechanisms in vision.",
        "dcterms:title": "ViT (Vision Transformer)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://blog.google/products/search/search-language-understanding-bert",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Vision transformer",
            "Image classification",
            "Self-attention"
        ],
        "dcat:landingPage": "https://blog.google/products/search/search-language-understanding-bert",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Q. Zhang",
            "H. Lu",
            "H. Sak",
            "A. Tripathi",
            "E. McDermott",
            "S. Koo",
            "S. Kumar"
        ],
        "dcterms:description": "T-T is a streamable speech recognition model that combines transformer encoders with RNN-T loss for efficient speech processing.",
        "dcterms:title": "T-T",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2002.02562",
        "dcat:theme": [
            "Speech Recognition",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Speech recognition",
            "Transformer",
            "RNN-T"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2002.02562",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "H. Wang",
            "J. Zhai",
            "M. Gao",
            "Z. Ma",
            "S. Tang",
            "L. Zheng",
            "Y. Li",
            "K. Rong",
            "Y. Chen",
            "Z. Jia"
        ],
        "dcterms:description": "PET is a framework that optimizes tensor programs using partially equivalent transformations and automated corrections.",
        "dcterms:title": "PET",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Tensor optimization",
            "Machine learning",
            "Automated corrections"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Z. Jia",
            "O. Padon",
            "J. Thomas",
            "T. Warszawski",
            "M. Zaharia",
            "A. Aiken"
        ],
        "dcterms:description": "TASO is a system that optimizes deep learning computation through automatic generation of graph substitutions.",
        "dcterms:title": "TASO",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Deep learning optimization",
            "Graph substitutions",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]