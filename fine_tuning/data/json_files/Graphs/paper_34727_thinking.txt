To extract datasets from the research paper titled "Self-Supervised Representation Learning with Spatial-Temporal Consistency for Sign Language Recognition" by Weichao Zhao et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the method is evaluated on **four public benchmarks**, which suggests that datasets are involved.

Next, I will look for specific mentions of datasets in the **experiments section**. In this section, the authors provide a summary of the datasets used for their experiments, which includes:

1. **WLASL Dataset**: This dataset consists of 21,330 training videos and 4,172 testing videos, focusing on American Sign Language (ASL) with a vocabulary of 2,000 words.

2. **MSASL Dataset**: This dataset includes 18,205 training videos and 2,878 testing videos, also focusing on ASL with a vocabulary of 1,000 words.

3. **NMFs-CSL Dataset**: This dataset contains 25,608 training videos and 6,402 testing videos, focusing on Chinese Sign Language (CSL) with a vocabulary of 1,067 words.

4. **SLR500 Dataset**: This dataset has 90,000 training videos and 35,000 testing videos, also focusing on CSL with 500 normal words.

I will then check the **References section** to find the full citations for these datasets. The citations are as follows:

- For **WLASL Dataset**:
  > Li, D., Rodriguez, C., Yu, X., & Li, H. (2020). Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison. In *WACV* (pp. 1459–1469).

- For **MSASL Dataset**:
  > Joze, H. R. V., & Koller, O. (2019). MS-ASL: A large-scale data set and benchmark for understanding American Sign Language. In *BMVC* (pp. 1–16).

- For **NMFs-CSL Dataset**:
  > Hu, H., Zhou, W., Pu, J., & Li, H. (2021). Global-local enhancement network for NMF-aware sign language recognition. *ACM TOMM*, 17(3), 1–19.

- For **SLR500 Dataset**:
  > Hu, H., Zhou, W., & Li, H. (2018). Attention-based 3D-CNNs for large-vocabulary sign language recognition. *IEEE TCSVT*, 29(9), 2822–2832.

After gathering this information, I will compile the dataset entries into a structured format that includes the dataset names, descriptions, and full citations, ensuring that each dataset is accurately represented with its respective citation.