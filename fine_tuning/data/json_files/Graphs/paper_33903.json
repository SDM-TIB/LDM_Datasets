[
    {
        "dcterms:creator": [
            "Carlos Busso",
            "Murtaza Bulut",
            "Chi-Chun Lee",
            "Abe Kazemzadeh",
            "Emily Mower",
            "Samuel Kim",
            "Jeannette N Chang",
            "Sungbok Lee",
            "Shrikanth S Narayanan"
        ],
        "dcterms:description": "The IEMOCAP database was collected using multi-modal sensors to capture motion, audio, and video of human interactions. The original corpus contains 10,039 utterances from ten subjects expressing various categorical emotions from improvised and scripted scenarios.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Multi-modal Data"
        ],
        "dcat:keyword": [
            "Emotion dataset",
            "Speech data",
            "Multi-modal interaction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video, Text",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Carlos Busso",
            "Srinivas Parthasarathy",
            "Alec Burmania",
            "Mohammed AbdelWahab",
            "Najmeh Sadoughi",
            "Emily Mower Provost"
        ],
        "dcterms:description": "The MSP-Improv corpus is a multi-modal emotion recognition data set captured from improvised scenarios. The data is collected from 12 speakers and includes audio and textual data of utterances spoken in different recording conditions.",
        "dcterms:title": "MSP-Improv",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Multi-modal Data"
        ],
        "dcat:keyword": [
            "Emotion dataset",
            "Speech data",
            "Multi-modal interaction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Text",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Xin Wang",
            "Devinder Kumar",
            "Nicolas Thome",
            "Matthieu Cord",
            "Frederic Precioso"
        ],
        "dcterms:description": "The UPMC Food101 dataset consists of web pages with textual recipe descriptions for 101 food labels automatically retrieved online. Each page was matched with a single image, where the images were obtained by querying Google Image Search for the given category.",
        "dcterms:title": "UPMC Food101",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Food Recognition",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Food dataset",
            "Recipe recognition",
            "Image data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Image Classification"
        ]
    }
]