[
    {
        "dcterms:creator": [
            "M. Bain",
            "A. Nagrani",
            "G. Varol",
            "A. Zisserman"
        ],
        "dcterms:description": "A dataset containing 2.5 million video-text pairs, used for video-text retrieval tasks.",
        "dcterms:title": "WebVid-2M",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Text retrieval",
            "Video-text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "P. Sharma",
            "N. Ding",
            "S. Goodman",
            "R. Soricut"
        ],
        "dcterms:description": "A dataset containing 3 million image-text pairs, used for automatic image captioning.",
        "dcterms:title": "Conceptual Captions 3M (CC3M)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Text pairs",
            "Automatic captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "J. Xu",
            "T. Mei",
            "T. Yao",
            "Y. Rui"
        ],
        "dcterms:description": "A large video description dataset for bridging video and language, containing 10,000 videos.",
        "dcterms:title": "MSRVTT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Description",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Language understanding",
            "Video-text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Description",
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "A. Hendricks",
            "L. Wang",
            "E. Shechtman",
            "J. Sivic",
            "T. Darrell",
            "B. Russell"
        ],
        "dcterms:description": "A dataset for localizing moments in video with natural language, containing 10,000 videos.",
        "dcterms:title": "DiDeMo",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Localization",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Moment localization",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Moment Localization"
        ]
    },
    {
        "dcterms:creator": [
            "A. Rohrbach",
            "M. Rohrbach",
            "N. Tandon",
            "B. Schiele"
        ],
        "dcterms:description": "A dataset for movie description, containing 118,081 clips from 202 movies.",
        "dcterms:title": "LSMDC",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Movie Description",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Movie dataset",
            "Video description",
            "Text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Movie Description"
        ]
    },
    {
        "dcterms:creator": [
            "D. Chen",
            "W.B. Dolan"
        ],
        "dcterms:description": "A dataset containing 1,970 YouTube videos with 80,000 descriptions for paraphrase evaluation.",
        "dcterms:title": "MSVD",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Description",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Paraphrase evaluation",
            "Text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Paraphrase Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "K. Soomro",
            "A.R. Zamir",
            "M. Shah"
        ],
        "dcterms:description": "A dataset of 101 human action classes from videos in the wild, containing 13,320 videos.",
        "dcterms:title": "UCF101",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Action dataset",
            "Human actions",
            "Video classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Kuehne",
            "H. Jhuang",
            "E. Garrote",
            "T. Poggio",
            "T. Serre"
        ],
        "dcterms:description": "A large video database for human motion recognition, containing 6,766 videos.",
        "dcterms:title": "HMDB51",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Motion Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Motion dataset",
            "Human actions",
            "Video classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Motion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "F. Caba Heilbron",
            "V. Escorcia",
            "B. Ghanem",
            "J.C. Niebles"
        ],
        "dcterms:description": "A large-scale video benchmark for human activity understanding, containing 20,000 videos.",
        "dcterms:title": "ActivityNet",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Understanding",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Activity dataset",
            "Video understanding",
            "Human activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "A. Miech",
            "D. Zhukov",
            "J.B. Alayrac",
            "M. Tapaswi",
            "I. Laptev",
            "J. Sivic"
        ],
        "dcterms:description": "A dataset containing 100 million narrated video clips for learning text-video embeddings.",
        "dcterms:title": "HowTo100M",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instructional Videos",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Instructional dataset",
            "Video embeddings",
            "Narrated videos"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-Video Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Consortium, E."
        ],
        "dcterms:description": "A large-scale first-person video database supporting research in multi-modal machine perception for daily life activity.",
        "dcterms:title": "Ego4D",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Egocentric Video",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Egocentric dataset",
            "Daily life activities",
            "Video perception"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Li",
            "K. Min",
            "S. Tripathi",
            "N. Vasconcelos"
        ],
        "dcterms:description": "A dataset for temporal learning of sparse video-text transformers.",
        "dcterms:title": "SViTT",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Learning",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Sparse transformers",
            "Video-text pairs",
            "Temporal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Learning"
        ]
    },
    {
        "dcterms:creator": [
            "G. Zhang",
            "J. Ren",
            "J. Gu",
            "V. Tresp"
        ],
        "dcterms:description": "A dataset for multi-event video-text retrieval.",
        "dcterms:title": "Multi-event Video-Text Retrieval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Text Retrieval",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Multi-event retrieval",
            "Video-text pairs",
            "Event understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Multi-event Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "P. Bagad",
            "M. Tapaswi",
            "C.G. Snoek"
        ],
        "dcterms:description": "A dataset for instilling video-language models with a sense of time.",
        "dcterms:title": "Test of Time",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Temporal Understanding",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Temporal logic",
            "Video-language models",
            "Event understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Understanding"
        ]
    }
]