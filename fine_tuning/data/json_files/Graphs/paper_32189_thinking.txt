To extract datasets from the research paper titled "Improving Reference-based Distinctive Image Captioning with Contrastive Rewards" by Yangjun Mao et al., I will follow a systematic approach to identify and document the datasets mentioned in the text.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets that are explicitly mentioned. In the abstract, the authors refer to the development of two new benchmarks for the Reference-based Distinctive Image Captioning (Ref-DIC) task, which suggests that datasets are involved.

Next, I will focus on the **introduction section**, where the authors mention the construction of two new benchmarks: **COCO-DIC** and **Flickr30K-DIC**. This indicates that these are the primary datasets used in their experiments.

In the **experiments section**, the authors provide details about the datasets. They state that COCO-DIC is based on the MS-COCO dataset, which contains 123,287 images, and Flickr30K-DIC is based on the Flickr30K dataset, which contains 31,014 images. Each image in both datasets is annotated with five ground-truth captions.

Now, I will check the **references section** to find the full citations for these datasets:

1. **MS-COCO Dataset**:
   > Chen, X., Fang, H., Lin, T. Y., Vedantam, R., Gupta, S., Dollar, P., & Zitnick, C. L. (2015). Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv preprint (2015).

2. **Flickr30K Dataset**:
   > Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., & Lazebnik, S. (2016). Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. In Int. Conf. Comput. Vis.

After gathering this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will include the dataset names, descriptions, and full citations, ready for structured output or further processing.