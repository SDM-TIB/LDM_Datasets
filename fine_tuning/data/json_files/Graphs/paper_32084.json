[
    {
        "dcterms:creator": [
            "Payal Bajaj",
            "Daniel Campos",
            "Nick Craswell",
            "Li Deng",
            "Jianfeng Gao",
            "Xiaodong Liu",
            "Rangan Majumder",
            "Andrew McNamara",
            "Bhaskar Mitra",
            "Tri Nguyen",
            "Mir Rosenberg",
            "Xia Song",
            "Alina Stoica",
            "Saurabh Tiwary",
            "Tong Wang"
        ],
        "dcterms:description": "A human generated machine reading comprehension dataset used for evaluating retrieval tasks.",
        "dcterms:title": "MS MARCO",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Reading Comprehension",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Retrieval",
            "Question Answering",
            "Machine Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Information Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Mandy Guo",
            "Yinfei Yang",
            "Daniel Cer",
            "Qinlan Shen",
            "Noah Constant"
        ],
        "dcterms:description": "A cross-domain evaluation dataset for retrieval question answering models.",
        "dcterms:title": "MultiReQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Cross-Domain Evaluation"
        ],
        "dcat:keyword": [
            "Retrieval",
            "Question Answering",
            "Cross-Domain"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Information Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Nandan Thakur",
            "Nils Reimers",
            "Andreas Rücklé",
            "Abhishek Srivastava",
            "Iryna Gurevych"
        ],
        "dcterms:description": "A heterogeneous benchmark for zero-shot evaluation of information retrieval models.",
        "dcterms:title": "BEIR",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Information Retrieval",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Zero-Shot Evaluation",
            "Information Retrieval",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Information Retrieval",
            "Evaluation"
        ]
    }
]