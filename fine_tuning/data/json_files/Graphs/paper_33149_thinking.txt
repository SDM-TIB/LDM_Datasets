To extract datasets from the research paper titled "Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition" by Dongyuan Li et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets mentioned. The abstract states that extensive experiments were conducted on three benchmark datasets, which indicates that specific datasets will be detailed later in the paper.

Next, I will focus on **section 3 (Methodology)**, where the authors mention the datasets used for evaluation. In this section, they specifically list three datasets:

1. **IEMOCAP**: This dataset is described as a conversational dataset where each utterance is labeled with one of six emotion categories (Anger, Excited, Sadness, Happiness, Frustrated, and Neutral). The paper mentions that two settings were used for testing: one with four emotions and one with six emotions.

2. **MELD**: This dataset is noted as a multimodal multi-party dataset for emotion recognition in conversations. It includes various emotions and is structured for training, validation, and testing.

3. **MOSEI**: This dataset is described as containing movie review clips from YouTube, annotated with sentiment scores and emotion labels.

In **section 4.1 (Experimental Settings)**, the authors provide additional details about the datasets, including their distribution and the number of samples for training, validation, and testing. This confirms the datasets mentioned earlier.

Now, I will check the **References section** to find the full citations for these datasets:

- For **IEMOCAP**, the citation is:
  > Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeanette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. *IEMOCAP: Interactive Emotional Dyadic Motion Capture Database*. Language Resources and Evaluation, 42(4):335–359, 2008.

- For **MELD**, the citation is:
  > Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. *MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations*. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 527–536, 2019.

- For **MOSEI**, the citation is:
  > Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. *Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph*. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 2236–2246, 2018.

After gathering all this information, I will compile the dataset entries, ensuring that each dataset is accurately described and includes its full citation. This will provide a comprehensive overview of the datasets used in the research paper.