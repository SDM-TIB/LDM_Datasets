To extract datasets from the research paper titled "A Two-stream Neural Network for Pose-based Hand Gesture Recognition" by Chuankun Li et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. In the abstract, the authors mention using the **Dynamic Hand Gesture dataset** and the **First-Person Hand Action dataset** to validate their method. This indicates that these datasets are crucial for the experiments.

Next, I will look for a dedicated **datasets section** or any relevant sections that describe the datasets in detail. In the **experiments section**, the authors provide specific information about both datasets:

1. **Dynamic Hand Gesture (DHG) 14/28 dataset**: This dataset is captured using Intel RealSense depth cameras and contains hand joint data and depth map sequences. It consists of 2800 gesture sequences with 14 gestures performed by 20 subjects, with each sequence ranging from 20 to 50 frames.

2. **First-Person Hand Action (FPHA) dataset**: This dataset includes 1175 sequences from 45 different gesture classes, captured in various scenarios with high variability in viewpoint and style. It is noted for its complexity due to the involvement of many different objects.

Now, I will check the **References section** to find the full citations for these datasets:

- For the **Dynamic Hand Gesture (DHG) dataset**, the citation is:
  > G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim. *First-person hand action benchmark with RGB-D videos and 3D hand pose annotations*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

- For the **First-Person Hand Action (FPHA) dataset**, the citation is:
  > G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim. *First-person hand action benchmark with RGB-D videos and 3D hand pose annotations*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

Since both datasets are cited from the same source, I will ensure to note that the DHG dataset is specifically referenced as "DHG 14/28" in the paper.

Finally, I will compile the extracted information into a structured format, ensuring that each dataset is clearly described along with its full citation. This will provide a comprehensive overview of the datasets used in the research.