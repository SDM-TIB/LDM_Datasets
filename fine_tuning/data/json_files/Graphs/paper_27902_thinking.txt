To extract datasets from the research paper titled "Self-supervised Incremental Deep Graph Learning for Ethereum Phishing Scam Detection" by Shucheng Li et al., I will follow a systematic approach to identify and document the datasets mentioned in the text.

First, I will read through the **abstract, introduction, methodology, and experiments sections** to locate any references to datasets. The abstract mentions that the authors collected transaction records from Ethereum, which indicates that there is a dataset involved.

Next, I will focus on the **introduction section**, where the authors discuss the challenges of phishing scams on Ethereum and the need for intelligent analytics at the transactional activity level. They mention the collection of transaction data over a period of time, which suggests that this data is crucial for their experiments.

In the **experiments section**, particularly in **section 4.1 (Experimental setup)**, the authors detail their data collection process. They state that they collected transaction data from Ethereum from January 2018 to May 2020, resulting in a dataset of over 75 million transactions. This dataset is essential for their experiments on phishing scam detection.

I will also look for any specific details about how the dataset was processed or split, as this information can be relevant for understanding the dataset's structure and usage in the experiments.

Now, I will consult the **References section** to find the full citation for the dataset. Since the dataset is based on Ethereum transaction data, I will note that the original source of this data is the Ethereum blockchain itself, and I will provide a citation for the Ethereum project:

- For the Ethereum transaction dataset, the citation is:
  > Gavin Wood et al. *Ethereum: A secure decentralised generalised transaction ledger*. Ethereum project yellow paper, 151(2014):1â€“32, 2014.

After gathering this information, I will summarize the dataset details, including its purpose, size, and any relevant characteristics, ensuring that I include the full citation for the dataset as required.

Finally, I will compile the dataset entries into a structured format that is ready for review or further processing.