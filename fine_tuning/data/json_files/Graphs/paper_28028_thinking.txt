To extract the datasets from the research paper titled "Lesion-based Contrastive Learning for Diabetic Retinopathy Grading from Fundus Images" by Yijin Huang et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on a publicly-accessible dataset called **EyePACS**, which suggests that this dataset is significant for their research.

Next, I will examine **section 3 (Experiment)**, where the authors explicitly mention two datasets used in their work:

1. **IDRiD**: This dataset consists of 81 fundus images with pixel-wise lesion annotations, including hemorrhages, microaneurysms, soft exudates, and hard exudates. The authors describe how this dataset is used to train an object detection network.

2. **EyePACS**: This dataset contains 35,000 training images, 11,000 validation images, and 43,000 testing images, classified into five grades of diabetic retinopathy. The authors mention that this dataset is used for training their self-supervised model.

To confirm the details and obtain full citations for these datasets, I will refer to the **References section** of the paper:

- For **IDRiD**, the citation is:
  > Porwal, P., et al. "Indian diabetic retinopathy image dataset (IDRID): a database for diabetic retinopathy screening research." *Data* 3(3), 25 (2018).

- For **EyePACS**, the citation is:
  > Kaggle diabetic retinopathy detection competition. *Kaggle*. https://www.kaggle.com/c/diabetic-retinopathy-detection.

Now that I have identified the datasets and their citations, I will summarize the information for each dataset, ensuring to include the full citations as required.

Finally, I will compile the dataset entries into a structured format for further processing or review.