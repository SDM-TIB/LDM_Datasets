To extract datasets from the research paper titled "Video Salient Object Detection via Contrastive Features and Attention Modules" by Yi-Wen Chen et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experimental results sections** to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on several benchmark datasets for video salient object detection, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on **section 4.1 (Datasets and Evaluation Metrics)**, where the authors explicitly list the datasets used for their experiments. Here, they mention:

1. **MSRA10K**: This dataset is used for training and consists of images for salient object detection.
2. **DUT-OMRON**: Another dataset utilized for training, which also contains images for salient object detection.
3. **DAVIS**: This dataset is specifically mentioned for video salient object detection, containing 30 training videos and 20 validation videos with pixel-wise annotations.

In the same section, the authors also mention the evaluation datasets:

1. **DAVIS**: Used for validation, with pixel-wise annotations.
2. **FBMS**: A benchmark for video object segmentation, consisting of 29 training videos and 30 testing videos.
3. **ViSal**: A dataset for video salient object detection with 17 video sequences and 193 annotated frames.
4. **DAVSOD**: The largest dataset for video salient object detection, containing 226 videos with 23,938 frames and pixel-wise annotations based on eye fixation records.

Now, I will check the **References section** to find the full citations for these datasets. The citations are as follows:

- **MSRA10K**: 
  > Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip H. S. Torr, and Shi-Min Hu. *Global contrast based salient region detection*. PAMI, 37(3):569–582, 2015.

- **DUT-OMRON**: 
  > Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. *Saliency detection via graph-based manifold ranking*. In CVPR, 2013.

- **DAVIS**: 
  > F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. *A benchmark dataset and evaluation methodology for video object segmentation*. In CVPR, 2016.

- **FBMS**: 
  > Thomas Brox and Jitendra Malik. *Object segmentation by long term analysis of point trajectories*. In ECCV, 2010.

- **ViSal**: 
  > Wenguan Wang, Jianbing Shen, and Ling Shao. *Consistent video saliency using local gradient flow optimization and global refinement*. IEEE Transactions on Image Processing, 24(11):4185–4196, 2015.

- **DAVSOD**: 
  > Wenguan Wang, Jianbing Shen, and Fatih Porikli. *Revisiting video saliency: A large-scale benchmark and a new model*. In CVPR, 2018.

After gathering all the necessary information, I will compile the dataset entries, ensuring to include the full citations for each dataset as required. This structured approach will ensure that I accurately capture all relevant datasets and their citations from the paper.