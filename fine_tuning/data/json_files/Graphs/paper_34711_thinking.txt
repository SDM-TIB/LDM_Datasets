To extract datasets from the research paper titled "SCDNet: Self-supervised Learning Feature based Speaker Change Detection" by Yue Li et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental settings sections** to identify any datasets mentioned. The abstract provides a brief overview of the study, but the details about datasets are likely to be found in the experimental sections.

Next, I will focus on **section 3 (Experimental settings)**, where the authors explicitly mention the datasets used for evaluation. Here, they list four real datasets:

1. **AMI Dataset**: This dataset is commonly used for speaker change detection tasks. The authors specify that they utilized the "headset mix" recordings from this dataset.

2. **AliMeeting Dataset**: The authors mention using channel 0 recordings from this dataset for their experiments.

3. **AISHELL-4 Dataset**: Similar to the previous datasets, they specify that channel 0 recordings are used for evaluation.

4. **DIHARD3 Dataset**: This dataset is also mentioned as part of their evaluation process.

Additionally, the authors note that they created an artificial dataset from the "train-other-500" subset of **LibriSpeech**. This is important as it indicates that they are using both real and synthetic data for their experiments.

Now, I will check the **References section** to gather the full citations for each dataset mentioned:

- For the **AMI Dataset**, the citation is:
  > W. Kraaij, T. Hain, M. Lincoln, and W. Post. "The AMI Meeting Corpus." In International Conference on Methods and Techniques in Behavioral Research, 2005, pp. 28–39.

- For the **AliMeeting Dataset**, the citation is:
  > F. Yu, S. Zhang, Y. Fu, L. Xie, S. Zheng, Z. Du, W. Huang, P. Guo, Z. Yan, B. Ma et al. "M2met: The ICASSP 2022 Multi-channel Multi-party Meeting Transcription Challenge." In International Conference on Acoustics, Speech and Signal Processing, 2022, pp. 6167–6171.

- For the **AISHELL-4 Dataset**, the citation is:
  > Y. Fu, L. Cheng, S. Lv, Y. Jv, Y. Kong, Z. Chen, Y. Hu, L. Xie, J. Wu, H. Bu et al. "AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario." In INTERSPEECH, 2021, pp. 3665–3669.

- For the **DIHARD3 Dataset**, the citation is:
  > N. Ryant, P. Singh, V. Krishnamohan, R. Varma, K. Church, C. Cieri, J. Du, S. Ganapathy, and M. Liberman. "The Third DIHARD Diarization Challenge." In INTERSPEECH, 2021, pp. 3570–3574.

- For the **LibriSpeech Dataset**, the citation is:
  > V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. "LibriSpeech: An ASR Corpus Based on Public Domain Audio Books." In International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 5206–5210.

After gathering all this information, I will compile the dataset entries, ensuring that each dataset is accurately described and includes its full citation. This will provide a comprehensive overview of the datasets used in the research paper, which is essential for reproducibility and further research.