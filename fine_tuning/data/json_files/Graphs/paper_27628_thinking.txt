To extract datasets from the research paper titled "Joint Representation Learning and Novel Category Discovery on Single- and Multi-modal Data" by Xuhui Jia et al., I will follow a systematic approach to identify and document the datasets mentioned throughout the paper.

First, I will read through the **abstract** and **introduction** sections to get an overview of the datasets referenced. In the abstract, the authors mention evaluating their framework on **large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound**, as well as image benchmarks **CIFAR10, CIFAR100, and ImageNet**. This indicates that these datasets are likely to be important for the experiments conducted in the paper.

Next, I will look for a dedicated **datasets section** or any mention of datasets in the **experiments section**. In the **experiments section**, the authors explicitly state that they evaluated their framework on the following datasets:

1. **Kinetics-400**: A large-scale dataset containing 400 action categories, with around 240,000 video clips.
2. **VGG-Sound**: A dataset designed for audio-visual learning, containing 183,000 video clips with sound.
3. **CIFAR10**: A dataset consisting of 60,000 32x32 color images in 10 classes, with 6,000 images per class.
4. **CIFAR100**: Similar to CIFAR10 but with 100 classes containing 600 images each.
5. **ImageNet**: A large visual database designed for use in visual object recognition software research, containing over 14 million images.

To ensure accuracy, I will check the **References section** for full citations of these datasets. The citations I find are:

- For **Kinetics-400**:
  > Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. *The Kinetics Human Action Video Dataset*. arXiv preprint arXiv:1705.06950, 2017.

- For **VGG-Sound**:
  > Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. *VGGSound: A Large-Scale Audio-Visual Dataset*. In ICASSP, 2020.

- For **CIFAR10**:
  > Alex Krizhevsky and Geoffrey Hinton. *Learning Multiple Layers of Features from Tiny Images*. Technical report, 2009.

- For **CIFAR100**:
  > Alex Krizhevsky and Geoffrey Hinton. *Learning Multiple Layers of Features from Tiny Images*. Technical report, 2009.

- For **ImageNet**:
  > Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. *ImageNet: A Large-Scale Hierarchical Image Database*. In CVPR, 2009.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further use or processing.