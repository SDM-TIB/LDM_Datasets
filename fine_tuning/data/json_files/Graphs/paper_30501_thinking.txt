To extract datasets from the research paper titled "Rethinking Rotation in Self-Supervised Contrastive Learning: Adaptive Positive or Negative Data Augmentation" by Atsuyuki Miyai et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract provides a brief overview of the study, and while it discusses the proposed method, it does not explicitly mention datasets. Therefore, I will move on to the introduction.

In the **introduction**, the authors discuss the importance of data augmentation in contrastive learning but do not specify any datasets. I will continue to the **experiments section**, where the authors typically describe the datasets used for their experiments.

In **section 5 (Experiments)**, the authors mention two datasets:

1. **CIFAR-100**: This dataset contains 50,000 training images and 10,000 test images, scaled down to 32×32 pixels across 100 different classes.

2. **Tiny ImageNet**: This dataset consists of 100,000 training images and 10,000 test images, scaled down to 64×64 pixels across 200 different classes, drawn from the original 1,000 classes of ImageNet.

Next, I will check the **References section** to find the full citations for these datasets:

- For **CIFAR-100**, the citation is:
  > Alex Krizhevsky. *Learning multiple layers of features from tiny images*. Technical report, 2009.

- For **Tiny ImageNet**, the citation is:
  > Ya Le and Xuan Yang. *Tiny ImageNet Visual Recognition Challenge*. 2015.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing. This includes ensuring that each dataset is clearly described along with its full citation, which is crucial for proper attribution in research contexts.