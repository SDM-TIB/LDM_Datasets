[
    {
        "dcterms:creator": [
            "P. S. H. Lewis",
            "B. Oguz",
            "R. Rinott",
            "S. Riedel",
            "H. Schwenk"
        ],
        "dcterms:description": "MLQA is a popular xMRC benchmark, which covers various languages and evaluates cross-lingual extractive question answering.",
        "dcterms:title": "MLQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-lingual Machine Reading Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Cross-lingual",
            "Extractive Question Answering",
            "Benchmark Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Machine Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "A. Asai",
            "A. Eriguchi",
            "K. Hashimoto",
            "Y. Tsuruoka"
        ],
        "dcterms:description": "XQUAD is a dataset for evaluating cross-lingual model performances, consisting of multiple languages and designed for extractive reading comprehension.",
        "dcterms:title": "XQUAD",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-lingual Machine Reading Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Cross-lingual",
            "Extractive Reading Comprehension",
            "Benchmark Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Machine Reading Comprehension"
        ]
    }
]