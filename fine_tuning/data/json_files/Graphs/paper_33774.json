[
    {
        "dcterms:creator": [
            "B. Wu",
            "S. Yu",
            "Z. Chen",
            "J. B. Tenenbaum",
            "C. Gan"
        ],
        "dcterms:description": "The STAR dataset comprises 60k situated reasoning questions accompanied by programs and answers, 24k candidate choices, and 22k trimmed situation video clips. It covers four types of questions: interaction, sequence, prediction, and feasibility, in which prediction and feasibility questions are strongly related to what will happen next.",
        "dcterms:title": "STAR",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://openreview.net/forum?id=EfgNF5-ZAjM",
        "dcat:theme": [
            "Video Reasoning",
            "Multi-modal Learning"
        ],
        "dcat:keyword": [
            "Situated reasoning",
            "Video clips",
            "Question answering"
        ],
        "dcat:landingPage": "https://openreview.net/forum?id=EfgNF5-ZAjM",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Xiao",
            "X. Shang",
            "A. Yao",
            "T.-S. Chua"
        ],
        "dcterms:description": "NExT-QA comprises 5440 videos, each with an average duration of 44 seconds. It includes approximately 52k manually annotated question-answer pairs, categorized into causal, temporal, and descriptive questions.",
        "dcterms:title": "NExT-QA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering",
            "Temporal Reasoning"
        ],
        "dcat:keyword": [
            "Question answering",
            "Temporal actions",
            "Video analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Kim",
            "A. Rohrbach",
            "T. Darrell",
            "J. Canny",
            "Z. Akata"
        ],
        "dcterms:description": "BDD-X is a textual autonomous driving dataset that annotates the descriptions and actions of 77 hours within 6,970 videos from the BDD dataset. The video captioning performance is evaluated by the BLEU-4 score, CIDEr score, and METEOR score.",
        "dcterms:title": "BDD-X",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Autonomous Driving",
            "Video Captioning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Driving actions",
            "Captioning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Video Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "F. Ebert",
            "C. Finn",
            "A. X. Lee",
            "S. Levine"
        ],
        "dcterms:description": "The BAIR dataset records 30k videos of a robot pushing multiple objects. The X, Y, and Z location of the robot gripper is provided for each frame, and it is used for prediction and planning tasks.",
        "dcterms:title": "BAIR",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Action Prediction"
        ],
        "dcat:keyword": [
            "Robot actions",
            "Trajectory prediction",
            "Planning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Action Prediction",
            "Planning"
        ]
    }
]