[
    {
        "dcterms:creator": [
            "Guilherme Penedo",
            "Quentin Malartic",
            "Daniel Hesslow"
        ],
        "dcterms:description": "CSEPrompts is a framework with hundreds of programming exercise prompts and multiple-choice questions retrieved from introductory CS and programming courses, designed to evaluate the performance of various LLMs in generating Python code and answering basic computer science questions.",
        "dcterms:title": "CSEPrompts",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/mraihan-gmu/CSEPrompts",
        "dcat:theme": [
            "Computer Science Education",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Programming prompts",
            "Multiple-choice questions",
            "Python code",
            "Introductory computer science"
        ],
        "dcat:landingPage": "https://github.com/mraihan-gmu/CSEPrompts",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Multiple Choice Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "HumanEval is a dataset containing a set of coding prompts paired with human-generated solutions and three test cases for each task, designed to evaluate code generation capabilities of LLMs.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Coding prompts",
            "Human-generated solutions",
            "Test cases"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Austin",
            "Augustus Odena",
            "Maxwell Nye",
            "Bosma"
        ],
        "dcterms:description": "MBPP is a dataset for program synthesis with large language models, containing a larger set of coding tasks paired with human-generated solutions and three test cases for each task.",
        "dcterms:title": "MBPP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Program synthesis",
            "Coding tasks",
            "Human-generated solutions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Yuhang Lai",
            "Chengxi Li",
            "Yiming Wang",
            "Tianyi Zhang",
            "Ruiqi Zhong"
        ],
        "dcterms:description": "DS-1000 is a natural and reliable benchmark for data science code generation, providing a dataset for evaluating code generation tasks.",
        "dcterms:title": "DS-1000",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Data Science",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Data science",
            "Code generation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Srinivasan Iyer",
            "Ioannis Konstas",
            "Alvin Cheung",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "CONCODE is a dataset that maps language to code in programmatic context, designed to evaluate code generation capabilities.",
        "dcterms:title": "CONCODE",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Language to code mapping",
            "Programmatic context"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jiawei Liu",
            "Chunqiu Steven Xia",
            "Yuyao Wang",
            "Lingming Zhang"
        ],
        "dcterms:description": "HumanEval+ is an extension of the HumanEval dataset, rigorously evaluating large language models for code generation.",
        "dcterms:title": "HumanEval+",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Code generation evaluation",
            "Large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Weidong Guo",
            "Jiuding Yang",
            "Kaitong Yang",
            "Li"
        ],
        "dcterms:description": "MBPP+ is an extension of the MBPP dataset, advancing prompt evolution through hybridization.",
        "dcterms:title": "MBPP+",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Prompt evolution",
            "Code generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    }
]