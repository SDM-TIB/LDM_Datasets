[
    {
        "dcterms:creator": [
            "T.B. Brown",
            "B. Mann",
            "N. Ryder",
            "M. Subbiah",
            "J. Kaplan",
            "P. Dhariwal",
            "A. Neelakantan",
            "P. Shyam",
            "G. Sastry",
            "A. Askell",
            "S. Agarwal",
            "A. Herbert-Voss",
            "G. Krueger",
            "T. Henighan",
            "R. Child",
            "A. Ramesh",
            "D.M. Ziegler",
            "J. Wu",
            "C. Winter",
            "C. Hesse",
            "M. Chen",
            "E. Sigler",
            "M. Litwin",
            "S. Gray",
            "B. Chess",
            "J. Clark",
            "C. Berner",
            "S. McCandlish",
            "A. Radford",
            "I. Sutskever",
            "D. Amodei"
        ],
        "dcterms:description": "A large language model that demonstrates few-shot learning capabilities.",
        "dcterms:title": "GPT-3",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "https://proceedings.neurips.cc/paper/2020/file/155f9964c1b86e04f2808eb69964af4c-Paper.pdf",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Few-shot learning",
            "Deep learning"
        ],
        "dcat:landingPage": "https://proceedings.neurips.cc/paper/2020/file/155f9964c1b86e04f2808eb69964af4c-Paper.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "C. Raffel",
            "N. Shazeer",
            "A. Roberts",
            "K. Lee",
            "S. Narang",
            "M. Matena",
            "Y. Zhou",
            "W. Li",
            "P.J. Liu"
        ],
        "dcterms:description": "A unified text-to-text transformer model that explores the limits of transfer learning.",
        "dcterms:title": "T5",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "https://proceedings.neurips.cc/paper/2019/hash/3f5ee243547dee91fbd1662f1e7a7ea2-Abstract.html",
        "dcat:theme": [
            "Natural Language Processing",
            "Transfer Learning"
        ],
        "dcat:keyword": [
            "Text-to-text",
            "Transfer learning",
            "Deep learning"
        ],
        "dcat:landingPage": "https://proceedings.neurips.cc/paper/2019/hash/3f5ee243547dee91fbd1662f1e7a7ea2-Abstract.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "A. Radford",
            "J. Wu",
            "R. Child",
            "D. Luan",
            "D. Amodei",
            "I. Sutskever"
        ],
        "dcterms:description": "A language model that is capable of performing multiple tasks without supervision.",
        "dcterms:title": "GPT-2",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Unsupervised learning",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "J. Devlin",
            "M.W. Chang",
            "K. Lee",
            "K. Toutanova"
        ],
        "dcterms:description": "A pre-trained model for deep bidirectional transformers aimed at language understanding.",
        "dcterms:title": "BERT-large",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Bidirectional transformers",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text classification"
        ]
    },
    {
        "dcterms:creator": [
            "S. Iyer",
            "X.V. Lin",
            "R. Pasunuru",
            "T. Mihaylov",
            "D. Simig",
            "P. Yu",
            "K. Shuster",
            "T. Wang",
            "Q. Liu",
            "P.S. Koura",
            "X. Li",
            "B. Oâ€™Horo",
            "G. Pereyra",
            "J. Wang",
            "C. Dewan",
            "A. Celikyilmaz",
            "L. Zettlemoyer",
            "V. Stoyanov"
        ],
        "dcterms:description": "A model that scales language model instruction meta learning through the lens of generalization.",
        "dcterms:title": "OPT",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Meta learning",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text generation"
        ]
    }
]