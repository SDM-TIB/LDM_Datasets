To extract the datasets mentioned in the research paper titled "Temporal Extension Module for Skeleton-Based Action Recognition" by Yuya Obinata and Takuma Yamamoto, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets that are explicitly mentioned. In the abstract, the authors state that they conducted experiments on **two large datasets, NTU RGB+D and Kinetics-Skeleton**. This indicates that these datasets are crucial for their research.

Next, I will look into **section V (Experiments)**, where the authors provide detailed descriptions of the datasets used:

1. **NTU RGB+D**: This dataset is described as a large-scale and multi-modality dataset for skeleton-based action recognition, consisting of 56,880 video clips across 60 action classes. The dataset includes sequences of 3D skeletons, RGB frames, depth maps, and IR sequences. The authors mention that they specifically use the sequences of 3D skeletons for their work.

2. **Kinetics-Skeleton**: This dataset is derived from the original Kinetics dataset, which contains about 300,000 video clips in 400 classes. The authors explain that they estimate the 2D joint coordinates using the OpenPose toolbox and that the released dataset is divided into training and test sets.

Now, I will check the **References section** to find the full citations for these datasets:

- For **NTU RGB+D**, the citation is:
  > A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang. *NTU RGB+D: A large scale dataset for 3D human activity analysis*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

- For **Kinetics-Skeleton**, the citation is:
  > W. Kay, et al. *The kinetics human action video dataset*. Arxiv, 2017.

Having gathered the necessary information, I will now compile the dataset entries, ensuring that I include the full citations for each dataset as required. This will provide a comprehensive overview of the datasets utilized in the research paper.