To extract datasets from the research paper titled "Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning" by Dorian Desblancs, I will follow a systematic approach to ensure that I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and related work sections** to identify any datasets that are mentioned. The abstract provides a brief overview of the research and may mention datasets used for evaluation. The introduction often elaborates on the context and significance of the datasets in relation to the research objectives.

Next, I will focus on **section 3.3.1 (Datasets)**, where the author explicitly lists the datasets used for beat tracking and downbeat estimation. This section is likely to contain detailed descriptions of each dataset, including the number of files, duration, and whether beat and downbeat annotations are available.

In this section, I will find the following datasets mentioned:

1. **Ballroom Dataset**: This dataset is comprised of dance music excerpts, such as tangos and waltzes, with a total of 685 files and a length of 5 hours and 57 minutes. Both beat and downbeat annotations are available.

2. **Hainsworth Dataset**: This dataset includes a wider variety of genres, such as classical and electronic music, with 222 files and a total length of 3 hours and 19 minutes. It also contains beat and downbeat annotations.

3. **GTZAN Dataset**: This dataset consists of 1000 files spanning 10 genres, including hip-hop, jazz, and disco, with a total length of 8 hours and 20 minutes. Beat annotations are available, but downbeat annotations are not.

4. **SMC Dataset**: This dataset contains 217 files and has a total length of 2 hours and 25 minutes. It includes beat annotations but lacks downbeat annotations.

After identifying these datasets, I will refer to the **References section** of the paper to gather the full citations for each dataset. This is crucial for proper attribution and to allow others to access the datasets.

The full citations for the datasets are as follows:

- **Ballroom Dataset**:
  > Fabien Gouyon, Anssi Klapuri, Simon Dixon, Miguel Alonso, George Tzanetakis, Christian Uhle, and Pedro Cano. *An experimental comparison of audio tempo induction algorithms*. In Proceedings of the IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2006.

- **Hainsworth Dataset**:
  > Stephen W Hainsworth and Malcolm D Macleod. *Particle filtering applied to musical tempo tracking*. In Proceedings of EURASIP, 2004.

- **GTZAN Dataset**:
  > George Tzanetakis and Perry Cook. *Musical genre classification of audio signals*. In IEEE Transactions on Speech and Audio Processing, 2002.

- **SMC Dataset**:
  > Andre Holzapfel, Matthew EP Davies, José R Zapata, João Lobato Oliveira, and Fabien Gouyon. *Selective sampling for beat tracking evaluation*. In IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2012.

Finally, I will compile the dataset entries into a structured format that includes the dataset names, descriptions, and full citations, ensuring that all relevant information is accurately captured for future reference or analysis.