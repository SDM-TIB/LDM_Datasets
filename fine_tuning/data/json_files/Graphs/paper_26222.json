[
    {
        "dcterms:creator": [
            "Alon Talmor",
            "Jonathan Herzig",
            "Nicholas Lourie",
            "Jonathan Berant"
        ],
        "dcterms:description": "A large-scale dataset for commonsense reasoning that consists of 12,102 natural language questions, each with five candidate answers, requiring human commonsense reasoning ability to answer.",
        "dcterms:title": "CommonsenseQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Natural language questions",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "v1.11",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Robert Speer",
            "Joshua Chin",
            "Catherine Havasi"
        ],
        "dcterms:description": "An open multilingual graph of general knowledge that provides a structured representation of commonsense knowledge.",
        "dcterms:title": "ConceptNet",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Knowledge Graph",
            "Commonsense Knowledge"
        ],
        "dcat:keyword": [
            "Knowledge graph",
            "Commonsense knowledge",
            "Multilingual"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "5.5",
        "dcterms:format": "Graph",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Yonatan Bisk",
            "Roy Schwartz",
            "Yejin Choi"
        ],
        "dcterms:description": "A large-scale adversarial dataset for grounded commonsense inference that includes multiple-choice questions requiring commonsense reasoning.",
        "dcterms:title": "SWAG",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Inference"
        ],
        "dcat:keyword": [
            "Grounded commonsense inference",
            "Adversarial dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Inference"
        ]
    },
    {
        "dcterms:creator": [
            "Hector J. Levesque"
        ],
        "dcterms:description": "A challenge designed to test a system's ability to resolve pronouns in sentences, requiring commonsense reasoning.",
        "dcterms:title": "WSC (Winograd Schema Challenge)",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Pronoun Resolution"
        ],
        "dcat:keyword": [
            "Pronoun resolution",
            "Commonsense reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pronoun Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard H. Hovy"
        ],
        "dcterms:description": "A large-scale reading comprehension dataset derived from examination questions, designed to evaluate reading comprehension abilities.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Examination dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Tzvetan Mihaylov",
            "Peter F. Clark",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "dcterms:description": "A dataset for open book question answering that challenges models to answer questions based on a set of facts.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Open Book Question Answering"
        ],
        "dcat:keyword": [
            "Open book QA",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]