[
    {
        "dcterms:creator": [
            "J. Ruckert",
            "A.B. Abacha",
            "A.G. Herrera",
            "L. Bloch",
            "R. Brungel",
            "A. I. Yaghir",
            "H. Schafer",
            "H. Muller",
            "C.M. Friedrich"
        ],
        "dcterms:description": "The ImageCLEF2022 image caption dataset is used for pretraining the vision language model, which is further divided into 83,275 training and 7,645 validation samples.",
        "dcterms:title": "ImageCLEF2022 image caption dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Image captioning",
            "Medical imaging",
            "Vision-language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. J. Lau",
            "S. Gayen",
            "A. B. Abacha",
            "D. Demner-Fushman"
        ],
        "dcterms:description": "VQA-RAD dataset contains 315 images associated with 3,064 question-answer pairs, with 451 pairs selected for testing.",
        "dcterms:title": "VQA-RAD",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Visual Question Answering",
            "Radiology",
            "Medical imaging"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "X. He",
            "Z. Cai",
            "W. Wei",
            "Y. Zhang",
            "L. Mou",
            "E. P. Xing",
            "P. Xie"
        ],
        "dcterms:description": "PathVQA is the largest dataset among the three, containing 32,799 QA pairs, divided into training, validation, and testing sets.",
        "dcterms:title": "PathVQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Visual Question Answering",
            "Pathology",
            "Medical imaging"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "B. Liu",
            "L.M. Zhan",
            "L. Xu",
            "L. Ma",
            "Y. Yang",
            "X.M. Wu"
        ],
        "dcterms:description": "Slake dataset is split into training, validation, and testing samples out of 14,028 QA pairs.",
        "dcterms:title": "Slake",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Visual Question Answering",
            "Medical imaging",
            "Knowledge-enhanced dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]