[
    {
        "dcterms:creator": [
            "Jun Xu",
            "Tao Mei",
            "Ting Yao",
            "Yong Rui"
        ],
        "dcterms:description": "The MSR-VTT dataset is composed of 10k video clips with 20 sentence captions per clip, one of the largest datasets in terms of video-text pairs and word vocabulary.",
        "dcterms:title": "MSR-VTT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Text captions",
            "Video-text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "David L Chen",
            "William B Dolan"
        ],
        "dcterms:description": "The MSVD dataset consists of 1970 video clips with around 40 multilingual sentence captions per clip. The authors only consider the English captions and stochastically choose five captions from it.",
        "dcterms:title": "MSVD",
        "dcterms:issued": "2011",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Multilingual captions",
            "Video-text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-Text Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "Visual Genome is an image region relation annotated dataset that provides dense annotations of objects and their relationships within each image.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Visual Understanding"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object relationships",
            "Dense annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": []
    }
]