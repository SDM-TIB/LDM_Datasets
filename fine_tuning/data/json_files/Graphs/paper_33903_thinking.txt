To extract the datasets mentioned in the research paper titled "Partial Federated Learning" by Tiantian Feng et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets that are explicitly mentioned. The abstract indicates that the authors evaluate their approach on two different multi-modal datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on the **experiments section** (section 3) where the authors describe their evaluation methodology. In this section, they mention two datasets used for the Speech Emotion Recognition task: **IEMOCAP** and **MSP-Improv**. They provide a brief description of each dataset, including the number of subjects and the nature of the data collected.

1. **IEMOCAP Dataset**: This dataset contains utterances from ten subjects expressing various categorical emotions. The authors specify that they focus on the improvised sessions and use four emotion labels: neutral, sad, happiness, and anger.

2. **MSP-Improv Dataset**: This dataset is described as a multi-modal emotion recognition dataset collected from 12 speakers, including audio and textual data of utterances spoken in different recording conditions.

Additionally, the authors mention another dataset used for image classification: **UPMC Food101**. They describe it as consisting of web pages with textual recipe descriptions for 101 food labels, matched with images obtained via Google Image Search.

Now, I will check the **References section** to find the full citations for these datasets:

- For **IEMOCAP**, the citation is:
  > Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. *IEMOCAP: Interactive emotional dyadic motion capture database*. Language resources and evaluation, vol. 42, no. 4, pp. 335–359, 2008.

- For **MSP-Improv**, the citation is:
  > Carlos Busso, Srinivas Parthasarathy, Alec Burmania, Mohammed AbdelWahab, Najmeh Sadoughi, and Emily Mower Provost. *MSP-Improv: An acted corpus of dyadic interactions to study emotion perception*. IEEE Transactions on Affective Computing, vol. 8, no. 1, pp. 67–80, 2016.

- For **UPMC Food101**, the citation is:
  > Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Frederic Precioso. *Recipe recognition with large multimodal food dataset*. In 2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW). IEEE, 2015, pp. 1–6.

After gathering all this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research, which is crucial for reproducibility and further research in the field.