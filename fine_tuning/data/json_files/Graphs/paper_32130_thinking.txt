To extract datasets from the research paper titled "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks" by Wei Liu et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract mentions evaluating the method on two tasks, which suggests that datasets are involved. 

Next, I will focus on **section 4.1 (Datasets)**, where the authors explicitly list the datasets used for their experiments. Here, they mention:

1. **Grammarly Corpus of Discourse Coherence (GCDC)**: This dataset contains texts from various domains, annotated with coherence scores by expert raters.
2. **Test of English as a Foreign Language (TOEFL) dataset**: This dataset consists of essays from multiple prompts, each assigned a score level.

I will then look at the **References section** to find the full citations for these datasets:

- For the **GCDC dataset**, the citation is:
  > Lai, A., & Tetreault, J. (2018). Discourse coherence in the wild: A dataset, evaluation and methods. In Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue (pp. 214â€“223). Association for Computational Linguistics.

- For the **TOEFL dataset**, the citation is:
  > Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., & Chodorow, M. (2014). ETS corpus of non-native written English ldc2014t06. Philadelphia, Penn.: Linguistic Data Consortium.

After gathering this information, I will summarize the datasets in a structured manner, ensuring that I include the full citations for each dataset as required. This will provide a clear and comprehensive overview of the datasets used in the research.