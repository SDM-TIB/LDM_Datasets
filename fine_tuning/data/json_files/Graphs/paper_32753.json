[
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "A dataset for open book question answering that includes questions requiring reasoning over a set of facts.",
        "dcterms:title": "OpenBookQA (OBQA)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Open book",
            "Question answering",
            "Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "A dataset designed to challenge models on their reasoning capabilities through multiple-choice questions.",
        "dcterms:title": "AI2 Reasoning Challenge (ARC)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1803.05457",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reasoning challenge",
            "Multiple choice",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "R. Zellers",
            "J. Gao",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset that focuses on reasoning about physical commonsense knowledge in natural language.",
        "dcterms:title": "Physical Interaction Question Answering (PIQA)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Physical commonsense",
            "Reasoning",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "B. Y. Lin",
            "Z. Wu",
            "Y. Yang",
            "D.-H. Lee",
            "X. Ren"
        ],
        "dcterms:description": "A dataset for reasoning about riddle questions that require linguistic creativity and commonsense knowledge.",
        "dcterms:title": "RiddleSense (Riddle)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Riddles",
            "Linguistic creativity",
            "Commonsense knowledge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Q. Jin",
            "B. Dhingra",
            "Z. Liu",
            "W. W. Cohen",
            "X. Lu"
        ],
        "dcterms:description": "A dataset for biomedical research question answering that includes questions derived from PubMed articles.",
        "dcterms:title": "PubMedQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Biomedical Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Biomedical questions",
            "Research",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "G. Tsatsaronis",
            "G. Balikas",
            "P. Malakasiotis",
            "I. Partalas",
            "M. Zschunke",
            "M. R. Alvers",
            "D. Weissenborn",
            "A. Krithara",
            "S. Petridis",
            "D. Polychronopoulos"
        ],
        "dcterms:description": "A large-scale biomedical semantic indexing and question answering competition dataset.",
        "dcterms:title": "BioASQ",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Biomedical Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Biomedical semantic indexing",
            "Question answering",
            "Competition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]