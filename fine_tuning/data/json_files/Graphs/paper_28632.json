[
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "MS-COCO is a dataset that contains images with corresponding captions, widely used for training and evaluating image captioning models.",
        "dcterms:title": "MS-COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Image captioning",
            "Object detection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalanatidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "Visual Genome is a dataset that connects language and vision using crowdsourced dense image annotations, providing rich information about objects, attributes, and relationships.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Annotation"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Dense annotations",
            "Object relationships"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "WebImageText (WIT) is a dataset used for training models like CLIP, consisting of image-text pairs collected from the web.",
        "dcterms:title": "WebImageText (WIT)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Image dataset",
            "Web-scale data",
            "Image-text pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Deng",
            "W. Dong",
            "R. Socher",
            "L.-J. Li",
            "K. Li",
            "L. Fei-Fei"
        ],
        "dcterms:description": "ImageNet is a large-scale hierarchical image database used for visual recognition tasks, containing millions of labeled images.",
        "dcterms:title": "ImageNet",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Object recognition",
            "Large-scale data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jong Wook Kim",
            "Chris Hallacy",
            "Aditya Ramesh",
            "Gabriel Goh",
            "Sandhini Agarwal",
            "Girish Sastry",
            "Amanda Askell",
            "Pamela Mishkin",
            "Jack Clark"
        ],
        "dcterms:description": "CLIP is a model that learns transferable visual models from natural language supervision, enabling zero-shot learning across various tasks.",
        "dcterms:title": "CLIP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Image-text matching",
            "Zero-shot learning",
            "Transfer learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Captioning",
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pengchuan Zhang",
            "Xiujun Li",
            "Xiaowei Hu",
            "Jianwei Yang",
            "Lei Zhang",
            "Lijuan Wang",
            "Yejin Choi",
            "Jianfeng Gao"
        ],
        "dcterms:description": "VinVL revisits visual representations in vision-language models, enhancing the performance of various tasks.",
        "dcterms:title": "VinVL",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Vision-Language Models"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Visual representations",
            "Vision-language tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Captioning",
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Ron Mokady",
            "Amir Hertz",
            "Amit H Bermano"
        ],
        "dcterms:description": "ClipCap is a method that utilizes CLIP for image captioning by using a prefix approach to generate captions.",
        "dcterms:title": "ClipCap",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Image captioning",
            "CLIP",
            "Prefix method"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Captioning"
        ]
    }
]