To extract datasets from the research paper titled "PYSKL: Towards Good Practices for Skeleton Action Recognition" by Haodong Duan et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations accurately.

First, I will start by reading through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract indicates that PYSKL supports training and testing on **nine skeleton-based action recognition benchmarks**, which suggests that there are multiple datasets involved.

Next, I will look for a dedicated **section on datasets** or any mention of datasets in the **experiments** section. In the introduction, the authors mention that PYSKL achieves state-of-the-art performance on eight benchmarks, which implies that these benchmarks are likely datasets.

Upon reviewing the paper, I find that the authors explicitly list the datasets in the **experiments** section. They mention the following datasets:

1. **NTURGB+D**: A large-scale dataset for 3D human activity understanding, which includes various action categories and is widely used in skeleton-based action recognition.
2. **NTURGB+D 120**: An extension of the NTURGB+D dataset that includes additional action categories.
3. **Kinetics-400**: A large-scale dataset for action recognition in videos, containing a diverse set of human actions.
4. **UCF101**: A dataset of 101 human action classes from videos in the wild, commonly used for action recognition tasks.
5. **HMDB51**: A dataset containing 51 action categories, also used for evaluating action recognition algorithms.
6. **FineGYM**: A hierarchical video dataset designed for fine-grained action understanding.
7. **Diving48**: A dataset focused on diving actions, which includes various annotations.

Next, I will check the **References section** to find the full citations for each of these datasets. The citations are crucial for proper attribution and to provide context for future researchers.

The full citations I find are:

- **NTURGB+D**:
  > Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. *NTU RGB+D: A large scale dataset for 3D human activity analysis*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1010–1019, 2016.

- **NTURGB+D 120**:
  > Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C Kot. *NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding*. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10):2684–2701, 2020.

- **Kinetics-400**:
  > Joao Carreira and Andrew Zisserman. *Quo vadis, action recognition? A new model and the kinetics dataset*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308, 2017.

- **UCF101**:
  > Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. *UCF101: A dataset of 101 human action classes from videos in the wild*. arXiv preprint arXiv:1212.0402, 2012.

- **HMDB51**:
  > Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. *HMDB: A large video database for human motion recognition*. In 2011 International conference on computer vision, pages 2556–2563. IEEE, 2011.

- **FineGYM**:
  > Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. *FineGYM: A hierarchical video dataset for fine-grained action understanding*. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2616–2625, 2020.

- **Diving48**:
  > Yingwei Li, Yi Li, and Nuno Vasconcelos. *Resound: Towards action recognition without representation bias*. In Proceedings of the European Conference on Computer Vision (ECCV), pages 513–528, 2018.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for easy reference and further processing. This will ensure that I have accurately captured all relevant details for each dataset mentioned in the paper.