To extract datasets from the research paper titled "Contrast-Phys+: Unsupervised and Weakly-supervised Video-based Remote Physiological Measurement via Spatiotemporal Contrast" by Zhaodong Sun and Xiaobai Li, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and methods sections** to identify any datasets mentioned. The abstract indicates that the authors evaluate their methods on **five publicly available datasets**, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on **section 5.1.1 (Datasets)**, where the authors explicitly list the datasets used in their experiments. Here, they mention:

1. **PURE Dataset**: This dataset comprises facial videos from ten subjects recorded in six distinct setups, including both static and dynamic tasks.

2. **UBFC-rPPG Dataset**: This dataset includes facial videos from 42 subjects who participated in a mathematical game designed to elevate their heart rates.

3. **OBF Dataset**: This dataset encompasses 200 videos from 100 healthy subjects recorded both before and after exercise sessions.

4. **MR-NIRP Dataset**: This dataset contains near-infrared videos of eight subjects, capturing instances of subjects remaining stationary as well as engaging in motion tasks.

5. **MMSE-HR Dataset**: This dataset includes 102 videos from 40 subjects recorded during emotion elicitation experiments.

6. **EquiPleth Dataset**: This dataset includes 91 participants with varying skin tones, where each subject has six recordings, including RGB videos and synchronized ground truth PPG signals.

Additionally, the authors mention two synthetic datasets:

7. **UCLA-Synth Dataset**: This dataset consists of 476 avatar subjects with rendered facial videos and ground truth signals.

8. **SCAMPS Dataset**: This dataset includes 2800 avatar subjects with diverse skin tones and facial appearances, each having a facial video and ground truth signal.

Now, I will check the **References section** to gather the full citations for each dataset mentioned. The citations are crucial for proper attribution and further exploration of the datasets.

- For **PURE Dataset**, the citation is:
  > Stricker, R., Müller, S., & Gross, H.-M. (2014). Non-contact video-based pulse rate measurement on a mobile service robot. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1056–1062).

- For **UBFC-rPPG Dataset**, the citation is:
  > Bobbia, S., Macwan, R., Benezeth, Y., Mansouri, A., & Dubois, J. (2019). Unsupervised skin tissue segmentation for remote photoplethysmography. Pattern Recognition Letters, 124, 82–90.

- For **OBF Dataset**, the citation is:
  > Li, X., Alikhani, I., Shi, J., Seppanen, T., Junttila, J., Majamaa-Voltti, K., Tulppo, M., & Zhao, G. (2018). The OBF database: A large face video database for remote physiological signal measurement and atrial fibrillation detection. In 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018) (pp. 242–249).

- For **MR-NIRP Dataset**, the citation is:
  > Nowara, E. M., Marks, T. K., Mansour, H., & Veeraraghavan, A. (2020). Near-infrared imaging photoplethysmography during driving. IEEE Transactions on Intelligent Transportation Systems.

- For **MMSE-HR Dataset**, the citation is:
  > Zhang, Z., Girard, J. M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., et al. (2016). Multimodal spontaneous emotion corpus for human behavior analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3438–3446).

- For **EquiPleth Dataset**, the citation is:
  > Vilesov, A., Chari, P., Armouti, A., Harish, A. B., Kulkarni, K., Deoghare, A., Jalilian, L., & Kadambi, A. (2022). Blending camera and 77 GHz radar sensing for equitable, robust plethysmography. ACM Transactions on Graphics (SIGGRAPH), 41(4), 1–14.

- For **UCLA-Synth Dataset**, the citation is:
  > Wang, Z., Ba, Y., Chari, P., Bozkurt, O. D., Brown, G., Patwa, P., Vaddi, N., Jalilian, L., & Kadambi, A. (2022). Synthetic generation of face videos with plethysmograph physiology. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20 587–20 596).

- For **SCAMPS Dataset**, the citation is:
  > McDuff, D., Wander, M., Liu, X., Hill, B., Hernandez, J., Lester, J., & Baltrusaitis, T. (2022). SCAMPS: Synthetics for camera measurement of physiological signals. Advances in Neural Information Processing Systems, 35, 3744–3757.

After gathering all this information, I will compile the dataset entries into a structured format for further processing or review.