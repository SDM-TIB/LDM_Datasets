[
    {
        "dcterms:creator": [],
        "dcterms:description": "Home Action Genome (HOMAGE) is a multi-view action dataset with multiple modalities and viewpoints, supplemented with hierarchical activity and atomic action labels along with dense scene composition labels. It aims to unify various aspects and challenges of action recognition, specifically targeting multi-modal and compositional perception for home actions.",
        "dcterms:title": "Home Action Genome (HOMAGE)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Multi-Modal Learning"
        ],
        "dcat:keyword": [
            "Multi-view dataset",
            "Hierarchical action labels",
            "Atomic action labels",
            "Scene graphs",
            "Home actions"
        ],
        "dcat:landingPage": "http://www.homeactiongenome.org",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition",
            "Compositional Action Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "J. Carreira",
            "E. Noland",
            "C. Hillier",
            "A. Zisserman"
        ],
        "dcterms:description": "Kinetics is a large-scale dataset for human action recognition, containing a diverse set of human actions in videos.",
        "dcterms:title": "Kinetics",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1907.06987",
        "dcat:theme": [
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Human actions",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "G. A. Sigurdsson",
            "G. Varol",
            "X. Wang",
            "A. Farhadi",
            "I. Laptev",
            "A. Gupta"
        ],
        "dcterms:description": "Charades is a dataset for activity understanding, collected through crowdsourcing, featuring various activities performed in home settings.",
        "dcterms:title": "Charades",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Understanding"
        ],
        "dcat:keyword": [
            "Crowdsourced dataset",
            "Home activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "B. G. Fabian Caba Heilbron",
            "Victor Escorcia",
            "J. C. Niebles"
        ],
        "dcterms:description": "ActivityNet is a large-scale video benchmark for human activity understanding, providing a diverse set of activities with temporal annotations.",
        "dcterms:title": "ActivityNet",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Understanding"
        ],
        "dcat:keyword": [
            "Video benchmark",
            "Human activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "K. Soomro",
            "A. R. Zamir",
            "M. Shah"
        ],
        "dcterms:description": "UCF101 is a dataset of 101 human action classes from videos in the wild, widely used for action recognition tasks.",
        "dcterms:title": "UCF101",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1212.0402",
        "dcat:theme": [
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Human actions",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "D. Damen",
            "H. Doughty",
            "G. Maria Farinella",
            "S. Fidler",
            "A. Furnari",
            "E. Kazakos",
            "D. Moltisanti",
            "J. Munro",
            "T. Perrett",
            "W. Price"
        ],
        "dcterms:description": "EPIC Kitchens is a dataset for ego-centric action recognition, featuring videos collected in kitchen environments with various actions.",
        "dcterms:title": "EPIC Kitchens",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Ego-Centric Action Recognition"
        ],
        "dcat:keyword": [
            "Ego-centric videos",
            "Kitchen activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "J. Ji",
            "R. Krishna",
            "L. Fei-Fei",
            "J. C. Niebles"
        ],
        "dcterms:description": "Action Genome is a dataset that focuses on actions as compositions of spatio-temporal scene graphs, providing rich annotations for action understanding.",
        "dcterms:title": "Action Genome",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Spatio-temporal scene graphs",
            "Action compositions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Q. Kong",
            "Z. Wu",
            "Z. Deng",
            "M. Klinkigt",
            "B. Tong",
            "T. Murakami"
        ],
        "dcterms:description": "MMAct is a large-scale dataset for cross-modal human action understanding, including RGB videos, keypoints, acceleration, and gyroscope data.",
        "dcterms:title": "MMAct",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-Modal Action Understanding"
        ],
        "dcat:keyword": [
            "Multi-modal dataset",
            "Human actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "B. Jia",
            "Y. Chen",
            "S. Huang",
            "Y. Zhu",
            "S.-C. Zhu"
        ],
        "dcterms:description": "LEMMA is a multi-view dataset for learning multi-agent multi-task activities, providing annotations for complex interactions.",
        "dcterms:title": "LEMMA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-Agent Activity Recognition"
        ],
        "dcat:keyword": [
            "Multi-view dataset",
            "Agent interactions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "L. Sigal",
            "A. O. Balan",
            "M. J. Black"
        ],
        "dcterms:description": "HumanEva is a synchronized video and motion capture dataset for evaluating articulated human motion.",
        "dcterms:title": "HumanEva",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human Motion Analysis"
        ],
        "dcat:keyword": [
            "Motion capture",
            "Articulated motion"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Motion Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "B. Ni",
            "G. Wang",
            "P. Moulin"
        ],
        "dcterms:description": "RGBD-HuDaAct is a color-depth video database for human daily activity recognition, providing RGB and depth data.",
        "dcterms:title": "RGBD-HuDaAct",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Recognition"
        ],
        "dcat:keyword": [
            "Color-depth dataset",
            "Daily activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Shahroudy",
            "J. Liu",
            "T.-T. Ng",
            "G. Wang"
        ],
        "dcterms:description": "NTU RGB+D is a large-scale dataset for 3D human activity analysis, featuring RGB, depth, and infrared frames with 3D human joint annotations.",
        "dcterms:title": "NTU RGB+D",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Human Activity Analysis"
        ],
        "dcat:keyword": [
            "3D dataset",
            "Human activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "C. Gu",
            "C. Sun",
            "D. A. Ross",
            "C. Vondrick",
            "C. Pantofaru",
            "Y. Li",
            "S. Vijayanarasimhan",
            "G. Toderici",
            "S. Ricco",
            "R. Sukthankar"
        ],
        "dcterms:description": "AVA is a video dataset of spatio-temporally localized atomic visual actions, providing annotations for various actions in videos.",
        "dcterms:title": "AVA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Localized actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Kuehne",
            "A. Arslan",
            "T. Serre"
        ],
        "dcterms:description": "Breakfast is a dataset focused on goal-directed human activities, providing annotations for various breakfast-related actions.",
        "dcterms:title": "Breakfast",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Recognition"
        ],
        "dcat:keyword": [
            "Goal-directed activities",
            "Breakfast actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "C. Liu",
            "Y. Hu",
            "Y. Li",
            "S. Song",
            "J. Liu"
        ],
        "dcterms:description": "PKU-MMD is a large-scale benchmark for continuous multi-modal human action understanding, providing a diverse set of actions.",
        "dcterms:title": "PKU-MMD",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1703.07475",
        "dcat:theme": [
            "Multi-Modal Action Understanding"
        ],
        "dcat:keyword": [
            "Multi-modal dataset",
            "Human actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]