To extract datasets from the research paper titled "Optimized Skeleton-based Action Recognition via Sparsified Graph Regression" by Xiang Gao et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract states that the proposed method achieves state-of-the-art performance on several datasets, which indicates that datasets are involved, but I need to find their specific names.

Next, I will look into the **experiments section** (Section VI) where the authors typically describe the datasets used for evaluation. Here, they mention four datasets: **NTU RGB+D**, **Florence 3D**, **UT-Kinect**, and **SYSU 3D**. I will extract details about each dataset, including the number of action sequences, the number of classes, and any specific characteristics mentioned.

1. **NTU RGB+D Dataset**: This dataset consists of 56,880 action sequences across 60 classes, captured from 40 subjects using 3 Kinect v2 cameras. The authors mention that it includes 25 joints per skeleton.

2. **Florence 3D Dataset**: This dataset contains 215 action sequences performed by 10 actors across 9 classes, with skeletons recorded from Kinect, including 15 joints.

3. **UT-Kinect Dataset**: This dataset includes 200 sequences across 10 classes, recorded using a single Kinect camera, with each skeleton containing 20 joints.

4. **SYSU 3D Dataset**: This dataset consists of 480 action videos performed by 40 actors across 12 activities, with skeleton data captured by Kinect.

After identifying the datasets, I will refer to the **References section** to find the full citations for each dataset. The citations are as follows:

- For **NTU RGB+D Dataset**:
  > Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis." In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 1010–1019.

- For **Florence 3D Dataset**:
  > Lorenzo Seidenari, Vincenzo Varano, Stefano Berretti, Alberto Bimbo, and Pietro Pala. "Recognizing Actions from Depth Cameras as Weakly Aligned Multi-Part Bag-of-Poses." In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2013, pp. 479–485.

- For **UT-Kinect Dataset**:
  > Lu Xia, Chia-Chih Chen, and Jake K Aggarwal. "View Invariant Human Action Recognition Using Histograms of 3D Joints." In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2012, pp. 20–27.

- For **SYSU 3D Dataset**:
  > Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. "Jointly Learning Heterogeneous Features for RGB-D Activity Recognition." In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 5344–5352.

Now that I have gathered all the necessary information about the datasets and their citations, I will compile this into a structured format for further processing or review.