[
    {
        "dcterms:creator": [
            "G. Moon",
            "S.-I. Yu",
            "H. Wen",
            "T. Shiratori",
            "K. M. Lee"
        ],
        "dcterms:description": "InterHand2.6M is a representative two-hand RGB image dataset with challenging hand interacting scenarios. It contains 1.36M train images and 849K test images. The ground-truth contains semi-automatically annotated 3D coordinates of 42 hand joints.",
        "dcterms:title": "InterHand2.6M",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "3D Hand Pose Estimation"
        ],
        "dcat:keyword": [
            "RGB images",
            "3D hand joints",
            "interacting hands"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "3D Hand Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "C. Zimmermann",
            "T. Brox"
        ],
        "dcterms:description": "RHP is a synthesized dataset containing two isolated hand data. It includes 41K training and 2.7K testing samples, used to approximate the generalization ability of models on in-the-wild conditions.",
        "dcterms:title": "RHP",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "3D Hand Pose Estimation"
        ],
        "dcat:keyword": [
            "synthesized dataset",
            "isolated hands",
            "generalization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "3D Hand Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Tompson",
            "M. Stein",
            "Y. Lecun",
            "K. Perlin"
        ],
        "dcterms:description": "NYU is a single-hand depth image dataset which has 72K training images and 8.2K testing images with 3D annotation on 36 hand joints. It is used for evaluating hand pose estimation methods.",
        "dcterms:title": "NYU",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "3D Hand Pose Estimation"
        ],
        "dcat:keyword": [
            "depth images",
            "3D hand joints",
            "single hand"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "3D Hand Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Yuan",
            "Q. Ye",
            "G. Garcia-Hernando",
            "T.-K. Kim"
        ],
        "dcterms:description": "HANDS 2017 is a single-hand depth image dataset which has 957K training images and 295K testing images combined from BigHand2.2M and First-Person Hand Action. The ground-truth contains 3D coordinates of 21 hand joints.",
        "dcterms:title": "HANDS 2017",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "3D Hand Pose Estimation"
        ],
        "dcat:keyword": [
            "depth images",
            "3D hand joints",
            "single hand"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "3D Hand Pose Estimation"
        ]
    }
]