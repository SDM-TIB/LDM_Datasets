To extract the datasets mentioned in the research paper titled "Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization" by Qianhan Feng et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets that are explicitly mentioned. The abstract states that the method achieves an average mAP of 50.8% on **THUMOS’14**, which indicates that this dataset is used in their evaluation.

Next, I will check the **experiments section** (specifically section 4.1) where the authors discuss the datasets used for their experiments. Here, they mention two datasets:

1. **THUMOS’14**: This dataset contains untrimmed videos from 20 categories, and the authors specify that they used 200 videos from the validation set for training and 213 videos from the test set for evaluation.

2. **ActivityNet v1.3**: This is another popular benchmark for temporal action localization with 200 categories. The authors mention using 10,024 untrimmed videos from the training set and 4,926 validation videos for evaluation.

Now, I will look at the **References section** to find the full citations for these datasets:

- For **THUMOS’14**, the citation is:
  > Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. *The THUMOS challenge on action recognition for videos "in the wild"*. Comput. Vis. Image Underst. 155 (2017), 1–23.

- For **ActivityNet v1.3**, the citation is:
  > Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. *ActivityNet: A large-scale video benchmark for human activity understanding*. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. IEEE Computer Society, 961–970.

Having gathered the necessary information, I will now compile the dataset entries with their respective citations for further processing.