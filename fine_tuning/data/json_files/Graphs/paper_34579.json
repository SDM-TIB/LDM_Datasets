[
    {
        "dcterms:creator": [
            "Zhendong Wang",
            "Jonathan J Hunt",
            "Mingyuan Zhou"
        ],
        "dcterms:description": "A multi-turn dialogue preference dataset comprising 7.6k entries, designed to enhance the conversational abilities of open-source LLMs. Each entry consists of multiple dialogue turns between a user and an assistant, with only the final message from the assistant considered as a response.",
        "dcterms:title": "Distilabel-Capybara",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized",
        "dcat:theme": [
            "Dialogue Systems",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-turn dialogue",
            "Conversational AI",
            "Preference learning"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational ability",
            "Preference learning"
        ]
    },
    {
        "dcterms:creator": [
            "Edward Beeching",
            "Clémentine Fourrier",
            "Nathan Habib",
            "Sheon Han",
            "Nathan Lambert",
            "Nazneen Rajani",
            "Omar Sanseviero",
            "Lewis Tunstall",
            "Thomas Wolf"
        ],
        "dcterms:description": "A comprehensive benchmark suite aggregating six popular datasets to assess diverse aspects of language model performance including reasoning, language understanding, and problem-solving through few-shot prompting.",
        "dcterms:title": "Open LLM Leaderboard",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Language model evaluation",
            "Benchmark suite"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question answering",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Jeffrey Zhou",
            "Tianjian Lu",
            "Swaroop Mishra",
            "Siddhartha Brahma",
            "Sujoy Basu",
            "Yi Luan",
            "Denny Zhou",
            "Le Hou"
        ],
        "dcterms:description": "A benchmark evaluating language models on their ability to follow instructions, featuring 541 prompts with verifiable directives such as length constraints and specific formats.",
        "dcterms:title": "IFEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction following",
            "Evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Zhendong Wang",
            "Jonathan J Hunt",
            "Mingyuan Zhou"
        ],
        "dcterms:description": "A benchmark testing language models with 80 multi-turn questions across domains like writing and coding, assessing conversational skill and understanding.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-turn questions",
            "Conversational ability"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Conversational ability"
        ]
    },
    {
        "dcterms:creator": [
            "Yann Dubois",
            "Balázs Galambosi",
            "Percy Liang",
            "Tatsunori B Hashimoto"
        ],
        "dcterms:description": "A dataset for evaluating response quality through a win-rate mechanism, focusing on controlling for length bias in evaluations.",
        "dcterms:title": "AlpacaEval 2.0",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Response quality",
            "Win-rate evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "2.0",
        "dcterms:format": "",
        "mls:task": [
            "Response evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Wei Liu",
            "Weihao Zeng",
            "Keqing He",
            "Yong Jiang",
            "Junxian He"
        ],
        "dcterms:description": "A comprehensive study of automatic data selection in instruction tuning, focusing on what constitutes good data for alignment.",
        "dcterms:title": "Deita 10k",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Data Selection",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Data alignment",
            "Instruction tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]