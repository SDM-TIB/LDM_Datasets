[
    {
        "dcterms:creator": [
            "Brian Christian"
        ],
        "dcterms:description": "A comprehensive exploration of the alignment problem in AI, discussing how machine learning can be aligned with human values.",
        "dcterms:title": "The Alignment Problem",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Alignment",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Alignment problem",
            "Human values",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Nick Bostrom"
        ],
        "dcterms:description": "An exploration of the potential paths, dangers, and strategies associated with the development of superintelligent AI.",
        "dcterms:title": "Superintelligence",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Safety",
            "Philosophy"
        ],
        "dcat:keyword": [
            "Superintelligence",
            "AI risks",
            "Philosophical implications"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Dario Amodei",
            "Chris Olah",
            "Jacob Steinhardt",
            "Paul Christiano",
            "John Schulman",
            "Dan Man√©"
        ],
        "dcterms:description": "A detailed examination of concrete problems in AI safety that need to be addressed to ensure safe AI development.",
        "dcterms:title": "Concrete problems in AI safety",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1606.06565",
        "dcat:theme": [
            "AI Safety",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "AI safety",
            "Concrete problems",
            "Research agenda"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jack Clark",
            "Dario Amodei"
        ],
        "dcterms:description": "An analysis of faulty reward functions in AI systems and their implications for AI safety.",
        "dcterms:title": "Faulty Reward Functions in the Wild",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://openai.com/blog/faulty-reward-functions/",
        "dcat:theme": [
            "AI Safety",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reward functions",
            "AI safety",
            "Faulty systems"
        ],
        "dcat:landingPage": "https://openai.com/blog/faulty-reward-functions/",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Allan Dafoe",
            "Yoram Bachrach",
            "Gillian Hadfield",
            "Eric Horvitz",
            "Kate Larson",
            "Thore Graepel"
        ],
        "dcterms:description": "A discussion on the importance of cooperation in AI systems and the need for machines to find common ground.",
        "dcterms:title": "Cooperative AI",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Cooperation",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Cooperative AI",
            "Social interaction",
            "AI alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Dylan Hadfield-Menell",
            "Gillian K Hadfield"
        ],
        "dcterms:description": "An exploration of the relationship between incomplete contracting and AI alignment.",
        "dcterms:title": "Incomplete contracting and AI alignment",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Alignment",
            "Contract Theory"
        ],
        "dcat:keyword": [
            "Incomplete contracting",
            "AI alignment",
            "Ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Matt Kusner",
            "Joshua Loftus",
            "Chris Russell",
            "Ricardo Silva"
        ],
        "dcterms:description": "A framework for assessing fairness in machine learning models through counterfactual reasoning.",
        "dcterms:title": "Counterfactual fairness",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fairness",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Counterfactuals",
            "Fairness",
            "Causal inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Matthew Botvinick",
            "Marc Toussaint"
        ],
        "dcterms:description": "A conceptual framework that treats planning as a form of inference.",
        "dcterms:title": "Planning as inference",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Science",
            "AI Planning"
        ],
        "dcat:keyword": [
            "Planning",
            "Inference",
            "Cognitive architectures"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Stuart Russell"
        ],
        "dcterms:description": "A discussion on the challenges of ensuring that AI systems are compatible with human values.",
        "dcterms:title": "Human Compatible",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Alignment",
            "Ethics"
        ],
        "dcat:keyword": [
            "Human compatibility",
            "AI control",
            "Ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Richard Sutton"
        ],
        "dcterms:description": "An exploration of the lessons learned from the history of AI research, emphasizing the importance of generalization.",
        "dcterms:title": "The Bitter Lesson",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
        "dcat:theme": [
            "AI Research",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Generalization",
            "AI lessons",
            "Research history"
        ],
        "dcat:landingPage": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]