[
    {
        "dcterms:creator": [],
        "dcterms:description": "SEC data encompasses a wide range of financial and regulatory information submitted by publicly-traded companies, including quarterly and annual reports, filings related to insider trading, proxy statements, and other critical documents.",
        "dcterms:title": "SEC Data",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Finance",
            "Regulatory Data"
        ],
        "dcat:keyword": [
            "Financial reports",
            "Regulatory filings",
            "Corporate governance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Financial analysis",
            "Risk assessment",
            "Regulatory compliance"
        ]
    },
    {
        "dcterms:creator": [
            "Chaoyi Wu",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "dcterms:description": "PMC-LLaMA is an open-source medical-specific large language model that incorporates data-centric knowledge injection with pure continual pre-training and medical-specific instruction tuning.",
        "dcterms:title": "PMC-LLaMA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Medicine",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Medical language model",
            "Open-source",
            "Knowledge injection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Medical question answering",
            "Medical text generation"
        ]
    },
    {
        "dcterms:creator": [
            "Fengbin Zhu",
            "Wenqiang Lei",
            "Youcheng Huang",
            "Chao Wang",
            "Shuo Zhang",
            "Jiancheng Lv",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "dcterms:description": "TAT-QA is a question answering benchmark on a hybrid of tabular and textual content in finance.",
        "dcterms:title": "TAT-QA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Finance",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Tabular data",
            "Textual content",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering",
            "Data interpretation"
        ]
    },
    {
        "dcterms:creator": [
            "Zhiyu Chen",
            "Shiyang Li",
            "Charese Smiley",
            "Zhiqiang Ma",
            "Sameena Shah",
            "William Yang Wang"
        ],
        "dcterms:description": "ConvFinQA explores the chain of numerical reasoning in conversational finance question answering.",
        "dcterms:title": "ConvFinQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Finance",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Conversational finance",
            "Numerical reasoning",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational question answering",
            "Numerical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi"
        ],
        "dcterms:description": "BIG-bench quantifies and extrapolates the capabilities of language models.",
        "dcterms:title": "BIG-bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Language model capabilities",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Capability assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "Amin Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "dcterms:description": "AGIEval is a human-centric benchmark for evaluating foundation models.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Model Evaluation",
            "Human-Centric AI"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Evaluation",
            "Foundation models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Human-centric assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Keiran Paster",
            "Marco Dos Santos",
            "Zhangir Azerbayev",
            "Jimmy Ba"
        ],
        "dcterms:description": "Openwebmath is an open dataset of high-quality mathematical web text.",
        "dcterms:title": "open-web-math/open-web-math",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Mathematical text",
            "Web dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical text generation",
            "Mathematical reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "dcterms:description": "Wikitext is a dataset used for language modeling and text generation.",
        "dcterms:title": "Wikitext",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text generation",
            "Language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "Yiming Chen"
        ],
        "dcterms:description": "Refinedweb-3m is a high-quality web dataset.",
        "dcterms:title": "refinedweb-3m",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Web Data",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Web dataset",
            "High-quality text"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Web text analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Luca Soldaini",
            "Kyle Lo"
        ],
        "dcterms:description": "peS2o (Pretraining Efficiently on S2ORC) is a dataset designed for efficient pretraining of language models.",
        "dcterms:title": "peS2o",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/allenai/pes2o",
        "dcat:theme": [
            "Natural Language Processing",
            "Pretraining"
        ],
        "dcat:keyword": [
            "Efficient pretraining",
            "Language models"
        ],
        "dcat:landingPage": "https://github.com/allenai/pes2o",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language model pretraining"
        ]
    }
]