[
    {
        "dcterms:creator": [
            "Srinivasan Iyer",
            "Xi Victoria Lin",
            "Ramakanth Pasunuru",
            "Todor Mihaylov",
            "Dániel Simig",
            "Ping Yu",
            "Kurt Shuster",
            "Tianlu Wang",
            "Qing Liu",
            "Punit Singh Koura",
            "Xian Li",
            "Brian O’Horo",
            "Gabriel Pereyra",
            "Jeﬀ Wang",
            "Christopher Dewan",
            "Asli Celikyilmaz",
            "Luke Zettlemoyer",
            "Ves Stoyanov"
        ],
        "dcterms:description": "A large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, used to evaluate model generalization abilities.",
        "dcterms:title": "OPT-IML Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Instruction Meta-Learning",
            "NLP tasks",
            "Model evaluation",
            "Generalization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Generalization assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Victor Sanh",
            "Albert Webson",
            "Colin Raﬀel",
            "Stephen Bach",
            "Lintang Sutawika",
            "Zaid Alyafeai",
            "Antoine Chaﬃn",
            "Arnaud Stiegler",
            "Arun Raja",
            "Manan Dey",
            "M Saiful Bari",
            "Canwen Xu",
            "Urmish Thakker",
            "Shanya Sharma",
            "Eliza Szczechla",
            "Taewoon Kim",
            "Gunjan Chhablani",
            "Nihal Nayak",
            "Debajyoti Datta",
            "Jonathan Chang",
            "Mike Tian-Jian Jiang",
            "Han Wang",
            "Matteo Manica",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Harshit Pandey",
            "Rachel Bawden",
            "Thomas Wang",
            "Trishala Neeraj",
            "Jos Rozen",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Thibault Fevry",
            "Jason Alan Fries",
            "Ryan Teehan",
            "Teven Le Scao",
            "Nihal Nayak",
            "Debajyoti Datta",
            "Jonathan Chang",
            "Mike Tian-Jian Jiang",
            "Han Wang",
            "Matteo Manica",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Harshit Pandey",
            "Rachel Bawden",
            "Thomas Wang",
            "Trishala Neeraj",
            "Jos Rozen",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Thibault Fevry",
            "Jason Alan Fries",
            "Ryan Teehan",
            "Teven Le Scao"
        ],
        "dcterms:description": "An Integrated Development Environment and Repository for Natural Language Prompts, providing a collection of prompts for various NLP tasks.",
        "dcterms:title": "PromptSource",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Prompt Engineering"
        ],
        "dcat:keyword": [
            "Prompt repository",
            "NLP tasks",
            "Natural language prompts"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Prompt generation",
            "Task formulation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Wei",
            "M. Bosma",
            "V. Zhao",
            "K. Guu",
            "A. W. Yu",
            "B. Lester",
            "N. Du",
            "A. M. Dai",
            "Q. V. Le"
        ],
        "dcterms:description": "A benchmark that demonstrates the zero-shot learning capabilities of fine-tuned language models across various tasks.",
        "dcterms:title": "FLAN",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Zero-shot learning",
            "Fine-tuning",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Zero-shot learning",
            "Task evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "A. Wang",
            "S. Prabhumoye",
            "A. W. Black"
        ],
        "dcterms:description": "A benchmark for instruction following in NLP, providing a comprehensive set of tasks and evaluation metrics.",
        "dcterms:title": "Super-NaturalInstructions",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "Instruction following",
            "NLP tasks",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following",
            "Task evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "T. Xie",
            "C. H. Wu",
            "P. Shi",
            "R. Zhong",
            "T. Scholak",
            "M. Yasunaga",
            "S. I. Wang"
        ],
        "dcterms:description": "A framework for unifying and multitasking structured knowledge grounding with text-to-text language models.",
        "dcterms:title": "UniﬁedSKG",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Knowledge Grounding"
        ],
        "dcat:keyword": [
            "Structured knowledge",
            "Text-to-text models",
            "Multi-task learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Knowledge grounding",
            "Multi-task learning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding, providing a comprehensive evaluation framework.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Task evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "A. Srivastava",
            "A. Rastogi",
            "A. Rao",
            "A. Abid"
        ],
        "dcterms:description": "A benchmark that quantifies and extrapolates the capabilities of language models beyond imitation.",
        "dcterms:title": "Big-Bench Hard (BBH)",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "Language model capabilities",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Capability assessment"
        ]
    },
    {
        "dcterms:creator": [
            "P. Rajpurkar",
            "R. Jia",
            "P. Liang"
        ],
        "dcterms:description": "A dataset providing over 100,000 questions for machine comprehension of text, widely used for evaluating reading comprehension models.",
        "dcterms:title": "SQuAD v1/v2",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Machine comprehension",
            "Question answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading comprehension",
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "V. Aribandi",
            "Y. Tay",
            "T. Schuster",
            "J. Rao",
            "H. S. Zheng",
            "S. V. Mehta",
            "H. Zhuang"
        ],
        "dcterms:description": "A framework for extreme multi-task scaling for transfer learning, focusing on diverse NLP tasks.",
        "dcterms:title": "ExMix",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Transfer Learning"
        ],
        "dcat:keyword": [
            "Multi-task learning",
            "Transfer learning",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Transfer learning",
            "Multi-task evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "V. Sanh",
            "A. Webson",
            "C. Raﬀel",
            "S. Bach",
            "L. Sutawika",
            "Z. Alyafeai"
        ],
        "dcterms:description": "A benchmark for natural language processing that evaluates models on various tasks.",
        "dcterms:title": "T0",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "Natural language processing",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Task assessment"
        ]
    }
]