[
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Vineet Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano"
        ],
        "dcterms:description": "A dataset for training verifiers to solve math word problems, demonstrating the capabilities of large language models in arithmetic reasoning.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2110.14168",
        "dcat:theme": [
            "Mathematical Reasoning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Math word problems",
            "Arithmetic reasoning",
            "Large language models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2110.14168",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "dcterms:description": "A dataset for evaluating reasoning in repeated games with large language models, focusing on commonsense reasoning.",
        "dcterms:title": "AQuA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2305.16867",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Game Theory"
        ],
        "dcat:keyword": [
            "Repeated games",
            "Large language models",
            "Reasoning"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2305.16867",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Ramakrishna Bairi",
            "Atharv Sonwane",
            "Aditya Kanade",
            "Arun Iyer",
            "Suresh Parthasarathy",
            "Sriram Rajamani",
            "B Ashok",
            "Shashank Shet"
        ],
        "dcterms:description": "A dataset designed for repository-level coding tasks using large language models and planning.",
        "dcterms:title": "SVAMP",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2309.12499",
        "dcat:theme": [
            "Code Generation",
            "Planning"
        ],
        "dcat:keyword": [
            "Repository-level coding",
            "Large language models",
            "Planning"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2309.12499",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Zhen Bi",
            "Ningyu Zhang",
            "Yinuo Jiang",
            "Shumin Deng",
            "Guozhou Zheng",
            "Huajun Chen"
        ],
        "dcterms:description": "A dataset for evaluating commonsense reasoning in question answering tasks.",
        "dcterms:title": "CSQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2308.15452",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Question answering",
            "Large language models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2308.15452",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Geva et al."
        ],
        "dcterms:description": "A question answering benchmark that requires implicit reasoning strategies.",
        "dcterms:title": "Strategy QA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://aclanthology.org/2021.tacl-1.21",
        "dcat:theme": [
            "Question Answering",
            "Implicit Reasoning"
        ],
        "dcat:keyword": [
            "Question answering",
            "Implicit reasoning",
            "Benchmark"
        ],
        "dcat:landingPage": "https://aclanthology.org/2021.tacl-1.21",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Wei et al."
        ],
        "dcterms:description": "A dataset that explores symbolic reasoning through concatenation of last letters.",
        "dcterms:title": "Last Letter Concatenation",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Symbolic Reasoning"
        ],
        "dcat:keyword": [
            "Symbolic reasoning",
            "Concatenation",
            "Large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Symbolic Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Wei et al."
        ],
        "dcterms:description": "A dataset that evaluates reasoning through coin flipping tasks.",
        "dcterms:title": "Coin Flip",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Symbolic Reasoning"
        ],
        "dcat:keyword": [
            "Symbolic reasoning",
            "Coin flipping",
            "Large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Symbolic Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Saurav Kadavath",
            "Akul Arora",
            "Steven Basart",
            "Eric Tang",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A dataset for measuring mathematical problem solving capabilities of models.",
        "dcterms:title": "MATH Dataset",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html",
        "dcat:theme": [
            "Mathematical Reasoning"
        ],
        "dcat:keyword": [
            "Mathematical problem solving",
            "Benchmark",
            "Large language models"
        ],
        "dcat:landingPage": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "dcterms:description": "A dataset that evaluates the ability of large language models to improve by self-critiquing their own plans.",
        "dcterms:title": "MultiMedQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2310.08118",
        "dcat:theme": [
            "Question Answering",
            "Medical Knowledge"
        ],
        "dcat:keyword": [
            "Medical question answering",
            "Self-critiquing",
            "Large language models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2310.08118",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Singhal et al."
        ],
        "dcterms:description": "A dataset that encodes clinical knowledge for evaluating language models in the medical domain.",
        "dcterms:title": "HealthSearchQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2212.13138",
        "dcat:theme": [
            "Medical Knowledge",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Clinical knowledge",
            "Medical question answering",
            "Large language models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2212.13138",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]