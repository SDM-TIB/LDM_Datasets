[
    {
        "dcterms:creator": [
            "Jieming Cui",
            "Ziren Gong",
            "Baoxiong Jia",
            "Siyuan Huang",
            "Zilong Zheng",
            "Jianzhu Ma",
            "Yixin Zhu"
        ],
        "dcterms:description": "ProBio is a protocol-guided multimodal dataset designed for studying activity understanding in Molecular Biology Labs. It includes fine-grained hierarchical annotations and video recordings of common biology experiments.",
        "dcterms:title": "ProBio",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://probio-dataset.github.io",
        "dcat:theme": [
            "Molecular Biology",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Multimodal dataset",
            "Activity understanding",
            "Biology experiments",
            "Hierarchical annotations"
        ],
        "dcat:landingPage": "https://probio-dataset.github.io",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition",
            "Solution tracking"
        ]
    },
    {
        "dcterms:creator": [
            "D. Damen",
            "H. Doughty",
            "G. M. Farinella",
            "A. Furnari",
            "E. Kazakos",
            "J. Ma",
            "D. Moltisanti",
            "J. Munro",
            "T. Perrett",
            "W. Price"
        ],
        "dcterms:description": "EPIC-Kitchen is a dataset for egocentric vision, focusing on understanding activities in kitchen environments through video recordings.",
        "dcterms:title": "EPIC-Kitchen",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2006.13256",
        "dcat:theme": [
            "Computer Vision",
            "Activity Recognition"
        ],
        "dcat:keyword": [
            "Egocentric video",
            "Kitchen activities",
            "Action recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Miech",
            "D. Zhukov",
            "J.-B. Alayrac",
            "M. Tapaswi",
            "I. Laptev",
            "J. Sivic"
        ],
        "dcterms:description": "YouCook2 is a dataset for learning video representations from instructional cooking videos, providing a large-scale collection of video clips with corresponding text descriptions.",
        "dcterms:title": "YouCook2",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Instructional Videos"
        ],
        "dcat:keyword": [
            "Cooking videos",
            "Instructional learning",
            "Video-text alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video understanding",
            "Text-video alignment"
        ]
    },
    {
        "dcterms:creator": [
            "B. Jia",
            "Y. Chen",
            "S. Huang",
            "S.-C. Zhu"
        ],
        "dcterms:description": "LEMMA is a multi-view dataset designed for learning multi-agent multi-task activities, providing a comprehensive framework for understanding complex interactions.",
        "dcterms:title": "LEMMA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-agent Systems",
            "Activity Recognition"
        ],
        "dcat:keyword": [
            "Multi-view dataset",
            "Multi-agent activities",
            "Task understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Sener",
            "D. Chatterjee",
            "D. Shelepov",
            "K. He",
            "D. Singhania",
            "R. Wang",
            "A. Yao"
        ],
        "dcterms:description": "COIN is a large-scale multi-view video dataset for understanding procedural activities, focusing on assembly tasks.",
        "dcterms:title": "COIN",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Procedural Understanding"
        ],
        "dcat:keyword": [
            "Multi-view video",
            "Procedural activities",
            "Assembly tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "A. Miech",
            "D. Zhukov",
            "J.-B. Alayrac",
            "M. Tapaswi",
            "I. Laptev",
            "J. Sivic"
        ],
        "dcterms:description": "HowTo100M is a large-scale dataset for learning text-video embeddings by watching narrated video clips, providing a rich resource for multimodal learning.",
        "dcterms:title": "HowTo100M",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Text-video embeddings",
            "Narrated videos",
            "Multimodal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Text-video alignment"
        ]
    },
    {
        "dcterms:creator": [
            "H. Jhuang",
            "J. Gall",
            "S. Zufﬁ",
            "C. Schmid",
            "M. J. Black"
        ],
        "dcterms:description": "HOMAGE is a dataset aimed at understanding action recognition, providing a collection of videos with annotated actions.",
        "dcterms:title": "HOMAGE",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Action recognition",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Jhuang",
            "J. Gall",
            "S. Zufﬁ",
            "C. Schmid",
            "M. J. Black"
        ],
        "dcterms:description": "IAW is a dataset for understanding action recognition, focusing on various action categories.",
        "dcterms:title": "IAW",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Action recognition",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Jhuang",
            "J. Gall",
            "S. Zufﬁ",
            "C. Schmid",
            "M. J. Black"
        ],
        "dcterms:description": "FineGYM is a dataset for fine-grained action recognition, providing a collection of videos with detailed action annotations.",
        "dcterms:title": "FineGYM",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fine-grained Action Recognition",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Fine-grained actions",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Jhuang",
            "J. Gall",
            "S. Zufﬁ",
            "C. Schmid",
            "M. J. Black"
        ],
        "dcterms:description": "FineDiving is a dataset for understanding fine-grained actions in diving, providing a collection of annotated videos.",
        "dcterms:title": "FineDiving",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fine-grained Action Recognition",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Diving actions",
            "Video dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action recognition"
        ]
    }
]