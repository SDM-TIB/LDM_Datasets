[
    {
        "dcterms:creator": [
            "Jonathan Berant",
            "Andrew Chou",
            "Roy Frostig",
            "Percy Liang"
        ],
        "dcterms:description": "A novel KGQA dataset requiring commonsense reasoning and long-tail knowledge about entities, consisting of two subtasks: question answering and claim verification.",
        "dcterms:title": "CR-LT-KGQA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/D3Mlab/cr-lt-kgqa",
        "dcat:theme": [
            "Knowledge Graph Question Answering",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "KGQA",
            "Commonsense Reasoning",
            "Long-Tail Knowledge",
            "Question Answering",
            "Claim Verification"
        ],
        "dcat:landingPage": "https://github.com/D3Mlab/cr-lt-kgqa",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering",
            "Claim Verification"
        ]
    },
    {
        "dcterms:creator": [
            "Mor Geva",
            "Daniel Khashabi",
            "Elad Segal",
            "Tushar Khot",
            "Dan Roth",
            "Jonathan Berant"
        ],
        "dcterms:description": "A true/false QA dataset where the required reasoning steps are implicit and should be inferred using a strategy.",
        "dcterms:title": "StrategyQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1162/TACL_A_00370",
        "dcat:theme": [
            "Question Answering",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "True/False Questions",
            "Implicit Reasoning",
            "QA Benchmark"
        ],
        "dcat:landingPage": "https://doi.org/10.1162/TACL_A_00370",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yasumasa Onoe",
            "Michael J. Q. Zhang",
            "Eunsol Choi",
            "Greg Durrett"
        ],
        "dcterms:description": "A dataset for commonsense reasoning over entity knowledge, requiring reasoning about entities to answer queries.",
        "dcterms:title": "CREAK",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract-round2.html",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Entity Knowledge"
        ],
        "dcat:keyword": [
            "Commonsense Reasoning",
            "Entity Knowledge",
            "QA Dataset"
        ],
        "dcat:landingPage": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract-round2.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    }
]