[
    {
        "dcterms:creator": [
            "Jifan Yu",
            "Xiaozhi Wang",
            "Shangqing Tu",
            "Shulin Cao",
            "Daniel Zhang-Li",
            "Xin Lv",
            "Hao Peng",
            "Zijun Yao",
            "Xiaohan Zhang",
            "Hanming Li",
            "Chunyang Li",
            "Zheyuan Zhang",
            "Yushi Bai",
            "Yantao Liu",
            "Amy Xin",
            "Kaifeng Yun",
            "Linlu GONG",
            "Nianyi Lin",
            "Jianhui Chen",
            "Zhili Wu",
            "Yunjia Qi",
            "Weikai Li",
            "Yong Guan",
            "Kaisheng Zeng",
            "Ji Qi",
            "Hailong Jin",
            "Jinxin Liu",
            "Yu Gu",
            "Yuan Yao",
            "Ning Ding",
            "Lei Hou",
            "Zhiyuan Liu",
            "Xu Bin",
            "Jie Tang",
            "Juanzi Li"
        ],
        "dcterms:description": "A large-scale quantitative benchmark for evaluating how well LLMs apply their world knowledge.",
        "dcterms:title": "KoLA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Knowledge application",
            "Large language models",
            "World knowledge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Knowledge evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Albert Q. Jiang",
            "Alexandre Sablayrolles",
            "Arthur Mensch",
            "Chris Bamford",
            "Devendra Singh Chaplot",
            "Diego de las Casas",
            "Florian Bressand",
            "Gianna Lengyel",
            "Guillaume Lample",
            "Lucile Saulnier",
            "Lélio Renard Lavaud",
            "Marie-Anne Lachaux",
            "Pierre Stock",
            "Teven Le Scao",
            "Thibaut Lavril",
            "Thomas Wang",
            "Timothée Lacroix",
            "William El Sayed"
        ],
        "dcterms:description": "A hand-built bias benchmark for question answering.",
        "dcterms:title": "BBQ",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias evaluation",
            "Question answering"
        ],
        "dcat:keyword": [
            "Bias",
            "Question answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Bias detection"
        ]
    },
    {
        "dcterms:creator": [
            "Lichao Sun",
            "Yue Huang",
            "Haoran Wang",
            "Siyuan Wu",
            "Qihui Zhang",
            "Yuan Li",
            "Chujie Gao",
            "Yixin Huang",
            "Wenhan Lyu",
            "Yixuan Zhang",
            "Xiner Li",
            "Zhengliang Liu",
            "Yixin Liu",
            "Yijue Wang",
            "Zhikun Zhang",
            "Bertie Vidgen",
            "Bhavya Kailkhura",
            "Caiming Xiong",
            "Chaowei Xiao",
            "Chunyuan Li",
            "Eric Xing",
            "Furong Huang",
            "Hao Liu",
            "Heng Ji",
            "Hongyi Wang",
            "Huan Zhang",
            "Huaxiu Yao",
            "Manolis Kellis",
            "Marinka Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Neil Zhenqiang Gong",
            "Philip S. Yu",
            "Pin-Yu Chen",
            "Quanquan Gu",
            "Ran Xu",
            "Rex Ying",
            "Shuiwang Ji",
            "Suman Jana",
            "Tianlong Chen",
            "Tianming Liu",
            "Tianyi Zhou",
            "William Wang",
            "Xiang Li",
            "Xiangliang Zhang",
            "Xiao Wang",
            "Xing Xie",
            "Xun Chen",
            "Xuyu Wang",
            "Yan Liu",
            "Yanfang Ye",
            "Yinzhi Cao",
            "Yong Chen",
            "Yue Zhao"
        ],
        "dcterms:description": "A dataset focused on trustworthiness in large language models.",
        "dcterms:title": "TrustLLM",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2401.05561",
        "dcat:theme": [
            "Trustworthiness",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Trust",
            "Large language models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Trust evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Albert Q. Jiang",
            "Alexandre Sablayrolles",
            "Arthur Mensch",
            "Chris Bamford",
            "Devendra Singh Chaplot",
            "Diego de las Casas",
            "Florian Bressand",
            "Gianna Lengyel",
            "Guillaume Lample",
            "Lucile Saulnier",
            "Lélio Renard Lavaud",
            "Marie-Anne Lachaux",
            "Pierre Stock",
            "Teven Le Scao",
            "Thibaut Lavril",
            "Thomas Wang",
            "Timothée Lacroix",
            "William El Sayed"
        ],
        "dcterms:description": "A dataset for evaluating the performance of large language models.",
        "dcterms:title": "Mistral",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.06825",
        "dcat:theme": [
            "Model evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Model performance",
            "Evaluation",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Sophia Yang",
            "Szymon Antoniak",
            "Teven Le Scao",
            "Théophile Gervet",
            "Thibaut Lavril",
            "Thomas Wang",
            "Timothée Lacroix",
            "William El Sayed"
        ],
        "dcterms:description": "A dataset that combines multiple expert models for evaluation.",
        "dcterms:title": "Mixtral",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2401.04088",
        "dcat:theme": [
            "Model evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Expert models",
            "Evaluation",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Meta"
        ],
        "dcterms:description": "The most capable openly available large language model to date.",
        "dcterms:title": "Llama-3",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Large Language Models",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Open model",
            "Language model",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Natural language processing"
        ]
    },
    {
        "dcterms:creator": [
            "Google"
        ],
        "dcterms:description": "Research and technology related to the Gemini project.",
        "dcterms:title": "Gemma",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2403.08295",
        "dcat:theme": [
            "Research",
            "Technology"
        ],
        "dcat:keyword": [
            "Gemini",
            "Research",
            "Technology"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Research evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "The introduction of ChatGPT, a conversational AI model.",
        "dcterms:title": "GPT-3.5",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Chatbot",
            "Conversational AI",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Conversational tasks"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "The introduction of GPT-4o, an advanced version of the GPT model.",
        "dcterms:title": "GPT-4o",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Advanced model",
            "Conversational AI",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Conversational tasks"
        ]
    }
]