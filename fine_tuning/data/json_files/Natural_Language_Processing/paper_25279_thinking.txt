To extract datasets from the research paper titled "Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning" by Haoye Dong et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations accurately.

First, I will read through the **abstract, introduction, and experiments sections** of the paper. The abstract mentions that the authors conducted extensive experiments on several benchmarks, which suggests that multiple datasets are involved. I will look for specific names of datasets in these sections.

Next, I will focus on the **Experiments section**, particularly the subsection titled **Datasets**. Here, the authors explicitly list the datasets used for training their model. They mention that they trained Hamba on 2.7 million training samples from multiple datasets, which include:

1. **FreiHAND**: A large-scale multiview hand dataset with 3D hand annotations.
2. **HO3D**: A markerless hand-object interaction dataset.
3. **MTC**: A dataset capturing both 3D body and hand poses.
4. **RHD**: A synthetic dataset for hand pose estimation.
5. **InterHand2.6M**: A large-scale dataset for hand interactions.
6. **H2O3D**: A dataset for hand-object interactions.
7. **DexYCB**: A dataset for hand manipulation of objects.
8. **COCO-Wholebody**: A dataset with annotations for whole-body keypoints.
9. **Halpe**: A dataset for full-body human-object interactions.
10. **MPII NZSL**: A mixed dataset containing various hand interactions.

I will then check the **References section** to find the full citations for each of these datasets. This is crucial for proper attribution and to provide readers with the necessary information to access these datasets.

The citations I will extract are as follows:

- **FreiHAND**:
  > Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and Thomas Brox. *FreiHAND: A dataset for markerless capture of hand pose and shape from single RGB images*. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 813–822, 2019.

- **HO3D**:
  > Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. *Honnotate: A method for 3D annotation of hand and object poses*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3196–3206, 2020.

- **MTC**:
  > Xingyu Chen, Baoyuan Wang, and Heung-Yeung Shum. *Hand avatar: Free-pose hand animation and rendering from monocular video*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8683–8693, 2023.

- **RHD**:
  > Iason Oikonomidis, Nikolaos Kyriazis, Antonis A Argyros. *Efficient model-based 3D tracking of hand articulations using Kinect*. In BMVC, volume 1, page 3, 2011.

- **InterHand2.6M**:
  > Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. *Interhand2.6M: A dataset and baseline for 3D interacting hand pose estimation from a single RGB image*. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16, pages 548–564. Springer, 2020.

- **H2O3D**:
  > Shreyas Hampali, Sayan Deb Sarkar, and Vincent Lepetit. *HO-3D: Improving the accuracy of hand-object annotations of the HO-3D dataset*. arXiv preprint arXiv:2107.00887, 2021.

- **DexYCB**:
  > Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. *DexYCB: A benchmark for capturing hand grasping of objects*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9044–9053, 2021.

- **COCO-Wholebody**:
  > Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo. *Whole-body human pose estimation in the wild*. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16, pages 196–214. Springer, 2020.

- **Halpe**:
  > Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. *AlphaPose: Whole-body regional multi-person pose estimation and tracking in real-time*. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

- **MPII NZSL**:
  > Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. *2D human pose estimation: New benchmark and state of the art analysis*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3686–3693, 2014.

After gathering all this information, I will compile the dataset entries into a structured format for easy reference and further processing.