[
    {
        "dcterms:creator": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalandidis",
            "Li-Jia Li",
            "David A Shamma"
        ],
        "dcterms:description": "A dataset that connects language and vision using crowdsourced dense image annotations, containing 75k object categories and 40k predicate categories.",
        "dcterms:title": "Visual Genome (VG)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Scene Graph Generation"
        ],
        "dcat:keyword": [
            "Image annotations",
            "Object categories",
            "Predicate categories",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Scene Graph Generation",
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Drew A Hudson",
            "Christopher D Manning"
        ],
        "dcterms:description": "A dataset for real-world visual reasoning and compositional question answering, featuring more than 3.8M relation annotations.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Visual reasoning",
            "Question answering",
            "Relation annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Reasoning",
            "Compositional Question Answering"
        ]
    }
]