[
    {
        "dcterms:creator": [
            "Klaus Greff",
            "Sjoerd Van Steenkiste",
            "JÃ¼rgen Schmidhuber"
        ],
        "dcterms:description": "VELOCITI is a benchmark designed to test perception and binding in video language models using complex movie clips and dense semantic role label annotations.",
        "dcterms:title": "VELOCITI",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://katha-ai.github.io/projects/velociti/",
        "dcat:theme": [
            "Video Understanding",
            "Language Processing"
        ],
        "dcat:keyword": [
            "Video-Language Models",
            "Compositionality",
            "Semantic Role Labeling"
        ],
        "dcat:landingPage": "https://katha-ai.github.io/projects/velociti/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Perception Testing",
            "Binding Testing"
        ]
    },
    {
        "dcterms:creator": [
            "Arka Sadhu",
            "Tanmay Gupta",
            "Mark Yatskar",
            "Ram Nevatia",
            "Aniruddha Kembhavi"
        ],
        "dcterms:description": "VidSitu is a video situation recognition dataset that consists of dense annotations with semantic role labels, sourced from movie clips, presenting challenging scenarios.",
        "dcterms:title": "VidSitu",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Semantic Role Labeling"
        ],
        "dcat:keyword": [
            "Video Situations",
            "Semantic Role Labels",
            "Complex Scenarios"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Situation Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Tristan Thrush",
            "Ryan Jiang",
            "Max Bartolo",
            "Amanpreet Singh",
            "Adina Williams",
            "Douwe Kiela",
            "Candace Ross"
        ],
        "dcterms:description": "Winoground is a benchmark that probes vision and language models for visio-linguistic compositionality using similar images and captions.",
        "dcterms:title": "Winoground",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Understanding"
        ],
        "dcat:keyword": [
            "Compositionality",
            "Vision-Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Compositional Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "dcterms:description": "SEED-Bench is a benchmark for evaluating multimodal large language models with generative comprehension.",
        "dcterms:title": "SEED-Bench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Generative Comprehension",
            "Multimodal Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Generative Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Kunchang Li",
            "Yali Wang",
            "Yinan He",
            "Yizhuo Li",
            "Yi Wang",
            "Yi Liu",
            "Zun Wang",
            "Jilan Xu",
            "Guo Chen",
            "Ping Luo"
        ],
        "dcterms:description": "MVBench is a comprehensive benchmark for multi-modal video understanding.",
        "dcterms:title": "MVBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Multi-modal Understanding",
            "Video Analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Video Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Madeleine Grunde-McLaughlin",
            "Ranjay Krishna",
            "Maneesh Agrawala"
        ],
        "dcterms:description": "AGQA is a benchmark for compositional spatio-temporal reasoning in videos.",
        "dcterms:title": "AGQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Spatio-Temporal Reasoning"
        ],
        "dcat:keyword": [
            "Compositional Reasoning",
            "Video Analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Spatio-Temporal Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Rohit Girdhar",
            "Deva Ramanan"
        ],
        "dcterms:description": "CATER is a diagnostic dataset for compositional actions and temporal reasoning.",
        "dcterms:title": "CATER",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Temporal Reasoning"
        ],
        "dcat:keyword": [
            "Compositional Actions",
            "Temporal Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Cheng-Yu Hsieh",
            "Jieyu Zhang",
            "Zixian Ma",
            "Aniruddha Kembhavi",
            "Ranjay Krishna"
        ],
        "dcterms:description": "SugarCrepe is a benchmark that addresses biases in vision-language compositionality.",
        "dcterms:title": "SugarCrepe",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Evaluation"
        ],
        "dcat:keyword": [
            "Vision-Language Compositionality",
            "Biases"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Bias Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Hritik Bansal",
            "Yonatan Bitton",
            "Idan Szpektor",
            "Kai-Wei Chang",
            "Aditya Grover"
        ],
        "dcterms:description": "VideoCon is a dataset for robust video-language alignment using contrast captions.",
        "dcterms:title": "VideoCon",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video-Language Alignment"
        ],
        "dcat:keyword": [
            "Contrastive Learning",
            "Video-Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Video-Language Alignment"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jong Wook Kim",
            "Chris Hallacy",
            "Aditya Ramesh",
            "Gabriel Goh",
            "Sandhini Agarwal",
            "Girish Sastry",
            "Amanda Askell",
            "Pamela Mishkin",
            "Jack Clark"
        ],
        "dcterms:description": "CLIP is a model that learns transferable visual representations from natural language supervision.",
        "dcterms:title": "CLIP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Representation Learning"
        ],
        "dcat:keyword": [
            "Transfer Learning",
            "Visual Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Representation Learning"
        ]
    }
]