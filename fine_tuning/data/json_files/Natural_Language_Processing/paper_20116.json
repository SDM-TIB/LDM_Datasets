[
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset comprising over 13,000 clips across 101 action categories, utilized for experiments in human action recognition.",
        "dcterms:title": "UCF101 Videos dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Video dataset",
            "Action recognition",
            "Human actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "C. Lea",
            "M. D. Flynn",
            "R. Vidal",
            "A. Reiter",
            "G. D. Hager"
        ],
        "dcterms:description": "Temporal Convolutional Networks (TCNs) are effective for action segmentation and detection in videos, capturing long-range temporal dependencies.",
        "dcterms:title": "Temporal Convolutional Networks (TCNs)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Temporal analysis",
            "Action segmentation",
            "Action detection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Segmentation",
            "Action Detection"
        ]
    },
    {
        "dcterms:creator": [
            "K. Simonyan",
            "A. Zisserman"
        ],
        "dcterms:description": "Two-Stream Convolutional Networks leverage both spatial and temporal dimensions for effective action recognition in videos.",
        "dcterms:title": "Two-Stream Convolutional Networks",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Spatial features",
            "Temporal dynamics",
            "Action recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "L. Zhao",
            "X. Peng",
            "Y. Tian",
            "M. Kapadia",
            "D. N. Metaxas"
        ],
        "dcterms:description": "Graph Convolutional Networks (GCNs) model spatial relationships between body parts, enhancing action recognition capabilities.",
        "dcterms:title": "Graph Convolutional Networks (GCNs)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Spatial relationships",
            "Human pose",
            "Action recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "V. G. T. da Costa",
            "G. Zara",
            "P. Rota",
            "T. Oliveira-Santos",
            "N. Sebe",
            "V. Murino",
            "E. Ricci"
        ],
        "dcterms:description": "Transformer models are applied for action recognition, focusing on unsupervised domain adaptation to generalize across different video domains.",
        "dcterms:title": "Transformer models",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Unsupervised domain adaptation",
            "Action recognition",
            "Video analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Sang",
            "Z. Zhao",
            "D. He"
        ],
        "dcterms:description": "The Two-Level Attention Model is designed to weigh the importance of different frames in a video sequence for improved action recognition.",
        "dcterms:title": "Two-Level Attention Model",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/ACCESS.2019.2936628",
        "dcat:theme": [],
        "dcat:keyword": [
            "Attention mechanisms",
            "Action recognition",
            "Video analysis"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/ACCESS.2019.2936628",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]