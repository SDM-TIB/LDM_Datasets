[
    {
        "dcterms:creator": [
            "Qing Li",
            "Qingyi Tao",
            "Shafiq Joty",
            "Jianfei Cai",
            "Jiebo Luo"
        ],
        "dcterms:description": "VQA-R is an explanatory VQA dataset synthesized from VQA-E, containing image/question pairs annotated with answers, textual rationales, and visual rationales.",
        "dcterms:title": "VQA-R",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Explanatory Datasets"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Textual Rationale",
            "Visual Rationale",
            "Dataset Synthesis"
        ],
        "dcat:landingPage": "https://github.com/lik1996/CRVQA2024",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Qing Li",
            "Qingyi Tao",
            "Shafiq Joty",
            "Jianfei Cai",
            "Jiebo Luo"
        ],
        "dcterms:description": "VQA-E is a dataset that provides explanations for visual questions, including answer/text pairs and related images.",
        "dcterms:title": "VQA-E",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Explanatory Datasets"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Textual Explanation",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Dong Huk Park",
            "Lisa Anne Hendricks",
            "Zeynep Akata",
            "Anna Rohrbach",
            "Bernt Schiele",
            "Trevor Darrell",
            "Marcus Rohrbach"
        ],
        "dcterms:description": "VQA-X is a dataset containing image/question pairs with corresponding textual explanations that are human-annotated.",
        "dcterms:title": "VQA-X",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Explanatory Datasets"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Textual Explanation",
            "Human Annotation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "VQAv2 is a widely-used benchmark dataset for visual question answering, consisting of image/question pairs for answer prediction evaluation.",
        "dcterms:title": "VQAv2",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Benchmark Datasets"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Benchmark",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll√°r",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "COCO is a dataset that provides common objects in context, widely used for various computer vision tasks including object detection.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Object Detection",
            "Dataset",
            "Common Objects"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection"
        ]
    }
]