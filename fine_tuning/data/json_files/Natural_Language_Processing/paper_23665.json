[
    {
        "dcterms:creator": [
            "J. Achiam",
            "S. Adler",
            "S. Agarwal",
            "L. Ahmad",
            "I. Akkaya",
            "F. L. Aleman",
            "D. Almeida",
            "J. Altenschmidt",
            "S. Altman",
            "S. Anadkat"
        ],
        "dcterms:description": "A cognition inspired multilingual and multimodal benchmark designed to evaluate the general intelligence ability of multimodal large language models (MLLMs) based on the Cattell-Horn-Carroll (CHC) model of intelligence.",
        "dcterms:title": "M3GIA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Assessment",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "General Intelligence",
            "Cognitive Factors",
            "Multilingual",
            "Benchmark"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/Songweii/M3GIA",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Cognitive Ability Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Raven"
        ],
        "dcterms:description": "A nonverbal assessment tool used to measure abstract reasoning and cognitive ability through pattern recognition.",
        "dcterms:title": "Ravenâ€™s Progressive Matrices Test",
        "dcterms:issued": "2003",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Assessment"
        ],
        "dcat:keyword": [
            "Abstract Reasoning",
            "Cognitive Ability"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Test",
        "mls:task": [
            "Cognitive Ability Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "F. A. Schrank",
            "B. J. Wendling"
        ],
        "dcterms:description": "A comprehensive assessment tool for measuring cognitive abilities across various domains, including verbal and nonverbal skills.",
        "dcterms:title": "Woodcock-Johnson IV Tests of Cognitive Abilities (WJ IV)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Assessment"
        ],
        "dcat:keyword": [
            "Cognitive Abilities",
            "Assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Test",
        "mls:task": [
            "Cognitive Ability Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "G. H. Roid",
            "M. Pomplun"
        ],
        "dcterms:description": "An intelligence test designed to measure cognitive abilities in individuals, focusing on various aspects of intelligence.",
        "dcterms:title": "Stanford-Binet Intelligence Scales, Fifth Edition (SB5)",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cognitive Assessment"
        ],
        "dcat:keyword": [
            "Intelligence Testing",
            "Cognitive Abilities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Test",
        "mls:task": [
            "Cognitive Ability Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Lu",
            "H. Bansal",
            "T. Xia",
            "J. Liu",
            "C. Li",
            "H. Hajishirzi",
            "H. Cheng",
            "K.-W. Chang",
            "M. Galley",
            "J. Gao"
        ],
        "dcterms:description": "A benchmark for evaluating the mathematical reasoning capabilities of foundation models in visual contexts.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematical Reasoning",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Mathematics",
            "Visual Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Test",
        "mls:task": [
            "Mathematical Reasoning Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Liu",
            "H. Duan",
            "Y. Zhang",
            "B. Li",
            "S. Zhang",
            "W. Zhao",
            "Y. Yuan",
            "J. Wang",
            "C. He",
            "Z. Liu"
        ],
        "dcterms:description": "A benchmark designed to evaluate the performance of multi-modal models across various tasks.",
        "dcterms:title": "MMBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-modal Learning",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Multi-modal Models",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Test",
        "mls:task": [
            "Multi-modal Evaluation"
        ]
    }
]