[
    {
        "dcterms:creator": [
            "S. R. Bowman",
            "G. Angeli",
            "C. Potts",
            "C. D. Manning"
        ],
        "dcterms:description": "A large annotated corpus for learning natural language inference, used for training models to understand and predict entailment relationships between sentences.",
        "dcterms:title": "NLI dataset",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/D15-1075",
        "dcat:theme": [
            "Natural Language Processing",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "Natural Language Inference",
            "Annotated Corpus",
            "Entailment"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/D15-1075",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "D. Cer",
            "M. Diab",
            "E. Agirre",
            "I. Lopez-Gazpio",
            "L. Specia"
        ],
        "dcterms:description": "The STS-B development set is part of the Semantic Textual Similarity evaluation, focusing on multilingual and cross-lingual tasks.",
        "dcterms:title": "STS-B development set",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/S17-2001",
        "dcat:theme": [
            "Natural Language Processing",
            "Semantic Textual Similarity"
        ],
        "dcat:keyword": [
            "Semantic Textual Similarity",
            "Multilingual Evaluation",
            "Cross-lingual Evaluation"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/S17-2001",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Textual Similarity"
        ]
    },
    {
        "dcterms:creator": [
            "E. Agirre",
            "C. Banea",
            "C. Cardie",
            "D. Cer",
            "M. Diab",
            "A. Gonzalez-Agirre",
            "W. Guo",
            "R. Mihalcea",
            "G. Rigau",
            "J. Wiebe"
        ],
        "dcterms:description": "The STS benchmarks provide a set of tasks for evaluating semantic textual similarity across multiple languages.",
        "dcterms:title": "STS benchmarks",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/S16-1081",
        "dcat:theme": [
            "Natural Language Processing",
            "Semantic Textual Similarity"
        ],
        "dcat:keyword": [
            "Semantic Textual Similarity",
            "Evaluation Benchmark",
            "Cross-Lingual Evaluation"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/S16-1081",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Textual Similarity"
        ]
    },
    {
        "dcterms:creator": [
            "D. Cer",
            "M. Diab",
            "E. Agirre",
            "I. Lopez-Gazpio",
            "L. Specia"
        ],
        "dcterms:description": "The STS-B test set is used for evaluating models on semantic textual similarity tasks, focusing on multilingual and cross-lingual aspects.",
        "dcterms:title": "STS-B test set",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/S17-2001",
        "dcat:theme": [
            "Natural Language Processing",
            "Semantic Textual Similarity"
        ],
        "dcat:keyword": [
            "Semantic Textual Similarity",
            "Evaluation Benchmark",
            "Cross-Lingual Evaluation"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/S17-2001",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Textual Similarity"
        ]
    },
    {
        "dcterms:creator": [
            "M. Marelli",
            "S. Menini",
            "M. Baroni",
            "L. Bentivogli",
            "R. Bernardi",
            "R. Zamparelli"
        ],
        "dcterms:description": "The SICK dataset is designed for evaluating compositional distributional semantic models, providing sentence pairs with similarity scores.",
        "dcterms:title": "SICK dataset",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Semantic Similarity"
        ],
        "dcat:keyword": [
            "Semantic Similarity",
            "Compositional Distributional Semantics",
            "Evaluation Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Semantic Similarity"
        ]
    },
    {
        "dcterms:creator": [
            "T. Gao",
            "X. Yao",
            "D. Chen"
        ],
        "dcterms:description": "The SimCSE dataset is used for training sentence embeddings through contrastive learning, focusing on improving the quality of embeddings.",
        "dcterms:title": "SimCSE dataset",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/2021.emnlp-main.552",
        "dcat:theme": [
            "Natural Language Processing",
            "Sentence Embeddings"
        ],
        "dcat:keyword": [
            "Contrastive Learning",
            "Sentence Embeddings",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/2021.emnlp-main.552",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence Representation"
        ]
    }
]