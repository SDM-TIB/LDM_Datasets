[
    {
        "dcterms:creator": [
            "C. Busso",
            "M. Bulut",
            "C. C. Lee",
            "A. Kazemzadeh",
            "E. Mower",
            "S. Kim",
            "S. S. Narayanan"
        ],
        "dcterms:description": "The IEMOCAP dataset comprises 10,039 instances performed by 10 actors, who engage in multiple scripted and spontaneous emotional dialogues, portraying 10 predefined emotional states: angry, sad, happy, neutral, disgust, surprise, fear, excited, other, and unmarked. It includes various modalities such as audio, video, and text.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Human-Robot Interaction"
        ],
        "dcat:keyword": [
            "Emotion dataset",
            "Multimodal",
            "Dialogue",
            "Emotional states"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio, Video, Text",
        "mls:task": [
            "Emotion Recognition",
            "Valence Regression",
            "Arousal Regression"
        ]
    }
]