[
    {
        "dcterms:creator": [
            "Piyush Sharma",
            "Nan Ding",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "dcterms:description": "A cleaned, hypernymed image alt-text dataset for automatic image captioning, containing over 3 million image URLs.",
        "dcterms:title": "Conceptual Captions Dataset",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Automatic captioning",
            "Alt-text"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Stanislaw Antol",
            "Margaret Mitchell",
            "C. Lawrence Zitnick",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "dcterms:description": "A benchmark for visual question answering that includes 214K image-question-answer validation triplets.",
        "dcterms:title": "VQA v2 Benchmark",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "VQA dataset",
            "Question answering",
            "Image understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]