[
    {
        "dcterms:creator": [
            "Y. Goyal",
            "T. Khot",
            "D. Summers-Stay",
            "D. Batra",
            "D. Parikh"
        ],
        "dcterms:description": "The VQA v2 dataset associates two images to every question, each leading to different answers, discouraging blind guesses and improving evaluation of VQA systems.",
        "dcterms:title": "VQA v2",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1612.00837",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Image Understanding",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "R. Krishna",
            "Y. Zhu",
            "O. Groth",
            "J. Johnson",
            "K. Hata",
            "J. Kravitz",
            "S. Chen",
            "Y. Kalantidis",
            "L.-J. Li",
            "D. A. Shamma",
            "M. Bernstein",
            "L. Fei-Fei"
        ],
        "dcterms:description": "Visual Genome is a multipurpose dataset containing annotations of images in the form of scene graphs, providing fine-grained descriptions of image contents and diverse question formulations.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1602.07332",
        "dcat:theme": [
            "Image Annotation",
            "Scene Graphs"
        ],
        "dcat:keyword": [
            "Image Annotations",
            "Scene Graphs",
            "Visual Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering",
            "Image Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "T.-Y. Lin",
            "M. Maire",
            "S. Belongie",
            "J. Hays",
            "P. Perona",
            "D. Ramanan",
            "P. Doll√°r",
            "C. L. Zitnick"
        ],
        "dcterms:description": "COCO is a large-scale dataset that contains images of common objects in context, providing a rich set of annotations for various computer vision tasks.",
        "dcterms:title": "COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Object Detection",
            "Image Captioning"
        ],
        "dcat:keyword": [
            "Common Objects",
            "Image Annotations",
            "Object Detection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Detection",
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "S. Antol",
            "A. Agrawal",
            "J. Lu",
            "M. Mitchell",
            "D. Batra",
            "C. L. Zitnick",
            "D. Parikh"
        ],
        "dcterms:description": "VQA-real is a dataset that serves as a benchmark for Visual Question Answering, containing images and questions with ground truth answers.",
        "dcterms:title": "VQA-real",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Benchmark Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]