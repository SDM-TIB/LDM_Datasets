[
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "ChatGPT is a powerful language model developed by OpenAI that has played a significant role in expanding the reach and accessibility of AI-generated text.",
        "dcterms:title": "ChatGPT",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Language Models",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "AI-generated text",
            "Language model",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Conversational AI"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jeffrey Wu",
            "Rewon Child",
            "David Luan",
            "Dario Amodei",
            "Ilya Sutskever"
        ],
        "dcterms:description": "This dataset presents a model that learns to perform various tasks without explicit supervision, showcasing the capabilities of unsupervised learning in language models.",
        "dcterms:title": "Language models are unsupervised multitask learners",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Unsupervised Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Unsupervised learning",
            "Multitask learning",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multitask learning"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Karthik Narasimhan",
            "Tim Salimans",
            "Ilya Sutskever"
        ],
        "dcterms:description": "This dataset focuses on improving language understanding through generative pre-training, highlighting the effectiveness of pre-trained models in language tasks.",
        "dcterms:title": "Improving language understanding by generative pre-training",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Pre-training",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Generative pre-training",
            "Language understanding",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Brown",
            "Benjamin Mann",
            "Nick Ryder",
            "Melanie Subbiah",
            "Jared D Kaplan",
            "Prafulla Dhariwal",
            "Arvind Neelakantan",
            "Pranav Shyam",
            "Girish Sastry",
            "Amanda Askell"
        ],
        "dcterms:description": "This dataset demonstrates the few-shot learning capabilities of language models, showcasing their ability to perform tasks with minimal examples.",
        "dcterms:title": "Language models are few-shot learners",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Few-shot Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Few-shot learning",
            "Language models",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Few-shot learning"
        ]
    },
    {
        "dcterms:creator": [
            "Ninareh Mehrabi",
            "Fred Morstatter",
            "Nripsuta Saxena",
            "Kristina Lerman",
            "Aram Galstyan"
        ],
        "dcterms:description": "This dataset provides a comprehensive survey on bias and fairness in machine learning, discussing various aspects and implications of bias in AI systems.",
        "dcterms:title": "A survey on bias and fairness in machine learning",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias and Fairness",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Bias",
            "Fairness",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias evaluation",
            "Fairness assessment"
        ]
    },
    {
        "dcterms:creator": [
            "Kushal Tirumala",
            "Aram Markosyan",
            "Luke Zettlemoyer",
            "Armen Aghajanyan"
        ],
        "dcterms:description": "This dataset analyzes the training dynamics of large language models, focusing on memorization without overfitting.",
        "dcterms:title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Training Dynamics",
            "Large Language Models"
        ],
        "dcat:keyword": [
            "Memorization",
            "Overfitting",
            "Training dynamics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model training analysis"
        ]
    }
]