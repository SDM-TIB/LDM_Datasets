[
    {
        "dcterms:creator": [
            "Siwei Wu",
            "Kang Zhu",
            "Yu Bai",
            "Yiming Liang",
            "Yizhi Li",
            "Haoning Wu",
            "J.H. Liu",
            "Ruibo Liu",
            "Xingwei Qu",
            "Xuxin Cheng",
            "Ge Zhang",
            "Wenhao Huang",
            "Chenghua Lin"
        ],
        "dcterms:description": "A meticulously curated benchmark for evaluating multi-image relational association capabilities in large visual language models, comprising 1,024 samples and 11 subtasks at two granularity levels (image and entity) based on relations in ConceptNet.",
        "dcterms:title": "MMRA (Multi-granularity Multi-image Relational Association)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/Wusiwei0410/MMRA",
        "dcat:theme": [
            "Multi-modal Learning",
            "Visual Language Models"
        ],
        "dcat:keyword": [
            "Multi-image association",
            "Benchmark",
            "Visual Language Models",
            "Relational Association"
        ],
        "dcat:landingPage": "https://github.com/Wusiwei0410/MMRA",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multi-image perception",
            "Relational reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Nils Reimers",
            "Iryna Gurevych"
        ],
        "dcterms:description": "A dataset used to form image pairs based on semantic similarity for the MMRA benchmark, employing SentenceBERT to calculate the semantic similarity of image captions.",
        "dcterms:title": "LLaVA-665k-multi",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Processing",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Image pairs",
            "Semantic similarity",
            "Sentence embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image pair selection",
            "Annotation"
        ]
    },
    {
        "dcterms:creator": [
            "Robyn Speer",
            "Joshua Chin",
            "Catherine Havasi"
        ],
        "dcterms:description": "A knowledge graph that provides relations among different entities and events, used as a basis for defining subtasks in the MMRA benchmark.",
        "dcterms:title": "ConceptNet",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Knowledge Graph",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Knowledge representation",
            "Commonsense knowledge",
            "Relations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Commonsense reasoning",
            "Relation extraction"
        ]
    },
    {
        "dcterms:creator": [
            "Amanpreet Singh",
            "Vivek Natarjan",
            "Meet Shah",
            "Yu Jiang",
            "Xinlei Chen",
            "Dhruv Batra",
            "Devi Parikh",
            "Marcus Rohrbach"
        ],
        "dcterms:description": "A dataset for visual question answering that focuses on the ability of models to read and understand text within images.",
        "dcterms:title": "TextVQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Image Understanding"
        ],
        "dcat:keyword": [
            "Visual question answering",
            "Text understanding",
            "Image-text relations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Siwei Wu",
            "Yizhi Li",
            "Kang Zhu",
            "Ge Zhang",
            "Yiming Liang",
            "Kaijing Ma",
            "Chenghao Xiao",
            "Haoran Zhang",
            "Bohao Yang",
            "Wenhu Chen",
            "Wenhao Huang",
            "Noura Al Moubayed",
            "Jie Fu",
            "Chenghua Lin"
        ],
        "dcterms:description": "A benchmark for evaluating scientific multi-modal information retrieval, focusing on the retrieval of scientific information across different modalities.",
        "dcterms:title": "SciMMIR",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Information Retrieval",
            "Multi-modal Learning"
        ],
        "dcat:keyword": [
            "Scientific retrieval",
            "Multi-modal information",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Information retrieval",
            "Multi-modal analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Fei Wang",
            "Xingyu Fu",
            "James Y. Huang",
            "Zekun Li",
            "Qin Liu",
            "Xiaogeng Liu",
            "Mingyu Derek Ma",
            "Nan Xu",
            "Wenxuan Zhou",
            "Kai Zhang",
            "Tianyi Lorena Yan",
            "Wenjie Jacky Mo",
            "Hsiang-Hui Liu",
            "Pan Lu",
            "Chunyuan Li",
            "Chaowei Xiao",
            "Kai-Wei Chang",
            "Dan Roth",
            "Sheng Zhang",
            "Hoifung Poon",
            "Muhao Chen"
        ],
        "dcterms:description": "A comprehensive benchmark for robust multi-image understanding, focusing on evaluating the performance of multi-image models.",
        "dcterms:title": "MuirBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-image Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multi-image evaluation",
            "Robustness",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multi-image understanding",
            "Benchmarking"
        ]
    },
    {
        "dcterms:creator": [
            "Xiangqing Shen",
            "Yurun Song",
            "Siwei Wu",
            "Rui Xia"
        ],
        "dcterms:description": "A dataset focused on knowledge base guided visual commonsense discovery in images, evaluating the ability of models to understand commonsense knowledge in visual contexts.",
        "dcterms:title": "VCD (Visual Commonsense Discovery)",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Commonsense Reasoning",
            "Image Understanding"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Visual understanding",
            "Knowledge base"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Commonsense reasoning",
            "Visual understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yuanhan Zhang",
            "Yuan Liu",
            "Haodong Duan",
            "Bo Li",
            "Yike Yuan",
            "Songyang Zhang",
            "Wangbo Zhao",
            "Jiaqi Wang",
            "Conghui He",
            "Ziwei Liu",
            "Dahua Lin",
            "Kai Chen"
        ],
        "dcterms:description": "A benchmark designed to evaluate the performance of multi-modal models across various tasks, assessing their capabilities in multi-modal understanding.",
        "dcterms:title": "MMBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-modal Learning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multi-modal evaluation",
            "Benchmark",
            "Performance assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multi-modal evaluation",
            "Benchmarking"
        ]
    }
]