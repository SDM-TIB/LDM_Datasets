[
    {
        "dcterms:creator": [
            "Alane Suhr",
            "Stephanie Zhou",
            "Ally Zhang",
            "Iris Zhang",
            "Huajun Bai",
            "Yoav Artzi"
        ],
        "dcterms:description": "A dataset used for data construction in the General Comparison task, which examines the model’s general understanding of each image and comparison across different images.",
        "dcterms:title": "NLVR2",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1811.00491",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Image comparison",
            "Natural language reasoning",
            "Visual grounding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Kai Zhang",
            "Lingbo Mo",
            "Wenhu Chen",
            "Huan Sun",
            "Yu Su"
        ],
        "dcterms:description": "A dataset used in the Subtle Difference task to evaluate the model’s ability to perceive subtle differences between similar images.",
        "dcterms:title": "MagicBrush",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Editing",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image editing",
            "Manual annotation",
            "Instruction-guided"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Editing"
        ]
    },
    {
        "dcterms:creator": [
            "Yuanzhi Liang",
            "Yalong Bai",
            "Wei Zhang",
            "Xueming Qian",
            "Li Zhu",
            "Tao Mei"
        ],
        "dcterms:description": "A dataset used in the Visual Referring task to evaluate the model’s ability to utilize referring information provided by input images.",
        "dcterms:title": "VrR-VG",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Relationship Understanding"
        ],
        "dcat:keyword": [
            "Visual relationships",
            "Referring expressions",
            "Object relationships"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Relationship Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Raghav Goyal",
            "Samira Ebrahimi Kahou",
            "Vincent Michalski",
            "Joanna Materzynska",
            "Susanne Westphal",
            "Heuna Kim",
            "Valentin Haenel",
            "Ingo Fruend",
            "Peter Yianilos",
            "Moritz Mueller-Freitag"
        ],
        "dcterms:description": "A dataset used in the Temporal Reasoning task to assess the model’s understanding of temporal relationships among a series of consecutive images.",
        "dcterms:title": "Something-Something V2",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Understanding",
            "Temporal Reasoning"
        ],
        "dcat:keyword": [
            "Temporal reasoning",
            "Video dataset",
            "Visual common sense"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Temporal Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Junbin Xiao",
            "Xindi Shang",
            "Angela Yao",
            "Tat-Seng Chua"
        ],
        "dcterms:description": "A dataset used in the Logical Reasoning task to evaluate the model’s ability to perform logical reasoning and analyze causal relationships between objects or events shown in images.",
        "dcterms:title": "NExT-QA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Logical Reasoning"
        ],
        "dcat:keyword": [
            "Logical reasoning",
            "Causal relationships",
            "Visual question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Aditya Khosla",
            "Nityananda Jayadevaprakash",
            "Bangpeng Yao",
            "Fei-Fei Li"
        ],
        "dcterms:description": "A combination of several fine-grained recognition datasets used in the Fine-Grained Visual Recognition task to evaluate the model’s ability to recognize objects in the query image when given multiple reference images.",
        "dcterms:title": "Fine-Grained Recognition Datasets",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fine-Grained Recognition",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Fine-grained recognition",
            "Image categorization",
            "Object recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Object Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Ryota Tanaka",
            "Kyosuke Nishida",
            "Kosuke Nishida",
            "Taku Hasegawa",
            "Itsumi Saito",
            "Kuniko Saito"
        ],
        "dcterms:description": "A dataset used in the Text-Rich Images task to evaluate the model’s ability to understand text-rich images and extract relevant information.",
        "dcterms:title": "Slide-VQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Document Understanding"
        ],
        "dcat:keyword": [
            "Text-rich images",
            "Visual question answering",
            "Document analysis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yang Chen",
            "Hexiang Hu",
            "Yi Luan",
            "Haitian Sun",
            "Soravit Changpinyo",
            "Alan Ritter",
            "Ming-Wei Chang"
        ],
        "dcterms:description": "A dataset used in the Vision-linked Textual Knowledge task to evaluate the model’s ability to link query images to relevant images and extract useful information from corresponding text.",
        "dcterms:title": "InfoSeek",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.11713",
        "dcat:theme": [
            "Visual Question Answering",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Visual information-seeking",
            "Knowledge extraction",
            "Image-text linking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yingshan Chang",
            "Mridu Narang",
            "Hisami Suzuki",
            "Guihong Cao",
            "Jianfeng Gao",
            "Yonatan Bisk"
        ],
        "dcterms:description": "A dataset used in the Text-linked Visual Knowledge task to evaluate the model’s ability to link text-only questions to visual attributes of specific objects.",
        "dcterms:title": "WebQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Knowledge Extraction"
        ],
        "dcat:keyword": [
            "Text-linked questions",
            "Visual attributes",
            "Knowledge retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Kenneth Marino",
            "Mohammad Rastegari",
            "Ali Farhadi",
            "Roozbeh Mottaghi"
        ],
        "dcterms:description": "A dataset used in the Open-ended VQA task to evaluate the model’s ability to answer open-ended questions that cannot be fully covered by the demos.",
        "dcterms:title": "OK-VQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Open-ended Questioning"
        ],
        "dcat:keyword": [
            "Open-ended questions",
            "Visual question answering",
            "External knowledge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Oriol Vinyals",
            "Charles Blundell",
            "Timothy Lillicrap",
            "Daan Wierstra"
        ],
        "dcterms:description": "A dataset used in the Close-ended VQA task to evaluate the model’s ability to answer from a predefined set of responses.",
        "dcterms:title": "Mini-ImageNet",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Few-shot Learning"
        ],
        "dcat:keyword": [
            "Close-ended questions",
            "Few-shot learning",
            "Image classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "dcterms:description": "A dataset used in the Hallucination task to study the impact of the model’s in-context learning ability on hallucination phenomena.",
        "dcterms:title": "POPE",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.10355",
        "dcat:theme": [
            "Visual Question Answering",
            "Hallucination Evaluation"
        ],
        "dcat:keyword": [
            "Hallucination",
            "In-context learning",
            "Visual question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]