[
    {
        "dcterms:creator": [
            "Jiasheng Zheng",
            "Boxi Cao",
            "Zhengzhao Ma",
            "Ruotong Pan",
            "Hongyu Lin",
            "Yaojie Lu",
            "Xianpei Han",
            "Le Sun"
        ],
        "dcterms:description": "The RACE benchmark comprehensively evaluates the quality of code generated by LLMs across four dimensions: Readability, Maintainability, Correctness, and Efficiency.",
        "dcterms:title": "RACE benchmark",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Quality Evaluation",
            "Machine Learning",
            "Software Engineering"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Large Language Models",
            "Benchmarking",
            "Multi-dimensional Evaluation"
        ],
        "dcat:landingPage": "https://github.com/jszheng21/RACE",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Evaluation",
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Steven Basart",
            "Saurav Kadavath",
            "Mantas Mazeika",
            "Akul Arora",
            "Ethan Guo",
            "Collin Burns",
            "Samir Puranik",
            "Horace He",
            "Dawn Song"
        ],
        "dcterms:description": "HumanEval+ is a dataset for evaluating the correctness of code generation tasks, focusing on coding exercises.",
        "dcterms:title": "HumanEval+",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Evaluation",
            "Programming Challenges"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Evaluation",
            "Programming Exercises"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Code Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Austin",
            "Augustus Odena",
            "Maxwell Nye",
            "Maarten Bosma",
            "Henryk Michalewski",
            "David Dohan",
            "Ellen Jiang",
            "Carrie Cai",
            "Michael Terry",
            "Quoc Le"
        ],
        "dcterms:description": "MBPP+ is a dataset designed for evaluating code generation capabilities, focusing on programming tasks.",
        "dcterms:title": "MBPP+",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2108.07732",
        "dcat:theme": [
            "Code Evaluation",
            "Programming Tasks"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Benchmarking",
            "Programming Tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Code Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Xueying Du",
            "Mingwei Liu",
            "Kaixin Wang",
            "Hanlin Wang",
            "Junwei Liu",
            "Yixuan Chen",
            "Jiayi Feng",
            "Chaofeng Sha",
            "Xin Peng",
            "Yiling Lou"
        ],
        "dcterms:description": "ClassEval is a manually-crafted benchmark for evaluating LLMs on class-level code generation.",
        "dcterms:title": "ClassEval",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2308.01861",
        "dcat:theme": [
            "Code Evaluation",
            "Class-level Code Generation"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Benchmarking",
            "Class-level Programming"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Code Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yue Wang",
            "Weishi Wang",
            "Shafiq Joty",
            "Steven CH Hoi"
        ],
        "dcterms:description": "DS-1000 is a natural and reliable benchmark for data science code generation.",
        "dcterms:title": "DS-1000",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Evaluation",
            "Data Science"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Data Science",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Code Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Steven Basart",
            "Saurav Kadavath",
            "Mantas Mazeika",
            "Akul Arora",
            "Ethan Guo",
            "Collin Burns",
            "Samir Puranik",
            "Horace He",
            "Dawn Song"
        ],
        "dcterms:description": "APPS is a dataset for measuring coding challenge competence, focusing on coding exercises.",
        "dcterms:title": "APPS",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Evaluation",
            "Programming Challenges"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Evaluation",
            "Programming Exercises"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Code Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Li, Bowen"
        ],
        "dcterms:description": "CodeContests is a benchmark for evaluating code generation models.",
        "dcterms:title": "CodeContests",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2201.12345",
        "dcat:theme": [
            "Code Evaluation",
            "Programming Competitions"
        ],
        "dcat:keyword": [
            "Code Generation",
            "Benchmarking",
            "Programming Competitions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Code Evaluation"
        ]
    }
]