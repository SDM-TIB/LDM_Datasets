[
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "M. Chen",
            "J. Austin",
            "X. Du",
            "H. Yu",
            "F. Cassano",
            "Y. Lai",
            "J. Liu",
            "S. Iyer",
            "T. Miah",
            "R. Agashe",
            "Y. Xie",
            "J. Li"
        ],
        "dcterms:description": "APPS is a benchmark designed to measure coding challenge competence, consisting of a large number of programming tasks extracted from various coding challenge platforms.",
        "dcterms:title": "APPS",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2105.09938",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Coding challenges",
            "Benchmark",
            "Competence measurement"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Chen"
        ],
        "dcterms:description": "HumanEval is a benchmark for evaluating large language models trained on code, consisting of programming tasks with associated test cases.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2107.03374",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Code evaluation",
            "Programming tasks",
            "Test cases"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Austin"
        ],
        "dcterms:description": "MBPP is a benchmark for program synthesis with large language models, focusing on generating code from natural language descriptions.",
        "dcterms:title": "MBPP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2108.07732",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Program synthesis",
            "Natural language processing",
            "Code generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Austin"
        ],
        "dcterms:description": "MathQA-Python is a benchmark for program synthesis that focuses on generating Python code for mathematical problems.",
        "dcterms:title": "MathQA-Python",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2108.07732",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Mathematics",
            "Python programming",
            "Code generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "X. Du"
        ],
        "dcterms:description": "ClassEval is a manually-crafted benchmark for evaluating large language models on class-level code generation tasks.",
        "dcterms:title": "ClassEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2308.01861",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Class-level code generation",
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Yu"
        ],
        "dcterms:description": "CoderEval is a benchmark for pragmatic code generation with generative pre-trained models, focusing on real-world coding tasks.",
        "dcterms:title": "CoderEval",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Pragmatic code generation",
            "Benchmark",
            "Real-world tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "F. Cassano"
        ],
        "dcterms:description": "MultiPL-E is a scalable and extensible approach to benchmarking neural code generation, providing a diverse set of programming tasks.",
        "dcterms:title": "MultiPL-E",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Benchmarking",
            "Neural code generation",
            "Scalability"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Lai"
        ],
        "dcterms:description": "DS-1000 is a natural and reliable benchmark for data science code generation, focusing on generating code for data analysis tasks.",
        "dcterms:title": "DS-1000",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Data science",
            "Code generation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Liu"
        ],
        "dcterms:description": "HumanEval+ is an extension of the HumanEval benchmark, rigorously evaluating the correctness of code generated by large language models.",
        "dcterms:title": "HumanEval+",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2107.03374",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Code evaluation",
            "Correctness",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Iyer"
        ],
        "dcterms:description": "CONCODE is a benchmark for mapping language to code in programmatic contexts, focusing on understanding code generation tasks.",
        "dcterms:title": "CONCODE",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Language to code mapping",
            "Benchmark",
            "Programmatic context"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "T. Miah"
        ],
        "dcterms:description": "R-benchmark is a user-centric evaluation of ChatGPT's capability of generating R program code, focusing on usability and correctness.",
        "dcterms:title": "R-benchmark",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.03130",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "R programming",
            "User evaluation",
            "Code generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "R. Agashe"
        ],
        "dcterms:description": "JuICe is a large-scale distantly supervised dataset for open-domain context-based code generation, focusing on generating code from various contexts.",
        "dcterms:title": "JuICe",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Open-domain",
            "Context-based code generation",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Xie"
        ],
        "dcterms:description": "Exec-CSN is a benchmark for creating scalable execution-based code generation benchmarks, focusing on execution correctness.",
        "dcterms:title": "Exec-CSN",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2404.00566",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Execution-based",
            "Code generation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Li"
        ],
        "dcterms:description": "EvoCodeBench is an evolving code generation benchmark aligned with real-world code repositories, focusing on practical code generation tasks.",
        "dcterms:title": "EvoCodeBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2404.00599",
        "dcat:theme": [
            "Machine Learning",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Real-world code",
            "Benchmark",
            "Code generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    }
]