[
    {
        "dcterms:creator": [
            "Jean-Baptiste Alayrac",
            "Jeff Donahue",
            "Pauline Luc",
            "Antoine Miech",
            "Iain Barr",
            "Yana Hasson",
            "Karel Lenc",
            "Arthur Mensch",
            "Katherine Millican",
            "Malcolm Reynolds"
        ],
        "dcterms:description": "Flamingo is a visual language model designed for few-shot learning, integrating visual and language understanding capabilities.",
        "dcterms:title": "Flamingo",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Language Models",
            "Few-shot Learning"
        ],
        "dcat:keyword": [
            "Visual language model",
            "Few-shot learning",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image understanding",
            "Image generation"
        ]
    },
    {
        "dcterms:creator": [
            "Deyao Zhu",
            "Jun Chen",
            "Xiaoqian Shen",
            "Xiang Li",
            "Mohamed Elhoseiny"
        ],
        "dcterms:description": "MiniGPT-4 enhances vision-language understanding by leveraging advanced large language models.",
        "dcterms:title": "MiniGPT-4",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.10592",
        "dcat:theme": [
            "Vision-Language Models",
            "Language Models"
        ],
        "dcat:keyword": [
            "Vision-language understanding",
            "Large language models",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Vision-language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Zhengxiao Du",
            "Yujie Qian",
            "Xiao Liu",
            "Ming Ding",
            "Jiezhong Qiu",
            "Zhilin Yang",
            "Jie Tang"
        ],
        "dcterms:description": "VisualGLM is a general language model pretraining framework that incorporates autoregressive blank infilling.",
        "dcterms:title": "VisualGLM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Models",
            "Pretraining"
        ],
        "dcat:keyword": [
            "General language model",
            "Pretraining",
            "Autoregressive models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Jinze Bai",
            "Shuai Bai",
            "Shusheng Yang",
            "Shijie Wang",
            "Sinan Tan",
            "Peng Wang",
            "Junyang Lin",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "dcterms:description": "Qwen-VL is a large vision-language model that showcases versatile abilities across various tasks.",
        "dcterms:title": "Qwen-VL",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2308.12966",
        "dcat:theme": [
            "Vision-Language Models"
        ],
        "dcat:keyword": [
            "Large vision-language model",
            "Versatile abilities",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Vision-language tasks"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Spark is a platform for various AI applications, including vision-language tasks.",
        "dcterms:title": "Spark",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://xinghuo.xfyun.cn/desk",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "https://xinghuo.xfyun.cn/desk",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "ERNIE Bot is a conversational AI model developed for various applications.",
        "dcterms:title": "ERNIE Bot",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://yiyan.baidu.com/welcome",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "https://yiyan.baidu.com/welcome",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Peng Gao",
            "Jiaming Han",
            "Renrui Zhang",
            "Ziyi Lin",
            "Shijie Geng",
            "Aojun Zhou",
            "Wei Zhang",
            "Pan Lu",
            "Conghui He",
            "Xiangyu Yue"
        ],
        "dcterms:description": "LLaVA is a parameter-efficient visual instruction model that enhances visual understanding.",
        "dcterms:title": "LLaVA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.15010",
        "dcat:theme": [
            "Visual Instruction Models"
        ],
        "dcat:keyword": [
            "Visual instruction",
            "Parameter-efficient models",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual instruction tasks"
        ]
    },
    {
        "dcterms:creator": [
            "Junnan Li",
            "Dongxu Li",
            "Caiming Xiong",
            "Steven Hoi"
        ],
        "dcterms:description": "BLIP is a model that bootstraps language-image pre-training for unified vision-language understanding and generation.",
        "dcterms:title": "BLIP",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Understanding"
        ],
        "dcat:keyword": [
            "Language-image pre-training",
            "Unified understanding",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Vision-language understanding",
            "Generation tasks"
        ]
    },
    {
        "dcterms:creator": [
            "Gelei Deng",
            "Yi Liu",
            "Yuekang Li",
            "Kailong Wang",
            "Ying Zhang",
            "Zefeng Li",
            "Haoyu Wang",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "dcterms:description": "Jailbreaker is a framework for automated jailbreak across multiple large language model chatbots.",
        "dcterms:title": "Jailbreaker",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Red Teaming",
            "Security Evaluation"
        ],
        "dcat:keyword": [
            "Automated jailbreak",
            "Large language models",
            "Security"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Security evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yichen Gong",
            "Delong Ran",
            "Jinyuan Liu",
            "Conglei Wang",
            "Tianshuo Cong",
            "Anyu Wang",
            "Sisi Duan",
            "Xiaoyun Wang"
        ],
        "dcterms:description": "FigStep is a method for jailbreaking large vision-language models via typographic visual prompts.",
        "dcterms:title": "FigStep",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2311.05608",
        "dcat:theme": [
            "Vision-Language Models",
            "Jailbreaking"
        ],
        "dcat:keyword": [
            "Jailbreaking",
            "Visual prompts",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Jailbreaking tasks"
        ]
    },
    {
        "dcterms:creator": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "dcterms:description": "AVSJ focuses on visual adversarial examples to jailbreak aligned large language models.",
        "dcterms:title": "AVSJ",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Adversarial Machine Learning"
        ],
        "dcat:keyword": [
            "Visual adversarial examples",
            "Jailbreaking",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Jailbreaking tasks"
        ]
    },
    {
        "dcterms:creator": [
            "Jiahao Yu",
            "Xingwei Lin",
            "Xinyu Xing"
        ],
        "dcterms:description": "GPTFuzzer is a tool for red teaming large language models with auto-generated jailbreak prompts.",
        "dcterms:title": "GPTFuzzer",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2309.10253",
        "dcat:theme": [
            "Red Teaming",
            "Security Evaluation"
        ],
        "dcat:keyword": [
            "Red teaming",
            "Jailbreak prompts",
            "Machine learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Security evaluation"
        ]
    }
]