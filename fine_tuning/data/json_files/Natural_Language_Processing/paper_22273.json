[
    {
        "dcterms:creator": [
            "Roei Schuster",
            "Congzheng Song",
            "Eran Tromer",
            "Vitaly Shmatikov"
        ],
        "dcterms:description": "Dataset used to explore poisoning vulnerabilities in neural code completion, focusing on how adversarial inputs can manipulate model outputs.",
        "dcterms:title": "You Autocomplete Me",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.usenix.org/conference/usenixsecurity21/presentation/schuster",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Code completion",
            "Data poisoning",
            "Neural networks"
        ],
        "dcat:landingPage": "https://www.usenix.org/conference/usenixsecurity21/presentation/schuster",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code completion vulnerabilities"
        ]
    },
    {
        "dcterms:creator": [
            "Zhensu Sun",
            "Xiaoning Du",
            "Fu Song",
            "Mingze Ni",
            "Li Li"
        ],
        "dcterms:description": "Dataset aimed at protecting open-source code against unauthorized training usage through data poisoning techniques.",
        "dcterms:title": "CoProtector",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1145/3485447.3512225",
        "dcat:theme": [
            "Security",
            "Open Source"
        ],
        "dcat:keyword": [
            "Data poisoning",
            "Open-source protection",
            "Unauthorized usage"
        ],
        "dcat:landingPage": "https://doi.org/10.1145/3485447.3512225",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Data protection",
            "Code security"
        ]
    },
    {
        "dcterms:creator": [
            "Yao Wan",
            "Shijie Zhang",
            "Hongyu Zhang",
            "Yulei Sui",
            "Guandong Xu",
            "Dezhong Yao",
            "Hai Jin",
            "Lichao Sun"
        ],
        "dcterms:description": "Dataset that investigates poisoning vulnerabilities in neural code search, focusing on how specific inputs can manipulate search results.",
        "dcterms:title": "You See What I Want You to See",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1145/3540250.3549153",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Code search",
            "Data poisoning",
            "Neural networks"
        ],
        "dcat:landingPage": "https://doi.org/10.1145/3540250.3549153",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code search vulnerabilities"
        ]
    },
    {
        "dcterms:creator": [
            "G. Ramakrishnan",
            "A. Albarghouthi"
        ],
        "dcterms:description": "Dataset that explores backdoors in neural models of source code, focusing on vulnerabilities introduced during training.",
        "dcterms:title": "Backdoors in Neural Models of Source Code",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.ieeecomputersociety.org/10.1109/ICPR56361.2022.9956690",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Backdoors",
            "Neural networks",
            "Source code"
        ],
        "dcat:landingPage": "https://doi.ieeecomputersociety.org/10.1109/ICPR56361.2022.9956690",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Backdoor detection"
        ]
    },
    {
        "dcterms:creator": [
            "Domenico Cotroneo",
            "Cristina Improta",
            "Pietro Liguori",
            "Roberto Natella"
        ],
        "dcterms:description": "Dataset that explores vulnerabilities in AI code generators, specifically targeting data poisoning attacks.",
        "dcterms:title": "Vulnerabilities in AI Code Generators",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "AI code generation",
            "Data poisoning",
            "Vulnerabilities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Vulnerability detection"
        ]
    },
    {
        "dcterms:creator": [
            "Yanzhou Li",
            "Shangqing Liu",
            "Kangjie Chen",
            "Xiaofei Xie",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "dcterms:description": "Dataset that investigates multi-target backdoor attacks for code pre-trained models, focusing on the effectiveness of such attacks.",
        "dcterms:title": "Multi-target Backdoor Attacks for Code Pre-trained Models",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/2023.acl-long.399",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Backdoor attacks",
            "Code models",
            "Pre-trained models"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/2023.acl-long.399",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Backdoor attack analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Yang",
            "B. Xu",
            "J. M. Zhang",
            "H. Kang",
            "J. Shi",
            "J. He",
            "D. Lo"
        ],
        "dcterms:description": "Dataset that explores stealthy backdoor attacks for code models, focusing on the subtlety and effectiveness of such attacks.",
        "dcterms:title": "Stealthy Backdoor Attack for Code Models",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1109/TSE.2024.3361661",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Stealthy attacks",
            "Backdoor attacks",
            "Code models"
        ],
        "dcat:landingPage": "https://doi.org/10.1109/TSE.2024.3361661",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Backdoor attack detection"
        ]
    },
    {
        "dcterms:creator": [
            "Jia Li",
            "Zhuo Li",
            "Huangzhao Zhang",
            "Ge Li",
            "Zhi Jin",
            "Xing Hu",
            "Xin Xia"
        ],
        "dcterms:description": "Dataset that investigates poison attacks and defenses on deep source code processing models, focusing on detection techniques.",
        "dcterms:title": "Poison Attack and Defense on Deep Source Code Processing Models",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1145/3630008",
        "dcat:theme": [
            "Security",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Poison attacks",
            "Defense mechanisms",
            "Source code processing"
        ],
        "dcat:landingPage": "https://doi.org/10.1145/3630008",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Poison attack detection"
        ]
    }
]