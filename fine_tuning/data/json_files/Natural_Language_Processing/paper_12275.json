[
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for evaluating multiple-choice question answering capabilities across a wide range of topics, consisting of roughly 16,000 examples.",
        "dcterms:title": "Massive Multitask Language Understanding (MMLU)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multiple-choice questions",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A synthetic dataset created for the study to evaluate the ability of models to choose the correct answer based on token identity without requiring factual knowledge.",
        "dcterms:title": "Synthetic multiple choice dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Synthetic dataset",
            "Multiple-choice questions",
            "Token identity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]