[
    {
        "dcterms:creator": [
            "Zichang Liu",
            "Jue Wang",
            "Tri Dao",
            "Tianyi Zhou",
            "Binhang Yuan",
            "Zhao Song",
            "Anshumali Shrivastava",
            "Ce Zhang",
            "Yuandong Tian",
            "Christopher Re",
            "Beidi Chen"
        ],
        "dcterms:description": "DejaVu utilizes a predictive router comprising a two-layer linear network to enhance inference speed by leveraging the sparsity introduced by ReLU activation functions.",
        "dcterms:title": "DejaVu",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Large Language Models",
            "Dynamic Activation"
        ],
        "dcat:keyword": [
            "Inference speed",
            "Sparsity",
            "ReLU",
            "Dynamic Activation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Bowen Pan",
            "Yikang Shen",
            "Haokun Liu",
            "Mayank Mishra",
            "Gaoyuan Zhang",
            "Aude Oliva",
            "Colin Raffel",
            "Rameswar Panda"
        ],
        "dcterms:description": "DS-MoE introduces a framework that utilizes dense computation during training and switches to sparse computation during inference, enhancing parameter efficiency.",
        "dcterms:title": "DS-MoE",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mixture of Experts",
            "Sparse Inference"
        ],
        "dcat:keyword": [
            "Dense training",
            "Sparse inference",
            "Parameter efficiency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Harry Dong",
            "Beidi Chen",
            "Yuejie Chi"
        ],
        "dcterms:description": "Griffin employs a TopK router to construct a mixture of experts from a dense model in a train-free manner, enhancing efficiency in LLM generation.",
        "dcterms:title": "Griffin",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Efficient LLM Generation",
            "Mixture of Experts"
        ],
        "dcat:keyword": [
            "TopK router",
            "Train-free",
            "Efficiency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "LLaMA-MoE Team"
        ],
        "dcterms:description": "LLaMA-MoE transforms feedforward networks into mixture-of-experts by constructing experts and training an additional gating network for expert routing.",
        "dcterms:title": "LLaMA-MoE",
        "dcterms:issued": "Dec 2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mixture of Experts",
            "Continual Pre-training"
        ],
        "dcat:keyword": [
            "Expert routing",
            "Feedforward networks",
            "Continual learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Zhenyu Zhang",
            "Ying Sheng",
            "Tianyi Zhou",
            "Tianlong Chen",
            "Lianmin Zheng",
            "Ruisi Cai",
            "Zhao Song",
            "Yuandong Tian",
            "Christopher RÂ´e",
            "Clark Barrett",
            "Zhangyang Wang",
            "Beidi Chen"
        ],
        "dcterms:description": "H2O is designed as a heavy-hitter oracle for efficient generative inference of large language models, focusing on optimizing inference processes.",
        "dcterms:title": "H2O",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Inference",
            "Large Language Models"
        ],
        "dcat:keyword": [
            "Heavy-hitter oracle",
            "Generative inference",
            "Efficiency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]