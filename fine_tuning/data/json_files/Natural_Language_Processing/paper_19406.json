[
    {
        "dcterms:creator": [
            "A. Köpft",
            "Y. Kilcher",
            "D. von Rütte",
            "S. Anagnostidis",
            "Z.-R. Tam",
            "K. Stevens",
            "A. Barhoum",
            "N. M. Duc",
            "O. Stanley",
            "R. Nagyfi"
        ],
        "dcterms:description": "OpenAssistant provides multi-term user-assistant conversations about different topics with human-annotated labels for the assistant replies.",
        "dcterms:title": "OpenAssistant",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.07327",
        "dcat:theme": [
            "Conversational AI",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "User-assistant conversations",
            "Human-annotated labels",
            "Language model alignment"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2304.07327",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversation analysis",
            "Model alignment"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Lin",
            "Z. Wang",
            "Y. Tong",
            "Y. Wang",
            "Y. Guo",
            "Y. Wang",
            "J. Shang"
        ],
        "dcterms:description": "ToxicChat unveils hidden challenges of toxicity detection in real-world user-AI conversations.",
        "dcterms:title": "ToxicChat",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Toxicity detection",
            "User-AI interaction"
        ],
        "dcat:keyword": [
            "Toxicity detection",
            "User-AI conversation",
            "Real-world challenges"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxicity detection",
            "Conversation analysis"
        ]
    },
    {
        "dcterms:creator": [
            "S. Lin",
            "J. Hilton",
            "O. Evans"
        ],
        "dcterms:description": "TruthfulQA is a benchmark for measuring how models mimic human falsehoods.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "10.18653/v1/2022.acl-long.229",
        "dcat:theme": [
            "Benchmarking",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Falsehood detection",
            "Benchmark",
            "Language models"
        ],
        "dcat:landingPage": "https://aclanthology.org/2022.acl-long.229",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Falsehood detection",
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A synthetic dataset created by the authors for evaluating appropriateness in AI responses.",
        "dcterms:title": "Appropriateness Dataset",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Synthetic dataset",
            "Appropriateness evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]