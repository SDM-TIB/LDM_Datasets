[
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "The MMLU benchmark is used to evaluate the performance of language models across a variety of tasks, demonstrating their understanding and reasoning capabilities.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Model Evaluation",
            "Multitask Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "R. Taori",
            "I. Gulrajani",
            "T. Zhang",
            "Y. Dubois",
            "X. Li",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "Alpaca is a dataset containing instruction-following data generated from text-davinci-003, designed to fine-tune language models for better instruction adherence.",
        "dcterms:title": "Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "Instruction Dataset",
            "Fine-tuning",
            "Language Model"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "S. Longpre",
            "L. Hou",
            "T. Vu",
            "A. Webson",
            "H. W. Chung",
            "Y. Tay",
            "D. Zhou",
            "Q. V. Le",
            "B. Zoph",
            "J. Wei"
        ],
        "dcterms:description": "Flan v2 is a collection of tasks designed for effective instruction tuning, combining various datasets to enhance model performance on instruction-based tasks.",
        "dcterms:title": "Flan v2",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction Dataset",
            "Task Collection",
            "Model Tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Tuning"
        ]
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "HellaSwag is a dataset designed to evaluate the ability of models to complete sentences in a coherent and contextually appropriate manner.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Sentence Completion",
            "Commonsense Reasoning",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence Completion",
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "R. Zellers",
            "J. Gao",
            "Y. Choi"
        ],
        "dcterms:description": "PIQA is a dataset focused on reasoning about physical commonsense in natural language, challenging models to understand and predict physical interactions.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Physical Commonsense",
            "Reasoning Dataset",
            "Natural Language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "K. Sakaguchi",
            "R. L. Bras",
            "C. Bhagavatula",
            "Y. W. Choi"
        ],
        "dcterms:description": "WinoGrande is an adversarial dataset designed to challenge models on the Winograd Schema Challenge at scale, focusing on pronoun resolution.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Pronoun Resolution"
        ],
        "dcat:keyword": [
            "Adversarial Dataset",
            "Pronoun Resolution",
            "Winograd Schema"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pronoun Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "ARC (AI2 Reasoning Challenge) is a dataset designed to evaluate question answering capabilities, focusing on reasoning and comprehension.",
        "dcterms:title": "ARC (AI2 Reasoning Challenge)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1803.05457",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Reasoning Dataset",
            "Comprehension"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1803.05457",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "C. Clark",
            "K. Lee",
            "M.-W. Chang",
            "T. Kwiatkowski",
            "M. Collins",
            "K. Toutanova"
        ],
        "dcterms:description": "BoolQ is a dataset that explores the challenges of answering yes/no questions in natural language, focusing on the complexity of such queries.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Yes/No Questions",
            "Dataset",
            "Natural Language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "OpenBookQA is a dataset for open book question answering, requiring models to leverage external knowledge to answer questions.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Open Book QA",
            "Knowledge-based Questions",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]