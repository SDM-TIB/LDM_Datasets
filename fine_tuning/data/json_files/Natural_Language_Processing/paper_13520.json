[
    {
        "dcterms:creator": [
            "Qinkai Zheng",
            "Xiao Xia",
            "Xu Zou",
            "Yuxiao Dong",
            "Shanshan Wang",
            "Yufei Xue",
            "Zi-Yuan Wang",
            "Lei Shen",
            "Andi Wang",
            "Yang Li",
            "Teng Su",
            "Zhilin Yang",
            "Jie Tang"
        ],
        "dcterms:description": "HumanEval-X is crafted by adapting HumanEval to other programming languages, used to evaluate the multilingual abilities of models in Python, JavaScript, C++, Java, and Go.",
        "dcterms:title": "HumanEval-X",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Programming Languages"
        ],
        "dcat:keyword": [
            "Code evaluation",
            "Multilingual code generation",
            "Programming tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code evaluation",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Mark Chen",
            "Jerry Tworek",
            "Heewoo Jun",
            "Qiming Yuan",
            "Henrique Ponde",
            "Jared Kaplan",
            "Harrison Edwards",
            "Yura Burda",
            "Nicholas Joseph",
            "Greg Brockman",
            "Alex Ray",
            "Raul Puri",
            "Gretchen Krueger",
            "Michael Petrov",
            "Heidy Khlaaf",
            "Girish Sastry",
            "Pamela Mishkin",
            "Brooke Chan",
            "Scott Gray",
            "Nick Ryder",
            "Mikhail Pavlov",
            "Alethea Power",
            "Lukasz Kaiser",
            "Mohammad Bavarian",
            "Clemens Winter",
            "Philippe Tillet",
            "Felipe Petroski Such",
            "David W. Cummings",
            "Matthias Plappert",
            "Fotios Chantzis",
            "Elizabeth Barnes",
            "Ariel Herbert-Voss",
            "William H. Guss",
            "Alex Nichol",
            "Igor Babuschkin",
            "S. Arun Balaji",
            "Shantanu Jain",
            "Andrew Carr",
            "Jan Leike",
            "Joshua Achiam",
            "Vedant Misra",
            "Evan Morikawa",
            "Alec Radford",
            "Matthew M. Knight",
            "Miles Brundage",
            "Mira Murati",
            "Katie Mayer",
            "Peter Welinder",
            "Bob McGrew",
            "Dario Amodei",
            "Sam McCandlish",
            "Ilya Sutskever",
            "Wojciech Zaremba"
        ],
        "dcterms:description": "HumanEval is a dataset used for evaluating large language models trained on code, specifically designed for Python programming tasks.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Programming Languages"
        ],
        "dcat:keyword": [
            "Python programming",
            "Code evaluation",
            "Programming tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code evaluation",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Erik Nijkamp",
            "Bo Pang",
            "Hiroaki Hayashi",
            "Lifu Tu",
            "Huan Wang",
            "Yingbo Zhou",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "dcterms:description": "HumanEval-C is a new version of HumanEval created for evaluating code generation in the C programming language.",
        "dcterms:title": "HumanEval-C",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Programming Languages"
        ],
        "dcat:keyword": [
            "C programming",
            "Code evaluation",
            "Programming tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code evaluation",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Erik Nijkamp",
            "Bo Pang",
            "Hiroaki Hayashi",
            "Lifu Tu",
            "Huan Wang",
            "Yingbo Zhou",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "dcterms:description": "HumanEval-TypeScript is a new version of HumanEval created for evaluating code generation in the TypeScript programming language.",
        "dcterms:title": "HumanEval-TypeScript",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Programming Languages"
        ],
        "dcat:keyword": [
            "TypeScript programming",
            "Code evaluation",
            "Programming tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code evaluation",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "CodeAlpaca 20K is a dataset used for training models on code generation tasks, specifically related to Python.",
        "dcterms:title": "CodeAlpaca 20K",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
        "dcat:theme": [
            "Code Generation",
            "Programming Languages"
        ],
        "dcat:keyword": [
            "Python programming",
            "Code generation",
            "Instruction set"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code generation"
        ]
    }
]