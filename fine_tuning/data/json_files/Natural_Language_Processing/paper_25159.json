[
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A mixed collection of high school and undergraduate-level questions across various subjects, used to measure massive multitask language understanding.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Language Understanding",
            "Education"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language models",
            "Educational assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Educational Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "David Rein",
            "Betty Li Hou",
            "Asa Cooper Stickland",
            "Jackson Petty",
            "Richard Yuanzhe Pang",
            "Julien Dirani",
            "Julian Michael",
            "Samuel R Bowman"
        ],
        "dcterms:description": "A graduate-level question and answer benchmark designed to evaluate the performance of language models on complex academic questions.",
        "dcterms:title": "GPQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2311.12022",
        "dcat:theme": [
            "Language Understanding",
            "Graduate Education"
        ],
        "dcat:keyword": [
            "Question answering",
            "Graduate-level assessment",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Zhengwei Tao",
            "Ting-En Lin",
            "Xiancai Chen",
            "Hangyu Li",
            "Yuchuan Wu",
            "Yongbin Li",
            "Zhi Jin",
            "Fei Huang",
            "Dacheng Tao",
            "Jingren Zhou"
        ],
        "dcterms:description": "A benchmark designed to evaluate the self-evolution capabilities of large language models.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2404.14387",
        "dcat:theme": [
            "Language Models",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Self-evolution",
            "Language models",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Carlos E Jimenez",
            "John Yang",
            "Alexander Wettig",
            "Shunyu Yao",
            "Kexin Pei",
            "Ofir Press",
            "Karthik R Narasimhan"
        ],
        "dcterms:description": "A benchmark that assesses the ability of language models to resolve real-world issues on GitHub.",
        "dcterms:title": "SWE",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Software Engineering",
            "Language Models"
        ],
        "dcat:keyword": [
            "GitHub issues",
            "Language models",
            "Software engineering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Issue Resolution",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Terry Yue Zhuo",
            "Minh Chien Vu",
            "Jenny Chim",
            "Han Hu",
            "Wenhao Yu",
            "Ratnadira Widyasari",
            "Imam Nur Bani Yusuf",
            "Haolan Zhan",
            "Junda He",
            "Indraneil Paul",
            "Simon Brunner",
            "Chen Gong",
            "Thong Hoang",
            "Armel Randy Zebaze",
            "Xiaoheng Hong",
            "Wen-Ding Li",
            "Jean Kaddour",
            "Ming Xu",
            "Zhihan Zhang",
            "Prateek Yadav",
            "Naman Jain",
            "Alex Gu",
            "Zhoujun Cheng",
            "Jiawei Liu",
            "Qian Liu",
            "Zijian Wang",
            "David Lo",
            "Binyuan Hui",
            "Niklas Muennighoff",
            "Daniel Fried",
            "Xiaoning Du",
            "Harm de Vries",
            "Leandro Von Werra"
        ],
        "dcterms:description": "A benchmark for evaluating code generation capabilities with diverse function calls and complex instructions.",
        "dcterms:title": "BigCodeBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Software Engineering"
        ],
        "dcat:keyword": [
            "Code generation",
            "Benchmarking",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation",
            "Software Development"
        ]
    },
    {
        "dcterms:creator": [
            "Harsha Nori",
            "Yin Tat Lee",
            "Sheng Zhang",
            "Dean Carignan",
            "Richard Edgar",
            "Nicolo Fusi",
            "Nicholas King",
            "Jonathan Larson",
            "Yuanzhi Li",
            "Weishung Liu"
        ],
        "dcterms:description": "A case study evaluating whether generalist foundation models can outperform specialized tuning in the medical domain.",
        "dcterms:title": "MedPrompt",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2311.16452",
        "dcat:theme": [
            "Medical AI",
            "Language Models"
        ],
        "dcat:keyword": [
            "Medical AI",
            "Generalist models",
            "Specialized tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Medical Question Answering",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Khaled Saab",
            "Tao Tu",
            "Wei-Hung Weng",
            "Ryutaro Tanno",
            "David Stutz",
            "Ellery Wulczyn",
            "Fan Zhang",
            "Tim Strother",
            "Chunjong Park",
            "Elahe Vedadi"
        ],
        "dcterms:description": "A study on the capabilities of Gemini models in the medical field.",
        "dcterms:title": "MedGemini",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2404.18416",
        "dcat:theme": [
            "Medical AI",
            "Language Models"
        ],
        "dcat:keyword": [
            "Medical AI",
            "Gemini models",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Medical Question Answering",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Di Zhang",
            "Wei Liu",
            "Qian Tan",
            "Jingdan Chen",
            "Hang Yan",
            "Yuliang Yan",
            "Jiatong Li",
            "Weiran Huang",
            "Xiangyu Yue",
            "Dongzhan Zhou"
        ],
        "dcterms:description": "A large language model specifically designed for chemical applications.",
        "dcterms:title": "ChemLLM",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.06852",
        "dcat:theme": [
            "Chemistry",
            "Language Models"
        ],
        "dcat:keyword": [
            "Chemical language model",
            "Chemistry applications",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Chemical Question Answering",
            "Language Understanding"
        ]
    }
]