[
    {
        "dcterms:creator": [],
        "dcterms:description": "Internal customer experience data used for multi-class classification tasks, consisting of 257k text documents across 16 languages labeled for Task-1.",
        "dcterms:title": "Internal customer experience data for Task-1",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Customer experience",
            "Text classification",
            "Multi-class classification",
            "Internal dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-class classification"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Internal customer experience data used for multi-class classification tasks, consisting of 127k text documents across 12 languages labeled for Task-2.",
        "dcterms:title": "Internal customer experience data for Task-2",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Customer experience",
            "Text classification",
            "Multi-class classification",
            "Internal dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-class classification"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Internal customer experience data used for multi-label classification tasks, consisting of 127k text documents across 12 languages labeled for Task-3.",
        "dcterms:title": "Internal customer experience data for Task-3",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Customer experience",
            "Text classification",
            "Multi-label classification",
            "Internal dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-label classification"
        ]
    },
    {
        "dcterms:creator": [
            "Alexis Conneau",
            "Kartikay Khandelwal",
            "Naman Goyal",
            "Vishrav Chaudhary",
            "Guillaume Wenzek",
            "Francisco Guzm√°n",
            "Edouard Grave",
            "Myle Ott",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "dcterms:description": "XLM-R is a cross-lingual representation learning model that supports multiple languages and tasks.",
        "dcterms:title": "XLM-R (XLM-R Base)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1911.02116",
        "dcat:theme": [
            "Natural Language Processing",
            "Cross-lingual Learning"
        ],
        "dcat:keyword": [
            "Cross-lingual model",
            "Representation learning",
            "Multi-task learning"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1911.02116",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Subhabrata Mukherjee",
            "Ahmed Hassan Awadallah",
            "Jianfeng Gao"
        ],
        "dcterms:description": "XtremeDistil is a model designed for task transfer and task-agnostic distillation.",
        "dcterms:title": "XtremeDistil",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2106.04563",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Distillation"
        ],
        "dcat:keyword": [
            "Model distillation",
            "Task transfer",
            "Cross-lingual learning"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2106.04563",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Wenhui Wang",
            "Hangbo Bao",
            "Shaohan Huang",
            "Li Dong",
            "Furu Wei"
        ],
        "dcterms:description": "MiniLM is a model that uses self-attention distillation for compressing pretrained transformers.",
        "dcterms:title": "MiniLM-L12 (mMiniLMv2)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2012.15828",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Compression"
        ],
        "dcat:keyword": [
            "Model compression",
            "Self-attention distillation",
            "Pretrained transformers"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2012.15828",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Wenhui Wang",
            "Hangbo Bao",
            "Shaohan Huang",
            "Li Dong",
            "Furu Wei"
        ],
        "dcterms:description": "MiniLM is a model that uses self-attention distillation for compressing pretrained transformers.",
        "dcterms:title": "MiniLM-L6 (mMiniLMv2)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2012.15828",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Compression"
        ],
        "dcat:keyword": [
            "Model compression",
            "Self-attention distillation",
            "Pretrained transformers"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2012.15828",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": []
    }
]