[
    {
        "dcterms:creator": [
            "N. Sengupta",
            "S. K. Sahu",
            "B. Jia",
            "S. Katipomu",
            "H. Li",
            "F. Koto",
            "O. M. Afzal",
            "S. Kamboj",
            "O. Pandit",
            "R. Pal",
            "L. Pradhan",
            "Z. M. Mujahid",
            "M. Baali",
            "A. F. Aji",
            "Z. Liu",
            "A. Hock",
            "A. Feldman",
            "J. Lee",
            "A. Jackson",
            "P. Nakov",
            "T. Baldwin",
            "E. J. Xing"
        ],
        "dcterms:description": "The AraV5 Arabic dataset includes documents from various sources such as web pages, Wikipedia, Arabic news outlets, books, and social media. It also includes high-quality English corpora, books, and Wikipedia, translated to Arabic.",
        "dcterms:title": "AraV5 Arabic dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "Arabic",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Arabic corpus",
            "bilingual dataset",
            "language adaptation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling",
            "Cross-lingual Transfer"
        ]
    },
    {
        "dcterms:creator": [
            "L. Gao",
            "S. Biderman",
            "S. Black",
            "L. Golding",
            "T. Hoppe",
            "C. Foster",
            "J. Phang",
            "H. He",
            "A. Thite",
            "A. Nabeshima",
            "S. Presser",
            "C. Leahy"
        ],
        "dcterms:description": "The Pile corpus comprises data from 22 diverse sources including ArXiv, Wikipedia, PubmedCentral, CommonCrawl, OpenWebText, and Github.",
        "dcterms:title": "Pile corpus",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "diverse text",
            "language modeling",
            "large dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "Hindi corpus",
        "dcterms:issued": "",
        "dcterms:language": "Hindi",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Conneau",
            "R. Rinott",
            "G. Lample",
            "A. Williams",
            "S. R. Bowman",
            "H. Schwenk",
            "V. Stoyanov"
        ],
        "dcterms:description": "XNLI is a dataset for evaluating cross-lingual sentence representations.",
        "dcterms:title": "XNLI Hindi",
        "dcterms:issued": "2018",
        "dcterms:language": "Hindi",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Cross-lingual Evaluation"
        ],
        "dcat:keyword": [
            "cross-lingual",
            "sentence representation",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Cross-lingual Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "MMLU is a benchmark for measuring massive multitask language understanding.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "multitask",
            "language understanding",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "G. Lai",
            "Q. Xie",
            "H. Liu",
            "Y. Yang",
            "E. Hovy"
        ],
        "dcterms:description": "RACE is a large-scale reading comprehension dataset from examinations.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "reading comprehension",
            "examination dataset",
            "large-scale"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "M. Hardalov",
            "T. Mihaylov",
            "D. Zlatkova",
            "Y. Dinkov",
            "I. Koychev",
            "P. Nakov"
        ],
        "dcterms:description": "EXAMS is a multi-subject high school examinations dataset for cross-lingual and multilingual question answering.",
        "dcterms:title": "EXAMS",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "examinations",
            "multilingual",
            "question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "S. Lin",
            "J. Hilton",
            "O. Evans"
        ],
        "dcterms:description": "TruthfulQA measures how models mimic human falsehoods.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Misinformation"
        ],
        "dcat:keyword": [
            "truthfulness",
            "falsehoods",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Misinformation Detection"
        ]
    },
    {
        "dcterms:creator": [
            "N. Nangia",
            "C. Vania",
            "R. Bhalerao",
            "S. R. Bowman"
        ],
        "dcterms:description": "CrowS-Pairs is a challenge dataset for measuring social biases in masked language models.",
        "dcterms:title": "CrowS-Pairs",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Bias Measurement"
        ],
        "dcat:keyword": [
            "social bias",
            "masked language models",
            "challenge dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Measurement"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "HellaSwag is a dataset designed to evaluate the ability of machines to complete sentences.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Sentence Completion"
        ],
        "dcat:keyword": [
            "sentence completion",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence Completion"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "R. Zellers",
            "R. L. Bras",
            "J. Gao",
            "Y. Choi"
        ],
        "dcterms:description": "PIQA is a dataset for reasoning about physical commonsense in natural language.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "commonsense reasoning",
            "physical reasoning",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Sap",
            "H. Rashkin",
            "D. Chen",
            "R. L. Bras",
            "Y. Choi"
        ],
        "dcterms:description": "Social IQa is a dataset for commonsense reasoning about social interactions.",
        "dcterms:title": "SIQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "social interactions",
            "commonsense reasoning",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "C. Clark",
            "K. Lee",
            "M.-W. Chang",
            "T. Kwiatkowski",
            "M. Collins",
            "K. Toutanova"
        ],
        "dcterms:description": "BoolQ is a dataset exploring the difficulty of natural yes/no questions.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "yes/no questions",
            "question answering",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "ARC Challenge is a dataset for evaluating question answering capabilities.",
        "dcterms:title": "ARC Challenge",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "question answering",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "OpenBookQA is a dataset for open book question answering.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "open book",
            "question answering",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "K. Sakaguchi",
            "R. L. Bras",
            "C. Bhagavatula",
            "Y. Choi"
        ],
        "dcterms:description": "Winogrande is an adversarial Winograd schema challenge at scale.",
        "dcterms:title": "Winogrande",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Winograd schema",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    }
]