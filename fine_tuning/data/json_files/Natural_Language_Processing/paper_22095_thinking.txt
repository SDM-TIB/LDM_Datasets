To extract datasets from the research paper titled "EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars" by Nikita Drobyshev et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and related work sections** to identify any mentions of datasets. The abstract indicates that the authors propose a novel dataset to address the scarcity of high-quality video data capturing a wide range of facial expressions. This suggests that a dataset is indeed introduced.

Next, I will focus on **section 1 (Introduction)**, where the authors discuss the limitations of existing datasets and the need for a new dataset. They mention the **FEED dataset**, which is designed to capture complex facial expressions, including extreme and asymmetric ones. This is a clear indication that I need to gather more details about this dataset.

In **section 3 (FEED dataset)**, the authors provide a detailed description of the dataset, including its structure and the types of expressions it captures. They state that the dataset consists of **520 multi-view videos of 23 subjects**, recorded with 3 cameras, and includes various tasks such as head rotations and extreme expressions. This section is crucial for understanding the dataset's content and significance.

I will also check the **References section** to find the full citation for the FEED dataset. The authors do not provide a separate citation for the dataset itself, as it is a new contribution of their work. However, they reference existing datasets for comparison, such as MEAD, which I will also note.

The citation for the MEAD dataset is:
> Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. *MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation*. In European Conference on Computer Vision, pages 700â€“717. Springer, 2020.

Now, I will compile the information about the datasets into a structured format, ensuring that I include the necessary details and citations.

1. **FEED Dataset**: A novel dataset introduced in this paper, consisting of 520 multi-view videos of 23 subjects, capturing a wide range of extreme and asymmetric facial expressions.

2. **MEAD Dataset**: A dataset referenced for comparison, which provides audio-visual data for emotional talking-face generation.

After gathering all this information, I will prepare the dataset entries for review or further processing, ensuring that the citations are complete and accurate.