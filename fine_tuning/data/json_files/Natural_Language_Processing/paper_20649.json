[
    {
        "dcterms:creator": [
            "Pan Lu",
            "Swaroop Mishra",
            "Tanglin Xia",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Song-Chun Zhu",
            "Oyvind Tafjord",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "dcterms:description": "The first multimodal scientific question-answering corpus comprising 21,000 inquiries paired with multiple choices and accompanying images, designed to evaluate multimodal reasoning tasks.",
        "dcterms:title": "ScienceQA-IMG (SQA-IMG)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Reasoning",
            "Science Question Answering"
        ],
        "dcat:keyword": [
            "Multimodal dataset",
            "Scientific questions",
            "Image-based reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yuan Liu",
            "Haodong Duan",
            "Yuanhan Zhang",
            "Bo Li",
            "Songyang Zhang",
            "Wangbo Zhao",
            "Yike Yuan",
            "Jiaqi Wang",
            "Conghui He",
            "Ziwei Liu"
        ],
        "dcterms:description": "A systematic and robust benchmark for evaluating multimodal reasoning capabilities of models, providing a more comprehensive assessment compared to existing benchmarks.",
        "dcterms:title": "MMbench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Reasoning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multimodal evaluation",
            "Reasoning tasks",
            "Benchmark dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text, Image",
        "mls:task": [
            "Reasoning Evaluation"
        ]
    }
]