[
    {
        "dcterms:creator": [
            "Z. Allen-Zhu",
            "Y. Li"
        ],
        "dcterms:description": "Synthetic data used to measure the capacity of transformer models computationally by training models of different sizes.",
        "dcterms:title": "Synthetic Data",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.48550/arXiv.2404.05405",
        "dcat:theme": [
            "Machine Learning",
            "Synthetic Data"
        ],
        "dcat:keyword": [
            "Transformer models",
            "Memory capacity",
            "Synthetic training data"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2404.05405",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Capacity measurement",
            "Model training"
        ]
    },
    {
        "dcterms:creator": [
            "T.B. Brown",
            "B. Mann",
            "N. Ryder",
            "M. Subbiah",
            "J. Kaplan",
            "P. Dhariwal",
            "A. Neelakantan",
            "P. Shyam",
            "G. Sastry",
            "A. Askell",
            "S. Agarwal",
            "A. Herbert-Voss",
            "G. Krueger",
            "T. Henighan",
            "R. Child",
            "A. Ramesh",
            "D.M. Ziegler",
            "J. Wu",
            "C. Winter",
            "C. Hesse",
            "M. Chen",
            "E. Sigler",
            "M. Litwin",
            "S. Gray",
            "B. Chess",
            "J. Clark",
            "C. Berner",
            "S. McCandlish",
            "A. Radford",
            "I. Sutskever",
            "D. Amodei"
        ],
        "dcterms:description": "The training data used for GPT-3, which consists of a diverse range of internet text.",
        "dcterms:title": "GPT-3 Training Data",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "http://arxiv.org/abs/2005.14165",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "GPT-3",
            "Training data",
            "Language modeling"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2005.14165",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language modeling",
            "Few-shot learning"
        ]
    },
    {
        "dcterms:creator": [
            "S. Mahdavi",
            "R. Liao",
            "C. Thrampoulidis"
        ],
        "dcterms:description": "A dataset used to analyze the memorization capacity of multi-head attention in transformers.",
        "dcterms:title": "Library Size K",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.48550/arXiv.2306.02010",
        "dcat:theme": [
            "Machine Learning",
            "Transformers"
        ],
        "dcat:keyword": [
            "Multi-head attention",
            "Memorization capacity",
            "Transformers"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2306.02010",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Capacity measurement",
            "Model training"
        ]
    },
    {
        "dcterms:creator": [
            "P. Wang"
        ],
        "dcterms:description": "Data used in autoregressive tasks for training transformer models.",
        "dcterms:title": "Autoregressive Task Data",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/lucidrains/x-transformers",
        "dcat:theme": [
            "Machine Learning",
            "Transformers"
        ],
        "dcat:keyword": [
            "Autoregressive tasks",
            "Transformer models",
            "Training data"
        ],
        "dcat:landingPage": "https://github.com/lucidrains/x-transformers",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Token prediction",
            "Language modeling"
        ]
    }
]