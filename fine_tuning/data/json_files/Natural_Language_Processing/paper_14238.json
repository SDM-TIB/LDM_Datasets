[
    {
        "dcterms:creator": [
            "T. B. Brown",
            "B. Mann",
            "N. Ryder",
            "M. Subbiah",
            "J. Kaplan",
            "P. Dhariwal",
            "A. Neelakantan",
            "P. Shyam",
            "G. Sastry",
            "A. Askell",
            "S. Agarwal",
            "A. Herbert-Voss",
            "G. Krueger",
            "T. Henighan",
            "R. Child",
            "A. Ramesh",
            "D. M. Ziegler",
            "J. Wu",
            "C. Winter",
            "C. Hesse",
            "M. Chen",
            "E. Sigler",
            "M. Litwin",
            "S. Gray",
            "B. Chess",
            "J. Clark",
            "C. Berner",
            "S. McCandlish",
            "A. Radford",
            "I. Sutskever",
            "D. Amodei"
        ],
        "dcterms:description": "A large language model that demonstrates few-shot learning capabilities, allowing it to generate human-like text and understand context.",
        "dcterms:title": "GPT-3",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2005.14165",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language model",
            "Few-shot learning",
            "Text generation"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2005.14165",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "C. Raffel",
            "N. Shazeer",
            "A. Roberts",
            "K. Lee",
            "S. Narang",
            "M. Matena",
            "Y. Zhou",
            "W. Li",
            "P. J. Liu"
        ],
        "dcterms:description": "A unified text-to-text transformer model that explores the limits of transfer learning across various NLP tasks.",
        "dcterms:title": "T5",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Transfer Learning"
        ],
        "dcat:keyword": [
            "Text-to-text",
            "Transfer learning",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Text classification",
            "Translation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Devlin",
            "M. Chang",
            "K. Lee",
            "K. Toutanova"
        ],
        "dcterms:description": "A pre-trained deep bidirectional transformer model designed for language understanding tasks.",
        "dcterms:title": "BERT",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Language understanding",
            "Bidirectional transformer",
            "Pre-training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text classification",
            "Question answering",
            "Named entity recognition"
        ]
    }
]