[
    {
        "dcterms:creator": [
            "H. Yu",
            "B. Shen",
            "D. Ran",
            "J. Zhang",
            "Q. Zhang",
            "Y. Ma",
            "G. Liang",
            "Y. Li",
            "T. Xie",
            "Q. Wang"
        ],
        "dcterms:description": "A dataset of 230 Python functions selected from 10 real-world open-source projects hosted on GitHub, used to analyze bugs in code generated by LLMs.",
        "dcterms:title": "CoderEval",
        "dcterms:issued": "2023",
        "dcterms:language": "Python",
        "dcterms:identifier": "https://github.com/CoderEval/CoderEval/tree/ec1177750cf10b5faa414a0e76d1430e75141a44",
        "dcat:theme": [
            "Code Generation",
            "Software Engineering"
        ],
        "dcat:keyword": [
            "Python functions",
            "bug patterns",
            "code generation",
            "LLMs"
        ],
        "dcat:landingPage": "https://github.com/CoderEval/CoderEval/tree/ec1177750cf10b5faa414a0e76d1430e75141a44",
        "dcterms:hasVersion": "",
        "dcterms:format": "Code",
        "mls:task": [
            "Bug Analysis",
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Chen",
            "J. Tworek",
            "H. Jun",
            "Q. Yuan",
            "H. P. de Oliveira Pinto",
            "J. Kaplan",
            "H. Edwards",
            "Y. Burda",
            "N. Joseph",
            "G. Brockman",
            "A. Ray",
            "R. Puri",
            "G. Krueger",
            "M. Petrov",
            "H. Khlaaf",
            "G. Sastry",
            "P. Mishkin",
            "B. Chan",
            "S. Gray",
            "N. Ryder",
            "M. Pavlov",
            "A. Power",
            "L. Kaiser",
            "M. Bavarian",
            "C. Winter",
            "P. Tillet",
            "F. P. Such",
            "D. Cummings",
            "M. Plappert",
            "F. Chantzis",
            "E. Barnes",
            "A. Herbert-Voss",
            "W. H. Guss",
            "A. Nichol",
            "A. Paino",
            "N. Tezak",
            "J. Tang",
            "I. Babuschkin",
            "S. Balaji",
            "S. Jain",
            "W. Saunders",
            "C. Hesse",
            "A. N. Carr",
            "J. Leike",
            "J. Achiam",
            "V. Misra",
            "E. Morikawa",
            "A. Radford",
            "M. Knight",
            "M. Brundage",
            "M. Murati",
            "K. Mayer",
            "P. Welinder",
            "B. McGrew",
            "D. Amodei",
            "S. McCandlish",
            "I. Sutskever",
            "W. Zaremba"
        ],
        "dcterms:description": "A benchmark for evaluating large language models trained on code, consisting of programming problems designed to assess the capabilities of these models.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "Python",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "code evaluation",
            "programming problems",
            "large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Code",
        "mls:task": [
            "Code Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Austin",
            "A. Odena",
            "M. Nye",
            "M. Bosma",
            "H. Michalewski",
            "D. Dohan",
            "E. Jiang",
            "C. Cai",
            "M. Terry",
            "Q. Le",
            "C. Sutton"
        ],
        "dcterms:description": "A multi-task benchmark for programming problems that evaluates the performance of large language models in program synthesis tasks.",
        "dcterms:title": "MBPP (Multi-Task Benchmark for Programming Problems)",
        "dcterms:issued": "2021",
        "dcterms:language": "Python",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "program synthesis",
            "benchmark",
            "large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Code",
        "mls:task": [
            "Program Synthesis"
        ]
    }
]