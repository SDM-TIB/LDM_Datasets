To extract datasets from the research paper titled "Federated Document Visual Question Answering: A Pilot Study" by Khanh Nguyen and Dimosthenis Karatzas, I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by reviewing the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract discusses the challenges of document analysis due to privacy and copyright issues, hinting at the use of existing datasets for Document Visual Question Answering (DocVQA). The introduction elaborates on the need for diverse datasets in this context.

Next, I will focus on the **experimental setup** section, specifically **section 4 (Experimental Setup)**, where the authors detail the datasets used for their experiments. Here, they explicitly mention three datasets:

1. **DocVQA**: A large-scale dataset for visual question answering on single-page administrative documents. The authors note that it includes various document types and has undergone quality control to ensure diversity.

2. **WikiTableQuestions (WTQ)**: This dataset consists of logical questions over HTML tables sourced from Wikipedia, requiring semi-structured document understanding.

3. **TabFact**: A Natural Language Inference (NLI) dataset designed for fact verification based on evidence from Wikipedia tables.

In **section 4.1 (Dataset Selection)**, the authors provide a summary of these datasets, including their characteristics and the number of documents and questions in each. This section is crucial for understanding the datasets' scope and relevance.

Now, I will check the **References section** to find the full citations for each dataset. The citations are as follows:

- For **DocVQA**:
  > Mathew, M., Karatzas, D., & Jawahar, C. (2021). *DocVQA: A dataset for VQA on document images*. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 2200–2209).

- For **WikiTableQuestions (WTQ)**:
  > Pasupat, P., & Liang, P. (2015). *Compositional semantic parsing on semi-structured tables*. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (pp. 1470–1480).

- For **TabFact**:
  > Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X., & Wang, W.Y. (2020). *TabFact: A large-scale dataset for table-based fact verification*. In International Conference on Learning Representations.

With this information, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research, which is essential for reproducibility and further research in the field.