[
    {
        "dcterms:creator": [],
        "dcterms:description": "A novel VQA dataset designed to facilitate comprehensive evaluation of large vision-language models on comprehension tasks, categorized based on Bloom's Taxonomy.",
        "dcterms:title": "BloomVQA",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://huggingface.co/datasets/ygong/BloomVQA",
        "dcat:theme": [
            "Visual Question Answering",
            "Education",
            "Cognitive Assessment"
        ],
        "dcat:keyword": [
            "VQA",
            "Bloom's Taxonomy",
            "multi-modal comprehension",
            "cognitive skills",
            "story comprehension"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/ygong/BloomVQA",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering",
            "Comprehension Assessment"
        ]
    },
    {
        "dcterms:creator": [
            "D. A. Hudson",
            "C. D. Manning"
        ],
        "dcterms:description": "A dataset that leverages scene graph representations to construct VQA tasks describing reasoning based on relations of different entities in single-frame images.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "scene graphs",
            "visual reasoning",
            "compositional question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Tapaswi",
            "Y. Zhu",
            "R. Stiefelhagen",
            "A. Torralba",
            "R. Urtasun",
            "S. Fidler"
        ],
        "dcterms:description": "A dataset containing large scale multiple-choice samples collected for story comprehension based on video clips.",
        "dcterms:title": "MovieQA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Story Comprehension"
        ],
        "dcat:keyword": [
            "video clips",
            "story comprehension",
            "multiple-choice questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "S. Yagcioglu",
            "A. Erdem",
            "E. Erdem",
            "N. Ikizler-Cinbis"
        ],
        "dcterms:description": "A challenge dataset for multimodal comprehension of cooking recipes, constructed based on recipes with procedural text and image instructions.",
        "dcterms:title": "RecipeQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Cooking"
        ],
        "dcat:keyword": [
            "recipes",
            "multimodal comprehension",
            "procedural tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "R. Tanaka",
            "K. Nishida",
            "K. Nishida",
            "T. Hasegawa",
            "I. Saito",
            "K. Saito"
        ],
        "dcterms:description": "A dataset for document visual question answering on multiple images, focusing on multi-hop reasoning.",
        "dcterms:title": "SlideVQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Document Understanding"
        ],
        "dcat:keyword": [
            "slide decks",
            "multi-hop reasoning",
            "document VQA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "R. Krishna",
            "Y. Zhu",
            "O. Groth",
            "J. Johnson",
            "K. Hata",
            "J. Kravitz",
            "S. Chen",
            "Y. Kalantidis",
            "L.-J. Li",
            "D. A. Shamma",
            "M. S. Bernstein",
            "F.-F. Li"
        ],
        "dcterms:description": "A dataset connecting language and vision using crowdsourced dense image annotations, providing a rich set of visual data.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Understanding",
            "Image Annotation"
        ],
        "dcat:keyword": [
            "dense annotations",
            "language and vision",
            "image understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Image Understanding"
        ]
    }
]