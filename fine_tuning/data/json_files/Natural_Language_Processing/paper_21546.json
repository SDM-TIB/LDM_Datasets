[
    {
        "dcterms:creator": [
            "R. Li",
            "F. Koto",
            "E. M. Ponti",
            "Y. Huang",
            "S. Lin",
            "K. Cobbe",
            "M. Suzgun",
            "N. Nangia"
        ],
        "dcterms:description": "MMLU is a comprehensive benchmark dataset for measuring massive multitask language understanding across various domains.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multitask",
            "Language Model",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "F. Koto",
            "N. Aisyah",
            "H. Li",
            "T. Baldwin"
        ],
        "dcterms:description": "IndoMMLU is a benchmark dataset designed to evaluate language understanding in Indonesian, covering a range of educational levels.",
        "dcterms:title": "IndoMMLU",
        "dcterms:issued": "2023",
        "dcterms:language": "Indonesian",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Education"
        ],
        "dcat:keyword": [
            "Indonesian Language",
            "Benchmark",
            "Education"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "E. M. Ponti",
            "G. Glavaš",
            "O. Majewska",
            "Q. Liu",
            "I. Vulić",
            "A. Korhonen"
        ],
        "dcterms:description": "XCOPA is a multilingual dataset designed for causal commonsense reasoning, covering various languages.",
        "dcterms:title": "XCOPA",
        "dcterms:issued": "2020",
        "dcterms:language": "Multilingual",
        "dcterms:identifier": "arXiv:2005.00333",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Multilingual"
        ],
        "dcat:keyword": [
            "Causal Reasoning",
            "Commonsense Knowledge",
            "Multilingual Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Huang",
            "R. Li",
            "H. Li",
            "T. Baldwin"
        ],
        "dcterms:description": "C-Eval is a comprehensive evaluation suite for foundation models in Chinese, covering multiple disciplines and difficulty levels.",
        "dcterms:title": "C-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "arXiv:2305.08322",
        "dcat:theme": [
            "Language Evaluation",
            "Chinese Language"
        ],
        "dcat:keyword": [
            "Evaluation Suite",
            "Chinese Language",
            "Foundation Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding",
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Lin",
            "H. Li",
            "T. Baldwin"
        ],
        "dcterms:description": "TruthfulQA is a benchmark designed to measure how well models mimic human falsehoods across various topics.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Ethics"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Falsehoods",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Ethical Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "K. Cobbe",
            "M. Bavarian",
            "M. Plappert"
        ],
        "dcterms:description": "GSM8K is a dataset of diverse grade school math word problems designed to evaluate models' mathematical reasoning capabilities.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2110.14168",
        "dcat:theme": [
            "Mathematics",
            "Education"
        ],
        "dcat:keyword": [
            "Math Problems",
            "Word Problems",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk"
        ],
        "dcterms:description": "HellaSwag is a dataset for commonsense inference questions that are easy for humans but challenging for models.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Inference"
        ],
        "dcat:keyword": [
            "Inference Questions",
            "Commonsense Knowledge",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Mihaylov",
            "P. Clark",
            "T. Khot"
        ],
        "dcterms:description": "OpenBookQA is a dataset for question answering based on scientific research papers, requiring understanding of core facts.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Science"
        ],
        "dcat:keyword": [
            "Scientific Knowledge",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Knowledge Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "ai2"
        ],
        "dcterms:description": "Winogrande is an adversarial dataset designed to challenge models on commonsense reasoning tasks.",
        "dcterms:title": "Winogrande",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Adversarial Evaluation"
        ],
        "dcat:keyword": [
            "Commonsense Reasoning",
            "Adversarial Dataset",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Amini",
            "S. Gabriel",
            "S. Lin"
        ],
        "dcterms:description": "MathQA is a dataset aimed at interpretable math word problem solving using operation-based formalisms.",
        "dcterms:title": "MathQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Problem Solving"
        ],
        "dcat:keyword": [
            "Math Problems",
            "Word Problems",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Schärli"
        ],
        "dcterms:description": "BBH is a challenging dataset consisting of tasks designed to test the limits of language models.",
        "dcterms:title": "BBH",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Challenging Tasks",
            "Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation",
            "Benchmarking"
        ]
    },
    {
        "dcterms:creator": [
            "N. Nangia",
            "C. Vania",
            "R. Bhalerao"
        ],
        "dcterms:description": "CrowS-Pairs is a challenge dataset for measuring social biases in masked language models.",
        "dcterms:title": "CrowS-Pairs",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Social Biases"
        ],
        "dcat:keyword": [
            "Social Biases",
            "Masked Language Models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Measurement",
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Lin",
            "H. Li",
            "T. Baldwin"
        ],
        "dcterms:description": "TruthfulQA is a benchmark for measuring how models mimic human falsehoods across various topics.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Truthfulness",
            "Ethics"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Falsehoods",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Ethical Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "K. Cobbe",
            "M. Bavarian",
            "M. Plappert"
        ],
        "dcterms:description": "GSM8K is a dataset of diverse grade school math word problems designed to evaluate models' mathematical reasoning capabilities.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2110.14168",
        "dcat:theme": [
            "Mathematics",
            "Education"
        ],
        "dcat:keyword": [
            "Math Problems",
            "Word Problems",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning",
            "Question Answering"
        ]
    }
]