[
    {
        "dcterms:creator": [
            "J. Shahroudy",
            "J. Liu",
            "T. T. Ng",
            "G. Wang"
        ],
        "dcterms:description": "The NTU-RGB+D 60 dataset comprises 60 motion categories and 56,880 video samples, extensively employed in diverse action recognition tasks. It includes 3D skeleton data, masked depth maps, full-depth maps, RGB videos, and IR data, but this study exclusively utilizes the 3D skeleton sequence data.",
        "dcterms:title": "NTU-RGB+D 60",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://rose1.ntu.edu.sg/dataset/actionRecognition/",
        "dcat:theme": [
            "Computer Vision",
            "Human Action Recognition"
        ],
        "dcat:keyword": [
            "3D skeleton data",
            "action recognition",
            "motion categories"
        ],
        "dcat:landingPage": "https://rose1.ntu.edu.sg/dataset/actionRecognition/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Human Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "J. Liu",
            "A. Shahroudy",
            "M. Perez",
            "G. Wang",
            "L. Duan",
            "A. C. Kot"
        ],
        "dcterms:description": "The NTU-RGB+D 120 dataset is an extension of the NTU-RGB+D 60 dataset, expanding the total number of human action categories to 120. It encompasses 3D skeleton data, masked depth maps, full-depth maps, RGB videos, and IR data, with this study focusing on the 3D skeleton sequence data.",
        "dcterms:title": "NTU-RGB+D 120",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Human Action Recognition"
        ],
        "dcat:keyword": [
            "3D skeleton data",
            "action recognition",
            "motion categories"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Human Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "W. Kay",
            "J. Carreira",
            "K. Simonyan",
            "B. Zhang",
            "C. Hillier",
            "S. Vijayanarasimhan",
            "F. Viola",
            "T. Green",
            "T. Back",
            "P. Natsev"
        ],
        "dcterms:description": "The Kinetics-400 dataset is the largest unconstrained action recognition dataset constructed by DeepMind, covering 400 action classes from human-object interactions to human-human interactions, sourced from YouTube videos. This study utilizes skeleton-based action data processed from the raw video clips.",
        "dcterms:title": "Kinetics-400",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1705.06950",
        "dcat:theme": [
            "Computer Vision",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "action recognition",
            "video dataset",
            "human-object interaction"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1705.06950",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]