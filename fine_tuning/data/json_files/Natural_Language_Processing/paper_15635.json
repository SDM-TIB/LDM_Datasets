[
    {
        "dcterms:creator": [
            "A. Zou",
            "Z. Wang",
            "J. Z. Kolter",
            "M. Fredrikson"
        ],
        "dcterms:description": "AdvBench is a benchmark consisting of 520 harmful instructions designed to evaluate the propensity of language models to follow harmful instructions.",
        "dcterms:title": "AdvBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Adversarial Attacks",
            "Language Models"
        ],
        "dcat:keyword": [
            "Harmful instructions",
            "Adversarial prompts",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of model safety"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "RefusalBench is a new benchmark created to evaluate the propensity of models to follow harmful instructions, consisting of 783 questions across 7 categories of misuse.",
        "dcterms:title": "RefusalBench",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Misuse Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "Harmful instructions",
            "Misuse categories",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of model safety"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "L. Martin",
            "K. Stone",
            "P. Albert",
            "A. Almahairi",
            "Y. Babaei",
            "N. Bashlykov",
            "S. Batra",
            "P. Bhargava",
            "S. Bhosale",
            "D. Bikel",
            "L. Blecher",
            "C. C. Ferrer",
            "M. Chen",
            "G. Cucurull",
            "D. Esiobu",
            "J. Fernandes",
            "J. Fu",
            "W. Fu",
            "B. Fuller",
            "C. Gao",
            "V. Goswami",
            "N. Goyal",
            "A. Hartshorn",
            "S. Hosseini",
            "R. Hou",
            "H. Inan",
            "M. Kardas",
            "V. Kerkez",
            "M. Khabsa",
            "I. Kloumann",
            "A. Korenev",
            "P. S. Koura",
            "M.-A. Lachaux",
            "T. Lavril",
            "J. Lee",
            "D. Liskovich",
            "Y. Lu",
            "Y. Mao",
            "X. Martinet",
            "T. Mihaylov",
            "P. Mishra",
            "I. Molybog",
            "Y. Nie",
            "A. Poulton",
            "J. Reizenstein",
            "R. Rungta",
            "K. Saladi",
            "A. Schelten",
            "R. Silva",
            "E. M. Smith",
            "R. Subramanian",
            "X. E. Tan",
            "B. Tang",
            "R. Taylor",
            "A. Williams",
            "J. X. Kuan",
            "P. Xu",
            "Z. Yan",
            "I. Zarov",
            "Y. Zhang",
            "A. Fan",
            "M. Kambadur",
            "S. Narang",
            "A. Rodriguez",
            "R. Stojnic",
            "S. Edunov",
            "T. Scialom"
        ],
        "dcterms:description": "Llama 2-Chat is a collection of large language models developed by Meta, fine-tuned to refuse harmful content.",
        "dcterms:title": "Llama 2-Chat",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Models",
            "AI Safety"
        ],
        "dcat:keyword": [
            "Large language models",
            "Safety fine-tuning",
            "AI ethics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language understanding",
            "Safety evaluation"
        ]
    }
]