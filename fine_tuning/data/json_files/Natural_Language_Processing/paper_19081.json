[
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "The Alpaca dataset offers 52,002 instruction samples developed via the self-instruct paradigm, leveraging the capabilities of the text-davinci-003 model.",
        "dcterms:title": "Alpaca Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Self-instruct",
            "Language model training"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Xu et al."
        ],
        "dcterms:description": "The WizardLM dataset is a refined collection encompassing a total of 250,000 instruction samples, enhanced with gpt-3.5-turbo-0613 during the refinement process.",
        "dcterms:title": "WizardLM Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Data refinement",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Xu et al."
        ],
        "dcterms:description": "A subset of the WizardLM dataset, specifically focusing on 70,000 samples.",
        "dcterms:title": "WizardLM-7b Subset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Subset",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Xu et al."
        ],
        "dcterms:description": "A subset of the WizardLM dataset, specifically focusing on a larger model with 13 billion parameters.",
        "dcterms:title": "WizardLM-13b Subset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Tuning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Subset",
            "Language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    }
]