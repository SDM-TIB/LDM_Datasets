[
    {
        "dcterms:creator": [
            "L. Soldaini",
            "R. Kinney",
            "A. Bhagia",
            "D. Schwenk",
            "D. Atkinson",
            "R. Authur",
            "B. Bogen",
            "K. Chandu",
            "J. Dumas",
            "Y. Elazar",
            "V. Hofmann",
            "A. H. Jha",
            "S. Kumar",
            "L. Lucy",
            "X. Lyu",
            "N. Lambert",
            "I. Magnusson",
            "J. Morrison",
            "N. Muennighoff",
            "A. Naik",
            "C. Nam",
            "M. E. Peters",
            "A. Ravichander",
            "K. Richardson",
            "Z. Shen",
            "E. Strubell",
            "N. Subramani",
            "O. Tafjord",
            "P. Walsh",
            "L. Zettlemoyer",
            "N. A. Smith",
            "H. Hajishirzi",
            "I. Beltagy",
            "D. Groeneveld",
            "J. Dodge",
            "K. Lo"
        ],
        "dcterms:description": "A colossal, clean version of Common Crawl designed to pretrain language models and word representations in English.",
        "dcterms:title": "C4",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Model Pretraining"
        ],
        "dcat:keyword": [
            "Language model",
            "Pretraining",
            "Common Crawl",
            "Text corpus"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/allenai/dolma",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Wang",
            "A. Singh",
            "J. Michael",
            "F. Hill",
            "O. Levy",
            "S. R. Bowman"
        ],
        "dcterms:description": "A multi-task benchmark and analysis platform for natural language understanding.",
        "dcterms:title": "GLUE",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Natural language understanding",
            "Benchmark",
            "Multi-task learning"
        ],
        "dcat:landingPage": "https://huggingface.co/datasets/nyu-mll/glue",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Taori",
            "I. Gulrajani",
            "T. Zhang",
            "Y. Dubois",
            "X. Li",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "An instruction-following LLaMA model.",
        "dcterms:title": "Alpaca",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Following"
        ],
        "dcat:keyword": [
            "Instruction following",
            "LLaMA model",
            "Natural language processing"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "W. Wei",
            "A. Bosma",
            "V. Y. Zhao",
            "K. Guu",
            "A. W. Yu",
            "B. Lester",
            "N. Du",
            "A. M. Dai",
            "Q. V. Le"
        ],
        "dcterms:description": "Finetuned language models that are zero-shot learners.",
        "dcterms:title": "FLAN",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2109.01652",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Model"
        ],
        "dcat:keyword": [
            "Finetuning",
            "Zero-shot learning",
            "Language model"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2109.01652",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Raffel",
            "N. Shazeer",
            "A. Roberts",
            "K. Lee",
            "S. Narang",
            "M. Matena",
            "Y. Zhou",
            "W. Li",
            "P. J. Liu"
        ],
        "dcterms:description": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "dcterms:title": "RealNews",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Transfer Learning"
        ],
        "dcat:keyword": [
            "Transfer learning",
            "Text-to-text transformer",
            "Natural language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "L. Soldaini",
            "R. Kinney",
            "A. Bhagia",
            "D. Schwenk",
            "D. Atkinson",
            "R. Authur",
            "B. Bogen",
            "K. Chandu",
            "J. Dumas",
            "Y. Elazar",
            "V. Hofmann",
            "A. H. Jha",
            "S. Kumar",
            "L. Lucy",
            "X. Lyu",
            "N. Lambert",
            "I. Magnusson",
            "J. Morrison",
            "N. Muennighoff",
            "A. Naik",
            "C. Nam",
            "M. E. Peters",
            "A. Ravichander",
            "K. Richardson",
            "Z. Shen",
            "E. Strubell",
            "N. Subramani",
            "O. Tafjord",
            "P. Walsh",
            "L. Zettlemoyer",
            "N. A. Smith",
            "H. Hajishirzi",
            "I. Beltagy",
            "D. Groeneveld",
            "J. Dodge",
            "K. Lo"
        ],
        "dcterms:description": "An open corpus of three trillion tokens for language model pretraining research.",
        "dcterms:title": "Dolma",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2402.00159",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Model Pretraining"
        ],
        "dcat:keyword": [
            "Language model",
            "Pretraining",
            "Open corpus"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2402.00159",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]