[
    {
        "dcterms:creator": [
            "Jwala Dhamala",
            "Tony Sun",
            "Varun Kumar",
            "Satyapriya Krishna",
            "Yada Pruksachatkun",
            "Kai-Wei Chang",
            "Rahul Gupta"
        ],
        "dcterms:description": "A dataset of prompts curated for measuring biases in language generation models, generated from different Wikipedia pages to detect biases against or for different groups.",
        "dcterms:title": "BOLD",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Language Generation"
        ],
        "dcat:keyword": [
            "Bias",
            "Language Models",
            "Open-ended Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Detection",
            "Language Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jing Xu",
            "Da Ju",
            "Margaret Li",
            "Y-Lan Boureau",
            "Jason Weston",
            "Emily Dinan"
        ],
        "dcterms:description": "A dataset used for training a classifier to detect adversarial dialogues in conversational agents, aimed at ensuring safety in dialogue systems.",
        "dcterms:title": "Bot-Adversarial Dialogue Classifier",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Systems",
            "Safety in AI"
        ],
        "dcat:keyword": [
            "Adversarial Dialogue",
            "Conversational Agents",
            "Safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Dialogue Safety",
            "Adversarial Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Max Bartolo",
            "Alastair Roberts",
            "Johannes Welbl",
            "Sebastian Riedel",
            "Pontus Stenetorp"
        ],
        "dcterms:description": "A dataset for reading comprehension that includes adversarial human annotations to investigate the robustness of AI models.",
        "dcterms:title": "AdversarialQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Adversarial Testing"
        ],
        "dcat:keyword": [
            "Adversarial Examples",
            "Reading Comprehension",
            "AI Robustness"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension",
            "Adversarial Testing"
        ]
    },
    {
        "dcterms:creator": [
            "Max Bartolo",
            "Alastair Roberts",
            "Johannes Welbl",
            "Sebastian Riedel",
            "Pontus Stenetorp"
        ],
        "dcterms:description": "A dataset containing toxic comments from Wikipedia, used to evaluate the performance of models in detecting harmful content.",
        "dcterms:title": "Wikipedia Toxic Comments",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Toxicity Detection",
            "Content Moderation"
        ],
        "dcat:keyword": [
            "Toxic Comments",
            "Content Moderation",
            "AI Safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Toxicity Detection",
            "Content Moderation"
        ]
    },
    {
        "dcterms:creator": [
            "Moin Nadeem",
            "Anna Bethke",
            "Siva Reddy"
        ],
        "dcterms:description": "A dataset designed to measure stereotypical bias in pretrained language models, providing a means to evaluate bias in AI systems.",
        "dcterms:title": "Crowd-sourced datasets for evaluating bias",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Language Models"
        ],
        "dcat:keyword": [
            "Stereotypical Bias",
            "Language Models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Evaluation",
            "Language Model Testing"
        ]
    },
    {
        "dcterms:creator": [
            "Nikita Nangia",
            "Clara Vania",
            "Rasika Bhalerao",
            "Samuel R. Bowman"
        ],
        "dcterms:description": "A challenge dataset for measuring social biases in masked language models, providing pairs of sentences to evaluate bias.",
        "dcterms:title": "CrowS-pairs",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Language Models"
        ],
        "dcat:keyword": [
            "Social Bias",
            "Masked Language Models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias Measurement",
            "Language Model Testing"
        ]
    }
]