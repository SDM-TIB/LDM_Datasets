[
    {
        "dcterms:creator": [
            "Alexander Bondarenko",
            "Maik Fröbe",
            "Meriem Beloucif",
            "Lukas Gienapp",
            "Yamen Ajjour",
            "Chris Biemann",
            "Benno Stein",
            "Henning Wachsmuth",
            "Martin Potthast",
            "Matthias Hagen"
        ],
        "dcterms:description": "The Touché 2020 dataset is used for evaluating argument retrieval models, containing a focused crawl of arguments for 49 test queries addressing socially important issues. It includes relevance judgments for 2,214 documents.",
        "dcterms:title": "Touché 2020",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1007/978-3-030-58219-7_26",
        "dcat:theme": [
            "Argument Retrieval",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Argument retrieval",
            "Relevance judgments",
            "Document collection"
        ],
        "dcat:landingPage": "https://doi.org/10.1007/978-3-030-58219-7_26",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Argument Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Yamen Ajjour",
            "Henning Wachsmuth",
            "Johannes Kiesel",
            "Martin Potthast",
            "Matthias Hagen",
            "Benno Stein"
        ],
        "dcterms:description": "The args.me corpus is a dataset used for argument search, containing a large collection of arguments that can be utilized for various argument retrieval tasks.",
        "dcterms:title": "args.me corpus",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1007/978-3-030-30179-8_4",
        "dcat:theme": [
            "Argument Retrieval",
            "Data Acquisition"
        ],
        "dcat:keyword": [
            "Argument search",
            "Corpus",
            "Data acquisition"
        ],
        "dcat:landingPage": "https://doi.org/10.1007/978-3-030-30179-8_4",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Argument Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Tri Nguyen",
            "Mir Rosenberg",
            "Xia Song",
            "Jianfeng Gao",
            "Saurabh Tiwary",
            "Rangan Majumder",
            "Li Deng"
        ],
        "dcterms:description": "MS MARCO is a human-generated machine reading comprehension dataset designed for evaluating machine reading models.",
        "dcterms:title": "MS MARCO",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf",
        "dcat:theme": [
            "Machine Reading",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Machine reading comprehension",
            "Dataset",
            "Evaluation"
        ],
        "dcat:landingPage": "https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Nandan Thakur",
            "Nils Reimers",
            "Andreas Rücklé",
            "Abhishek Srivastava",
            "Iryna Gurevych"
        ],
        "dcterms:description": "The BEIR benchmark is a heterogeneous benchmark for zero-shot evaluation of information retrieval models, providing a diverse set of datasets for testing.",
        "dcterms:title": "BEIR benchmark",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html",
        "dcat:theme": [
            "Information Retrieval",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Zero-shot evaluation",
            "Information retrieval",
            "Benchmark"
        ],
        "dcat:landingPage": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Information Retrieval Evaluation"
        ]
    }
]