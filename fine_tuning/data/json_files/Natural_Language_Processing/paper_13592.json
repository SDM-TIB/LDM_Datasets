[
    {
        "dcterms:creator": [
            "S. Lin",
            "J. Hilton",
            "O. Evans"
        ],
        "dcterms:description": "TruthfulQA is a benchmark designed to measure how well language models can generate truthful answers to questions, assessing their ability to mimic human falsehoods.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Truthfulness Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Muhlgay",
            "O. Ram",
            "I. Magar",
            "Y. Levine",
            "N. Ratner",
            "Y. Belinkov",
            "O. Abend",
            "K. Leyton-Brown",
            "A. Shashua",
            "Y. Shoham"
        ],
        "dcterms:description": "FACTOR is a dataset designed for evaluating the factuality of language models, featuring long paragraphs and multiple completions for factual assessment.",
        "dcterms:title": "FACTOR",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.06908",
        "dcat:theme": [
            "Natural Language Processing",
            "Factuality Evaluation"
        ],
        "dcat:keyword": [
            "Factuality",
            "Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Factuality Evaluation",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Geva",
            "D. Khashabi",
            "E. Segal",
            "T. Khot",
            "D. Roth",
            "J. Berant"
        ],
        "dcterms:description": "StrategyQA is a question answering benchmark that requires implicit reasoning strategies, challenging models to answer questions that involve multi-hop reasoning.",
        "dcterms:title": "StrategyQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Reasoning",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "K. Cobbe",
            "V. Kosaraju",
            "M. Bavarian",
            "M. Chen",
            "H. Jun",
            "L. Kaiser",
            "M. Plappert",
            "J. Tworek",
            "J. Hilton",
            "R. Nakano"
        ],
        "dcterms:description": "GSM8K is a benchmark for evaluating the ability of models to solve math word problems, focusing on arithmetic reasoning and factual knowledge.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2110.14168",
        "dcat:theme": [
            "Natural Language Processing",
            "Mathematics"
        ],
        "dcat:keyword": [
            "Math Problems",
            "Reasoning",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Math Problem Solving",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "C. H. Chiang",
            "H. Lee"
        ],
        "dcterms:description": "Vicuna QA is a dataset that evaluates instruction-following abilities of chatbots, using human evaluations to assess performance.",
        "dcterms:title": "Vicuna QA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.01937",
        "dcat:theme": [
            "Natural Language Processing",
            "Chatbot Evaluation"
        ],
        "dcat:keyword": [
            "Instruction Following",
            "Chatbot",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Chatbot Evaluation",
            "Instruction Following"
        ]
    }
]