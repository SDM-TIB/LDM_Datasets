[
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "SQuAD1.1 is a large reading comprehension dataset for QA, where the test set is not publicly available, and the benchmark does not provide an evaluation on the span-based metric.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "QA dataset",
            "Reading comprehension",
            "Extractive QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "1.1",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Adam Trischler",
            "Tong Wang",
            "Xingdi Yuan",
            "Justin Harris",
            "Alessandro Sordoni",
            "Philip Bachman",
            "Kaheer Suleman"
        ],
        "dcterms:description": "NewsQA consists of answer-spans to questions generated in a way that reflects realistic information seeking processes in the news domain.",
        "dcterms:title": "NewsQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "News Domain"
        ],
        "dcat:keyword": [
            "QA dataset",
            "NewsQA",
            "Extractive QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee"
        ],
        "dcterms:description": "NaturalQuestions is a QA benchmark in a real-world scenario with Google search queries for naturally-occurring questions and passages from Wikipedia for annotating answer-spans.",
        "dcterms:title": "NaturalQuestions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Real-world QA"
        ],
        "dcat:keyword": [
            "QA dataset",
            "Natural Questions",
            "Extractive QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Extractive Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Zhilin Yang",
            "Peng Qi",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "William Cohen",
            "Ruslan Salakhutdinov",
            "Christopher D Manning"
        ],
        "dcterms:description": "HotpotQA aims to measure complex reasoning performance of QA models and requires finding relevant sentences from the given passages.",
        "dcterms:title": "HotpotQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Multi-hop Reasoning"
        ],
        "dcat:keyword": [
            "QA dataset",
            "HotpotQA",
            "Multi-hop QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-hop Question Answering"
        ]
    }
]