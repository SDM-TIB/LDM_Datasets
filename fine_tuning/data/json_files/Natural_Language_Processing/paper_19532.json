[
    {
        "dcterms:creator": [
            "Yi Zong",
            "Xipeng Qiu"
        ],
        "dcterms:description": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising 646 questions across 8 subjects and 897 images of various types, designed to evaluate the comprehensive capabilities of large vision-language models (LVLMs) in perception, understanding, knowledge, and reasoning.",
        "dcterms:title": "GAOKAO-MM",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "https://github.com/OpenMOSS/GAOKAO-MM",
        "dcat:theme": [
            "Education",
            "Multimodal Learning",
            "Artificial Intelligence"
        ],
        "dcat:keyword": [
            "Chinese College Entrance Examination",
            "Multimodal Benchmark",
            "Vision-Language Models",
            "Perception",
            "Understanding",
            "Reasoning"
        ],
        "dcat:landingPage": "https://github.com/OpenMOSS/GAOKAO-MM",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text and Image",
        "mls:task": [
            "Question Answering",
            "Image Understanding",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "VQA is a dataset designed for visual question answering, focusing on the role of image understanding in answering questions about images.",
        "dcterms:title": "VQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Image Understanding",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Kenneth Marino",
            "Mohammad Rastegari",
            "Ali Farhadi",
            "Roozbeh Mottaghi"
        ],
        "dcterms:description": "OK-VQA is a visual question answering benchmark that requires external knowledge to answer questions about images.",
        "dcterms:title": "OK-VQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Knowledge-Based Reasoning"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "External Knowledge",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Amanpreet Singh",
            "Vivek Natarajan",
            "Meet Shah",
            "Yu Jiang",
            "Xinlei Chen",
            "Dhruv Batra",
            "Devi Parikh",
            "Marcus Rohrbach"
        ],
        "dcterms:description": "TextVQA is a dataset aimed at developing visual question answering models that can read text within images.",
        "dcterms:title": "TextVQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Text Reading",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pan Lu",
            "Hritik Bansal",
            "Tony Xia",
            "Jiacheng Liu",
            "Chunyan Li",
            "Hannaneh Hajishirzi",
            "Hao Cheng",
            "Kai-Wei Chang",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "dcterms:description": "MathVista is a dataset designed to evaluate the mathematical reasoning capabilities of foundation models in visual contexts.",
        "dcterms:title": "MathVista",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Mathematical Reasoning",
            "Visual Contexts",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text and Image",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Chaoyou Fu",
            "Peixian Chen",
            "Yunhang Shen",
            "Yulei Qin",
            "Mengdan Zhang",
            "Xu Lin",
            "Jinrui Yang",
            "Xiawu Zheng",
            "Ke Li",
            "Xing Sun",
            "Yunsheng Wu",
            "Rongrong Ji"
        ],
        "dcterms:description": "MME is a comprehensive evaluation benchmark for multimodal large language models, assessing their performance across various tasks.",
        "dcterms:title": "MME",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Evaluation",
            "Large Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text and Image",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Weihao Yu",
            "Zhengyuan Yang",
            "Linjie Li",
            "Jianfeng Wang",
            "Kevin Lin",
            "Zicheng Liu",
            "Xinchao Wang",
            "Lijuan Wang"
        ],
        "dcterms:description": "MM-Vet is a benchmark for evaluating large multimodal models, focusing on integrated capabilities across various tasks.",
        "dcterms:title": "MM-Vet",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "Multimodal Models",
            "Evaluation",
            "Integrated Capabilities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text and Image",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Pan Lu",
            "Liang Qiu",
            "Jiaqi Chen",
            "Tony Xia",
            "Yizhou Zhao",
            "Wei Zhang",
            "Zhou Yu",
            "Xiaodan Liang",
            "Song-Chun Zhu"
        ],
        "dcterms:description": "IconQA is a benchmark for abstract diagram understanding and visual language reasoning.",
        "dcterms:title": "IconQA",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning",
            "Diagram Understanding"
        ],
        "dcat:keyword": [
            "Diagram Understanding",
            "Visual Reasoning",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Aniruddha Kembhavi",
            "Mike Salvato",
            "Eric Kolve",
            "Minjoon Seo",
            "Hannaneh Hajishirzi",
            "Ali Farhadi"
        ],
        "dcterms:description": "Ai2D is a dataset that emphasizes the importance of diagrams in visual understanding and reasoning.",
        "dcterms:title": "Ai2D",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Understanding",
            "Diagram Reasoning"
        ],
        "dcat:keyword": [
            "Diagrams",
            "Visual Reasoning",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Samira Ebrahimi Kahou",
            "Vincent Michalski",
            "Adam Atkinson",
            "Akos Kadar",
            "Adam Trischler",
            "Yoshua Bengio"
        ],
        "dcterms:description": "FigureQA is an annotated figure dataset designed for visual reasoning tasks.",
        "dcterms:title": "FigureQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning",
            "Annotation"
        ],
        "dcat:keyword": [
            "Visual Reasoning",
            "Annotated Figures",
            "Multimodal Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "ScienceQA is a dataset for science question answering, focusing on the integration of visual and textual information.",
        "dcterms:title": "ScienceQA",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Xiang Yue",
            "Yuansheng Ni",
            "Kai Zhang",
            "Tianyu Zheng",
            "Ruoqi Liu",
            "Ge Zhang",
            "Samuel Stevens",
            "Dongfu Jiang",
            "Weiming Ren",
            "Yuxuan Sun",
            "Cong Wei",
            "Botao Yu",
            "Ruibin Yuan",
            "Renliang Sun",
            "Ming Yin",
            "Boyuan Zheng",
            "Zhenzhu Yang",
            "Yibo Liu",
            "Wenhao Huang",
            "Huan Sun",
            "Yu Su",
            "Wenhu Chen"
        ],
        "dcterms:description": "MMMU is a massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.",
        "dcterms:title": "MMMU",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Artificial General Intelligence"
        ],
        "dcat:keyword": [
            "Multimodal Understanding",
            "Reasoning",
            "Expert AGI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text and Image",
        "mls:task": [
            "Understanding",
            "Reasoning"
        ]
    }
]