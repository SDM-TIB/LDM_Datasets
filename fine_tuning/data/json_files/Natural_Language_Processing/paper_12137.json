[
    {
        "dcterms:creator": [
            "T. Bunk",
            "D. Varshneya",
            "V. Vlasov",
            "A. Nichol"
        ],
        "dcterms:description": "A custom dataset containing 1092 sentences collected to train a Natural Language Understanding (NLU) model for dialogue systems, focusing on various expressions including misspelled and phonetically similar phrases.",
        "dcterms:title": "Custom dataset for NLU training",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Dialogue Systems"
        ],
        "dcat:keyword": [
            "NLU",
            "Dialogue",
            "Natural Language Understanding",
            "Speech Recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Understanding",
            "Dialogue Management"
        ]
    },
    {
        "dcterms:creator": [
            "S. Antol",
            "A. Agrawal",
            "J. Lu",
            "M. Mitchell",
            "D. Batra",
            "C. L. Zitnick",
            "D. Parikh"
        ],
        "dcterms:description": "A custom dataset of 10252 (image, question, answer) triplets collected to fine-tune a Visual Question Answering (VQA) model, focusing on questions that users with visual impairments might ask.",
        "dcterms:title": "Custom dataset for VQA model fine-tuning",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "VQA",
            "Visual Question Answering",
            "Image-Question Pairs",
            "Assistive Technology"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Radford",
            "J. W. Kim",
            "C. Hallacy",
            "A. Ramesh",
            "G. Goh",
            "S. Agarwal",
            "G. Sastry",
            "A. Askell",
            "P. Mishkin",
            "J. Clark"
        ],
        "dcterms:description": "A custom dataset containing 544 image and text description pairs used to fine-tune the CLIP model for landmark recognition, enabling the association of user descriptions with visual landmarks.",
        "dcterms:title": "Custom dataset for CLIP model fine-tuning",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Visual-Language Models"
        ],
        "dcat:keyword": [
            "CLIP",
            "Landmark Recognition",
            "Image-Text Matching",
            "Visual Language Grounding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Landmark Recognition",
            "Visual Language Grounding"
        ]
    }
]