[
    {
        "dcterms:creator": [
            "A. Köpf",
            "Y. Kilcher",
            "D. von Rütte",
            "S. Anagnostidis",
            "Z.-R. Tam",
            "K. Stevens",
            "A. Barhoum",
            "N. M. Duc",
            "O. Stanley",
            "R. Nagyfi",
            "S. ES",
            "S. Suri",
            "D. Glushkov",
            "A. Dantuluri",
            "A. Maguire",
            "C. Schuhmann",
            "H. Nguyen",
            "A. Mattick"
        ],
        "dcterms:description": "A human-generated, human-annotated assistant-style conversation corpus with over 10,000 complete and fully annotated conversation trees.",
        "dcterms:title": "OpenAssistant Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Conversational AI"
        ],
        "dcat:keyword": [
            "Human-annotated dataset",
            "Conversational corpus",
            "Assistant-style conversations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational AI",
            "Dialogue generation"
        ]
    },
    {
        "dcterms:creator": [
            "C. Zhou",
            "P. Liu",
            "P. Xu",
            "S. Iyer",
            "J. Sun",
            "Y. Mao",
            "X. Ma",
            "A. Efrat",
            "P. Yu",
            "L. Yu",
            "S. Zhang",
            "G. Ghosh",
            "M. Lewis",
            "L. Zettlemoyer",
            "O. Levy"
        ],
        "dcterms:description": "A dataset that emphasizes the importance of fewer examples for effective alignment in language models.",
        "dcterms:title": "LIMA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Alignment"
        ],
        "dcat:keyword": [
            "Alignment dataset",
            "Language model training",
            "Few-shot learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Schärli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. V. Le",
            "E. H. Chi",
            "D. Zhou",
            "J. Wei"
        ],
        "dcterms:description": "A benchmark dataset that challenges models with big-bench tasks and evaluates their ability to solve them using chain-of-thought reasoning.",
        "dcterms:title": "BBH",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.09261",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Benchmark dataset",
            "Chain-of-thought reasoning",
            "Task evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "N. Carlini",
            "J. Schulman",
            "J. Steinhardt"
        ],
        "dcterms:description": "A dataset designed to measure massive multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Natural Language Processing",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multitask dataset",
            "Language understanding",
            "Evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Dua",
            "Y. Wang",
            "P. Dasigi",
            "G. Stanovsky",
            "S. Singh",
            "M. Gardner"
        ],
        "dcterms:description": "A reading comprehension benchmark that requires discrete reasoning over paragraphs.",
        "dcterms:title": "DROP",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Discrete reasoning",
            "Benchmark dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Lin",
            "J. Hilton",
            "O. Evans"
        ],
        "dcterms:description": "A dataset that measures how models mimic human falsehoods in their responses.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Truthfulness Evaluation"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Falsehood measurement",
            "Evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]