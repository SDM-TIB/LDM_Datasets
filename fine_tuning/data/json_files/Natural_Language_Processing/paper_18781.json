[
    {
        "dcterms:creator": [
            "R. Taori",
            "I. Gulrajani",
            "T. Zhang",
            "Y. Dubois",
            "X. Li",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "A dataset providing 52k sets of instruction-following data obtained from the instruction-tuned GPT 3.5 model.",
        "dcterms:title": "Stanford-Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Following",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "GPT-3.5",
            "Language model fine-tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language comprehension",
            "Few-shot learning"
        ]
    },
    {
        "dcterms:creator": [
            "L. Gao"
        ],
        "dcterms:description": "A framework for evaluating few-shot language models, including various commonsense question answering tasks.",
        "dcterms:title": "Commonsense Question Answering (Commonsense QA)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "10.5281/zenodo.5371629",
        "dcat:theme": [
            "Question Answering",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense QA",
            "Few-shot evaluation",
            "Natural language understanding"
        ],
        "dcat:landingPage": "https://doi.org/10.5281/zenodo.5371629",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of machines to complete sentences in a commonsense context.",
        "dcterms:title": "Hellaswag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Sentence completion",
            "Commonsense reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentence completion"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bisk",
            "R. Zellers",
            "R. L. Bras",
            "J. Gao",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset for reasoning about physical commonsense in natural language.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Physical Commonsense Reasoning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Physical reasoning",
            "Commonsense reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Physical commonsense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "A dataset designed to challenge question answering systems with a focus on reasoning.",
        "dcterms:title": "ARC-challenge and ARC-easy",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reasoning"
        ],
        "dcat:keyword": [
            "AI2 Reasoning Challenge",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "K. Sakaguchi",
            "R. L. Bras",
            "C. Bhagavatula",
            "Y. Choi"
        ],
        "dcterms:description": "An adversarial dataset designed to evaluate models on the Winograd schema challenge at scale.",
        "dcterms:title": "Winogrande",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Adversarial Evaluation"
        ],
        "dcat:keyword": [
            "Winograd schema",
            "Adversarial dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "C. Clark",
            "K. Lee",
            "M.-W. Chang",
            "T. Kwiatkowski",
            "M. Collins",
            "K. Toutanova"
        ],
        "dcterms:description": "A dataset exploring the difficulty of natural yes/no questions.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Yes/No questions",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding across various subjects.",
        "dcterms:title": "Massively Multitask Language Understanding (MMLU)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multitask Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multitask benchmark",
            "Language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    }
]