[
    {
        "dcterms:creator": [
            "L. Zheng",
            "W.-L. Chiang",
            "Y. Sheng",
            "S. Zhuang",
            "Z. Wu",
            "Y. Zhuang",
            "Z. Lin",
            "Z. Li",
            "D. Li",
            "E. P. Xing",
            "H. Zhang",
            "J. E. Gonzalez",
            "I. Stoica"
        ],
        "dcterms:description": "MT-Bench is a multi-turn benchmark that presents 80 challenging queries, divided into two sub-questions each, covering 8 different categories: writing, role-playing, reasoning, math, coding, extraction, STEM, and humanities. It appraises the conversational capabilities of tested LLMs using another LLM as a judge.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Conversational AI",
            "Benchmark",
            "Evaluation",
            "Large Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of LLMs"
        ]
    },
    {
        "dcterms:creator": [
            "A. Q. Jiang",
            "A. Sablayrolles",
            "A. Mensch",
            "C. Bamford",
            "D. S. Chaplot",
            "D. de las Casas",
            "F. Bressand",
            "G. Lengyel",
            "G. Lample",
            "L. Saulnier",
            "L. R. Lavaud",
            "M.-A. Lachaux",
            "P. Stock",
            "T. L. Scao",
            "T. Lavril",
            "T. Wang",
            "T. Lacroix",
            "W. E. Sayed"
        ],
        "dcterms:description": "Mistral 7B is an open-source large language model with seven billion parameters, designed to outperform other models on various benchmarks, particularly in math and coding tasks.",
        "dcterms:title": "Mistral 7B",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Large Language Model",
            "Open-source",
            "Transformer Architecture"
        ],
        "dcat:landingPage": "https://mistral.ai/",
        "dcterms:hasVersion": "v0.2",
        "dcterms:format": "",
        "mls:task": [
            "Language Generation",
            "Reasoning",
            "Coding"
        ]
    }
]