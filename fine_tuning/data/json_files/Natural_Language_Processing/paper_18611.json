[
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark designed to measure massive multitask language understanding across various domains.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Percy Liang",
            "Rishi Bommasani",
            "Tony Lee",
            "Dimitris Tsipras",
            "Dilara Soylu",
            "Michihiro Yasunaga",
            "Yian Zhang",
            "Deepak Narayanan",
            "Yuhuai Wu",
            "Ananya Kumar"
        ],
        "dcterms:description": "A holistic evaluation framework for language models that assesses various dimensions of performance.",
        "dcterms:title": "HELM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2211.09110",
        "dcat:theme": [
            "Language Evaluation",
            "Holistic Assessment"
        ],
        "dcat:keyword": [
            "Evaluation framework",
            "Language models",
            "Performance metrics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adri√† Garriga-Alonso"
        ],
        "dcterms:description": "A benchmark that quantifies and extrapolates the capabilities of language models through a diverse set of tasks.",
        "dcterms:title": "Big-Bench",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2206.04615",
        "dcat:theme": [
            "Language Benchmarking",
            "Model Capabilities"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language models",
            "Task diversity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel R Bowman"
        ],
        "dcterms:description": "A multi-task benchmark and analysis platform for natural language understanding.",
        "dcterms:title": "GLUE",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1804.07461",
        "dcat:theme": [
            "Natural Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multi-task learning",
            "Language understanding",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P Xing",
            "Hao Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "dcterms:description": "A dataset for evaluating language models in a peer-review setting, focusing on human preference alignment.",
        "dcterms:title": "Chatbot Arena",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "Peer review",
            "Human preferences",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Human evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P Xing",
            "Hao Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "dcterms:description": "A dataset designed for evaluating language models through multi-turn interactions and human feedback.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "Multi-turn evaluation",
            "Human feedback",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Human evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Xuechen Li",
            "Tianyi Zhang",
            "Yann Dubois",
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B Hashimoto"
        ],
        "dcterms:description": "An automatic evaluator for instruction-following models, designed to assess their performance.",
        "dcterms:title": "AlpacaEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Following",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "Automatic evaluation",
            "Instruction-following",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation"
        ]
    }
]