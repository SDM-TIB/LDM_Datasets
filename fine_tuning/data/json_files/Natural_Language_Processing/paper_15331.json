[
    {
        "dcterms:creator": [
            "A. Srivastava",
            "A. Rastogi",
            "A. Rao",
            "A. A. M. Shoeb",
            "A. Abid",
            "A. Fisch",
            "A. R. Brown",
            "A. Santoro",
            "A. Gupta",
            "A. Garriga-Alonso"
        ],
        "dcterms:description": "A collection of over 200 tasks designed to evaluate the capabilities of language models across various domains.",
        "dcterms:title": "BIG-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Model Evaluation",
            "Task Collection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "General AI Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Li",
            "X. Cheng",
            "W. X. Zhao",
            "J.-Y. Nie",
            "J.-R. Wen"
        ],
        "dcterms:description": "A large-scale hallucination evaluation benchmark for large language models, aimed at assessing their ability to generate accurate and reliable information.",
        "dcterms:title": "HALUEVAL",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "Hallucination Evaluation",
            "Language Model Assessment",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation of Language Models"
        ]
    },
    {
        "dcterms:creator": [
            "S. Gehrmann",
            "A. Bhattacharjee",
            "A. Mahendiran",
            "A. Wang",
            "A. Papangelis",
            "A. Madaan",
            "A. McMillan-Major",
            "A. Shvets",
            "A. Upadhyay",
            "B. Yao"
        ],
        "dcterms:description": "A multilingual natural language generation benchmarking framework that allows for easy evaluation of language models across different languages.",
        "dcterms:title": "GEMv2",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Evaluation",
            "Multilingual NLG"
        ],
        "dcat:keyword": [
            "Benchmarking",
            "Multilingual Evaluation",
            "Natural Language Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Generation Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Liang",
            "R. Bommasani",
            "T. Lee",
            "D. Tsipras",
            "D. Soylu",
            "M. Yasunaga",
            "Y. Zhang",
            "D. Narayanan",
            "Y. Wu",
            "A. Kumar"
        ],
        "dcterms:description": "A comprehensive framework for the holistic evaluation of language models, focusing on various dimensions of model performance.",
        "dcterms:title": "Holistic Evaluation of Language Models",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "AI Evaluation",
            "Language Models"
        ],
        "dcat:keyword": [
            "Holistic Evaluation",
            "Language Model Assessment",
            "Performance Metrics"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Comprehensive AI Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "R. Burnell",
            "H. Hao",
            "A. R. A. Conway",
            "J. H. Orallo"
        ],
        "dcterms:description": "A dataset aimed at revealing the structure of language model capabilities through various cognitive tasks.",
        "dcterms:title": "Cognitive Tasks Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.10062",
        "dcat:theme": [
            "AI Evaluation",
            "Cognitive Assessment"
        ],
        "dcat:keyword": [
            "Cognitive Tasks",
            "Language Model Capabilities",
            "Evaluation Framework"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Cognitive Evaluation"
        ]
    }
]