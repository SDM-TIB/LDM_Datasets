To extract datasets from the research paper titled "Gorgeous: Create Your Desired Character Facial Makeup from Any Ideas" by Jia Wei Sii and Chee Seng Chan, I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, methodology, and experiments sections** of the paper. The abstract mentions that the authors conducted comprehensive experiments, which suggests that datasets are involved. I will look for any explicit mentions of datasets or references to data sources.

In the **introduction**, the authors discuss the limitations of existing makeup transfer methods and introduce their novel approach, Gorgeous. They imply that their method utilizes various datasets but do not specify them yet. I will need to look further into the paper.

Next, I will focus on **section 4.1 (Datasets and Evaluation Metrics)**, where the authors explicitly list the datasets used for training and evaluation. Here, they mention:

1. **BeautyFace Dataset**: This dataset contains 2,447 makeup images and is used to train the Makeup Formatting (MaFor) module of their method.

2. **Reference Sets**: The authors categorize their reference sets into two styles:
   - **Style 1**: Facial images with makeup, which includes images generated from the SDXL model and curated images from Pinterest.
   - **Style 2**: Non-facial images with arbitrary styles, sourced from various online platforms.

In the **experiments section**, the authors confirm the use of the BeautyFace dataset for training and provide additional context about the reference sets used for their experiments.

Now, I will consult the **References section** to retrieve full citations for the datasets mentioned:

- For the **BeautyFace Dataset**, the citation is:
  > Qixin Yan, Chunle Guo, Jixin Zhao, Yuekun Dai, Chen Change Loy, and Chongyi Li. *BeautyRec: Robust, Efficient, and Component-Specific Makeup Transfer*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.

- For the **SDXL model**, the citation is:
  > Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. *SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis*. arXiv preprint arXiv:2307.01952, 2023.

- For the **Pinterest images**, since they are sourced from a website, I will note that they are not formally cited in the references but can be accessed at:
  > Pinterest. https://www.pinterest.com/

Now that I have identified the datasets and their citations, I will compile this information into a structured format for clarity and future reference. This will ensure that I have accurately captured the datasets used in the research and their respective citations.