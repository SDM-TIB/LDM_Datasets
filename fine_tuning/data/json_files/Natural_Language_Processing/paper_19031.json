[
    {
        "dcterms:creator": [
            "J. Achiam",
            "S. Adler",
            "S. Agarwal",
            "L. Ahmad",
            "I. Akkaya",
            "F. L. Aleman",
            "D. Almeida",
            "J. Altenschmidt",
            "S. Altman",
            "S. Anadkat"
        ],
        "dcterms:description": "A dataset consisting of harmful prompts used to evaluate GPT-4's response to harmful questions, including prompts from both OpenAI and Anthropic.",
        "dcterms:title": "OpenAI and Anthropic Red Teaming Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2303.08774",
        "dcat:theme": [
            "Natural Language Processing",
            "Safety Evaluation"
        ],
        "dcat:keyword": [
            "Harmful prompts",
            "GPT-4",
            "Red teaming"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation",
            "Prompt testing"
        ]
    },
    {
        "dcterms:creator": [
            "G. Deng",
            "Y. Liu",
            "Y. Li",
            "K. Wang",
            "Y. Zhang",
            "Z. Li",
            "H. Wang",
            "T. Zhang",
            "Y. Liu"
        ],
        "dcterms:description": "This dataset encompasses 10 distinct restrictive scenarios, with five-question prompts crafted for each scenario, totaling 50 instances.",
        "dcterms:title": "MasterKey",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.08715",
        "dcat:theme": [
            "Natural Language Processing",
            "Safety Evaluation"
        ],
        "dcat:keyword": [
            "Restrictive scenarios",
            "Harmful content",
            "Privacy"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation",
            "Prompt testing"
        ]
    },
    {
        "dcterms:creator": [
            "P. Chao",
            "A. Robey",
            "E. Dobriban",
            "H. Hassani",
            "G. J. Pappas",
            "E. Wong"
        ],
        "dcterms:description": "A curated subset of prompts from the AdvBench benchmark, focusing on harmful behaviors, consisting of 50 harmful prompts.",
        "dcterms:title": "AdvBench Subset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2310.08419",
        "dcat:theme": [
            "Natural Language Processing",
            "Safety Evaluation"
        ],
        "dcat:keyword": [
            "Harmful behaviors",
            "Prompt testing",
            "AdvBench"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation",
            "Prompt testing"
        ]
    }
]