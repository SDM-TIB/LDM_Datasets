[
    {
        "dcterms:creator": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "dcterms:description": "A dataset used for evaluating language models, focusing on perplexity and language generation tasks.",
        "dcterms:title": "WikiText2",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Language generation",
            "Perplexity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Colin Raffel",
            "Noam Shazeer",
            "Adam Roberts",
            "Katherine Lee",
            "Sharan Narang",
            "Michael Matena",
            "Yanqi Zhou",
            "Wei Li",
            "Peter J Liu"
        ],
        "dcterms:description": "A large-scale dataset for training and evaluating text-to-text models, focusing on various NLP tasks.",
        "dcterms:title": "C4",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text-to-Text Transfer"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Transfer learning",
            "NLP tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text-to-Text Transfer"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset designed for evaluating physical commonsense reasoning in natural language.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Physical reasoning",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "A dataset for evaluating question answering systems, focusing on reasoning challenges.",
        "dcterms:title": "ARC",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Reasoning",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Christopher Clark",
            "Kenton Lee",
            "Ming-Wei Chang",
            "Tom Kwiatkowski",
            "Michael Collins",
            "Kristina Toutanova"
        ],
        "dcterms:description": "A dataset for exploring the challenges of natural yes/no questions.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Yes/No questions",
            "Text dataset",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset for evaluating the ability of machines to complete sentences.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Sentence completion",
            "Text dataset",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "An adversarial dataset designed to challenge models on the Winograd schema.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Winograd schema",
            "Adversarial dataset",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multitask benchmark",
            "Language understanding",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    }
]