[
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "A state-of-the-art transformer-based model that delivers significantly higher accuracy in downstream tasks such as question-answering and multi-task language understanding over its immediate predecessor.",
        "dcterms:title": "GPT-4",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2303.08774",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Large Language Model",
            "Question Answering",
            "Multi-task Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "A transformer-based model that serves as the predecessor to GPT-4, used for various language tasks.",
        "dcterms:title": "GPT-3",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2303.08774",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Large Language Model",
            "Language Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A large transformer model with 13 billion parameters used for pre-training in the study.",
        "dcterms:title": "mt5-XXL",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Transformer Model",
            "Pre-training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Samyam Rajbhandari",
            "Jeff Rasley",
            "Olatunji Ruwase",
            "Yuxiong He"
        ],
        "dcterms:description": "A memory optimization technique designed to enable the training of trillion parameter models.",
        "dcterms:title": "ZeRO",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1910.02054",
        "dcat:theme": [
            "Machine Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Memory Optimization",
            "Large Scale Training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Training"
        ]
    },
    {
        "dcterms:creator": [
            "Jeff Rasley",
            "Samyam Rajbhandari",
            "Olatunji Ruwase",
            "Yuxiong He"
        ],
        "dcterms:description": "A system optimization framework that enables the training of deep learning models with over 100 billion parameters.",
        "dcterms:title": "DeepSpeed",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://deepspeed.readthedocs.io/en/latest/zero3.html",
        "dcat:theme": [
            "Machine Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "System Optimization",
            "Deep Learning"
        ],
        "dcat:landingPage": "https://deepspeed.readthedocs.io/en/latest/zero3.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Training"
        ]
    }
]