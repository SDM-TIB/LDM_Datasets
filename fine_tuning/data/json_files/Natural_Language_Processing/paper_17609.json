[
    {
        "dcterms:creator": [
            "Houwei Cao",
            "David G Cooper",
            "Michael K Keutmann",
            "Ruben C Gur",
            "Ani Nenkova",
            "Ragini Verma"
        ],
        "dcterms:description": "CREMA-D is an audio-visual dataset for emotion recognition research, consisting of both facial and vocal emotional expressions. The emotional states can be divided into 6 classes: happy, sad, angry, fear, disgust, and neutral. There are 7,442 video clips in total, which are randomly divided into 6,698 samples as the training set and 744 samples as the testing set.",
        "dcterms:title": "CREMA-D",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Emotion Recognition",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Audio-visual dataset",
            "Emotion recognition",
            "Facial expressions",
            "Vocal expressions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Emotion Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Yapeng Tian",
            "Jing Shi",
            "Bochen Li",
            "Zhiyao Duan",
            "Chenliang Xu"
        ],
        "dcterms:description": "AVE is an audio-visual video dataset for event localization. There are a total of 28 event classes and 4,143 10-second video clips with both auditory and visual tracks as well as second-level annotations which are collected from YouTube.",
        "dcterms:title": "AVE",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Event Localization",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Audio-visual dataset",
            "Event localization",
            "Video clips",
            "Annotations"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Event Localization"
        ]
    },
    {
        "dcterms:creator": [
            "Firoj Alam",
            "Ferda Ofli",
            "Muhammad Imran"
        ],
        "dcterms:description": "CrisisMMD is an image-text crisis dataset, which consists of annotated image-tweet pairs collected using event-specific keywords and hashtags during seven natural disasters in 2017. The dataset is used for the classification task of Informative vs. Not Informative.",
        "dcterms:title": "CrisisMMD",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Crisis Management",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Image-text dataset",
            "Crisis events",
            "Natural disasters",
            "Informative classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Classification"
        ]
    }
]