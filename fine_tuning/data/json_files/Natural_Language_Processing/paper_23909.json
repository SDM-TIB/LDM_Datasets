[
    {
        "dcterms:creator": [
            "Yuhang Wu",
            "Wenmeng Yu",
            "Yean Cheng",
            "Yan Wang",
            "Xiaohan Zhang",
            "Jiazheng Xu",
            "Ming Ding",
            "Yuxiao Dong"
        ],
        "dcterms:description": "AlignMMBench is a comprehensive alignment benchmark specifically designed for evaluating the capabilities of Chinese Vision-Language Models (VLMs). It includes 1,054 images and 4,978 question-answer pairs across thirteen specific tasks, focusing on both single-turn and multi-turn dialogue scenarios.",
        "dcterms:title": "AlignMMBench",
        "dcterms:issued": "",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "https://alignmmbench.github.io/",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Vision-Language Models"
        ],
        "dcat:keyword": [
            "Chinese VLMs",
            "alignment benchmark",
            "multimodal tasks",
            "dialogue evaluation"
        ],
        "dcat:landingPage": "https://alignmmbench.github.io/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image and Text",
        "mls:task": [
            "Evaluation of Vision-Language Models",
            "Dialogue Context Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "C. Fu",
            "P. Chen",
            "Y. Shen",
            "Y. Qin",
            "M. Zhang",
            "X. Lin",
            "J. Yang",
            "X. Zheng",
            "K. Li",
            "X. Sun",
            "Y. Wu",
            "R. Ji"
        ],
        "dcterms:description": "MME is a comprehensive evaluation benchmark for multimodal large language models, focusing on their performance across various tasks.",
        "dcterms:title": "MME",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2306.13394",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Large Language Models"
        ],
        "dcat:keyword": [
            "evaluation benchmark",
            "multimodal models",
            "large language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation of Multimodal Models"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Liu",
            "H. Duan",
            "Y. Zhang",
            "B. Li",
            "S. Zhang",
            "W. Zhao",
            "Y. Yuan",
            "J. Wang",
            "C. He",
            "Z. Liu"
        ],
        "dcterms:description": "MMBench is designed to evaluate the performance of multimodal models across various tasks, assessing their capabilities in a comprehensive manner.",
        "dcterms:title": "MMBench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2307.06281",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Model Performance Assessment"
        ],
        "dcat:keyword": [
            "multimodal models",
            "evaluation benchmark",
            "performance assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Lu",
            "H. Bansal",
            "T. Xia",
            "J. Liu",
            "C. Li",
            "H. Hajishirzi",
            "H. Cheng",
            "K.-W. Chang",
            "M. Galley",
            "J. Gao"
        ],
        "dcterms:description": "MathVista evaluates the mathematical reasoning capabilities of foundation models in visual contexts, providing a benchmark for assessing their performance.",
        "dcterms:title": "MathVista",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematical Reasoning",
            "Visual Context Evaluation"
        ],
        "dcat:keyword": [
            "mathematical reasoning",
            "visual evaluation",
            "foundation models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "X. Yue",
            "Y. Ni",
            "K. Zhang",
            "T. Zheng",
            "R. Liu",
            "G. Zhang",
            "S. Stevens",
            "D. Jiang",
            "W. Ren",
            "Y. Sun"
        ],
        "dcterms:description": "MMMU is a massive benchmark for multimodal understanding and reasoning across multiple disciplines, aimed at evaluating expert AGI.",
        "dcterms:title": "MMMU",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2311.16502",
        "dcat:theme": [
            "Multimodal Understanding",
            "Reasoning Benchmark"
        ],
        "dcat:keyword": [
            "multimodal understanding",
            "reasoning benchmark",
            "expert AGI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multimodal Understanding Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Goyal",
            "T. Khot",
            "D. Summers-Stay",
            "D. Batra",
            "D. Parikh"
        ],
        "dcterms:description": "VQAv2 enhances the role of image understanding in visual question answering, providing a benchmark for evaluating VQA models.",
        "dcterms:title": "VQAv2",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Image Understanding"
        ],
        "dcat:keyword": [
            "VQA benchmark",
            "image understanding",
            "visual question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Hiippala",
            "M. Alikhani",
            "J. Haverinen",
            "T. Kalliokoski",
            "E. Logacheva",
            "S. Orekhova",
            "A. Tuomainen",
            "M. Stone",
            "J. A. Bateman"
        ],
        "dcterms:description": "Ai2D is a multimodal corpus consisting of primary school science diagrams, aimed at enhancing language resources and evaluation.",
        "dcterms:title": "Ai2D",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Corpus",
            "Science Education"
        ],
        "dcat:keyword": [
            "science diagrams",
            "multimodal corpus",
            "language resources"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Educational Resource Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "C. Li",
            "Q. Wu",
            "Y. J. Lee"
        ],
        "dcterms:description": "LLaVABench focuses on visual instruction tuning, providing a benchmark for evaluating visual instruction models.",
        "dcterms:title": "LLaVABench",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Instruction Tuning",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "visual instruction",
            "benchmark",
            "model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Instruction Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "L. Chen",
            "J. Li",
            "X. Dong",
            "P. Zhang",
            "Y. Zang",
            "Z. Chen",
            "H. Duan",
            "J. Wang",
            "Y. Qiao",
            "D. Lin"
        ],
        "dcterms:description": "MMStar evaluates large vision-language models, providing insights into their performance and capabilities.",
        "dcterms:title": "MMStar",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2403.20330",
        "dcat:theme": [
            "Vision-Language Models",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "evaluation benchmark",
            "vision-language models",
            "model performance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "W. Yu",
            "Z. Yang",
            "L. Li",
            "J. Wang",
            "K. Lin",
            "Z. Liu",
            "X. Wang",
            "L. Wang"
        ],
        "dcterms:description": "MM-Vet evaluates large multimodal models for integrated capabilities, providing a benchmark for assessing their performance.",
        "dcterms:title": "MM-Vet",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2308.02490",
        "dcat:theme": [
            "Multimodal Evaluation",
            "Model Performance Assessment"
        ],
        "dcat:keyword": [
            "multimodal models",
            "evaluation benchmark",
            "integrated capabilities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Liu",
            "Z. Li",
            "H. Li",
            "W. Yu",
            "M. Huang",
            "D. Peng",
            "M. Liu",
            "M. Chen",
            "C. Li",
            "L. Jin"
        ],
        "dcterms:description": "OCRBench investigates the role of Optical Character Recognition in large multimodal models, providing insights into their capabilities.",
        "dcterms:title": "OCRBench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2305.07895",
        "dcat:theme": [
            "Optical Character Recognition",
            "Multimodal Models"
        ],
        "dcat:keyword": [
            "OCR evaluation",
            "multimodal models",
            "character recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "OCR Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Bai",
            "S. Yang",
            "J. Bai",
            "P. Wang",
            "X. Zhang",
            "J. Lin",
            "X. Wang",
            "C. Zhou",
            "J. Zhou"
        ],
        "dcterms:description": "TouchStone evaluates vision-language models by leveraging language models, providing a benchmark for assessing their performance.",
        "dcterms:title": "TouchStone",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2308.16890",
        "dcat:theme": [
            "Vision-Language Evaluation",
            "Model Assessment"
        ],
        "dcat:keyword": [
            "evaluation benchmark",
            "vision-language models",
            "language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bitton",
            "H. Bansal",
            "J. Hessel",
            "R. Shao",
            "W. Zhu",
            "A. Awadalla",
            "J. Gardner",
            "R. Taori",
            "L. Schmidt"
        ],
        "dcterms:description": "VisIT-Bench is a benchmark for vision-language instruction following, inspired by real-world use cases.",
        "dcterms:title": "VisIT-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2308.06595",
        "dcat:theme": [
            "Vision-Language Instruction",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "instruction following",
            "vision-language models",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following Evaluation"
        ]
    }
]