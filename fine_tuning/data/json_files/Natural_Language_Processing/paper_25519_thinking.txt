To extract datasets from the research paper titled "Unsupervised Video Highlight Detection by Learning from Audio and Visual Recurrence" by Zahidul Islam et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any datasets mentioned. The abstract indicates that the authors conducted experiments on "three highlight detection benchmarks," which suggests that specific datasets are used.

Next, I will focus on the **experiments section** (section 4.1) where the authors explicitly list the datasets used for their experiments. Here, they mention three benchmark datasets:

1. **YouTube Highlights**: This dataset is constructed by mining YouTube videos related to six specific categories (dog, parkour, gymnastics, skiing, skating, and surfing) with about 100 videos in each category.

2. **TVSum**: This dataset consists of 50 videos across 10 diverse categories, such as changing a vehicle tire and making a sandwich. The authors mention that they randomly split this dataset with 80% for training and 20% for testing.

3. **QVHighlights**: This is a larger dataset with over 10,000 videos designed for query-focused video highlight detection and moment retrieval. It includes standard train, validation, and test splits with a ratio of 70:15:15.

After identifying the datasets, I will check the **References section** to find the full citations for each dataset. The citations are crucial for proper attribution and to allow others to access the datasets.

The full citations for the datasets are as follows:

- For **YouTube Highlights**, the citation is:
  > Min Sun, Ali Farhadi, and Steve Seitz. *Ranking domain-specific highlights by analyzing edited videos*. In European Conference on Computer Vision, 2014.

- For **TVSum**, the citation is:
  > Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. *TVSum: Summarizing web videos using titles*. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.

- For **QVHighlights**, the citation is:
  > Jie Lei, Tamara L Berg, and Mohit Bansal. *Detecting moments and highlights in videos via natural language queries*. In Advances in Neural Information Processing Systems, 2021.

Now that I have gathered all the necessary information about the datasets and their citations, I will compile this information into a structured format for further processing or review.