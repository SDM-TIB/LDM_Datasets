[
    {
        "dcterms:creator": [
            "S. Merity",
            "C. Xiong",
            "J. Bradbury",
            "R. Socher"
        ],
        "dcterms:description": "The wikitext-2 dataset is used for evaluating the perplexity of language models, providing a benchmark for their performance.",
        "dcterms:title": "wikitext-2",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1609.07843",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Text dataset",
            "Language modeling",
            "Perplexity evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "L. Martin",
            "K. Stone"
        ],
        "dcterms:description": "Llama2-7B-hf is a large language model that serves as a foundation and fine-tuned chat model, used for various natural language processing tasks.",
        "dcterms:title": "Llama2-7B-hf",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2307.09288",
        "dcat:theme": [
            "Natural Language Processing",
            "Large Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Chat model",
            "Fine-tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Generation",
            "Conversational AI"
        ]
    }
]