[
    {
        "dcterms:creator": [
            "Matt Mahoney"
        ],
        "dcterms:description": "The enwik9 dataset contains the first 109 bytes of English Wikipedia with about 120 million words after being processed by Matt Mahoneyâ€™s text preprocessing script.",
        "dcterms:title": "enwik9",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Processing"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Text corpus",
            "Word embeddings"
        ],
        "dcat:landingPage": "http://mattmahoney.net/dc/textdata.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding",
            "Text Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Matt Mahoney"
        ],
        "dcterms:description": "The text8 dataset contains the first 108 bytes of the preprocessed enwik9 with about 17 million words.",
        "dcterms:title": "text8",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Processing"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Text corpus",
            "Word embeddings"
        ],
        "dcat:landingPage": "http://mattmahoney.net/dc/textdata.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word Embedding",
            "Text Modeling"
        ]
    }
]