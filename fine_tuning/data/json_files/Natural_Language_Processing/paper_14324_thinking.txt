To extract datasets from the research paper titled "Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature" by Shengze Jin et al., I will follow a systematic approach to identify and document each dataset mentioned in the paper, ensuring to include full citations for each.

First, I will read through the **abstract, introduction, and experiments sections** to locate any references to datasets. The abstract mentions that Q-REG sets a new state-of-the-art on the **3DMatch, KITTI, and ModelNet benchmarks**, indicating that these are likely datasets used in the research.

Next, I will examine **section 4 (Experiments)**, where the authors typically describe the datasets used for evaluation. Here, they explicitly mention the following datasets:

1. **3DMatch Dataset**: This dataset contains 62 scenes, with specific splits for training, validation, and testing. The authors detail how they preprocess the data for their experiments.

2. **KITTI Dataset**: The authors describe using the KITTI odometry dataset, which consists of 11 sequences of LiDAR-scanned outdoor driving scenarios. They specify how they split the dataset into training, validation, and testing sets.

3. **ModelNet Dataset**: This dataset comprises 12,311 3D CAD models of man-made objects from 40 categories, with specific splits for training, validation, and testing.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset:

- For the **3DMatch Dataset**, the citation is:
  > Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. *3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2020–2027, 2017.

- For the **KITTI Dataset**, the citation is:
  > Andreas Geiger, Philip Lenz, and Raquel Urtasun. *Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite*. In 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3354–3361, 2012.

- For the **ModelNet Dataset**, the citation is:
  > Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. *ShapeNet: A Deep Representation for Volumetric Shapes*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1912–1920, 2015.

Now, I will compile the dataset entries, ensuring that each entry includes the dataset name, a brief description, and the full citation. This structured approach will ensure that I accurately capture all relevant information regarding the datasets used in the research paper.