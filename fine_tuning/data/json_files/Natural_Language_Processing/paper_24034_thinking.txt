To extract the datasets from the research paper titled "Reactor Mk.1 performances: MMLU, HumanEval and BBH test results" by TJ Dunham and Henry Syahputra, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and datasets sections** of the paper to identify any datasets mentioned. The abstract provides a brief overview of the performance of Reactor Mk.1 on various benchmarks, which suggests that datasets are involved.

Next, I will focus on the **Datasets section** where the authors explicitly mention three datasets used for benchmarking:

1. **MMLU**: The paper describes MMLU as a benchmark designed to assess a model's world knowledge and problem-solving ability across 57 different subjects, including mathematics and US history. This dataset is crucial for evaluating multitasking accuracy in language models.

2. **HumanEval**: This dataset is created to measure the functional correctness of code generated from docstrings. It serves as a benchmark for comparing the performance of different language models in code generation tasks.

3. **BBH**: The BBH dataset is a subset of the BIG-Bench benchmark, aimed at evaluating LLM capabilities across various domains, including traditional NLP and commonsense reasoning. It consists of over 200 tasks designed to challenge current language models.

After identifying the datasets, I will check the **References section** for full citations. The citations for the datasets are as follows:

- For **MMLU**, the citation is:
  > D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. "Measuring massive multitask language understanding," arXiv preprint arXiv:2009.03300, 2020.

- For **HumanEval**, the citation is:
  > M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, and A. Ray. "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021.

- For **BBH**, the citation is:
  > M. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. "Challenging big-bench tasks and whether chain-of-thought can solve them," arXiv preprint arXiv:2210.09261, 2022.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured the datasets and their respective references from the paper.