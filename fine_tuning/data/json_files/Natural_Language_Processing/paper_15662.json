[
    {
        "dcterms:creator": [
            "Suraj Jyothi Unni",
            "Raha Moraffah",
            "Huan Liu"
        ],
        "dcterms:description": "VQA-GEN is a novel large-scale dataset for multi-modal distribution shifts, constructed through a three-stage data generation pipeline that introduces controlled visual and textual variations.",
        "dcterms:title": "VQA-GEN",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Domain Generalization"
        ],
        "dcat:keyword": [
            "Multi-modal dataset",
            "Distribution shift",
            "Visual reasoning",
            "Textual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Agrawal",
            "D. Batra",
            "D. Parikh",
            "A. Kembhavi"
        ],
        "dcterms:description": "VQA-CP is a dataset that tests generalization with counterfactual questions, focusing on the answer distribution of the VQA v2 dataset.",
        "dcterms:title": "VQA-CP",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1712.00377",
        "dcat:theme": [
            "Visual Question Answering",
            "Domain Generalization"
        ],
        "dcat:keyword": [
            "Counterfactual questions",
            "Generalization"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1712.00377",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Gokhale",
            "P. Banerjee",
            "C. Baral",
            "Y. Yang"
        ],
        "dcterms:description": "VQA-Compose focuses on logically composed binary questions, providing a dataset for evaluating reasoning in visual question answering.",
        "dcterms:title": "VQA-Compose",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2002.08325",
        "dcat:theme": [
            "Visual Question Answering",
            "Logical Reasoning"
        ],
        "dcat:keyword": [
            "Binary questions",
            "Logical composition"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2002.08325",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Ren",
            "R. Kiros",
            "R. Zemel"
        ],
        "dcterms:description": "COCO-QA is a dataset focusing on visual question answering with diverse images and question-answer sets.",
        "dcterms:title": "COCO-QA",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Diverse images",
            "Question-answer pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "R. Krishna",
            "Y. Zhu",
            "O. Groth",
            "J. Johnson",
            "K. Hata",
            "J. Kravitz",
            "S. Chen",
            "Y. Kalantidis",
            "L.-J. Li",
            "D. A. Shamma",
            "M. S. Bernstein",
            "F.-F. Li"
        ],
        "dcterms:description": "Visual Genome connects language and vision using crowd-sourced dense image annotations, providing extensive image annotations for scene understanding.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Scene Understanding"
        ],
        "dcat:keyword": [
            "Dense annotations",
            "Language and vision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Johnson",
            "B. Hariharan",
            "L. van der Maaten",
            "L. Fei-Fei",
            "C. L. Zitnick",
            "R. Girshick"
        ],
        "dcterms:description": "CLEVR is a diagnostic dataset for compositional language and elementary visual reasoning, focusing on reasoning with complex questions about 3D scenes.",
        "dcterms:title": "CLEVR",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Compositional language",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Zhu",
            "O. Groth",
            "M. Bernstein",
            "L. Fei-Fei"
        ],
        "dcterms:description": "Visual7W tests natural language understanding and reasoning in images, providing a dataset for grounded question answering.",
        "dcterms:title": "Visual7W",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Grounded Question Answering"
        ],
        "dcat:keyword": [
            "Natural language understanding",
            "Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Agrawal",
            "J. Lu",
            "S. Antol",
            "M. Mitchell",
            "C. L. Zitnick",
            "D. Batra",
            "D. Parikh"
        ],
        "dcterms:description": "VQA Abstract challenges models with abstract images and questions, providing a dataset for visual question answering.",
        "dcterms:title": "VQA Abstract",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Abstract images",
            "Challenging questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. Gurari",
            "Q. Li",
            "A. J. Stangl",
            "A. Guo",
            "C. Lin",
            "K. Grauman",
            "J. Luo",
            "J. P. Bigham"
        ],
        "dcterms:description": "VizWiz addresses real-world VQA for blind users, providing a dataset for answering visual questions.",
        "dcterms:title": "VizWiz",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Real-world VQA"
        ],
        "dcat:keyword": [
            "Blind users",
            "Visual questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering"
        ]
    }
]