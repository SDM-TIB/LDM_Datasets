[
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P. Xing",
            "Haotong Zhang",
            "Joseph Gonzalez"
        ],
        "dcterms:description": "MTBench is used to evaluate the alignment performance of Generative Language Models (GLMs) by analyzing preference noise in the data.",
        "dcterms:title": "MTBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Preference Learning"
        ],
        "dcat:keyword": [
            "Generative Language Models",
            "Preference Noise",
            "Alignment Performance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Alignment Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Nisan Stiennon",
            "Long Ouyang",
            "Jeffrey Wu",
            "Daniel Ziegler",
            "Ryan Lowe",
            "Chelsea Voss",
            "Alec Radford",
            "Dario Amodei",
            "Paul F Christiano"
        ],
        "dcterms:description": "TL;DR is utilized for summarization tasks, providing a dataset of document-summary pairs for training and evaluating models.",
        "dcterms:title": "TL;DR",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Summarization"
        ],
        "dcat:keyword": [
            "Summarization",
            "Human Feedback",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Yao Zhao",
            "Rishabh Joshi",
            "Tianqi Liu",
            "Misha Khalman",
            "Mohammad Saleh",
            "Peter J Liu"
        ],
        "dcterms:description": "CBArena is used for evaluating models based on sequence likelihood calibration with human feedback.",
        "dcterms:title": "CBArena",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Sequence Likelihood",
            "Calibration",
            "Human Feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "Tom Henighan"
        ],
        "dcterms:description": "AntHH is a dataset used for training models to be helpful and harmless through reinforcement learning from human feedback.",
        "dcterms:title": "AntHH",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reinforcement Learning",
            "Human Feedback",
            "Safety"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Guanming Yao",
            "Wei Zhu",
            "Yuan Ni",
            "Guotong Xie",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "dcterms:description": "MetaHS is a dataset aimed at boosting language models with high-quality feedback.",
        "dcterms:title": "MetaHS",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Feedback Learning"
        ],
        "dcat:keyword": [
            "High-Quality Feedback",
            "Language Models",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Feedback Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Louis Martin",
            "Kevin Stone",
            "Peter Albert",
            "Amjad Almahairi",
            "Yasmine Babaei",
            "Nikolay Bashlykov",
            "Soumya Batra",
            "Prajjwal Bhargava",
            "Shruti Bhosale"
        ],
        "dcterms:description": "WebGPT is a dataset used for training models in dialogue generation and web-based tasks.",
        "dcterms:title": "WebGPT",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Dialogue Generation"
        ],
        "dcat:keyword": [
            "Dialogue Generation",
            "Web-based Tasks",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Dialogue Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Harrison Lee",
            "Samrat Phatale",
            "Hassan Mansoor",
            "Kellie Lu",
            "Thomas Mesnard",
            "Colton Bishop",
            "Victor Carbune",
            "Abhinav Rastogi"
        ],
        "dcterms:description": "StanfordHumanPreference is used for scaling reinforcement learning from human feedback with AI feedback.",
        "dcterms:title": "StanfordHumanPreference",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reinforcement Learning",
            "Human Feedback",
            "AI Feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Saurav Kadavath",
            "Sandipan Kundu",
            "Amanda Askell",
            "Jackson Kernion",
            "Andy Jones",
            "Anna Chen",
            "Anna Goldie",
            "Azalia Mirhoseini",
            "Cameron McKinnon"
        ],
        "dcterms:description": "AnthropicHH is a dataset focused on ensuring harmlessness from AI feedback.",
        "dcterms:title": "AnthropicHH",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "NLP",
            "Safety"
        ],
        "dcat:keyword": [
            "Harmlessness",
            "AI Feedback",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Safety in AI"
        ]
    }
]