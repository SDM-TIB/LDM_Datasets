[
    {
        "dcterms:creator": [
            "M. Li",
            "Y. Zhao",
            "B. Yu",
            "Y. Sun",
            "H. Tian",
            "N. Zhang",
            "H. Wu"
        ],
        "dcterms:description": "API-Bank is a comprehensive benchmark designed for evaluating tool-augmented large language models (LLMs). It provides a structured dataset to assess the capabilities of LLMs in utilizing APIs effectively.",
        "dcterms:title": "API-Bank",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "API usage",
            "Large Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "API evaluation",
            "Tool usage assessment"
        ]
    },
    {
        "dcterms:creator": [
            "S. G. Patil",
            "T. Zhang",
            "X. Wang",
            "J. E. Gonzalez"
        ],
        "dcterms:description": "APIBench is a benchmark for evaluating the usage of APIs in language models. It focuses on assessing how well language models can understand and utilize APIs in various tasks.",
        "dcterms:title": "APIBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.15491",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "API evaluation",
            "Language models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "API usage evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "T. Xu",
            "C. Wu",
            "Y. Zhang"
        ],
        "dcterms:description": "ToolBench1 is a benchmark for evaluating tool usage in language models. It provides a structured dataset to assess how effectively language models can utilize various tools.",
        "dcterms:title": "ToolBench1",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.15491",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Tool usage",
            "Language models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Tool usage evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Tang",
            "H. Lin",
            "X. Han",
            "Q. Liang",
            "L. Sun"
        ],
        "dcterms:description": "ToolAlpaca is a dataset designed for generalized tool learning for language models, featuring 3000 simulated cases to enhance the tool usage capabilities of LLMs.",
        "dcterms:title": "ToolAlpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.05301",
        "dcat:theme": [
            "Tool Learning",
            "Simulation"
        ],
        "dcat:keyword": [
            "Tool learning",
            "Language models",
            "Simulation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Tool learning",
            "Simulation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Song",
            "W. Xiong",
            "D. Zhu",
            "C. Li",
            "K. Wang",
            "Y. Tian",
            "S. Li"
        ],
        "dcterms:description": "RestBench connects large language models with real-world applications via restful APIs, providing a benchmark for evaluating the integration of LLMs with RESTful services.",
        "dcterms:title": "RestBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.06624",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "RESTful APIs",
            "Language models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "API integration",
            "Tool usage evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Qin",
            "S. Hu",
            "Y. Lin",
            "N. Chen",
            "N. Ding",
            "G. Cui",
            "Z. Zeng",
            "Y. Huang",
            "C. Xiao",
            "C. Han"
        ],
        "dcterms:description": "ToolBench2 is a comprehensive benchmark for tool-augmented LLMs, providing a large dataset for evaluating the capabilities of LLMs in utilizing tools effectively.",
        "dcterms:title": "ToolBench2",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Tool usage",
            "Large Language Models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Tool usage evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "T. Huang",
            "D. Jung",
            "M. Chen"
        ],
        "dcterms:description": "MetaTool is a benchmark for large language models that focuses on deciding whether to use tools and which tools to use, enhancing the decision-making capabilities of LLMs.",
        "dcterms:title": "MetaTool",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Tool decision-making",
            "Language models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Tool decision-making"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Zhuang",
            "X. Chen",
            "T. Yu",
            "S. Mitra",
            "V. Bursztyn",
            "R. A. Rossi",
            "S. Sarkhel",
            "C. Zhang"
        ],
        "dcterms:description": "ToolQA is a dataset designed for question answering with external tools, enabling LLMs to answer questions that require tool usage.",
        "dcterms:title": "ToolQA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Question answering",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Ruan",
            "H. Chen",
            "Y. Zhang"
        ],
        "dcterms:description": "ToolEmu is a benchmark for evaluating tool usage in language models, providing a structured dataset to assess how well LLMs can utilize tools.",
        "dcterms:title": "ToolEmu",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.15491",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Tool usage",
            "Language models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Tool usage evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "N. Farn",
            "R. Shin"
        ],
        "dcterms:description": "ToolTalk evaluates tool usage in a conversational setting, assessing how well language models can utilize tools in dialogue.",
        "dcterms:title": "ToolTalk",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Conversational AI",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational tool usage"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Zhong",
            "M. Qi",
            "R. Wang",
            "Y. Qiu",
            "Y. Zhang",
            "H. Ma"
        ],
        "dcterms:description": "VIoT is a benchmark for evaluating tool usage in vision-centric tasks, assessing how well language models can utilize tools in visual contexts.",
        "dcterms:title": "VIoT",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Vision tasks",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Vision-centric tool usage"
        ]
    },
    {
        "dcterms:creator": [
            "J. Ye",
            "G. Li",
            "S. Gao",
            "C. Huang",
            "Y. Wu",
            "Q. Zhang",
            "T. Gui",
            "X. Huang"
        ],
        "dcterms:description": "RoTBench is a multi-level benchmark for evaluating the robustness of large language models in tool learning, focusing on their ability to handle various challenges.",
        "dcterms:title": "RoTBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2401.08326",
        "dcat:theme": [
            "Robustness",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Robustness",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Robustness evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Wang",
            "Y. Zhang",
            "Y. Liu",
            "Y. Zhang"
        ],
        "dcterms:description": "MLLM-Tool is a benchmark for evaluating tool usage capabilities of LLMs in multi-modal contexts, assessing their performance across different modalities.",
        "dcterms:title": "MLLM-Tool",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.11451",
        "dcat:theme": [
            "Multi-modal",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Multi-modal tasks",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-modal tool usage evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Ye",
            "G. Li",
            "S. Gao",
            "C. Huang",
            "Y. Wu",
            "Q. Zhang",
            "T. Gui"
        ],
        "dcterms:description": "ToolSword unveils safety issues of large language models in tool learning across three stages, providing insights into the risks associated with tool usage.",
        "dcterms:title": "ToolSword",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.10753",
        "dcat:theme": [
            "Safety",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Safety issues",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Ye",
            "Y. Wu",
            "S. Gao",
            "X. Li",
            "G. Li",
            "Q. Zhang",
            "T. Gui"
        ],
        "dcterms:description": "StableToolBench aims for stable large-scale benchmarking on tool learning of large language models, providing a comprehensive evaluation framework.",
        "dcterms:title": "StableToolBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2403.07714",
        "dcat:theme": [
            "Benchmarking",
            "Tool Learning"
        ],
        "dcat:keyword": [
            "Stable benchmarking",
            "Tool usage",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Benchmarking evaluation"
        ]
    }
]