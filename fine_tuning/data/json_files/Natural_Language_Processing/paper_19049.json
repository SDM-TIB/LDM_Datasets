[
    {
        "dcterms:creator": [
            "X. Chen",
            "H. Fang",
            "T.-Y. Lin",
            "R. Vedantam",
            "S. Gupta",
            "P. Doll√°r",
            "C. L. Zitnick"
        ],
        "dcterms:description": "A dataset for image captioning that includes a large number of images with multiple captions, enabling the evaluation of image understanding and caption generation.",
        "dcterms:title": "Microsoft COCO Captions",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1504.00325",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Captioning",
            "Visual understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Goyal",
            "T. Khot",
            "D. Summers-Stay",
            "D. Batra",
            "D. Parikh"
        ],
        "dcterms:description": "A dataset designed for visual question answering that emphasizes the importance of image understanding in answering questions about images.",
        "dcterms:title": "VQAv2",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Question answering",
            "Visual understanding",
            "Image dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "D. A. Hudson",
            "C. D. Manning"
        ],
        "dcterms:description": "A dataset for visual reasoning and compositional question answering, focusing on real-world scenarios.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Reasoning",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Visual reasoning",
            "Compositional questions",
            "Image dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Visual Reasoning",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Suhr",
            "S. Zhou",
            "A. Zhang",
            "I. Zhang",
            "H. Bai",
            "Y. Artzi"
        ],
        "dcterms:description": "A corpus for reasoning about natural language grounded in photographs, facilitating the study of language understanding in visual contexts.",
        "dcterms:title": "NVLR2",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Natural language",
            "Visual grounding",
            "Image dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text/Image",
        "mls:task": [
            "Visual Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "J. Lei",
            "L. Yu",
            "M. Bansal",
            "T. Berg"
        ],
        "dcterms:description": "A dataset for localized, compositional video question answering, focusing on understanding video content in response to questions.",
        "dcterms:title": "TVQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Question answering",
            "Temporal reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Lei",
            "L. Yu",
            "T. L. Berg",
            "M. Bansal"
        ],
        "dcterms:description": "A large-scale dataset for video-subtitle moment retrieval, focusing on aligning video content with corresponding subtitles.",
        "dcterms:title": "How2QA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Retrieval",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Subtitle alignment",
            "Moment retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video/Text",
        "mls:task": [
            "Video Retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "L. Li",
            "J. Lei",
            "Z. Gan",
            "L. Yu",
            "Y.-C. Chen",
            "R. Pillai",
            "Y. Cheng",
            "L. Zhou",
            "X. E. Wang",
            "W. Y. Wang"
        ],
        "dcterms:description": "A multi-task benchmark for evaluating video-and-language understanding, encompassing various tasks and modalities.",
        "dcterms:title": "VALUE",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video and Language Understanding",
            "Multimodal Learning"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video/Text",
        "mls:task": [
            "Video and Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "R. Taori",
            "I. Gulrajani",
            "T. Zhang",
            "Y. Dubois",
            "X. Li",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "An instruction-following model based on the LLaMA architecture, designed for various tasks.",
        "dcterms:title": "Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Instruction Following",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Language model",
            "Fine-tuning"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "R. Krishna",
            "Y. Zhu",
            "O. Groth",
            "J. Johnson",
            "K. Hata",
            "J. Kravitz",
            "S. Chen",
            "Y. Kalantidis",
            "L.-J. Li",
            "D. A. Shamma"
        ],
        "dcterms:description": "A dataset that connects language and vision through dense image annotations, facilitating various vision-language tasks.",
        "dcterms:title": "Visual Genome",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Annotation",
            "Vision-Language Understanding"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Dense annotations",
            "Vision-language tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Vision-Language Understanding"
        ]
    }
]