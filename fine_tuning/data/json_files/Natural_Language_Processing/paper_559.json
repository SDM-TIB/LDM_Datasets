[
    {
        "dcterms:creator": [
            "A. Conneau",
            "G. Lample",
            "M. Ranzato",
            "L. Denoyer",
            "H. JÃ©gou"
        ],
        "dcterms:description": "The MUSE dataset is used for evaluating bilingual lexicon induction methods, providing a standard benchmark for cross-lingual word embeddings.",
        "dcterms:title": "MUSE dataset",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Bilingual Lexicon Induction"
        ],
        "dcat:keyword": [
            "Bilingual lexicon",
            "Cross-lingual embeddings",
            "Evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Bilingual Lexicon Induction"
        ]
    },
    {
        "dcterms:creator": [
            "P. Bojanowski",
            "E. Grave",
            "A. Joulin",
            "T. Mikolov"
        ],
        "dcterms:description": "fastText embeddings are used for training word embeddings that incorporate subword information, enhancing the quality of the embeddings for various languages.",
        "dcterms:title": "fastText embeddings",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Subword information",
            "FastText"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Word Embedding Training"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Wikipedia is used as a source for extracting monolingual corpora to train word embeddings.",
        "dcterms:title": "Wikipedia",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Monolingual corpus",
            "Text data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "WikiExtractor is a tool used for extracting plain text from Wikipedia dumps to create a clean monolingual corpus.",
        "dcterms:title": "WikiExtractor",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Text extraction",
            "Wikipedia dumps"
        ],
        "dcat:landingPage": "https://github.com/attardi/wikiextractor",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]