[
    {
        "dcterms:creator": [
            "Bohao Li",
            "Yuying Ge",
            "Yixiao Ge",
            "Guangzhi Wang",
            "Rui Wang",
            "Ruimao Zhang",
            "Ying Shan"
        ],
        "dcterms:description": "A benchmark specifically designed for evaluating text-rich visual comprehension of Multimodal Large Language Models (MLLMs), comprising 2.3K multiple-choice questions with precise human annotations across three broad categories: Charts, Maps, and Webs.",
        "dcterms:title": "SEED-Bench-2-Plus",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/AILab-CVC/SEED-Bench",
        "dcat:theme": [
            "Multimodal Learning",
            "Visual Comprehension"
        ],
        "dcat:keyword": [
            "MLLM",
            "text-rich visual data",
            "benchmark",
            "multiple-choice questions"
        ],
        "dcat:landingPage": "https://github.com/AILab-CVC/SEED-Bench",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Comprehension",
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Haotian Liu",
            "Chunyuan Li",
            "Qingyang Wu",
            "Yong Jae Lee"
        ],
        "dcterms:description": "A benchmark for evaluating visual instruction tuning, focusing on the performance of models in understanding visual content and instructions.",
        "dcterms:title": "LLaVA-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Instruction Tuning"
        ],
        "dcat:keyword": [
            "visual instruction",
            "benchmark",
            "evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Understanding",
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Chaoyou Fu",
            "Peixian Chen",
            "Yunhang Shen",
            "Yulei Qin",
            "Mengdan Zhang",
            "Xu Lin",
            "Zhenyu Qiu",
            "Wei Lin",
            "Jinrui Yang",
            "Xiawu Zheng",
            "Ke Li",
            "Xing Sun",
            "Rongrong Ji"
        ],
        "dcterms:description": "A comprehensive evaluation benchmark for multimodal large language models, focusing on various evaluation dimensions.",
        "dcterms:title": "MME",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Evaluation"
        ],
        "dcat:keyword": [
            "evaluation benchmark",
            "multimodal models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Wenxuan Zhang",
            "Sharifah Mahani Aljunied",
            "Chang Gao",
            "Yew Ken Chia",
            "Lidong Bing"
        ],
        "dcterms:description": "A multilingual, multimodal, multilevel benchmark for examining large language models, designed to assess their capabilities across different languages and modalities.",
        "dcterms:title": "M3Exam",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "multilingual",
            "multimodal",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Examination"
        ]
    },
    {
        "dcterms:creator": [
            "Peng Xu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Peng Gao",
            "Shuo Liu",
            "Meng Lei",
            "Fanqing Meng",
            "Siyuan Huang",
            "Yu Qiao",
            "Ping Luo"
        ],
        "dcterms:description": "A comprehensive evaluation benchmark for large vision-language models, focusing on their performance across various tasks.",
        "dcterms:title": "LVLM-eHub",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Models"
        ],
        "dcat:keyword": [
            "evaluation benchmark",
            "vision-language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Shuai Bai",
            "Shusheng Yang",
            "Jinze Bai",
            "Peng Wang",
            "Xingxuan Zhang",
            "Junyang Lin",
            "Xinggang Wang",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "dcterms:description": "A benchmark for evaluating vision-language models by language models, focusing on their performance in various tasks.",
        "dcterms:title": "Touchstone",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Vision-Language Evaluation"
        ],
        "dcat:keyword": [
            "evaluation",
            "vision-language",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Pan Lu",
            "Hritik Bansal",
            "Tony Xia",
            "Jiacheng Liu",
            "Chunyuan Li",
            "Hannaneh Hajishirzi",
            "Hao Cheng",
            "Kai-Wei Chang",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "dcterms:description": "A benchmark for evaluating mathematical reasoning of foundation models in visual contexts.",
        "dcterms:title": "MathVista",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematical Reasoning"
        ],
        "dcat:keyword": [
            "mathematical reasoning",
            "visual contexts",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Xiang Yue",
            "Yuansheng Ni",
            "Kai Zhang",
            "Tianyu Zheng",
            "Ruoqi Liu",
            "Ge Zhang",
            "Samuel Stevens",
            "Dongfu Jiang",
            "Weiming Ren",
            "Yuxuan Sun"
        ],
        "dcterms:description": "A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.",
        "dcterms:title": "MMMU",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Understanding"
        ],
        "dcat:keyword": [
            "multimodal",
            "understanding",
            "reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Understanding",
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Zhenfei Yin",
            "Jiong Wang",
            "Jianjian Cao",
            "Zhelun Shi",
            "Dingning Liu",
            "Mukai Li",
            "Lu Sheng",
            "Lei Bai",
            "Xiaoshui Huang",
            "Zhiyong Wang"
        ],
        "dcterms:description": "A language-assisted multimodal instruction-tuning dataset, framework, and benchmark.",
        "dcterms:title": "LAMM",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "instruction tuning",
            "multimodal",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Instruction Tuning"
        ]
    }
]