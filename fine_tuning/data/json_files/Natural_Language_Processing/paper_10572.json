[
    {
        "dcterms:creator": [
            "Seungone Kim",
            "Se June Joo",
            "Doyoung Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Jamin Shin",
            "Minjoon Seo"
        ],
        "dcterms:description": "The COT COLLECTION is an instruction-tuning dataset that includes 1.84 million rationales augmented across 1,060 tasks, aimed at improving the reasoning capabilities of smaller language models through CoT fine-tuning.",
        "dcterms:title": "COT COLLECTION",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Chain-of-Thought",
            "Rationales",
            "Instruction Tuning",
            "Language Models"
        ],
        "dcat:landingPage": "https://github.com/kaistAI/CoT-Collection",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Zero-shot Learning",
            "Few-shot Learning"
        ]
    },
    {
        "dcterms:creator": [
            "S. Longpre",
            "A. Webson",
            "C. Raffel",
            "S. H. Bach",
            "L. Sutawika",
            "Z. Alyafeai",
            "A. Chaffin",
            "A. Stiegler",
            "T. Le Scao",
            "A. Raja"
        ],
        "dcterms:description": "The Flan Collection is a diverse set of NLP tasks designed for instruction tuning, which includes 1,836 tasks and is used as a source for augmenting rationales in the COT COLLECTION.",
        "dcterms:title": "Flan Collection",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2301.13688",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction Tuning",
            "NLP Tasks",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Schärli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. V. Le",
            "E. H. Chi",
            "D. Zhou"
        ],
        "dcterms:description": "BIG-Bench-Hard (BBH) is a benchmark that includes challenging tasks to evaluate the reasoning capabilities of language models, particularly in the context of chain-of-thought prompting.",
        "dcterms:title": "BIG-Bench-Hard (BBH)",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2210.09261",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Reasoning Tasks",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Zero-shot Learning"
        ]
    },
    {
        "dcterms:creator": [
            "V. Sanh",
            "A. Webson",
            "C. Raffel",
            "S. H. Bach",
            "L. Sutawika",
            "Z. Alyafeai",
            "A. Chaffin",
            "A. Stiegler",
            "T. Le Scao",
            "A. Raja"
        ],
        "dcterms:description": "The P3 Evaluation benchmark is a collection of datasets used to evaluate the performance of language models on various tasks, particularly in the context of instruction tuning.",
        "dcterms:title": "P3 Evaluation benchmark",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2110.08207",
        "dcat:theme": [
            "Natural Language Processing",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Evaluation Benchmark",
            "Language Models",
            "Instruction Tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "F. Shi",
            "M. Suzgun",
            "M. Freitag",
            "X. Wang",
            "S. Srivats",
            "S. Vosoughi",
            "H. W. Chung",
            "Y. Tay",
            "S. Ruder",
            "D. Zhou"
        ],
        "dcterms:description": "The MGSM benchmark is a multilingual reasoning benchmark that evaluates the performance of language models across different languages, focusing on their reasoning capabilities.",
        "dcterms:title": "MGSM benchmark",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2210.03057",
        "dcat:theme": [
            "Natural Language Processing",
            "Multilingual Evaluation"
        ],
        "dcat:keyword": [
            "Multilingual",
            "Reasoning",
            "Language Models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Tuggener",
            "P. Von Däniken",
            "T. Peetz",
            "M. Cieliebak"
        ],
        "dcterms:description": "LEDGAR is a large-scale multi-label corpus for text classification of legal provisions in contracts, designed to assist in legal text analysis.",
        "dcterms:title": "LEDGAR",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Legal Text Analysis",
            "Text Classification"
        ],
        "dcat:keyword": [
            "Legal Dataset",
            "Text Classification",
            "Multi-label"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "L. Zheng",
            "Y. Liu",
            "Y. Zhang"
        ],
        "dcterms:description": "Case Hold is a dataset for legal case classification, providing a structured approach to categorize legal cases based on their characteristics.",
        "dcterms:title": "Case Hold",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Legal Text Analysis",
            "Case Classification"
        ],
        "dcat:keyword": [
            "Legal Dataset",
            "Case Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Romanov",
            "C. Shivade"
        ],
        "dcterms:description": "MedNLI is a dataset focused on natural language inference in the clinical domain, designed to evaluate models on medical reasoning tasks.",
        "dcterms:title": "MedNLI",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Medical Text Analysis",
            "Natural Language Inference"
        ],
        "dcat:keyword": [
            "Medical Dataset",
            "Natural Language Inference"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Q. Jin",
            "B. Dhingra",
            "Z. Liu",
            "W. Cohen",
            "X. Lu"
        ],
        "dcterms:description": "PubMedQA is a dataset for biomedical research question answering, providing a structured approach to evaluate models on medical question answering tasks.",
        "dcterms:title": "PubMedQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Biomedical Text Analysis",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Biomedical Dataset",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]