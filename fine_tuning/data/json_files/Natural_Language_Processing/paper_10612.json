[
    {
        "dcterms:creator": [
            "Y. Yu",
            "Z. Zhang",
            "Y. Liu",
            "J. Zhang"
        ],
        "dcterms:description": "A reading comprehension dataset requiring logical reasoning, designed to evaluate the logical reasoning capabilities of models.",
        "dcterms:title": "ReClor",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Logical Reasoning"
        ],
        "dcat:keyword": [
            "Logical reasoning",
            "Reading comprehension",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Logical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "H. Liu",
            "J. Liu",
            "L. Cui",
            "N. Duan",
            "M. Zhou",
            "Y. Zhang"
        ],
        "dcterms:description": "A challenge dataset for machine reading comprehension that focuses on logical reasoning tasks.",
        "dcterms:title": "LogiQA-v2",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Logical Reasoning"
        ],
        "dcat:keyword": [
            "Logical reasoning",
            "Machine reading comprehension",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Logical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "G. Lai",
            "Q. Xie",
            "H. Liu",
            "Y. Yang",
            "E. H. Hovy"
        ],
        "dcterms:description": "A large-scale reading comprehension dataset derived from examinations, designed to assess general reasoning abilities.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "General Reasoning"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Examinations",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A dataset designed to measure massive multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language understanding",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Sch√§rli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. V. Le",
            "E. H. Chi",
            "D. Zhou",
            "J. Wei"
        ],
        "dcterms:description": "A collection of challenging tasks designed to evaluate the capabilities of large language models, particularly in reasoning.",
        "dcterms:title": "BIG-Bench-Hard (BBH)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Benchmarking",
            "Reasoning"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Challenging tasks",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning"
        ]
    }
]