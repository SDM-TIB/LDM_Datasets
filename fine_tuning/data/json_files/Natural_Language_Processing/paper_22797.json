[
    {
        "dcterms:creator": [
            "Siva Reddy",
            "Danqi Chen",
            "Christopher D. Manning"
        ],
        "dcterms:description": "A dataset designed for conversational question answering, containing questions and answers in a dialogue format.",
        "dcterms:title": "CoQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1162/tacl_a_00266",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Conversational AI",
            "Dialogue",
            "Question Answering"
        ],
        "dcat:landingPage": "https://doi.org/10.1162/tacl_a_00266",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "A large-scale dataset for reading comprehension, featuring questions and answers derived from trivia.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18653/v1/P17-1147",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Trivia",
            "Question Answering",
            "Reading Comprehension"
        ],
        "dcat:landingPage": "https://doi.org/10.18653/v1/P17-1147",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Johannes Welbl",
            "Nelson F. Liu",
            "Matt Gardner"
        ],
        "dcterms:description": "A dataset for multiple choice science questions, designed to assess understanding of scientific concepts.",
        "dcterms:title": "SciQ",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1707.06209",
        "dcat:theme": [
            "Science Education",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Science Questions",
            "Multiple Choice",
            "Education"
        ],
        "dcat:landingPage": "arXiv:1707.06209",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Matthew Kelcey",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina N. Toutanova",
            "Llion Jones",
            "Ming-Wei Chang",
            "Andrew Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "A benchmark dataset for question answering research, containing real questions and answers from Google search.",
        "dcterms:title": "Natural Questions (NQ)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Natural Questions",
            "Question Answering",
            "Research Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    }
]