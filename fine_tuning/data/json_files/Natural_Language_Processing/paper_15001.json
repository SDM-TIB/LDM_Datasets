[
    {
        "dcterms:creator": [
            "Steven Morad",
            "Linxi Fan",
            "Yuke Zhu"
        ],
        "dcterms:description": "POPGym is a benchmark suite designed for evaluating long-term memory and generalization in reinforcement learning across various partially observable environments.",
        "dcterms:title": "POPGym",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://openreview.net/forum?id=chDrutUTs0K",
        "dcat:theme": [
            "Reinforcement Learning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Long-term memory",
            "Generalization",
            "Partially observable environments"
        ],
        "dcat:landingPage": "https://openreview.net/forum?id=chDrutUTs0K",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation of RL agents"
        ]
    },
    {
        "dcterms:creator": [
            "Danijar Hafner"
        ],
        "dcterms:description": "Crafter is a simplified version of Minecraft designed for evaluating multi-task capabilities in procedurally generated environments.",
        "dcterms:title": "Crafter",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2109.06780",
        "dcat:theme": [
            "Reinforcement Learning",
            "Multi-task Learning"
        ],
        "dcat:keyword": [
            "Procedurally generated environments",
            "Multi-task capabilities",
            "Minecraft"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2109.06780",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multi-task learning",
            "Exploration"
        ]
    },
    {
        "dcterms:creator": [
            "Tianhe Yu",
            "Yuke Zhu",
            "Tianwei Ni"
        ],
        "dcterms:description": "Meta-World is a benchmark for evaluating multi-task and meta reinforcement learning in robotic environments.",
        "dcterms:title": "Meta-World",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Meta Learning"
        ],
        "dcat:keyword": [
            "Robotics",
            "Multi-task learning",
            "Meta reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Meta-learning",
            "Task generalization"
        ]
    },
    {
        "dcterms:creator": [
            "Michael Laskin",
            "Danijar Hafner"
        ],
        "dcterms:description": "Dark Key-To-Door is a sparse-reward multi-episodic meta-reinforcement learning problem designed to test adaptation and exploration capabilities.",
        "dcterms:title": "Dark Key-To-Door",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.14215",
        "dcat:theme": [
            "Reinforcement Learning",
            "Meta Learning"
        ],
        "dcat:keyword": [
            "Sparse rewards",
            "Meta-learning",
            "Exploration"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2210.14215",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Meta-learning",
            "Exploration"
        ]
    },
    {
        "dcterms:creator": [
            "Tianwei Ni",
            "Benjamin Eysenbach",
            "Ruslan Salakhutdinov"
        ],
        "dcterms:description": "Passive T-Maze is a benchmark designed to isolate memory capabilities in reinforcement learning agents.",
        "dcterms:title": "Passive T-Maze",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.03864",
        "dcat:theme": [
            "Reinforcement Learning",
            "Memory Evaluation"
        ],
        "dcat:keyword": [
            "Memory",
            "Recall",
            "Benchmark"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2307.03864",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Memory evaluation",
            "Recall tasks"
        ]
    },
    {
        "dcterms:creator": [
            "Tianwei Ni",
            "Benjamin Eysenbach",
            "Ruslan Salakhutdinov"
        ],
        "dcterms:description": "Wind Environment is a continuous action environment used to evaluate reinforcement learning agents in a sparse-reward setting.",
        "dcterms:title": "Wind Environment",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2110.05038",
        "dcat:theme": [
            "Reinforcement Learning",
            "Continuous Control"
        ],
        "dcat:keyword": [
            "Sparse rewards",
            "Continuous actions",
            "Benchmark"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2110.05038",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Continuous control",
            "Sparse reward tasks"
        ]
    }
]