[
    {
        "dcterms:creator": [
            "M. Saif",
            "F. Bravo-Marquez",
            "M. Salameh",
            "S. Kiritchenko"
        ],
        "dcterms:description": "A dataset for valence ordinal classification based on tweets, used to evaluate sentiment analysis and bias in language models.",
        "dcterms:title": "SemEval-2018 Task 1-Valence Ordinal Classification",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sentiment Analysis",
            "Bias Evaluation"
        ],
        "dcat:keyword": [
            "Tweets",
            "Sentiment",
            "Valence",
            "Ordinal Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "R. Socher",
            "A. Perelygin",
            "J. Wu",
            "J. Chuang",
            "C. Manning",
            "A. Ng",
            "C. Potts"
        ],
        "dcterms:description": "A sentiment analysis dataset consisting of movie reviews, providing five-way sentiment labels for evaluation.",
        "dcterms:title": "Stanford Sentiment Treebank Five-way (SST-5)",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Sentiment Analysis",
            "Bias Evaluation"
        ],
        "dcat:keyword": [
            "Movie Reviews",
            "Sentiment",
            "Five-way Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Sentiment Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "A. Parrish",
            "A. Chen",
            "N. Nangia",
            "V. Padmakumar",
            "J. Phang",
            "J. Thompson",
            "P. Htut",
            "S. Bowman"
        ],
        "dcterms:description": "A benchmark designed to evaluate biases in question answering systems, focusing on various sensitive attributes.",
        "dcterms:title": "Bias Benchmark for QA evaluation task",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Evaluation",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Bias",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Bias Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Czarnowska",
            "Y. Vyas",
            "K. Shah"
        ],
        "dcterms:description": "Templates designed for evaluating social biases in NLP, providing a framework for empirical comparison of fairness metrics.",
        "dcterms:title": "Templates designed by Czarnowska et al.",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Evaluation",
            "Fairness Metrics"
        ],
        "dcat:keyword": [
            "Templates",
            "Social Biases",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Bias Evaluation"
        ]
    }
]