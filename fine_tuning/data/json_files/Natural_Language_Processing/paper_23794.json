[
    {
        "dcterms:creator": [
            "Nouha Dziri",
            "Ehsan Kamalloo",
            "Sivan Milton",
            "Osmar Zaiane",
            "Mo Yu",
            "Edoardo M Ponti",
            "Siva Reddy"
        ],
        "dcterms:description": "HalluDial is a comprehensive large-scale benchmark for automatic dialogue-level hallucination evaluation, encompassing both spontaneous and induced hallucination scenarios, covering factuality and faithfulness hallucinations. The dataset includes 4,094 dialogues with a total of 146,856 samples, providing detailed results for hallucination detection, localization, and rationale provision.",
        "dcterms:title": "HalluDial",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "https://github.com/FlagOpen/HalluDial",
        "dcat:theme": [
            "Natural Language Processing",
            "Dialogue Systems",
            "Hallucination Evaluation"
        ],
        "dcat:keyword": [
            "Dialogue-level hallucination",
            "Large Language Models",
            "Evaluation benchmark",
            "Factuality hallucination",
            "Faithfulness hallucination"
        ],
        "dcat:landingPage": "https://github.com/FlagOpen/HalluDial",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Hallucination detection",
            "Hallucination localization",
            "Rationale provision"
        ]
    },
    {
        "dcterms:creator": [
            "Nouha Dziri",
            "Ehsan Kamalloo",
            "Sivan Milton",
            "Osmar Zaiane",
            "Mo Yu",
            "Edoardo M Ponti",
            "Siva Reddy"
        ],
        "dcterms:description": "FaithDial is a benchmark for information-seeking dialogue, providing a faithful evaluation of dialogue systems. It serves as a foundational dataset for generating responses in the HalluDial dataset.",
        "dcterms:title": "FaithDial",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Dialogue Systems"
        ],
        "dcat:keyword": [
            "Information-seeking dialogue",
            "Dialogue evaluation",
            "Benchmark dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "dcterms:description": "HaluEval is a large-scale hallucination evaluation benchmark for large language models, focusing on hallucination detection across various tasks including dialogue, question answering, and summarization.",
        "dcterms:title": "HaluEval",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Hallucination detection",
            "Large language models",
            "Evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]