[
    {
        "dcterms:creator": [
            "Zhiyuan Zeng",
            "Jiatong Yu",
            "Tianyu Gao",
            "Yu Meng",
            "Tanya Goyal",
            "Danqi Chen"
        ],
        "dcterms:description": "LLMBar is a meta-evaluation dataset designed to assess the ability of large language models to judge text based on adherence to instructions, featuring pairs of model outputs where one follows the instructions correctly and the other is adversarially designed to appear favorable but fails to do so.",
        "dcterms:title": "LLMBar",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Meta-evaluation",
            "Instruction following",
            "Adversarial examples"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation of language models"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric Xing"
        ],
        "dcterms:description": "MT-Bench is a meta-evaluation dataset that consists of 80 multi-turn instructions and generated outputs from various models, used to assess the performance of language models in pairwise evaluations.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Meta-evaluation",
            "Pairwise comparison",
            "Language model assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Evaluation of language models"
        ]
    }
]