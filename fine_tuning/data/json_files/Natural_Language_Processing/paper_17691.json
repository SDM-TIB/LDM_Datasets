[
    {
        "dcterms:creator": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "dcterms:description": "A dataset used for instruction tuning, containing ranked responses generated by the GPT-4 model.",
        "dcterms:title": "GPT-4-LLM",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.03277",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction tuning",
            "GPT-4",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Instruction following",
            "Response ranking"
        ]
    },
    {
        "dcterms:creator": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard H. Hovy"
        ],
        "dcterms:description": "A large-scale reading comprehension dataset derived from examinations, used for evaluating language models.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1704.04683",
        "dcat:theme": [
            "Reading Comprehension",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Examinations",
            "Language model evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multitask Learning",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language understanding",
            "Multitask evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "Amin Saied Sanosi Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "dcterms:description": "A human-centric benchmark for evaluating foundation models, focusing on various tasks and metrics.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.06364",
        "dcat:theme": [
            "Model Evaluation",
            "Human-Centric AI"
        ],
        "dcat:keyword": [
            "Model evaluation",
            "Human-centric",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model evaluation",
            "Benchmarking"
        ]
    },
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adri√† Garriga-Alonso"
        ],
        "dcterms:description": "A dataset that quantifies and extrapolates the capabilities of language models beyond imitation.",
        "dcterms:title": "BBH",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2206.04615",
        "dcat:theme": [
            "Language Model Evaluation",
            "Capability Assessment"
        ],
        "dcat:keyword": [
            "Language model",
            "Capabilities",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Capability assessment",
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset designed to evaluate the ability of machines to complete sentences in a coherent manner.",
        "dcterms:title": "HellaSWAG",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Understanding",
            "Sentence Completion"
        ],
        "dcat:keyword": [
            "Sentence completion",
            "Natural language understanding",
            "Evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sentence completion",
            "Natural language understanding"
        ]
    }
]