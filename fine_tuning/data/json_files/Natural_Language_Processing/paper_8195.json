[
    {
        "dcterms:creator": [
            "J. Wei",
            "Y. Tay",
            "B. Rishi",
            "C. Raffel",
            "B. Zoph",
            "S. Borgeaud",
            "D. Yogatama",
            "M. Bosma",
            "D. Zhou",
            "D. Metzler"
        ],
        "dcterms:description": "A benchmark for evaluating emergent abilities of large language models across various tasks.",
        "dcterms:title": "BIG-Bench",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Emergent abilities",
            "Language models",
            "Benchmark tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Cobbe",
            "V. Kosaraju",
            "M. Bavarian",
            "J. Hilton",
            "R. Nakano",
            "C. Hesse",
            "J. Schulman"
        ],
        "dcterms:description": "A dataset for training verifiers to solve math word problems.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2110.14168",
        "dcat:theme": [
            "Mathematics",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Math word problems",
            "Verification",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Math problem solving"
        ]
    },
    {
        "dcterms:creator": [
            "F. Shi",
            "M. Suzgun",
            "M. Freitag",
            "X. Wang",
            "S. Srivats",
            "S. Vosoughi",
            "H. W. Chung",
            "Y. Tay",
            "S. Ruder",
            "D. Zhou",
            "Q. V. Le"
        ],
        "dcterms:description": "A multilingual dataset that evaluates language models' reasoning capabilities.",
        "dcterms:title": "MGSM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.03057",
        "dcat:theme": [
            "Multilingual Processing",
            "Reasoning"
        ],
        "dcat:keyword": [
            "Multilingual",
            "Reasoning",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multilingual reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "C. Clark",
            "E. Choi",
            "M. Collins",
            "D. Garrette",
            "T. Kwiatkowski",
            "V. Nikolaev",
            "J. Palomaki"
        ],
        "dcterms:description": "A benchmark for information-seeking question answering in diverse languages.",
        "dcterms:title": "TydiQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Diversity"
        ],
        "dcat:keyword": [
            "Information-seeking",
            "Question answering",
            "Diverse languages"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Wang",
            "Y. Pruksachatkun",
            "N. Nangia",
            "A. Singh",
            "J. Michael",
            "F. Hill",
            "O. Levy",
            "S. R. Bowman"
        ],
        "dcterms:description": "A benchmark for evaluating general-purpose language understanding systems.",
        "dcterms:title": "SuperGLUE",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1905.00537",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Language understanding",
            "Benchmark",
            "NLP tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. S. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "A large-scale dataset for reading comprehension with distant supervision.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1705.03551",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Distant supervision",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "T. Kwiatkowski",
            "J. Palomaki",
            "O. Redfield",
            "M. Collins",
            "A. Parikh",
            "C. Alberti",
            "K. Toutanova"
        ],
        "dcterms:description": "A benchmark for question answering research.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Research Benchmark"
        ],
        "dcat:keyword": [
            "Question answering",
            "Benchmark",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "A dataset for evaluating the ability of machines to complete sentences.",
        "dcterms:title": "HellaSWAG",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1905.07830",
        "dcat:theme": [
            "Natural Language Processing",
            "Sentence Completion"
        ],
        "dcat:keyword": [
            "Sentence completion",
            "Dataset",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Sakaguchi",
            "R. Le Bras",
            "C. Bhagavatula",
            "Y. Choi"
        ],
        "dcterms:description": "An adversarial challenge dataset for evaluating commonsense reasoning.",
        "dcterms:title": "Winogrande",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1907.10641",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Adversarial dataset",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring massive multitask language understanding.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Multitask Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Benchmark",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]