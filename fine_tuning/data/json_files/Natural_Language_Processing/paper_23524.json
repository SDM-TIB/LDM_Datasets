[
    {
        "dcterms:creator": [
            "Zafar Rafii",
            "Yusong Wu"
        ],
        "dcterms:description": "The MUSDB18 dataset is used for music separation tasks, providing a collection of audio stems for various songs.",
        "dcterms:title": "MUSDB18",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "10.5281/zenodo.1117372",
        "dcat:theme": [
            "Music Separation",
            "Audio Processing"
        ],
        "dcat:keyword": [
            "Audio stems",
            "Music separation",
            "Dataset"
        ],
        "dcat:landingPage": "https://doi.org/10.5281/zenodo.1117372",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Music Mixing",
            "Audio Encoding"
        ]
    },
    {
        "dcterms:creator": [
            "Shawn Hershey"
        ],
        "dcterms:description": "VGGish is a model for large-scale audio classification, used to encode audio data into a latent space.",
        "dcterms:title": "VGGish",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Classification"
        ],
        "dcat:keyword": [
            "Audio classification",
            "Neural network",
            "Feature extraction"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1609.09430",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio Encoding"
        ]
    },
    {
        "dcterms:creator": [
            "Yusong Wu"
        ],
        "dcterms:description": "CLAP is a model for contrastive language-audio pretraining, used to enhance audio representation learning.",
        "dcterms:title": "CLAP",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Representation Learning"
        ],
        "dcat:keyword": [
            "Contrastive learning",
            "Audio representation",
            "Language-audio models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio Encoding"
        ]
    },
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Quoc V Le",
            "Ilya Sutskever"
        ],
        "dcterms:description": "Word2Vec is a model that captures semantic relationships among words, used in natural language processing.",
        "dcterms:title": "Word2Vec",
        "dcterms:issued": "2013",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Semantic relationships",
            "Machine translation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine Translation"
        ]
    }
]