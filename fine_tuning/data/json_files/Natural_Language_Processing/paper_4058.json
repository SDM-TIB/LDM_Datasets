[
    {
        "dcterms:creator": [
            "Hai Hu",
            "Kyle Richardson",
            "Liang Xu",
            "Lu Li",
            "Sandra KÃ¼bler",
            "Lawrence Moss"
        ],
        "dcterms:description": "OCNLI is the first large-scale, human-annotated Chinese NLI dataset, collected from scratch following the MNLI procedure, with 50k training examples.",
        "dcterms:title": "OCNLI (Original Chinese NLI)",
        "dcterms:issued": "2020",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Inference",
            "Chinese Language Processing"
        ],
        "dcat:keyword": [
            "NLI",
            "Chinese",
            "Dataset",
            "Natural Language Processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "R Thomas McCoy",
            "Ellie Pavlick",
            "Tal Linzen"
        ],
        "dcterms:description": "HANS is designed to diagnose syntactic heuristics in natural language inference, focusing on lexical overlap and subsequence heuristics.",
        "dcterms:title": "HANS",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Inference",
            "Diagnostic Testing"
        ],
        "dcat:keyword": [
            "NLI",
            "Heuristics",
            "Syntactic Bias",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "Aakanksha Naik",
            "Abhilasha Ravichander",
            "Norman Sadeh",
            "Carolyn Rose",
            "Graham Neubig"
        ],
        "dcterms:description": "NLI stress-tests are designed to evaluate the robustness of NLI models against various linguistic phenomena.",
        "dcterms:title": "NLI stress-tests",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Inference",
            "Robustness Testing"
        ],
        "dcat:keyword": [
            "NLI",
            "Stress Tests",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "Liang Xu",
            "Hai Hu",
            "Xuanwei Zhang",
            "Lu Li",
            "Chenjie Cao",
            "Yudong Li",
            "Yechen Xu",
            "Kai Sun",
            "Dian Yu",
            "Cong Yu",
            "Yin Tian",
            "Qianqian Dong",
            "Weitang Liu",
            "Bo Shi",
            "Yiming Cui",
            "Junyi Li",
            "Jun Zeng",
            "Rongzhao Wang",
            "Weijian Xie",
            "Yanting Li",
            "Yina Patterson",
            "Zuoyu Tian",
            "Yiwen Zhang",
            "He Zhou",
            "Shaoweihua Liu",
            "Zhe Zhao",
            "Qipeng Zhao",
            "Cong Yue",
            "Xinrui Zhang",
            "Zhengliang Yang",
            "Kyle Richardson",
            "Zhenzhong Lan"
        ],
        "dcterms:description": "CLUE is a Chinese language understanding evaluation benchmark.",
        "dcterms:title": "CLUE",
        "dcterms:issued": "2020",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Chinese Language Processing"
        ],
        "dcat:keyword": [
            "Chinese",
            "Benchmark",
            "Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Wei Zhao",
            "Mingyue Shang",
            "Yang Liu",
            "Liang Wang",
            "Jingming Liu"
        ],
        "dcterms:description": "Ape210k is a large-scale and template-rich dataset of math word problems.",
        "dcterms:title": "Ape210k",
        "dcterms:issued": "2020",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "arXiv:2009.11506",
        "dcat:theme": [
            "Mathematics",
            "Education"
        ],
        "dcat:keyword": [
            "Math Word Problems",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Alex Wang",
            "Yada Pruksachatkun",
            "Nikita Nangia",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill",
            "Omer Levy",
            "Samuel Bowman"
        ],
        "dcterms:description": "SuperGLUE is a benchmark for general-purpose language understanding systems.",
        "dcterms:title": "SuperGLUE",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Tushar Khot",
            "Ashish Sabharwal",
            "Peter Clark"
        ],
        "dcterms:description": "SciTail is a textual entailment dataset derived from science question answering.",
        "dcterms:title": "SciTail",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Inference",
            "Science"
        ],
        "dcat:keyword": [
            "Textual Entailment",
            "Science",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    }
]