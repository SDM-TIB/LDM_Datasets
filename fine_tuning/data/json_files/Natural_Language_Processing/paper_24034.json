[
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "The MMLU dataset is proposed to assess a model's world knowledge and problem-solving ability. It covers 57 different subjects, including elementary subjects, mathematics, US history, computer science, and law, with questions collected from various sources.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multitask",
            "Knowledge Assessment",
            "Problem Solving"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Knowledge Assessment",
            "Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "M. Chen",
            "J. Tworek",
            "H. Jun",
            "Q. Yuan",
            "H. P. D. O. Pinto",
            "J. Kaplan",
            "H. Edwards",
            "Y. Burda",
            "N. Joseph",
            "G. Brockman",
            "A. Ray"
        ],
        "dcterms:description": "HumanEval is an evaluation set designed to measure functional correctness in synthesizing programs from docstrings. It assesses the ability of language models to generate code that passes provided unit tests.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2107.03374",
        "dcat:theme": [
            "Code Generation",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Functional Correctness",
            "Code Synthesis",
            "Programming Challenges"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation",
            "Functional Testing"
        ]
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Sch√§rli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. V. Le",
            "E. H. Chi",
            "D. Zhou",
            "J. Wei"
        ],
        "dcterms:description": "The BBH dataset is a subset of the BIG-Bench benchmark, designed to evaluate the capabilities of language models across various domains, including traditional NLP, mathematics, and commonsense reasoning, with over 200 tasks.",
        "dcterms:title": "BBH",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.09261",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Complex Reasoning",
            "NLP Tasks",
            "Benchmark Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reasoning",
            "Language Understanding"
        ]
    }
]