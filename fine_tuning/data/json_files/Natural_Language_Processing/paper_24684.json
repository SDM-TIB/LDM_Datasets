[
    {
        "dcterms:creator": [],
        "dcterms:description": "A benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evaluation method and an efficient mechanism for task and evaluator construction, featuring 100 tasks across computer desktop and mobile phone environments.",
        "dcterms:title": "Crab Benchmark-v0",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/camel-ai/crab",
        "dcat:theme": [],
        "dcat:keyword": [
            "Cross-environment tasks",
            "Benchmark framework",
            "Multimodal agents"
        ],
        "dcat:landingPage": "https://github.com/camel-ai/crab",
        "dcterms:hasVersion": "v0",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tianlin Shi",
            "Yongchao Chen",
            "Guohao Li"
        ],
        "dcterms:description": "An open-domain platform for web-based agents, providing a comprehensive dataset for evaluating agent performance on web tasks.",
        "dcterms:title": "MINIWOB++",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "https://proceedings.mlr.press/v70/shi17a.html",
        "dcat:theme": [
            "Web-based agents",
            "Open-domain tasks"
        ],
        "dcat:keyword": [
            "Web tasks",
            "Agent evaluation"
        ],
        "dcat:landingPage": "https://proceedings.mlr.press/v70/shi17a.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Web interaction",
            "Task completion"
        ]
    },
    {
        "dcterms:creator": [
            "Gr√©goire Mialon"
        ],
        "dcterms:description": "A benchmark for general AI assistants, focusing on evaluating their performance in various tasks.",
        "dcterms:title": "GAIA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2311.12983",
        "dcat:theme": [
            "AI assistants",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "General AI",
            "Performance evaluation"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2311.12983",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Xiang Deng"
        ],
        "dcterms:description": "A dataset aimed at developing a generalist agent for the web, focusing on real-world interactions and tasks.",
        "dcterms:title": "MIND2WEB",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.06070",
        "dcat:theme": [
            "Generalist agents",
            "Web interactions"
        ],
        "dcat:keyword": [
            "Web tasks",
            "Real-world scenarios"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2306.06070",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Xiao Liu"
        ],
        "dcterms:description": "A dataset for evaluating large language models as agents, focusing on their performance across various tasks.",
        "dcterms:title": "AGENTBENCH",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://openreview.net/forum?id=zAdUB0aCTQ",
        "dcat:theme": [
            "Language models",
            "Agent evaluation"
        ],
        "dcat:keyword": [
            "LLMs",
            "Task performance"
        ],
        "dcat:landingPage": "https://openreview.net/forum?id=zAdUB0aCTQ",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "John Yang"
        ],
        "dcterms:description": "A dataset for standardizing and benchmarking interactive coding with execution feedback.",
        "dcterms:title": "INTERCODE",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2306.14898",
        "dcat:theme": [
            "Interactive coding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Coding tasks",
            "Execution feedback"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2306.14898",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Carlos E. Jimenez"
        ],
        "dcterms:description": "A benchmark focusing on the ability of language models to resolve real-world GitHub issues.",
        "dcterms:title": "SWEBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2310.06770",
        "dcat:theme": [
            "Software engineering",
            "Language models"
        ],
        "dcat:keyword": [
            "GitHub issues",
            "Language model evaluation"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2310.06770",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Raghav Kapoor"
        ],
        "dcterms:description": "A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web environments.",
        "dcterms:title": "OMNIACT",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2402.17553",
        "dcat:theme": [
            "Multimodal agents",
            "Autonomous systems"
        ],
        "dcat:keyword": [
            "Generalist agents",
            "Desktop and web tasks"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2402.17553",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tianbao Xie"
        ],
        "dcterms:description": "A benchmark for evaluating multimodal agents in open-ended tasks within real computer environments.",
        "dcterms:title": "OSWorld",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2404.07972",
        "dcat:theme": [
            "Multimodal agents",
            "Open-ended tasks"
        ],
        "dcat:keyword": [
            "Computer environments",
            "Task evaluation"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2404.07972",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Mingzhe Xing"
        ],
        "dcterms:description": "A dataset aimed at understanding the weaknesses of large language model agents within a complex Android environment.",
        "dcterms:title": "Android Arena",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2402.06596",
        "dcat:theme": [
            "Android applications",
            "Agent evaluation"
        ],
        "dcat:keyword": [
            "Android tasks",
            "Language model weaknesses"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2402.06596",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]