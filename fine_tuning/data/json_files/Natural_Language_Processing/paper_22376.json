[
    {
        "dcterms:creator": [
            "H. Touvron",
            "T. Lavril",
            "G. Izacard",
            "X. Martinet",
            "M.-A. Lachaux",
            "T. Lacroix",
            "B. Rozi`ere",
            "N. Goyal",
            "E. Hambro",
            "F. Azhar",
            "A. Rodriguez",
            "A. Joulin",
            "E. Grave",
            "G. Lampl"
        ],
        "dcterms:description": "Llama 7B is a large language model that serves as an open and efficient foundation for various language tasks.",
        "dcterms:title": "Llama 7B",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Open-source",
            "Foundation model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "E. Almazrouei",
            "H. Alobeidli",
            "A. Alshamsi",
            "A. Cappelli",
            "R. Cojocaru",
            "M. Debbah",
            "E. Goffinet",
            "D. Heslow",
            "J. Launay",
            "Q. Malartic",
            "B. Noune",
            "B. Pannier",
            "G. Penedo"
        ],
        "dcterms:description": "Falcon 7B is a large language model that demonstrates state-of-the-art performance in various language tasks.",
        "dcterms:title": "Falcon 7B",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "State-of-the-art",
            "Open-source"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text generation",
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "T. Zhang",
            "F. Ladhak",
            "E. Durmus",
            "P. Liang",
            "K. McKeown",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "ImageNet1k is a benchmark dataset used for evaluating the performance of large language models in news summarization tasks.",
        "dcterms:title": "ImageNet1k",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Image Classification"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Benchmark",
            "Classification"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image classification",
            "Object recognition"
        ]
    },
    {
        "dcterms:creator": [
            "J. Ainslie",
            "J. Lee-Thorp",
            "M. de Jong",
            "Y. Zemlyanskiy",
            "F. Lebrón",
            "S. Sanghai"
        ],
        "dcterms:description": "Multi-Query-Attention (MQA) is a technique for training generalized multi-query transformer models from multi-head checkpoints.",
        "dcterms:title": "Multi-Query-Attention (MQA)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Attention mechanism",
            "Transformer models",
            "Multi-query"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model training",
            "Attention optimization"
        ]
    },
    {
        "dcterms:creator": [
            "J. Ainslie",
            "J. Lee-Thorp",
            "M. de Jong",
            "Y. Zemlyanskiy",
            "F. Lebrón",
            "S. Sanghai"
        ],
        "dcterms:description": "Group-Query-Attention (GQA) is a technique for training generalized multi-query transformer models from multi-head checkpoints.",
        "dcterms:title": "Group-Query-Attention (GQA)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Attention mechanism",
            "Transformer models",
            "Group-query"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model training",
            "Attention optimization"
        ]
    }
]