[
    {
        "dcterms:creator": [
            "R. A. Bradley",
            "M. E. Terry"
        ],
        "dcterms:description": "A statistical model used for analyzing preferences in paired comparisons, particularly useful in ranking tasks.",
        "dcterms:title": "Bradley-Terry preference model",
        "dcterms:issued": "1952",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistics",
            "Preference Modeling"
        ],
        "dcat:keyword": [
            "Ranking",
            "Pairwise Comparisons",
            "Statistical Model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "W. Zhang",
            "M. Zhu",
            "K. G. Derpanis"
        ],
        "dcterms:description": "A dataset containing video clips annotated with human actions, used for action recognition tasks.",
        "dcterms:title": "Penn Action Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Video Dataset",
            "Action Recognition",
            "Human Actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "H. Jhuang",
            "J. Gall",
            "S. ZufÔ¨Å",
            "C. Schmid",
            "M. J. Black"
        ],
        "dcterms:description": "A dataset designed for understanding action recognition, containing video clips with annotated actions.",
        "dcterms:title": "Sub-JHMDB Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Video Dataset",
            "Action Recognition",
            "Human Actions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Yue",
            "J. Broder",
            "R. Kleinberg",
            "T. Joachims"
        ],
        "dcterms:description": "A problem setting in which two options are compared, and the goal is to learn from the outcomes of these comparisons.",
        "dcterms:title": "Dueling Bandit Problem",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Bandit Algorithms"
        ],
        "dcat:keyword": [
            "Dueling Bandits",
            "Learning from Comparisons",
            "Bandit Problem"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R. Rafailov",
            "A. Sharma",
            "E. Mitchell",
            "S. Ermon",
            "C. D. Manning",
            "C. Finn"
        ],
        "dcterms:description": "A method for optimizing preferences directly without the need for reward modeling, focusing on policy optimization.",
        "dcterms:title": "Direct Preference Optimization (DPO)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Preference Learning"
        ],
        "dcat:keyword": [
            "Preference Optimization",
            "Policy Optimization",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "D. M. Ziegler",
            "N. Stiennon",
            "J. Wu",
            "T. B. Brown",
            "A. Radford",
            "D. Amodei",
            "P. Christiano",
            "G. Irving"
        ],
        "dcterms:description": "A framework for fine-tuning language models based on human preferences, utilizing reinforcement learning techniques.",
        "dcterms:title": "Reinforcement Learning from Human Feedback (RLHF)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Human Feedback",
            "Language Models",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language Model Fine-tuning"
        ]
    },
    {
        "dcterms:creator": [
            "R. Agarwal",
            "D. Schuurmans",
            "M. Norouzi"
        ],
        "dcterms:description": "A perspective on offline reinforcement learning that emphasizes optimistic approaches to learning from fixed datasets.",
        "dcterms:title": "Offline Reinforcement Learning",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Offline Learning",
            "Reinforcement Learning",
            "Optimistic Approaches"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "X. Chen",
            "H. Zhong",
            "Z. Yang",
            "Z. Wang",
            "L. Wang"
        ],
        "dcterms:description": "A framework for preference-based reinforcement learning that incorporates human feedback into the learning process.",
        "dcterms:title": "Human-in-the-loop",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Human Feedback",
            "Preference Learning",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]