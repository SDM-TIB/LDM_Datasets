[
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Schärli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. V. Le",
            "E. Chi",
            "D. Zhou",
            "J. Wei"
        ],
        "dcterms:description": "BIG-Bench Hard (BBH) comprises 27 challenging tasks covering arithmetic, symbolic reasoning, etc., from BIG-Bench. The majority of the data involve multiple-choice questions, with a few being open-ended.",
        "dcterms:title": "BIG-Bench Hard (BBH)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reasoning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multi-choice questions",
            "Symbolic reasoning",
            "Arithmetic tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning tasks"
        ]
    },
    {
        "dcterms:creator": [
            "M. Suzgun",
            "N. Scales",
            "N. Schärli",
            "S. Gehrmann",
            "Y. Tay",
            "H. W. Chung",
            "A. Chowdhery",
            "Q. V. Le",
            "E. Chi",
            "D. Zhou",
            "J. Wei"
        ],
        "dcterms:description": "BIG-Bench Sub (BB-sub) is a subset of the BIG-Bench benchmark consisting of 61 subtasks related to multiple-choice and reasoning tasks, filtered from the original dataset.",
        "dcterms:title": "BIG-Bench Sub (BB-sub)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reasoning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Subtasks",
            "Multiple-choice",
            "Reasoning tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning tasks"
        ]
    },
    {
        "dcterms:creator": [
            "W. Zhong",
            "R. Cui",
            "Y. Guo",
            "Y. Liang",
            "S. Lu",
            "Y. Wang",
            "A. Saied",
            "W. Chen",
            "N. Duan"
        ],
        "dcterms:description": "AGIEval is a human-centric benchmark used to assess language models' reasoning abilities, covering various domains including college entrance exams and logic tests.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Education",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Reasoning abilities",
            "Human-centric evaluation",
            "Logic tests"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning tasks",
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "The AI2 Reasoning Challenge (ARC) consists of two subsets: ARC-Easy (ARC-E) with simpler questions and ARC-Challenge (ARC-C) with more complex questions, designed to evaluate reasoning in science exams.",
        "dcterms:title": "AI2 Reasoning Challenge (ARC)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Education",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Science exams",
            "Reasoning tasks",
            "Multiple-choice questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning tasks"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "ARC-Easy (ARC-E) consists of relatively simpler questions from middle and high school science exams, aimed at evaluating basic reasoning skills.",
        "dcterms:title": "ARC-Easy (ARC-E)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Education",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Science exams",
            "Easy questions",
            "Reasoning tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning tasks"
        ]
    },
    {
        "dcterms:creator": [
            "P. Clark",
            "I. Cowhey",
            "O. Etzioni",
            "T. Khot",
            "A. Sabharwal",
            "C. Schoenick",
            "O. Tafjord"
        ],
        "dcterms:description": "ARC-Challenge (ARC-C) consists of more complex and challenging questions designed to evaluate advanced reasoning skills in science exams.",
        "dcterms:title": "ARC-Challenge (ARC-C)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Education",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Science exams",
            "Challenging questions",
            "Reasoning tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning tasks"
        ]
    }
]