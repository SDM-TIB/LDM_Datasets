[
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "PIQA assesses large modelsâ€™ ability to understand everyday physics with over 16,000 training question-answer pairs.",
        "dcterms:title": "PIQA (Everyday Physics Questions)",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Physics",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Physics questions",
            "Commonsense reasoning",
            "Question-answer pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "HellaSwag challenges language models with 70,000 multiple-choice questions that test their ability to extract information and summarize it.",
        "dcterms:title": "HellaSwag (Commonsense Reasoning Dataset)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Multiple-choice questions",
            "Commonsense reasoning",
            "Information extraction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "WinoGrande is a sizable dataset with 44,000 questions designed to be more extensive and challenging.",
        "dcterms:title": "WinoGrande (Large-Scale Question Dataset)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Large-scale questions",
            "Challenging questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "ARC is a multiple-choice question-answering dataset covering science questions from grades three to nine, with an easy version (i.e., ARC-e).",
        "dcterms:title": "ARC-e (Reasoning Challenge Dataset)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Education",
            "Science"
        ],
        "dcat:keyword": [
            "Multiple-choice questions",
            "Science questions",
            "Educational assessment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]