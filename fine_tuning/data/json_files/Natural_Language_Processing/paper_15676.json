[
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "MMLU is a benchmark for measuring multitask language understanding ability, covering 57 tasks requiring diverse knowledge such as math, history, science, and law.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/hendrycks/test",
        "dcat:theme": [
            "Language Understanding",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "multitask",
            "language understanding",
            "benchmark"
        ],
        "dcat:landingPage": "https://github.com/hendrycks/test",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Felix Hill",
            "Antoine Bordes",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "dcterms:description": "The Children’s Book Test dataset is used to evaluate the ability of models to read children's books with explicit memory representations.",
        "dcterms:title": "Children’s Book Test",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "children's literature",
            "reading comprehension",
            "memory representation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Christopher Clark",
            "Kenton Lee",
            "Ming-Wei Chang",
            "Tom Kwiatkowski",
            "Michael Collins",
            "Kristina Toutanova"
        ],
        "dcterms:description": "BoolQ is a dataset for exploring the difficulty of natural yes/no questions, designed to evaluate models' ability to answer such questions.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "yes/no questions",
            "natural language processing",
            "question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Ronan Le Bras",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "PIQA is a dataset for reasoning about physical commonsense in natural language, focusing on physical interactions.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "physical commonsense",
            "reasoning",
            "natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Rowan Zellers",
            "Ari Holtzman",
            "Yonatan Bisk",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "dcterms:description": "Hellaswag is a dataset designed to evaluate the ability of models to complete sentences in a coherent manner.",
        "dcterms:title": "Hellaswag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Generation",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "sentence completion",
            "coherence",
            "language generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "WinoGrande is an adversarial dataset designed to challenge models on the Winograd schema, focusing on pronoun resolution.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Pronoun Resolution",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Winograd schema",
            "pronoun resolution",
            "adversarial dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pronoun Resolution"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "ARC Easy and Challenge are datasets designed for evaluating models on question answering tasks, focusing on reasoning.",
        "dcterms:title": "ARC Easy and Challenge",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "reasoning",
            "question answering",
            "evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Todor Mihaylov",
            "Peter Clark",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "dcterms:description": "OpenBookQA is a dataset for open book question answering, requiring models to answer questions based on knowledge from a provided set of facts.",
        "dcterms:title": "OpenBookQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "open book",
            "question answering",
            "knowledge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Alon Talmor",
            "Jonathan Herzig",
            "Nicholas Lourie",
            "Jonathan Berant"
        ],
        "dcterms:description": "CommonsenseQA is a question answering challenge targeting commonsense knowledge, designed to evaluate models' understanding of everyday situations.",
        "dcterms:title": "CommonsenseQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Commonsense Reasoning",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "commonsense knowledge",
            "question answering",
            "evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard H. Hovy"
        ],
        "dcterms:description": "RACE is a large-scale reading comprehension dataset derived from examinations, designed to evaluate models' reading comprehension abilities.",
        "dcterms:title": "RACE",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "reading comprehension",
            "examinations",
            "dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Siva Reddy",
            "Danqi Chen",
            "Christopher D. Manning"
        ],
        "dcterms:description": "CoQA is a conversational question answering challenge that evaluates models' ability to answer questions in a dialogue context.",
        "dcterms:title": "CoQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Conversational AI",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "conversational question answering",
            "dialogue",
            "evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yiming Cui",
            "Ting Liu",
            "Wanxiang Che",
            "Li Xiao",
            "Zhipeng Chen",
            "Wentao Ma",
            "Shijin Wang",
            "Guoping Hu"
        ],
        "dcterms:description": "CMRC2018 is a span-extraction dataset for Chinese machine reading comprehension, designed to evaluate models' understanding of Chinese texts.",
        "dcterms:title": "CMRC2018",
        "dcterms:issued": "2019",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "Chinese",
            "machine reading comprehension",
            "span extraction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Denis Paperno",
            "Germán Kruszewski",
            "Angeliki Lazaridou",
            "Quan Ngoc Pham",
            "Raffaella Bernardi",
            "Sandro Pezzelle",
            "Marco Baroni",
            "Gemma Boleda",
            "Raquel Fernández"
        ],
        "dcterms:description": "LAMBADA is a dataset for word prediction that requires a broad discourse context, testing models' ability to predict the last word of a sentence.",
        "dcterms:title": "LAMBADA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Modeling",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "word prediction",
            "discourse context",
            "language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Shashi Narayan",
            "Shay B. Cohen",
            "Mirella Lapata"
        ],
        "dcterms:description": "XSum is a dataset for extreme summarization, requiring models to generate concise summaries from long documents.",
        "dcterms:title": "XSum",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "extreme summarization",
            "text summarization",
            "evaluation benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Chen, J.",
            "Tworek, J.",
            "Jun, H.",
            "Yuan, Q.",
            "Pinto, H. P. d. O.",
            "Kaplan, J.",
            "Edwards, H.",
            "Burda, Y.",
            "Joseph, N.",
            "Brockman, G."
        ],
        "dcterms:description": "HumanEval is a dataset for evaluating code generation capabilities of models, focusing on the ability to write code that solves specific problems.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Evaluation Benchmark"
        ],
        "dcat:keyword": [
            "code generation",
            "evaluation benchmark",
            "programming"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    }
]