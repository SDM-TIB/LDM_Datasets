To extract datasets from the research paper titled "Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence," I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** and **introduction** sections to identify any mentions of datasets. The abstract highlights that the authors conducted extensive experiments on three large-scale 3D skeleton-based action recognition datasets, which indicates that datasets are indeed present.

Next, I will focus on the **experiments section** (specifically section 4.1) where the authors explicitly list the datasets used for evaluation. Here, they mention:

1. **NTU RGB+D 60**: This dataset contains 56,880 skeleton sequences across 60 action categories performed by 40 subjects. The authors follow specific evaluation protocols for cross-subject and cross-view testing.

2. **NTU RGB+D 120**: An extension of the NTU RGB+D 60 dataset, it includes 114,480 skeleton sequences across 120 action categories performed by 106 subjects. The authors also describe a challenging cross-setup evaluation protocol.

3. **PKU Multi-Modality Dataset (PKUMMD)**: This dataset consists of nearly 20,000 skeleton sequences across 52 action categories, with a specific cross-subject protocol for training and testing.

After identifying these datasets, I will check the **References section** to find the full citations for each dataset:

- For **NTU RGB+D 60**, the citation is:
  > Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. *NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1010–1019, 2016.

- For **NTU RGB+D 120**, the citation is:
  > Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C. Kot. *NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding*. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10):2684–2701, 2019.

- For **PKU Multi-Modality Dataset (PKUMMD)**, the citation is:
  > Chunhui Liu, Yueyu Hu, Yanghao Li, Sijie Song, and Jiaying Liu. *PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding*. arXiv preprint arXiv:1703.07475, 2017.

Now that I have gathered the dataset names and their corresponding citations, I will prepare to compile this information into the required structured format for further processing or review.