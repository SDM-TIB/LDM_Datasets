[
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "T. J. Henighan",
            "Nicholas Joseph",
            "Saurav Kadavath",
            "John Kernion",
            "Tom Conerly",
            "Sheer El-Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Tristan Hume",
            "Scott Johnston",
            "Shauna Kravec",
            "Liane Lovitt",
            "Neel Nanda",
            "Catherine Olsson",
            "Dario Amodei",
            "Tom B. Brown",
            "Jack Clark",
            "Sam McCandlish",
            "Christopher Olah",
            "Benjamin Mann",
            "Jared Kaplan"
        ],
        "dcterms:description": "A dataset used to train reward models focusing on harmless responses.",
        "dcterms:title": "Anthropic-HH-Harmless (HH-harmless)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2204.05862",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Reward Model",
            "Human Preferences",
            "Safety"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2204.05862",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reward Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Andy Jones",
            "Kamal Ndousse",
            "Amanda Askell",
            "Anna Chen",
            "Nova DasSarma",
            "Dawn Drain",
            "Stanislav Fort",
            "Deep Ganguli",
            "T. J. Henighan",
            "Nicholas Joseph",
            "Saurav Kadavath",
            "John Kernion",
            "Tom Conerly",
            "Sheer El-Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Tristan Hume",
            "Scott Johnston",
            "Shauna Kravec",
            "Liane Lovitt",
            "Neel Nanda",
            "Catherine Olsson",
            "Dario Amodei",
            "Tom B. Brown",
            "Jack Clark",
            "Sam McCandlish",
            "Christopher Olah",
            "Benjamin Mann",
            "Jared Kaplan"
        ],
        "dcterms:description": "A dataset used to train reward models focusing on helpful responses.",
        "dcterms:title": "Anthropic-HH-Helpful (HH-Helpful)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2204.05862",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Reward Model",
            "Human Preferences",
            "Helpfulness"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2204.05862",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reward Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Jiaming Ji",
            "Mickel Liu",
            "Juntao Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Ce Bian",
            "Ruiyang Sun",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "dcterms:description": "A dataset aimed at improving safety alignment of language models through human preferences.",
        "dcterms:title": "Beaver Safe (BS)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2307.04657",
        "dcat:theme": [
            "Safety",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Safety Alignment",
            "Human Preferences",
            "Language Models"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2307.04657",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Yann Dubois",
            "Xuechen Li",
            "Rohan Taori",
            "Tianyi Zhang",
            "Ishaan Gulrajani",
            "Jimmy Ba",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori Hashimoto"
        ],
        "dcterms:description": "A dataset designed for training models that learn from human feedback.",
        "dcterms:title": "Alpaca Human Pref (AHP)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2305.14387",
        "dcat:theme": [
            "Human Feedback",
            "Learning from Feedback"
        ],
        "dcat:keyword": [
            "Human Preferences",
            "Learning Framework",
            "Feedback"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2305.14387",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P. Xing",
            "Haotong Zhang",
            "Joseph Gonzalez",
            "Ion Stoica"
        ],
        "dcterms:description": "A dataset for evaluating language models as judges using a benchmarking framework.",
        "dcterms:title": "Chatbot Arena (CA)",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2306.05685",
        "dcat:theme": [
            "Evaluation",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Evaluation Framework",
            "Language Models",
            "Judging"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2306.05685",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    }
]