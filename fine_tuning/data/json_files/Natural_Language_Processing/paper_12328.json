[
    {
        "dcterms:creator": [],
        "dcterms:description": "SCIBENCH is a benchmark suite for evaluating college-level scientific problem-solving abilities of large language models, featuring a curated dataset of collegiate-level scientific problems from mathematics, chemistry, and physics domains.",
        "dcterms:title": "SCIBENCH",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Scientific Problem Solving",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Large Language Models",
            "Scientific Reasoning",
            "Mathematics",
            "Chemistry",
            "Physics"
        ],
        "dcat:landingPage": "https://scibench-ucla.github.io",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Problem Solving",
            "Scientific Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "P. Lu",
            "S. Mishra",
            "T. Xia",
            "L. Qiu",
            "K.-W. Chang",
            "S.-C. Zhu",
            "O. Tafjord",
            "P. Clark",
            "A. Kalyan"
        ],
        "dcterms:description": "ScienceQA is a multimodal question-answering dataset designed to evaluate scientific reasoning abilities, including explanations and annotations.",
        "dcterms:title": "ScienceQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Scientific Reasoning"
        ],
        "dcat:keyword": [
            "Multimodal",
            "Science",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "K. Cobbe",
            "V. Kosaraju",
            "M. Bavarian",
            "M. Chen",
            "H. Jun",
            "L. Kaiser",
            "M. Plappert",
            "J. Tworek",
            "J. Hilton",
            "R. Nakano"
        ],
        "dcterms:description": "GSM8K is a dataset containing 8.5K grade school math word problems designed to evaluate the mathematical reasoning capabilities of language models.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Word Problems"
        ],
        "dcat:keyword": [
            "Math",
            "Word Problems",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Kadavath",
            "A. Arora",
            "S. Basart",
            "E. Tang",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "MATH is a dataset designed to measure mathematical problem-solving abilities, containing a variety of math problems.",
        "dcterms:title": "MATH",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Problem Solving"
        ],
        "dcat:keyword": [
            "Math Problems",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "MMLU is a benchmark for measuring massive multitask language understanding across various tasks and subjects.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multitask",
            "Language Understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "W. Zhong",
            "R. Cui",
            "Y. Guo",
            "Y. Liang",
            "S. Lu",
            "Y. Wang",
            "A. Saied",
            "W. Chen",
            "N. Duan"
        ],
        "dcterms:description": "AGIEval is a human-centric benchmark for evaluating foundation models on various tasks.",
        "dcterms:title": "AGIEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Human-Centric"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Arora",
            "H. G. Singh"
        ],
        "dcterms:description": "JEEBench is a benchmark for evaluating large language models on engineering-level scientific problems.",
        "dcterms:title": "JEEBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Engineering",
            "Problem Solving"
        ],
        "dcat:keyword": [
            "Engineering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Engineering Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "P. Lu",
            "S. Mishra",
            "T. Xia",
            "L. Qiu",
            "K.-W. Chang",
            "S.-C. Zhu",
            "O. Tafjord",
            "P. Clark",
            "A. Kalyan"
        ],
        "dcterms:description": "MathVista evaluates the mathematical reasoning of foundation models in visual contexts.",
        "dcterms:title": "MathVista",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Visual Reasoning"
        ],
        "dcat:keyword": [
            "Visual Contexts",
            "Mathematical Reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "L. Sun",
            "Y. Han",
            "Z. Zhao",
            "D. Ma",
            "Z. Shen",
            "B. Chen",
            "L. Chen",
            "K. Yu"
        ],
        "dcterms:description": "SciEval is a multi-level evaluation benchmark for large language models in scientific research.",
        "dcterms:title": "SciEval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Scientific Research",
            "Evaluation"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Scientific Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Scientific Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "S. Mishra",
            "M. Finlayson",
            "P. Lu",
            "L. Tang",
            "S. Welleck",
            "C. Baral",
            "T. Rajpurohit",
            "O. Tafjord",
            "A. Sabharwal",
            "P. Clark"
        ],
        "dcterms:description": "LILA is a unified benchmark for mathematical reasoning across various tasks.",
        "dcterms:title": "LILA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Mathematical Reasoning",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical Reasoning"
        ]
    }
]