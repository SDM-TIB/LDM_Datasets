[
    {
        "dcterms:creator": [
            "A. Mallen",
            "A. Asai",
            "V. Zhong",
            "R. Das",
            "H. Hajishirzi",
            "D. Khashabi"
        ],
        "dcterms:description": "A dataset for evaluating the performance of language models in the context of knowledge conflicts, particularly focusing on context-memory conflicts.",
        "dcterms:title": "PopQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Knowledge Conflicts"
        ],
        "dcat:keyword": [
            "Knowledge Conflicts",
            "Language Models",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Geva",
            "D. Khashabi",
            "E. Segal",
            "T. Khot",
            "D. Roth",
            "J. Berant"
        ],
        "dcterms:description": "A question answering benchmark that requires implicit reasoning strategies, focusing on the ability of models to handle complex reasoning tasks.",
        "dcterms:title": "STRATEGYQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Reasoning"
        ],
        "dcat:keyword": [
            "Implicit Reasoning",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "H. Trivedi",
            "N. Balasubramanian",
            "T. Khot",
            "A. Sabharwal"
        ],
        "dcterms:description": "A dataset designed for multi-hop question answering through the composition of single-hop questions.",
        "dcterms:title": "MuSiQue",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Multi-hop Reasoning"
        ],
        "dcat:keyword": [
            "Multi-hop Questions",
            "Question Composition",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "P. Rajpurkar",
            "J. Zhang",
            "K. Lopyrev",
            "P. Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text, including unanswerable questions.",
        "dcterms:title": "SQuAD2.0",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Comprehension",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Machine Comprehension",
            "Question Answering",
            "Unanswerable Questions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Du",
            "L. Ding",
            "Y. Zhang"
        ],
        "dcterms:description": "A dataset aimed at exploring explainable causal reasoning, providing a framework for understanding causal relationships.",
        "dcterms:title": "e-CARE",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Causal Reasoning",
            "Explainability"
        ],
        "dcat:keyword": [
            "Causal Reasoning",
            "Explainability",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Causal Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "C. Clark",
            "K. Lee",
            "M.-W. Chang",
            "T. Kwiatkowski",
            "M. Collins",
            "K. Toutanova"
        ],
        "dcterms:description": "A dataset exploring the challenges of natural yes/no questions, focusing on the surprising difficulty of such queries.",
        "dcterms:title": "BoolQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Yes/No Questions"
        ],
        "dcat:keyword": [
            "Yes/No Questions",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "T. Kwiatkowski",
            "J. Palomaki",
            "O. Redfield",
            "M. Collins",
            "A. Parikh",
            "K. Toutanova"
        ],
        "dcterms:description": "A benchmark dataset for question answering research, focusing on natural questions that require factual answers.",
        "dcterms:title": "NQ",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Questions"
        ],
        "dcat:keyword": [
            "Natural Questions",
            "Question Answering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. J. Q. Lin",
            "E. Choi"
        ],
        "dcterms:description": "A dataset designed to measure how models mimic human falsehoods, focusing on the generation of truthful responses.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Truthfulness",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Falsehoods",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "M. Joshi",
            "E. Choi",
            "D. S. Weld",
            "L. Zettlemoyer"
        ],
        "dcterms:description": "A large-scale dataset for reading comprehension, created through distant supervision, focusing on trivia questions.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Trivia Questions"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Trivia Questions",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "K. M. Hermann",
            "T. Kociský",
            "E. Grefenstette",
            "L. Espeholt",
            "W. Kay",
            "P. Blunsom"
        ],
        "dcterms:description": "A dataset for teaching machines to read and comprehend, focusing on summarization tasks.",
        "dcterms:title": "CNN-DailyMail",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text Summarization",
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Text Summarization",
            "Reading Comprehension",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "T. Koˇciský",
            "J. Schwarz",
            "P. Blunsom",
            "C. Dyer",
            "K. M. Hermann",
            "G. Melis"
        ],
        "dcterms:description": "A reading comprehension challenge dataset that requires understanding of narratives and storylines.",
        "dcterms:title": "NarrativeQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension",
            "Narrative Understanding"
        ],
        "dcat:keyword": [
            "Reading Comprehension",
            "Narrative Understanding",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "H. Elsahar",
            "P. Vougiouklis",
            "A. Remaci",
            "C. Gravier",
            "J. Hare",
            "E. Simperl"
        ],
        "dcterms:description": "A large-scale dataset aligning natural language with knowledge base triples, facilitating knowledge representation tasks.",
        "dcterms:title": "T-REx",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Knowledge Representation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Knowledge Representation",
            "Natural Language Processing",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Knowledge Representation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Ko",
            "I. Seong",
            "H. Lee",
            "J. Park",
            "M. Chang",
            "M. Seo"
        ],
        "dcterms:description": "A dataset for comparing and contrasting claims on contentious issues, focusing on argumentative analysis.",
        "dcterms:title": "ClaimDiff",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Argumentation",
            "Contentious Issues"
        ],
        "dcat:keyword": [
            "Argumentation",
            "Contentious Issues",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Argumentation Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "J. Li",
            "V. Raheja",
            "D. Kumar"
        ],
        "dcterms:description": "A dataset for understanding self-contradictions in documents using large language models, focusing on contradiction detection.",
        "dcterms:title": "CONTRADOC",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Contradiction Detection",
            "Document Analysis"
        ],
        "dcat:keyword": [
            "Contradiction Detection",
            "Document Analysis",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Contradiction Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Jin",
            "Z. Cao",
            "Y. Chen",
            "J. Jiang",
            "J. Xu",
            "J. Zhao"
        ],
        "dcterms:description": "A dataset exploring and resolving knowledge conflicts in retrieval-augmented language models, focusing on conflicting information.",
        "dcterms:title": "CONFLICTINGQA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Knowledge Conflicts",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Knowledge Conflicts",
            "Retrieval-Augmented Models",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]