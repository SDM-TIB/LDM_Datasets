[
    {
        "dcterms:creator": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "dcterms:description": "A dataset used for language modeling, consisting of a collection of Wikipedia articles.",
        "dcterms:title": "Wikitext",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Wikipedia",
            "Language model",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Ronan Le Bras",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset designed for evaluating commonsense reasoning in question answering.",
        "dcterms:title": "Common Sense Question Answering (CSQA)",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "Commonsense reasoning",
            "Question answering",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark for measuring language understanding across multiple tasks.",
        "dcterms:title": "Massive Multitask Language Understanding (MMLU)",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multitask learning",
            "Language understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Stella Biderman",
            "Sid Black",
            "Laurence Golding",
            "Travis Hoppe",
            "Charles Foster",
            "Jason Phang",
            "Horace He",
            "Anish Thite",
            "Noa Nabeshima"
        ],
        "dcterms:description": "A large dataset of diverse text for language modeling, consisting of various sources.",
        "dcterms:title": "C4 Dataset",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2101.00027",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Diverse text",
            "Language model",
            "Text dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Yonatan Bisk",
            "Rowan Zellers",
            "Ronan Le Bras",
            "Jianfeng Gao",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset for reasoning about physical commonsense in natural language.",
        "dcterms:title": "PIQA",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reasoning"
        ],
        "dcat:keyword": [
            "Physical commonsense",
            "Reasoning",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "An adversarial dataset for evaluating commonsense reasoning based on the Winograd schema.",
        "dcterms:title": "WinoGrande",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Adversarial dataset",
            "Commonsense reasoning",
            "Winograd schema"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Commonsense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Clark",
            "Isaac Cowhey",
            "Oren Etzioni",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Carissa Schoenick",
            "Oyvind Tafjord"
        ],
        "dcterms:description": "A dataset designed for evaluating AI's performance on reasoning tasks.",
        "dcterms:title": "ARC Easy",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1803.05457v1",
        "dcat:theme": [
            "Natural Language Processing",
            "Reasoning"
        ],
        "dcat:keyword": [
            "Question answering",
            "Reasoning",
            "AI evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Brown",
            "Benjamin Mann",
            "Nick Ryder",
            "Melanie Subbiah",
            "Jared D Kaplan",
            "Prafulla Dhariwal",
            "Arvind Neelakantan",
            "Pranav Shyam",
            "Girish Sastry",
            "Amanda Askell",
            "Sandhini Agarwal",
            "Ariel Herbert-Voss",
            "Gretchen Krueger",
            "Tom Henighan",
            "Rewon Child",
            "Aditya Ramesh",
            "Daniel Ziegler",
            "Jeffrey Wu",
            "Clemens Winter",
            "Chris Hesse",
            "Mark Chen",
            "Eric Sigler",
            "Mateusz Litwin",
            "Scott Gray",
            "Benjamin Chess",
            "Jack Clark",
            "Christopher Berner",
            "Sam McCandlish",
            "Alec Radford",
            "Ilya Sutskever",
            "Dario Amodei"
        ],
        "dcterms:description": "A large language model that demonstrates few-shot learning capabilities.",
        "dcterms:title": "GPT-3",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Language model",
            "Few-shot learning",
            "AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling",
            "Few-shot Learning"
        ]
    }
]