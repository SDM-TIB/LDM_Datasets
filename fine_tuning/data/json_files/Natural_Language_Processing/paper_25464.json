[
    {
        "dcterms:creator": [
            "Vlad-Andrei Bădoiu",
            "Mihai-Valentin Dumitru",
            "Alexandru M. Gherghescu",
            "Alexandru Agache",
            "Costin Raiciu"
        ],
        "dcterms:description": "FuLG is a 150B-token Romanian corpus extracted from CommonCrawl, aimed at improving the representation of the Romanian language in language models.",
        "dcterms:title": "FuLG",
        "dcterms:issued": "2023",
        "dcterms:language": "Romanian",
        "dcterms:identifier": "https://hf.co/datasets/faur-ai/fulg",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Romanian corpus",
            "language model pretraining",
            "CommonCrawl",
            "deduplication",
            "data filtering"
        ],
        "dcat:landingPage": "https://hf.co/datasets/faur-ai/fulg",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining",
            "Fine-tuning"
        ]
    },
    {
        "dcterms:creator": [
            "Luca Soldaini",
            "Rodney Kinney",
            "Akshita Bhagia",
            "Dustin Schwenk",
            "David Atkinson",
            "Russell Authur",
            "Ben Bogin",
            "Khyathi Chandu",
            "Jennifer Dumas",
            "Yanai Elazar"
        ],
        "dcterms:description": "Dolma is an open corpus of three trillion tokens designed for language model pretraining research.",
        "dcterms:title": "Dolma",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2402.00159",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "language model pretraining",
            "large corpus",
            "open dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Guilherme Penedo",
            "Hynek Kydlíček",
            "Loubna Ben allal",
            "Anton Lozhkov",
            "Margaret Mitchell",
            "Colin Raﬀel",
            "Leandro Von Werra",
            "Thomas Wolf"
        ],
        "dcterms:description": "FineWeb is a dataset aimed at providing high-quality text data extracted from the web for language modeling.",
        "dcterms:title": "FineWeb",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2406.17557",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "web data",
            "text data",
            "language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Together Computer"
        ],
        "dcterms:description": "RedPajama v2 is an updated version of the RedPajama dataset, which is used for training large language models.",
        "dcterms:title": "RedPajama v2",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/togethercomputer/RedPajama-Data",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "large language models",
            "open dataset"
        ],
        "dcat:landingPage": "https://github.com/togethercomputer/RedPajama-Data",
        "dcterms:hasVersion": "v2",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Training"
        ]
    },
    {
        "dcterms:creator": [
            "Jesse Dodge",
            "Maarten Sap",
            "Ana Marasović",
            "William Agnew",
            "Gabriel Ilharco",
            "Dirk Groeneveld",
            "Margaret Mitchell",
            "Matt Gardner"
        ],
        "dcterms:description": "C4 is a large-scale dataset derived from web text, designed for training language models.",
        "dcterms:title": "C4",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2104.08758",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "web text",
            "language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Leo Gao",
            "Stella Biderman",
            "Sid Black",
            "Laurence Golding",
            "Travis Hoppe",
            "Charles Foster",
            "Jason Phang",
            "Horace He",
            "Anish Thite",
            "Noa Nabeshima"
        ],
        "dcterms:description": "The Pile is a diverse dataset of text for language modeling, comprising various sources.",
        "dcterms:title": "The Pile",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2101.00027",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "diverse text",
            "language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Hugo Laurençon",
            "Lucile Saulnier",
            "Thomas Wang",
            "Christopher Akiki",
            "Albert Villanova del Moral",
            "Teven Le Scao",
            "Leandro Von Werra",
            "Chenghao Mou",
            "Eduardo González Ponferrada",
            "Huu Nguyen"
        ],
        "dcterms:description": "ROOTS is a multilingual dataset designed for language modeling, comprising a large volume of text.",
        "dcterms:title": "ROOTS",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "10.5555/31809",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "multilingual dataset",
            "language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Pedro Javier Ortiz Suárez",
            "Benoît Sagot",
            "Laurent Romary"
        ],
        "dcterms:description": "OSCAR is a multilingual dataset designed for processing large corpora.",
        "dcterms:title": "OSCAR",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "10.14618/ids-pub-9021",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "multilingual corpus",
            "language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Colin Raffel",
            "Noam Shazeer",
            "Adam Roberts",
            "Katherine Lee",
            "Sharan Narang",
            "Michael Matena",
            "Yanqi Zhou",
            "Wei Li",
            "Peter J. Liu"
        ],
        "dcterms:description": "mC4 is a multilingual dataset designed for language model training.",
        "dcterms:title": "mC4",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1910.10683",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "multilingual corpus",
            "language model training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "Marie-Anne Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar"
        ],
        "dcterms:description": "Llama is an open and efficient foundation language model.",
        "dcterms:title": "Llama",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "foundation model",
            "language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Training"
        ]
    },
    {
        "dcterms:creator": [
            "Zhengzhong Liu",
            "Aurick Qiao",
            "Willie Neiswanger",
            "Hongyi Wang",
            "Bowen Tan",
            "Tianhua Tao",
            "Junbo Li",
            "Yuqi Wang",
            "Suqi Sun",
            "Omkar Pangarkar"
        ],
        "dcterms:description": "LLM360 is a dataset aimed at providing transparency in open-source language models.",
        "dcterms:title": "LLM360",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2312.06550",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "open-source models",
            "transparency"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Training"
        ]
    },
    {
        "dcterms:creator": [
            "Yury Tokpanov",
            "Beren Millidge",
            "Paolo Glorioso",
            "Jonathan Pilault",
            "Adam Ibrahim",
            "James Whittington",
            "Quentin Anthony"
        ],
        "dcterms:description": "Zyda is a dataset designed for open language modeling, comprising 1.3 trillion tokens.",
        "dcterms:title": "Zyda",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2406.01981",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "open language modeling",
            "large dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Ge Zhang",
            "Scott Qu",
            "Jiaheng Liu",
            "Chenchen Zhang",
            "Chenghua Lin",
            "Chou Leuang Yu",
            "Danny Pan",
            "Esther Cheng",
            "Jie Liu",
            "Qunshu Lin"
        ],
        "dcterms:description": "MAP-Neo is a series of bilingual large language models designed for various tasks.",
        "dcterms:title": "MAP-Neo",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2405.19327",
        "dcat:theme": [
            "Language Modeling",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "bilingual models",
            "language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Model Training"
        ]
    }
]