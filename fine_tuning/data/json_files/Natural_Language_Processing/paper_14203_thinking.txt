To extract datasets from the research paper titled "FaceGemma: Enhancing Image Captioning with Facial Attributes for Portrait Images" by Naimul Haque et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will start by reviewing the **abstract** and **introduction** sections. The abstract mentions the use of **FaceAttDB data** for generating descriptions for 2000 faces, which indicates that this dataset is crucial for the study. The introduction further elaborates on the creation of the **FaceAttDB** dataset, which contains comprehensive descriptions of 40 facial attributes.

Next, I will look into **section 3 (Dataset)**, where the authors provide detailed information about the **FaceAttDB** dataset. This section states that the dataset comprises 2,000 curated portrait images sourced from the **CelebA dataset**, each paired with five captions based on 40 attribute annotations. This confirms the significance of both datasets in the research.

I will also check the **References section** to gather the full citations for both datasets mentioned:

1. **FaceAttDB**: The dataset is introduced in the paper itself, and its citation is:
   > Naimul Haque, Iffat Labiba, and Sadia Akter. “FaceAtt: Enhancing Image Captioning with Facial Attributes for Portrait Images.” arXiv preprint arXiv:2309.13601 (2023).

2. **CelebA Dataset**: The citation for the CelebA dataset, which is referenced in the paper, is:
   > Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. "Deep Learning Face Attributes in the Wild." Proceedings of International Conference on Computer Vision (ICCV), December 2015.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for clarity and future reference. This will ensure that the datasets are properly documented and can be easily accessed for further research or analysis.