[
    {
        "dcterms:creator": [
            "C. Wang",
            "X. Luo",
            "K. W. Ross",
            "D. Li"
        ],
        "dcterms:description": "InternVid is a large-scale video-text dataset designed for multimodal understanding and generation, providing a diverse range of video content paired with textual descriptions.",
        "dcterms:title": "InternVid",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2307.06942",
        "dcat:theme": [
            "Multimodal Learning",
            "Video Understanding"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Text-video pairs",
            "Multimodal generation"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2307.06942",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video, Text",
        "mls:task": [
            "Multimodal understanding",
            "Text-video generation"
        ]
    },
    {
        "dcterms:creator": [
            "K. Grauman",
            "A. Westbury",
            "E. Byrne",
            "Z. Q. Chavis",
            "A. Furnari",
            "E. Kazakos",
            "D. Moltisanti",
            "J. Munro",
            "T. Perrett",
            "W. Price"
        ],
        "dcterms:description": "Ego4D is a dataset consisting of egocentric videos collected from diverse participants performing daily activities, aimed at understanding human behavior from a first-person perspective.",
        "dcterms:title": "Ego4D",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Egocentric Vision",
            "Human Activity Recognition"
        ],
        "dcat:keyword": [
            "Egocentric video",
            "Daily activities",
            "Human behavior"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition",
            "Behavior understanding"
        ]
    },
    {
        "dcterms:creator": [
            "P. Sermanet",
            "T. Ding",
            "J. Zhao",
            "F. Xia",
            "D. Dwibedi",
            "K. Gopalakrishnan",
            "C. Chan",
            "G. Dulac-Arnold",
            "S. Maddineni",
            "N. J. Joshi"
        ],
        "dcterms:description": "RoboVQA is a dataset designed for multimodal long-horizon reasoning in robotics, providing video data paired with questions and answers to facilitate understanding of complex tasks.",
        "dcterms:title": "RoboVQA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Visual Question Answering"
        ],
        "dcat:keyword": [
            "Robotics",
            "Video reasoning",
            "Multimodal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video, Text",
        "mls:task": [
            "Visual question answering",
            "Long-horizon reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Damen",
            "H. Doughty",
            "G. M. Farinella",
            "A. Furnari",
            "E. Kazakos",
            "J. Ma",
            "D. Moltisanti",
            "J. Munro",
            "T. Perrett",
            "W. Price"
        ],
        "dcterms:description": "Epic-Kitchens-100 is a dataset that contains long-form video of cooking activities in real households, aimed at advancing the understanding of egocentric vision.",
        "dcterms:title": "Epic-Kitchens-100",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Egocentric Vision",
            "Cooking Activities"
        ],
        "dcat:keyword": [
            "Cooking video",
            "Egocentric activities",
            "Long-form video"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition",
            "Task understanding"
        ]
    },
    {
        "dcterms:creator": [
            "R. Goyal",
            "S. E. Kahou",
            "V. Michalski",
            "J. Materzynska",
            "S. Westphal",
            "H. Kim",
            "V. Haenel",
            "I. Fr√ºnd",
            "P. N. Yianilos",
            "M. Mueller-Freitag"
        ],
        "dcterms:description": "Something-Something V2 is a video dataset designed for learning and evaluating visual common sense, featuring short clips of object-centric actions.",
        "dcterms:title": "Something-Something V2",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Common Sense",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Object-centric actions",
            "Video dataset",
            "Visual reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action recognition",
            "Common sense reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "A. Miech",
            "D. Zhukov",
            "J.-B. Alayrac",
            "M. Tapaswi",
            "I. Laptev",
            "J. Sivic"
        ],
        "dcterms:description": "HowTo100M is a large-scale dataset for learning a text-video embedding by watching narrated video clips, aimed at improving multimodal understanding.",
        "dcterms:title": "HowTo100M",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multimodal Learning",
            "Instructional Videos"
        ],
        "dcat:keyword": [
            "Text-video embedding",
            "Narrated videos",
            "Multimodal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video, Text",
        "mls:task": [
            "Multimodal understanding",
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "D. Shan",
            "J. Geng",
            "M. Shu",
            "D. F. Fouhey"
        ],
        "dcterms:description": "100 Days of Hands is a dataset focused on understanding human hands in contact at internet scale, providing a large collection of hand-centric video data.",
        "dcterms:title": "100 Days of Hands",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human-Computer Interaction",
            "Hand Tracking"
        ],
        "dcat:keyword": [
            "Hand-centric video",
            "Human interaction",
            "Gesture recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Hand tracking",
            "Gesture recognition"
        ]
    },
    {
        "dcterms:creator": [
            "K. Grauman",
            "A. Westbury",
            "L. Torresani",
            "K. Kitani",
            "J. Malik",
            "T. Afouras",
            "A. Ashutosh",
            "V. Baiyya",
            "S. Bansal",
            "B. Boote"
        ],
        "dcterms:description": "Ego-Exo4D is a dataset that combines first-person and third-person perspectives to understand skilled human activity, providing a comprehensive view of actions.",
        "dcterms:title": "Ego-Exo4D",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Egocentric Vision",
            "Human Activity Recognition"
        ],
        "dcat:keyword": [
            "First-person video",
            "Third-person video",
            "Skilled activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Activity recognition",
            "Behavior understanding"
        ]
    }
]