[
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe 6B is a pre-trained word embedding dataset that represents words as vectors in a continuous vector space, capturing semantic meanings based on their co-occurrence in a large corpus.",
        "dcterms:title": "GloVe 6B",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "GloVe",
            "Pre-trained embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe 27B is a larger version of the GloVe word embedding dataset, trained on a more extensive corpus to provide richer semantic representations.",
        "dcterms:title": "GloVe 27B",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "GloVe",
            "Pre-trained embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe 42B is an even larger pre-trained word embedding dataset that captures a wide range of semantic meanings through extensive training on a large corpus.",
        "dcterms:title": "GloVe 42B",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "GloVe",
            "Pre-trained embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe 840B is the largest version of the GloVe word embeddings, trained on a vast corpus to provide comprehensive semantic representations of words.",
        "dcterms:title": "GloVe 840B",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "GloVe",
            "Pre-trained embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Piotr Bojanowski",
            "Edouard Grave",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "dcterms:description": "FastText Wiki is a pre-trained word embedding dataset that incorporates subword information to improve the representation of rare words.",
        "dcterms:title": "FastText Wiki",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "FastText",
            "Subword information"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Piotr Bojanowski",
            "Edouard Grave",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "dcterms:description": "FastText Crawl is another pre-trained word embedding dataset that uses subword information to enhance the representation of words, particularly in low-resource scenarios.",
        "dcterms:title": "FastText Crawl",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "FastText",
            "Subword information"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "Greg S. Corrado",
            "Jeff Dean"
        ],
        "dcterms:description": "Google News is a popular pre-trained word embedding dataset derived from news articles, widely used for various natural language processing tasks.",
        "dcterms:title": "Google News",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word vectors",
            "Google News",
            "Pre-trained embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]