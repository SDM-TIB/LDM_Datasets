[
    {
        "dcterms:creator": [
            "Yizhong Wang",
            "Swaroop Mishra",
            "Pegah Alipoormolabashi",
            "Yeganeh Kordi",
            "Amirreza Mirzaei",
            "Atharva Naik",
            "Arjun Ashok",
            "Arut Selvan Dhanasekaran",
            "Anjana Arunkumar",
            "David Stap",
            "Eshaan Pathak",
            "Giannis Karamanolakis",
            "Haizhi Lai",
            "Ishan Purohit",
            "Ishani Mondal",
            "Jacob Anderson",
            "Kirby Kuznia",
            "Krima Doshi",
            "Kuntal Kumar Pal",
            "Maitreya Patel",
            "Mehrad Moradshahi",
            "Mihir Parmar",
            "Mirali Purohit",
            "Neeraj Varshney",
            "Phani Rohitha Kaza",
            "Pulkit Verma",
            "Ravsehaj Singh Puri",
            "Rushang Karia",
            "Savan Doshi",
            "Shailaja Keyur Sampat",
            "Siddhartha Mishra",
            "Sujan Reddy A",
            "Sumanta Patro",
            "Tanay Dixit",
            "Xudong Shen"
        ],
        "dcterms:description": "A dataset containing over 1600 NLP tasks with diverse human-written formats and instructions, used to evaluate LLM performance across various prompt formats.",
        "dcterms:title": "Super-NaturalInstructions",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "10.18653/v1/2022.emnlp-main.340",
        "dcat:theme": [
            "Natural Language Processing",
            "Task Generalization"
        ],
        "dcat:keyword": [
            "NLP tasks",
            "prompt design",
            "language models"
        ],
        "dcat:landingPage": "https://aclanthology.org/2022.emnlp-main.340",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Task evaluation",
            "Prompt formatting"
        ]
    },
    {
        "dcterms:creator": [
            "Moin Nadeem",
            "Anna Bethke",
            "Siva Reddy"
        ],
        "dcterms:description": "A dataset designed to measure stereotypical bias in pretrained language models, providing a benchmark for evaluating model performance on stereotype-related tasks.",
        "dcterms:title": "StereoSet",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "stereotypical bias",
            "language models",
            "evaluation dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias evaluation",
            "Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Armen Aghajanyan"
        ],
        "dcterms:description": "A dataset used for evaluating the performance of language models on a variety of tasks, focusing on the impact of prompt formatting on model accuracy.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://twitter.com/ArmenAgha/status/1669084129261162497",
        "dcat:theme": [
            "Model Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "language model evaluation",
            "prompt formatting",
            "accuracy measurement"
        ],
        "dcat:landingPage": "https://twitter.com/ArmenAgha/status/1669084129261162497",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation",
            "Task performance measurement"
        ]
    }
]