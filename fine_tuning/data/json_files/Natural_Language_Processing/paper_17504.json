[
    {
        "dcterms:creator": [
            "Carlos Busso",
            "Murtaza Bulut",
            "Chi-Chun Lee",
            "Abe Kazemzadeh",
            "Emily Mower",
            "Samuel Kim",
            "Jeannette N Chang",
            "Sungbok Lee",
            "Shrikanth S Narayanan"
        ],
        "dcterms:description": "IEMOCAP is a well-known multi-modal emotion corpus, which includes audio, visual and lexical modalities. In this work, we only use the data of audio modality. The corpus contains five recording sessions, each session has one male and one female speaker. It includes 5531 audio utterances of four emotion categories: happy (happy & excited), sad, neutral, and angry.",
        "dcterms:title": "IEMOCAP",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Speech Emotion Recognition",
            "Multimodal Emotion Analysis"
        ],
        "dcat:keyword": [
            "Emotion corpus",
            "Audio data",
            "Speech emotion recognition",
            "Multi-modal data"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Speech Emotion Recognition"
        ]
    }
]