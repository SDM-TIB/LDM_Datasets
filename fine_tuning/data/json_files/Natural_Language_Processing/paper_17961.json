[
    {
        "dcterms:creator": [
            "J. Xiao",
            "X. Shang",
            "A. Yao",
            "T.-S. Chua"
        ],
        "dcterms:description": "NExT-QA contains 34,132/4,996 video-question pairs for train/val. Each question is annotated with a question type (causal/temporal/descriptive) and 5 answer candidates. The dataset is used for evaluating the effectiveness of the proposed method.",
        "dcterms:title": "NExT-QA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Question answering",
            "Temporal reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Video Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Lei",
            "L. Yu",
            "T. L. Berg",
            "M. Bansal"
        ],
        "dcterms:description": "TVQA+ is an enhanced version of TVQA, augmented with 310.8K bounding boxes to link visual concepts in questions and answers to depicted objects in videos. It is used for evaluating video question answering tasks.",
        "dcterms:title": "TVQA+",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Question Answering"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Spatio-temporal grounding",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Video Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "S. Seo",
            "J.-Y. Lee",
            "B. Han"
        ],
        "dcterms:description": "Ref-YouTube-VOS is a large-scale referring video object segmentation dataset with approximately 15,000 referential expressions associated with over 3,900 videos, covering diverse scenarios. It is used for validating effectiveness in pixel-wise spatial-temporal segmentation.",
        "dcterms:title": "Ref-YouTube-VOS",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Object Segmentation"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Referring expressions",
            "Object segmentation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Referring Video Object Segmentation"
        ]
    }
]