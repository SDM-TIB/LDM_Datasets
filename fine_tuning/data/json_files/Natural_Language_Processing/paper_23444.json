[
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "Commonly used to evaluate the common sense reasoning ability of LLMs, MMLU consists of multiple-choice questions from various domains. It includes 57 subsets covering subjects in science, technology, humanities, and other areas. Each subset has four demonstrations.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Common Sense Reasoning"
        ],
        "dcat:keyword": [
            "Multiple-choice questions",
            "Common sense reasoning",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Common Sense Reasoning"
        ]
    },
    {
        "dcterms:creator": [
            "Mirac Suzgun",
            "Nathan Scales",
            "Nathanael Sch√§rli",
            "Sebastian Gehrmann",
            "Yi Tay",
            "Hyung Won Chung",
            "Aakanksha Chowdhery",
            "Quoc V Le",
            "Ed H Chi",
            "Denny Zhou"
        ],
        "dcterms:description": "Derived from a subset of tasks within the BIG-Bench benchmark, BBH includes tasks where existing LLMs struggle to reach average human-rater performance. The demonstrations are provided in Chen et al. (2023), and each subset has three demonstrations.",
        "dcterms:title": "BBH",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.09261",
        "dcat:theme": [
            "Benchmarking",
            "Language Understanding"
        ],
        "dcat:keyword": [
            "Multiple-choice tasks",
            "Benchmark",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multiple-choice tasks"
        ]
    }
]