To extract datasets from the research paper titled "VoxelKP: A Voxel-based Network Architecture for Human Keypoint Estimation in LiDAR Data" by Jian Shi and Peter Wonka, I will follow a systematic approach.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. In the abstract, the authors mention evaluating their method on the **Waymo dataset**, which indicates that a dataset is involved.

Next, I will look closely at **section 4.1 (Implementation Details)**, where the authors describe the dataset used for training. They specifically mention using the **Waymo v1.4.2 dataset** for their experiments. This section provides crucial details about the dataset, including the number of human examples with keypoint annotations and the specific keypoints labeled.

I will also check the **References section** to find the full citation for the Waymo dataset. The citation is as follows:
> Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. *Scalability in perception for autonomous driving: Waymo open dataset*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

Now, I will summarize the dataset information:

1. **Waymo Dataset**: This dataset is used for training and contains keypoint annotations for LiDAR-collected point cloud scenes. It includes 8,125 human examples with keypoint annotations, which are crucial for the experiments conducted in the paper.

Finally, I will compile the dataset entries into a structured format for review or further processing, ensuring that I include the full citation for the Waymo dataset as it is essential for proper attribution.