[
    {
        "dcterms:creator": [
            "W. Luo",
            "S. Ma",
            "X. Liu",
            "X. Guo",
            "C. Xiao"
        ],
        "dcterms:description": "RedTeam-2K consists of 2000 diverse and high-quality harmful textual questions across 16 harmful categories, used to evaluate the effectiveness of jailbreak attacks on multimodal large language models.",
        "dcterms:title": "RedTeam-2K",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Jailbreak Attacks",
            "Multimodal Large Language Models"
        ],
        "dcat:keyword": [
            "Harmful questions",
            "Jailbreak evaluation",
            "Textual dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Jailbreak Attack Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Mazeika",
            "L. Phan",
            "X. Yin",
            "A. Zou",
            "Z. Wang",
            "N. Mu",
            "E. Sakhaee",
            "N. Li",
            "S. Basart",
            "B. Li",
            "D. Forsyth",
            "D. Hendrycks"
        ],
        "dcterms:description": "HarmBench is an open-source framework for automated red teaming, containing 320 textual harmful questions for testing the robustness of multimodal large language models.",
        "dcterms:title": "HarmBench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Automated Red Teaming",
            "Robustness Evaluation"
        ],
        "dcat:keyword": [
            "Harmful questions",
            "Red teaming",
            "Evaluation framework"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Robustness Testing"
        ]
    }
]