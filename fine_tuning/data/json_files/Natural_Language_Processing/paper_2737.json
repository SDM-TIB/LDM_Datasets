[
    {
        "dcterms:creator": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "Greg S Corrado",
            "Jeff Dean"
        ],
        "dcterms:description": "Word2vec is a word embedding algorithm that learns vector representations of words by predicting context words given a target word, or vice versa, using a shallow neural network.",
        "dcterms:title": "Word2vec",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Neural networks",
            "Contextual prediction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word representation learning"
        ]
    },
    {
        "dcterms:creator": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher Manning"
        ],
        "dcterms:description": "GloVe is a global vectors for word representation algorithm that generates word embeddings by aggregating global word-word co-occurrence statistics from a corpus.",
        "dcterms:title": "GloVe",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Co-occurrence statistics",
            "Global vectors"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word representation learning"
        ]
    },
    {
        "dcterms:creator": [
            "Armand Joulin",
            "Edouard Grave",
            "Piotr Bojanowski",
            "Tomas Mikolov"
        ],
        "dcterms:description": "FastText is a word embedding algorithm that represents words as bags of character n-grams, allowing it to generate embeddings for out-of-vocabulary words.",
        "dcterms:title": "FastText",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Word embeddings",
            "Character n-grams",
            "Out-of-vocabulary words"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word representation learning"
        ]
    },
    {
        "dcterms:creator": [
            "Sanjeev Arora",
            "Yuanzhi Li",
            "Yingyu Liang",
            "Tengyu Ma",
            "Andrej Risteski"
        ],
        "dcterms:description": "LDS (Latent Discourse Space) is a model that provides a generative perspective on word embeddings, focusing on the latent structure of discourse.",
        "dcterms:title": "LDS (Latent Discourse Space)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Word Embeddings"
        ],
        "dcat:keyword": [
            "Latent discourse",
            "Generative modeling",
            "Word embeddings"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Word representation learning"
        ]
    },
    {
        "dcterms:creator": [
            "Sanjeev Arora",
            "Yuanzhi Li",
            "Yingyu Liang",
            "Tengyu Ma",
            "Andrej Risteski"
        ],
        "dcterms:description": "SIF (Smooth Inverse Frequency) is a document embedding algorithm that uses a weighted average of word embeddings, adjusted by their frequency in the corpus.",
        "dcterms:title": "SIF (Smooth Inverse Frequency)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Document Embeddings"
        ],
        "dcat:keyword": [
            "Document embeddings",
            "Word embeddings",
            "Frequency weighting"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Document representation learning"
        ]
    },
    {
        "dcterms:creator": [
            "Egidio Terra",
            "Charles LA Clarke"
        ],
        "dcterms:description": "PMI (Pointwise Mutual Information) is a statistical measure used to quantify the association between two words based on their co-occurrence in a corpus.",
        "dcterms:title": "PMI (Pointwise Mutual Information)",
        "dcterms:issued": "2003",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Statistical Measures"
        ],
        "dcat:keyword": [
            "Statistical association",
            "Co-occurrence",
            "Word similarity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Statistical analysis"
        ]
    }
]