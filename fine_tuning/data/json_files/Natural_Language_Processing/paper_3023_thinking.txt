To extract datasets from the research paper titled "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic" by Muhammad Abdul-Mageed et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets mentioned. The abstract states that the authors introduce **ARLUE**, a new benchmark for multi-dialectal Arabic language understanding evaluation, which indicates that multiple datasets are involved.

Next, I will focus on the **introduction** where the authors mention that ARLUE is built using **42 datasets targeting six different task clusters**. This suggests that there are numerous datasets to extract, and I need to look for specific names and details.

In the **methodology section**, particularly under the **"Datasets"** subsection, I will find detailed descriptions of the datasets used for training and evaluation. The authors provide a comprehensive list of datasets across various tasks, including:

1. **AraBERT**: Trained on 23GB of Arabic text from multiple sources, including Arabic Wikipedia and the Open Source International dataset (OSIAN).
   - Citation: Antoun, W., Baly, F., & Hajj, H. (2020). *AraBERT: Transformer-based model for Arabic language understanding*. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 9–15.

2. **OSIAN**: The Open Source International Arabic News Corpus, consisting of 3.5 million articles from 31 news sources in 24 Arab countries.
   - Citation: Zeroual, I., Goldhahn, D., Eckart, T., & Lakhouaja, A. (2019). *OSIAN: Open Source International Arabic News Corpus - Preparation and Integration into the CLARIN-infrastructure*. In Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 175–182.

3. **Gigaword**: A comprehensive archive of newswire text from multiple Arabic news sources.
   - Citation: Linguistic Data Consortium. (2011). *Arabic Gigaword 5th Edition*. LDC2011T11.

4. **Twitter Dataset**: A large dataset of Arabic tweets used for pre-training MARBERT, consisting of 1 billion tweets.
   - Citation: Not explicitly cited in the paper, but it is mentioned that it is an in-house dataset.

5. **ARLUE**: A benchmark composed of 42 datasets across six task clusters, which includes datasets for sentiment analysis, dialect identification, and more.
   - Citation: Abdul-Mageed, M., Elmadany, A., & Nagoudi, E. M. B. (2023). *ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic*. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 769–782.

After identifying these datasets, I will ensure to check the **References section** for full citations of each dataset mentioned in the paper. This will help in compiling a comprehensive list of datasets with their respective citations.

Finally, I will summarize the datasets in a structured format, ensuring that each dataset is clearly identified along with its citation, ready for further processing or review.