[
    {
        "dcterms:creator": [
            "Christoph Schuhmann",
            "Romain Beaumont",
            "Richard Vencu",
            "Cade Gordon",
            "Ross Wightman",
            "Mehdi Cherti",
            "Theo Coombes",
            "Aarush Katta",
            "Clayton Mullis",
            "Mitchell Wortsman"
        ],
        "dcterms:description": "A large dataset containing 400 million image-text pairs filtered using CLIP.",
        "dcterms:title": "LAION-400M",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2111.02114",
        "dcat:theme": [
            "Image-Text Pairs",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Text dataset",
            "CLIP",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Image-Text Matching",
            "Multi-modal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Piyush Sharma",
            "Nan Ding",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "dcterms:description": "A cleaned, hypernymed image alt-text dataset for automatic image captioning.",
        "dcterms:title": "Conceptual Captions",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Image captions",
            "Automatic captioning",
            "Hypernymed dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Vicente Ordonez",
            "Girish Kulkarni",
            "Tamara Berg"
        ],
        "dcterms:description": "A dataset containing 1 million captioned photographs for describing images.",
        "dcterms:title": "SBU",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Captioned photographs",
            "Image description",
            "Large dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Junnan Li",
            "Dongxu Li",
            "Silvio Savarese",
            "Steven Hoi"
        ],
        "dcterms:description": "A dataset used for bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "dcterms:title": "BLIP-Capfilt",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2301.12597",
        "dcat:theme": [
            "Image-Text Pairs",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Image-text dataset",
            "Pre-training",
            "Language-image models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Multi-modal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Christoph Schuhmann",
            "Romain Beaumont",
            "Richard Vencu",
            "Cade Gordon",
            "Ross Wightman",
            "Mehdi Cherti",
            "Theo Coombes",
            "Aarush Katta",
            "Clayton Mullis",
            "Mitchell Wortsman"
        ],
        "dcterms:description": "Aesthetic image subset of LAION-5B for training next generation image-text models.",
        "dcterms:title": "LAION-Aesthetics",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image-Text Pairs",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Aesthetic images",
            "Image-text dataset",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text",
        "mls:task": [
            "Image-Text Matching",
            "Multi-modal Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Together Computer"
        ],
        "dcterms:description": "An open-source recipe to reproduce the LLaMA training dataset.",
        "dcterms:title": "Redpajama",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/togethercomputer/RedPajama-Data",
        "dcat:theme": [
            "Text Dataset",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Open-source dataset",
            "LLaMA training",
            "Text corpus"
        ],
        "dcat:landingPage": "https://github.com/togethercomputer/RedPajama-Data",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Harsh Agrawal",
            "Karan Desai",
            "Yufei Wang",
            "Xinlei Chen",
            "Rishabh Jain",
            "Mark Johnson",
            "Dhruv Batra",
            "Devi Parikh",
            "Stefan Lee",
            "Peter Anderson"
        ],
        "dcterms:description": "A dataset for novel object captioning at scale.",
        "dcterms:title": "NoCaps",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Novel object captioning",
            "Image dataset",
            "Large-scale dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Bryan A Plummer",
            "Liwei Wang",
            "Chris M Cervantes",
            "Juan C Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "dcterms:description": "A dataset containing region-to-phrase correspondences for richer image-to-sentence models.",
        "dcterms:title": "Flickr30k",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Region-to-phrase correspondences",
            "Image-to-sentence models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Image Captioning"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "A dataset for visual question answering that emphasizes the role of image understanding.",
        "dcterms:title": "VQAv2",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual question answering",
            "Image understanding",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Kenneth Marino",
            "Mohammad Rastegari",
            "Ali Farhadi",
            "Roozbeh Mottaghi"
        ],
        "dcterms:description": "A visual question answering benchmark requiring external knowledge.",
        "dcterms:title": "OKVQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual question answering",
            "External knowledge",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Drew A Hudson",
            "Christopher D Manning"
        ],
        "dcterms:description": "A dataset for real-world visual reasoning and compositional question answering.",
        "dcterms:title": "GQA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual reasoning",
            "Compositional question answering",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Danna Gurari",
            "Qing Li",
            "Abigale J Stangl",
            "Anhong Guo",
            "Chi Lin",
            "Kristen Grauman",
            "Jiebo Luo",
            "Jeffrey P Bigham"
        ],
        "dcterms:description": "A dataset for answering visual questions from blind people.",
        "dcterms:title": "VizWiz",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Accessibility"
        ],
        "dcat:keyword": [
            "Visual questions",
            "Blind accessibility",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "Piotr Doll´ar",
            "C Lawrence Zitnick"
        ],
        "dcterms:description": "A dataset of common objects in context for image captioning and object detection.",
        "dcterms:title": "MS-COCO",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Captioning",
            "Object Detection"
        ],
        "dcat:keyword": [
            "Common objects",
            "Image dataset",
            "Contextual images"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Image Captioning",
            "Object Detection"
        ]
    }
]