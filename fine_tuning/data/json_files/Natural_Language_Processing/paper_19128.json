[
    {
        "dcterms:creator": [],
        "dcterms:description": "A benchmark designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. It aims to measure the falsehood of the final output of LLMs based on the reasoning steps.",
        "dcterms:title": "R2PE",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/XinXU-USTC/R2PE",
        "dcat:theme": [
            "Reasoning Assessment",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "LLM",
            "reasoning chains",
            "performance evaluation"
        ],
        "dcat:landingPage": "https://github.com/XinXU-USTC/R2PE",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reasoning Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Vineet Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano"
        ],
        "dcterms:description": "A dataset containing a collection of grade school math word problems, used as a benchmark for evaluating mathematical reasoning capabilities.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematical Reasoning"
        ],
        "dcat:keyword": [
            "math word problems",
            "benchmark",
            "education"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Mathematical Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Saurav Kadavath",
            "Akul Arora",
            "Steven Basart",
            "Eric Tang",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A dataset consisting of competition-level mathematics problems, used to measure mathematical problem-solving abilities.",
        "dcterms:title": "MATH",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematical Reasoning"
        ],
        "dcat:keyword": [
            "competition problems",
            "mathematics",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Mathematical Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "Mor Geva",
            "Daniel Khashabi",
            "Elad Segal",
            "Tushar Khot",
            "Dan Roth",
            "Jonathan Berant"
        ],
        "dcterms:description": "A question answering benchmark focusing on implicit reasoning strategies, requiring the inference of reasoning steps from the questions.",
        "dcterms:title": "StrategyQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "implicit reasoning",
            "benchmark",
            "QA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adrià Garriga-Alonso"
        ],
        "dcterms:description": "A dataset designed to assess the model’s ability to deduce character consistency and dialogue progression within Shakespearean plays.",
        "dcterms:title": "Play",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Common Sense Reasoning"
        ],
        "dcat:keyword": [
            "dialogue",
            "character consistency",
            "literature"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Dialogue Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adrià Garriga-Alonso"
        ],
        "dcterms:description": "A dataset sourced from BIG-bench collections, designed to evaluate a model’s understanding of physics through word problems.",
        "dcterms:title": "Physics",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Physical Reasoning"
        ],
        "dcat:keyword": [
            "physics problems",
            "benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Physics Problem Solving"
        ]
    },
    {
        "dcterms:creator": [
            "James Thorne",
            "Andreas Vlachos",
            "Christos Christodoulopoulos",
            "Arpit Mittal"
        ],
        "dcterms:description": "A large-scale dataset for fact extraction and verification, categorizing claims based on evidence from Wikipedia.",
        "dcterms:title": "FEVER",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fact Verification"
        ],
        "dcat:keyword": [
            "fact extraction",
            "verification",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Fact Verification"
        ]
    },
    {
        "dcterms:creator": [
            "Zhilin Yang",
            "Peng Qi",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "William Cohen",
            "Ruslan Salakhutdinov",
            "Christopher D. Manning"
        ],
        "dcterms:description": "A dataset for diverse, explainable multi-hop question answering, requiring reasoning across multiple documents.",
        "dcterms:title": "HotpotQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Open-Domain QA"
        ],
        "dcat:keyword": [
            "multi-hop QA",
            "explainable reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multi-Hop Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Albert Q. Jiang",
            "Alexandre Sablayrolles",
            "Antoine Roux",
            "Arthur Mensch",
            "Blanche Savary",
            "Chris Bamford",
            "Devendra Singh Chaplot",
            "Diego de las Casas",
            "Emma Bou Hanna",
            "Florian Bressand"
        ],
        "dcterms:description": "A dataset designed for multi-hop question-answering tasks, leveraging structured data from Wikidata.",
        "dcterms:title": "2WikiMultihop",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Open-Domain QA"
        ],
        "dcat:keyword": [
            "multi-hop QA",
            "Wikidata"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Multi-Hop Question Answering"
        ]
    }
]