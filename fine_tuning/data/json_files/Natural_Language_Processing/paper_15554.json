[
    {
        "dcterms:creator": [
            "A. Zou",
            "Z. Wang",
            "J. Z. Kolter",
            "M. Fredrikson"
        ],
        "dcterms:description": "AdvBench is a small benchmark that consists of 520 harmful instructions used to evaluate the robustness of safety training in language models.",
        "dcterms:title": "AdvBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Adversarial Attacks",
            "Safety Evaluation"
        ],
        "dcat:keyword": [
            "Harmful instructions",
            "Safety training",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Safety evaluation",
            "Adversarial testing"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "RefusalBench was created by generating categories of misuse and writing seed prompts, resulting in 783 questions across 7 categories to evaluate refusal rates of language models.",
        "dcterms:title": "RefusalBench",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Refusal Evaluation",
            "Safety Assessment"
        ],
        "dcat:keyword": [
            "Refusal rates",
            "Misuse categories",
            "Synthetic prompts"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Refusal evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "C. Burns",
            "S. Basart",
            "A. Zou",
            "M. Mazeika",
            "D. Song",
            "J. Steinhardt"
        ],
        "dcterms:description": "MMLU is a benchmark designed to measure massive multitask language understanding across various tasks.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Multitask evaluation",
            "Language models",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "R. Zellers",
            "A. Holtzman",
            "Y. Bisk",
            "A. Farhadi",
            "Y. Choi"
        ],
        "dcterms:description": "HellaSwag is a benchmark that tests the ability of models to complete sentences in a coherent and contextually appropriate manner.",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Language Understanding",
            "Sentence Completion"
        ],
        "dcat:keyword": [
            "Sentence completion",
            "Contextual understanding",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Sentence completion"
        ]
    },
    {
        "dcterms:creator": [
            "A. Q. Jiang",
            "A. Sablayrolles",
            "A. Roux",
            "A. Mensch",
            "B. Savary",
            "C. Bamford",
            "D. S. Chaplot",
            "D. de las Casas",
            "E. B. Hanna",
            "F. Bressand",
            "G. Lengyel",
            "G. Bour",
            "G. Lample",
            "L. R. Lavaud",
            "L. Saulnier",
            "M.-A. Lachaux",
            "P. Stock",
            "S. Subramanian",
            "S. Yang",
            "S. Antoniak",
            "T. L. Scao",
            "T. Gervet",
            "T. Lavril",
            "T. Wang",
            "T. Lacroix",
            "W. E. Sayed"
        ],
        "dcterms:description": "Mixtral is a powerful new open model that utilizes the Mixture-of-experts architecture.",
        "dcterms:title": "Mixtral",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Model Architecture"
        ],
        "dcat:keyword": [
            "Mixture-of-experts",
            "Model efficiency",
            "Open model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model training",
            "Language modeling"
        ]
    }
]