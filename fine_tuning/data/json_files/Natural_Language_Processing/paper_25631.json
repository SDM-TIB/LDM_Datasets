[
    {
        "dcterms:creator": [
            "Shivansh Sharma",
            "Mathew Huang",
            "Sanat Nair",
            "Alan Wen",
            "Christina Petlowany",
            "Selma Wanna",
            "Mitch Pryor"
        ],
        "dcterms:description": "The HAGS dataset provides 1200 challenging examples for hand and glove segmentation in industrial human-robot collaboration scenarios, including pixel-level labels and diverse participant data.",
        "dcterms:title": "HAGS: Hand and Glove Segmentation Dataset",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.18738/T8/85R7KQ",
        "dcat:theme": [
            "Human-Robot Collaboration",
            "Hand Segmentation",
            "Industrial Applications"
        ],
        "dcat:keyword": [
            "Hand segmentation",
            "Glove segmentation",
            "Human-robot collaboration",
            "Industrial environments",
            "Active safety systems"
        ],
        "dcat:landingPage": "https://doi.org/10.18738/T8/85R7KQ",
        "dcterms:hasVersion": "1.0",
        "dcterms:format": "Video",
        "mls:task": [
            "Segmentation",
            "Uncertainty quantification"
        ]
    },
    {
        "dcterms:creator": [
            "Roy Shilkrot",
            "Supreeth Narasimhaswamy",
            "Saif Vazir",
            "Minh Hoai"
        ],
        "dcterms:description": "WorkingHands is a hand-tool assembly dataset designed for image segmentation and activity mining.",
        "dcterms:title": "WorkingHands",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Image Segmentation",
            "Activity Recognition"
        ],
        "dcat:keyword": [
            "Hand-tool assembly",
            "Image segmentation",
            "Activity mining"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image segmentation",
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Stefan Grushko",
            "Aleš Vysocký",
            "Jakub Chlebek",
            "Petr Prokop"
        ],
        "dcterms:description": "HaDR is a synthetic multimodal dataset generated for hand instance segmentation in cluttered industrial environments.",
        "dcterms:title": "HaDR",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "eprint: 2304.05826",
        "dcat:theme": [
            "Synthetic Data",
            "Hand Segmentation"
        ],
        "dcat:keyword": [
            "Synthetic dataset",
            "Hand instance segmentation",
            "Industrial environments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Hand segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Seyedomid Sajedi",
            "Wansong Liu",
            "Kareem Eltouny",
            "Sara Behdad",
            "Minghui Zheng",
            "Xiao Liang"
        ],
        "dcterms:description": "HRC focuses on uncertainty-assisted image processing for human-robot close collaboration.",
        "dcterms:title": "HRC",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human-Robot Collaboration",
            "Image Processing"
        ],
        "dcat:keyword": [
            "Uncertainty estimation",
            "Human-robot collaboration",
            "Image processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Image processing"
        ]
    },
    {
        "dcterms:creator": [
            "Francesco Ragusa",
            "Antonino Furnari",
            "Salvatore Livatino",
            "Giovanni Maria Farinella"
        ],
        "dcterms:description": "The MECCANO dataset is designed to understand human-object interactions from egocentric videos in an industrial-like domain.",
        "dcterms:title": "MECCANO",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "eprint: 2010.05654",
        "dcat:theme": [
            "Human-Object Interaction",
            "Egocentric Videos"
        ],
        "dcat:keyword": [
            "Human-object interaction",
            "Egocentric videos",
            "Industrial domain"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Sven Bambach",
            "Stefan Lee",
            "David J. Crandall",
            "Chen Yu"
        ],
        "dcterms:description": "GTEA dataset is focused on detecting hands and recognizing activities in complex egocentric interactions.",
        "dcterms:title": "GTEA",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Recognition",
            "Egocentric Vision"
        ],
        "dcat:keyword": [
            "Hand detection",
            "Activity recognition",
            "Egocentric interactions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Aisha Urooj Khan",
            "Ali Borji"
        ],
        "dcterms:description": "EgoHands dataset focuses on hand segmentation in the wild.",
        "dcterms:title": "EgoHands",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "eprint: 1803.03317",
        "dcat:theme": [
            "Hand Segmentation",
            "Real-world Applications"
        ],
        "dcat:keyword": [
            "Hand segmentation",
            "Real-world data",
            "Ego-centric vision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Hand segmentation"
        ]
    },
    {
        "dcterms:creator": [
            "Hamed Pirsiavash",
            "Deva Ramanan"
        ],
        "dcterms:description": "ADL dataset is designed for detecting activities of daily living in first-person camera views.",
        "dcterms:title": "ADL",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Activity Recognition",
            "First-Person Vision"
        ],
        "dcat:keyword": [
            "Activity detection",
            "First-person camera",
            "Daily living activities"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Activity recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Cheng Li",
            "Kris M. Kitani"
        ],
        "dcterms:description": "EDSH dataset focuses on pixel-level hand detection in ego-centric videos.",
        "dcterms:title": "EDSH",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Hand Detection",
            "Ego-centric Videos"
        ],
        "dcat:keyword": [
            "Pixel-level detection",
            "Ego-centric videos",
            "Hand detection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Hand detection"
        ]
    },
    {
        "dcterms:creator": [
            "Yifan Zhang",
            "Congqi Cao",
            "Jian Cheng",
            "Hanqing Lu"
        ],
        "dcterms:description": "EgoGesture is a new dataset and benchmark for egocentric hand gesture recognition.",
        "dcterms:title": "EgoGesture",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Gesture Recognition",
            "Egocentric Vision"
        ],
        "dcat:keyword": [
            "Gesture recognition",
            "Egocentric videos",
            "Hand gestures"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Gesture recognition"
        ]
    }
]