[
    {
        "dcterms:creator": [
            "Seungjun Moon",
            "Hyungjoo Chae",
            "Dongjin Kang",
            "Kai Tzu-iunn Ong",
            "Yongho Song",
            "Seung-won Hwang",
            "Taeyoon Kwon",
            "Jinyoung Yeo"
        ],
        "dcterms:description": "COFFEE is a dataset specifically designed for code editing with feedback, including diverse solutions to programming problems and annotated natural language feedback that guides the solution of critical errors in erroneous source codes.",
        "dcterms:title": "COFFEE",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/Lune-Blue/COFFEE",
        "dcat:theme": [
            "Code Editing",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Code fixing",
            "Feedback generation",
            "Programming problems",
            "Natural language feedback"
        ],
        "dcat:landingPage": "https://github.com/Lune-Blue/COFFEE",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Editing"
        ]
    },
    {
        "dcterms:creator": [
            "Niklas Muennighoff",
            "Qian Liu",
            "Armel Zebaze",
            "Qinkai Zheng",
            "Binyuan Hui",
            "Terry Yue Zhuo",
            "Swayam Singh",
            "Xiangru Tang",
            "Leandro von Werra",
            "Shayne Longpre"
        ],
        "dcterms:description": "HumanEvalFix is a dataset designed to check whether models are capable of fixing various types of errors in code, consisting of 164 samples with human-authored bugs across six different programming languages.",
        "dcterms:title": "HumanEvalFix",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Editing"
        ],
        "dcat:keyword": [
            "Error fixing",
            "Code evaluation",
            "Programming languages"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Editing"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Steven Basart",
            "Saurav Kadavath",
            "Mantas Mazeika",
            "Akul Arora",
            "Ethan Guo",
            "Collin Burns",
            "Samir Puranik",
            "Horace He",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "HumanEvalSynthesize is a benchmark designed to assess code synthesis models, prompting them to synthesize correct codes given a natural language description.",
        "dcterms:title": "HumanEvalSynthesize",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Code synthesis",
            "Natural language processing",
            "Programming tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Steven Basart",
            "Saurav Kadavath",
            "Mantas Mazeika",
            "Akul Arora",
            "Ethan Guo",
            "Collin Burns",
            "Samir Puranik",
            "Horace He",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "APPS consists of 10,000 coding problems along with 232,421 human-authored golden solutions, designed to evaluate the performance of code generation models.",
        "dcterms:title": "APPS",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Coding problems",
            "Human solutions",
            "Programming challenges"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Austin",
            "Augustus Odena",
            "Maxwell Nye",
            "Maarten Bosma",
            "Henryk Michalewski",
            "David Dohan",
            "Ellen Jiang",
            "Carrie Cai",
            "Michael Terry",
            "Quoc Le"
        ],
        "dcterms:description": "MBPP is a benchmark designed to address entry-level programming problems in Python, consisting of 974 samples across various domains.",
        "dcterms:title": "MBPP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation"
        ],
        "dcat:keyword": [
            "Entry-level programming",
            "Python problems",
            "Programming fundamentals"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Code Generation"
        ]
    }
]