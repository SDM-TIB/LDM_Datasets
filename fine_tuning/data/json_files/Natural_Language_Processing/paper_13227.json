[
    {
        "dcterms:creator": [
            "Alon Talmor",
            "Jonathan Herzig",
            "Nicholas Lourie",
            "Jonathan Berant"
        ],
        "dcterms:description": "A commonsense multiple-choice question answering dataset, where each question is accompanied by 5 options.",
        "dcterms:title": "CSQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1811.00937",
        "dcat:theme": [
            "Natural Language Processing",
            "Commonsense Reasoning"
        ],
        "dcat:keyword": [
            "Commonsense QA",
            "Multiple-choice questions",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Dan Hendrycks",
            "Collin Burns",
            "Steven Basart",
            "Andy Zou",
            "Mantas Mazeika",
            "Dawn Song",
            "Jacob Steinhardt"
        ],
        "dcterms:description": "A benchmark that includes multiple-choice questions from various domains such as Abstract Algebra, High School Chemistry, and Professional Law.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2009.03300",
        "dcat:theme": [
            "Natural Language Processing",
            "Multitask Learning"
        ],
        "dcat:keyword": [
            "Multiple-choice questions",
            "Benchmark",
            "Domain diversity"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adri√† Garriga-Alonso"
        ],
        "dcterms:description": "A benchmark that offers multiple-choice questions with 3 options for each question, focusing on logical deduction.",
        "dcterms:title": "Big-Bench (Logical Deduction)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2206.04615",
        "dcat:theme": [
            "Natural Language Processing",
            "Logical Reasoning"
        ],
        "dcat:keyword": [
            "Logical deduction",
            "Multiple-choice questions",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    }
]