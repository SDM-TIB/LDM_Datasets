[
    {
        "dcterms:creator": [
            "Andy Zou",
            "Zifan Wang",
            "J Zico Kolter",
            "Matt Fredrikson"
        ],
        "dcterms:description": "A dataset containing adversarial prompts designed to test the robustness of aligned language models against various attack methods.",
        "dcterms:title": "AdvBench",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2307.15043",
        "dcat:theme": [
            "Adversarial Attacks",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Adversarial prompts",
            "Language models",
            "Robustness testing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Adversarial Testing"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A dataset used in the Trojan Detection Competition 2023 to analyze harmful prompts and responses.",
        "dcterms:title": "Trojan Detection Competition 2023",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Adversarial Attacks",
            "Model Safety"
        ],
        "dcat:keyword": [
            "Trojan detection",
            "Harmful prompts",
            "Language models"
        ],
        "dcat:landingPage": "https://trojandetection.ai/",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Detection"
        ]
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric Xing"
        ],
        "dcterms:description": "A benchmark for evaluating the performance of language models in various tasks, including their ability to judge responses.",
        "dcterms:title": "MT-bench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language models",
            "Evaluation tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Bill Yuchen Lin",
            "Abhilasha Ravichander",
            "Ximing Lu",
            "Nouha Dziri",
            "Melanie Sclar",
            "Khyathi Chandu",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "dcterms:description": "A dataset aimed at evaluating the alignment of language models through in-context learning.",
        "dcterms:title": "Just-Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2312.01552",
        "dcat:theme": [
            "Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Alignment",
            "Language models",
            "In-context learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    }
]