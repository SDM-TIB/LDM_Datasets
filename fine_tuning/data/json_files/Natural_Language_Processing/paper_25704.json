[
    {
        "dcterms:creator": [
            "Mark Chen",
            "Jerry Tworek",
            "Heewoo Jun",
            "Qiming Yuan",
            "Henrique Ponde de Oliveira Pinto",
            "Jared Kaplan",
            "Harri Edwards",
            "Yuri Burda",
            "Nicholas Joseph",
            "Greg Brockman"
        ],
        "dcterms:description": "A benchmark for evaluating the performance of large language models on coding tasks, focusing on function completion and simple Python problems.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2107.03374",
        "dcat:theme": [
            "Code Evaluation",
            "Programming"
        ],
        "dcat:keyword": [
            "Code generation",
            "Python",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Function completion",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Jacob Austin",
            "Augustus Odena",
            "Maxwell Nye",
            "Maarten Bosma",
            "Henryk Michalewski",
            "David Dohan",
            "Ellen Jiang",
            "Carrie Cai",
            "Michael Terry",
            "Quoc Le"
        ],
        "dcterms:description": "A dataset for program synthesis using large language models, focusing on generating code from natural language descriptions.",
        "dcterms:title": "MBPP",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2108.07732",
        "dcat:theme": [
            "Code Generation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Program synthesis",
            "Code generation",
            "Natural language"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation",
            "Natural language understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Yuhang Lai",
            "Chengxi Li",
            "Yiming Wang",
            "Tianyi Zhang",
            "Ruiqi Zhong",
            "Luke Zettlemoyer",
            "Wen-tau Yih",
            "Daniel Fried",
            "Sida Wang",
            "Tao Yu"
        ],
        "dcterms:description": "A natural and reliable benchmark for data science code generation, focusing on various data science tasks.",
        "dcterms:title": "DS-1000",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Data Science",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Data science",
            "Code generation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Data science tasks",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Bowen Li",
            "Wenhan Wu",
            "Ziwei Tang",
            "Lin Shi",
            "John Yang",
            "Jinyang Li",
            "Shunyu Yao",
            "Chen Qian",
            "Binyuan Hui",
            "Qicheng Zhang"
        ],
        "dcterms:description": "A comprehensive benchmark for software development, assessing various aspects of coding tasks.",
        "dcterms:title": "DevBench",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2403.08604",
        "dcat:theme": [
            "Software Development",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Software development",
            "Benchmark",
            "Coding tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Software development",
            "Code evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Carlos E Jimenez",
            "John Yang",
            "Alexander Wettig",
            "Shunyu Yao",
            "Kexin Pei",
            "Ofir Press",
            "Karthik Narasimhan"
        ],
        "dcterms:description": "A benchmark to evaluate language models' ability to resolve real-world GitHub issues.",
        "dcterms:title": "SWE-Bench",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2310.06770",
        "dcat:theme": [
            "Software Engineering",
            "Code Evaluation"
        ],
        "dcat:keyword": [
            "GitHub issues",
            "Software engineering",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Issue resolution",
            "Code evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Qinkai Zheng",
            "Xiao Xia",
            "Xu Zou",
            "Yuxiao Dong",
            "Shan Wang",
            "Yufei Xue",
            "Zihan Wang",
            "Lei Shen",
            "Andi Wang",
            "Yang Li"
        ],
        "dcterms:description": "A dataset that integrates code generation with execution and refinement, focusing on providing feedback for code.",
        "dcterms:title": "CodeFeedback",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2402.14658",
        "dcat:theme": [
            "Code Generation",
            "Feedback Mechanism"
        ],
        "dcat:keyword": [
            "Code feedback",
            "Code generation",
            "Execution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation",
            "Feedback integration"
        ]
    },
    {
        "dcterms:creator": [
            "Xingyao Wang",
            "Zihan Wang",
            "Jiateng Liu",
            "Yangyi Chen",
            "Lifan Yuan",
            "Hao Peng",
            "Heng Ji"
        ],
        "dcterms:description": "A dataset designed to improve LLM's abilities in multi-turn tasks through executable code actions.",
        "dcterms:title": "CodeActInstruct",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2402.01030",
        "dcat:theme": [
            "Multi-turn Interaction",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Multi-turn tasks",
            "Executable code",
            "Instruction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Multi-turn interaction",
            "Code generation"
        ]
    },
    {
        "dcterms:creator": [
            "Ning Ding",
            "Yulin Chen",
            "Bokai Xu",
            "Yujia Qin",
            "Zhi Zheng",
            "Shengding Hu",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Bowen Zhou"
        ],
        "dcterms:description": "A large-scale dataset of high-quality multi-turn instructional dialogues aimed at enhancing conversational models.",
        "dcterms:title": "UltraChat",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2305.14233",
        "dcat:theme": [
            "Conversational AI",
            "Instructional Dialogue"
        ],
        "dcat:keyword": [
            "Multi-turn dialogue",
            "Conversational models",
            "Instruction"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversational interaction",
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Anton Lozhkov",
            "Raymond Li",
            "Loubna Ben Allal",
            "Federico Cassano",
            "Joel Lamy-Poirier",
            "Nouamane Tazi",
            "Ao Tang",
            "Dmytro Pykhtar",
            "Jiawei Liu",
            "Yuxiang Wei"
        ],
        "dcterms:description": "A code-rich corpus of Jupyter notebooks designed to enhance code generation capabilities.",
        "dcterms:title": "The-stack-v2",
        "dcterms:issued": "2024",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Jupyter Notebooks"
        ],
        "dcat:keyword": [
            "Code corpus",
            "Jupyter notebooks",
            "Code generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation",
            "Notebook execution"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "A synthesized dataset created by the authors to enhance the model's performance on homologous tasks.",
        "dcterms:title": "PyInstruct",
        "dcterms:issued": "",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Homologous Data",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Homologous dataset",
            "Code generation",
            "Task synthesis"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation",
            "Task synthesis"
        ]
    }
]