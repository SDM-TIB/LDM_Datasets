[
    {
        "dcterms:creator": [
            "Yizhong Wang",
            "Yeganeh Kordi",
            "Swaroop Mishra",
            "Alisa Liu",
            "Noah A. Smith",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "dcterms:description": "Self-instruct consists of 52,000 training and 252 testing instructions, generated from seed tasks using InstructGPT to create diverse inputs and outputs.",
        "dcterms:title": "Self-Instruct",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Natural language processing",
            "Self-generated instructions"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "dcterms:description": "Alpaca consists of 52,002 samples generated using text-davinci-003 to fine-tune LLaMA for instruction-following capabilities.",
        "dcterms:title": "Alpaca",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford-alpaca",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Fine-tuning",
            "LLaMA"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford-alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Can Xu",
            "Qingfeng Sun",
            "Kai Zheng",
            "Xiubo Geng",
            "Pu Zhao",
            "Jiazhan Feng",
            "Chongyang Tao",
            "Daxin Jiang"
        ],
        "dcterms:description": "WizardLM consists of 250,000 samples generated using evolutionary algorithms to create complex and diverse high-quality instruction data.",
        "dcterms:title": "WizardLM",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/2304.12244",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Evolutionary algorithms",
            "Complex instructions"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2304.12244",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Chunting Zhou",
            "Pengfei Liu",
            "Puxin Xu",
            "Srini Iyer",
            "Jiao Sun",
            "Yuning Mao",
            "Xuezhe Ma",
            "Avia Efrat",
            "Ping Yu",
            "LILI YU",
            "Susan Zhang",
            "Gargi Ghosh",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Omer Levy"
        ],
        "dcterms:description": "LIMA consists of 1,000 training samples, 300 test samples, and 50 development samples, meticulously compiled for high-quality instruction following.",
        "dcterms:title": "LIMA",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "High-quality data",
            "Manual selection"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Mike Conover",
            "Matt Hayes",
            "Ankit Mathur",
            "Jianwei Xie",
            "Jun Wan",
            "Sam Shah",
            "Ali Ghodsi",
            "Patrick Wendell",
            "Matei Zaharia",
            "Reynold Xin"
        ],
        "dcterms:description": "Dolly-v2 consists of 15,000 instructions across various tasks like brainstorming, classification, QA, and summarization, authored manually.",
        "dcterms:title": "Dolly-v2",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Manual authorship",
            "Various tasks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Victor Sanh",
            "Albert Webson",
            "Colin Raffel",
            "Stephen H. Bach",
            "Lintang Sutawika",
            "Zaid Alyafeai",
            "Antoine Chaffin",
            "Arnaud Stiegler",
            "Arun Raja",
            "Manan Dey",
            "M Saiful Bari",
            "Canwen Xu",
            "Urmish Thakker",
            "Shanya Sharma Sharma",
            "Eliza Szczechla",
            "Taewoon Kim",
            "Gunjan Chhablani",
            "Nihal V. Nayak",
            "Debajyoti Datta",
            "Jonathan Chang",
            "Mike Tian-Jian Jiang",
            "Han Wang",
            "Matteo Manica",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Harshit Pandey",
            "Rachel Bawden",
            "Thomas Wang",
            "Trishala Neeraj",
            "Jos Rozen",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Thibault FÃ©vr",
            "Jason Alan Fries",
            "Ryan Teehan",
            "Teven Le Scao",
            "Stella Biderman",
            "Leo Gao",
            "Thomas Wolf",
            "Alexander M. Rush"
        ],
        "dcterms:description": "P3 integrates 170 NLP datasets with 2,052 prompts, converting conventional NLP tasks into natural language input-output pairs.",
        "dcterms:title": "P3",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Instruction Tuning"
        ],
        "dcat:keyword": [
            "Multitask dataset",
            "NLP tasks",
            "Prompt templates"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    }
]