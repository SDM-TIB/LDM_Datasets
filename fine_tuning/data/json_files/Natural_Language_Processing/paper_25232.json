[
    {
        "dcterms:creator": [
            "H. Chen",
            "W. Xie",
            "A. Vedaldi",
            "A. Zisserman"
        ],
        "dcterms:description": "A large-scale audio-visual dataset containing videos curated to maximize the audio-visual correspondence while remaining unconstrained in the nature of their content.",
        "dcterms:title": "VGGSound",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio-Visual Data",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Audio-Visual Correspondence",
            "Video Clips",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Video-to-Audio Generation"
        ]
    },
    {
        "dcterms:creator": [
            "R. Kumar",
            "P. Seetharaman",
            "A. Luebs",
            "I. Kumar",
            "K. Kumar"
        ],
        "dcterms:description": "A dataset used for evaluating audio quality, semantic matching, and temporal alignment in the context of music synthesis.",
        "dcterms:title": "MUSIC",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Music Data",
            "Audio Generation"
        ],
        "dcat:keyword": [
            "Music Synthesis",
            "Audio Quality",
            "Temporal Alignment"
        ],
        "dcat:landingPage": "https://github.com/roudimit/MUSIC_dataset",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Audio Quality Evaluation",
            "Semantic Matching",
            "Temporal Alignment"
        ]
    },
    {
        "dcterms:creator": [
            "J. F. Gemmeke",
            "D. P. Ellis",
            "D. Freedman",
            "A. Jansen",
            "W. Lawrence",
            "R. C. Moore",
            "M. Plakal",
            "M. Ritter"
        ],
        "dcterms:description": "An ontology and human-labeled dataset for audio events, used for evaluating temporal alignment and audio-visual synchronization.",
        "dcterms:title": "AudioSet",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Audio Events",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Audio Events",
            "Human-Labeled Data",
            "Ontology"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Audio",
        "mls:task": [
            "Audio Event Detection",
            "Audio-Visual Synchronization"
        ]
    },
    {
        "dcterms:creator": [
            "W. Kay",
            "J. Carreira",
            "K. Simonyan",
            "B. Zhang",
            "C. Hillier",
            "S. Vijayanarasimhan",
            "F. Viola",
            "T. Green",
            "T. Back",
            "P. Natsev"
        ],
        "dcterms:description": "A dataset for human action recognition in videos, used for evaluating video activity recognition.",
        "dcterms:title": "Kinetics 400",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Video Data",
            "Human Action Recognition"
        ],
        "dcat:keyword": [
            "Human Actions",
            "Video Dataset",
            "Action Recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]