To extract datasets from the research paper titled "NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation" by Tim Meinhardt et al., I will follow a systematic approach to ensure I capture all relevant datasets and their citations accurately.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors provide new state-of-the-art results on both **YouTube-VIS (2019/2021)** and the **OVIS** benchmarks, suggesting that these are the datasets used in their experiments.

Next, I will delve into the **experiments section** (section 5) where the authors discuss the benchmarks in detail. Here, they explicitly mention:

1. **YouTube-VIS 2019**: This dataset provides instance ground truth masks for 40 object categories across a total of 2883 sequences.
2. **YouTube-VIS 2021**: Similar to the 2019 version, this dataset includes 3859 sequences and also provides instance ground truth masks for the same 40 object categories.
3. **OVIS (Occluded Video Instance Segmentation)**: This benchmark includes significantly longer sequences and is noted as the most challenging public VIS benchmark, labeling 25 object categories.

In the **references section**, I will look for the full citations of these datasets. The citations for the datasets are as follows:

- For **YouTube-VIS 2019**, the citation is:
  > Yang, L., Fan, Y., & Xu, N. (2019). Video instance segmentation. In *Proceedings of the International Conference on Computer Vision (ICCV)*.

- For **YouTube-VIS 2021**, the citation is the same as for 2019 since it is an updated version of the same dataset:
  > Yang, L., Fan, Y., & Xu, N. (2019). Video instance segmentation. In *Proceedings of the International Conference on Computer Vision (ICCV)*.

- For **OVIS**, the citation is:
  > Qi, J., Gao, Y., Hu, Y., Wang, X., Liu, X., Bai, X., Belongie, S., Yuille, A., Torr, P., & Bai, S. (2022). Occluded video instance segmentation: A benchmark. In *International Journal of Computer Vision*.

Now, I will compile the dataset entries with their full citations, ensuring that I maintain clarity and accuracy in the descriptions. This will prepare the information for structured output or further processing.