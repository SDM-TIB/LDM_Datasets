[
    {
        "dcterms:creator": [
            "Stanislaw Antol",
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Margaret Mitchell",
            "Dhruv Batra",
            "C Lawrence Zitnick",
            "Devi Parikh"
        ],
        "dcterms:description": "A dataset for Visual Question Answering containing 1.1 million visual-question-answer triplets, with each image having at least 3 questions and 10 answers per question.",
        "dcterms:title": "VQA v2",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Image-Question pairs",
            "Multi-modal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Danna Gurari",
            "Qing Li",
            "Abigale J Stangl",
            "Anhong Guo",
            "Chi Lin",
            "Kristen Grauman",
            "Jiebo Luo",
            "Jeffrey P Bigham"
        ],
        "dcterms:description": "A dataset designed to answer visual questions posed by blind individuals, containing 19,425 train image/question pairs.",
        "dcterms:title": "VizWiz VQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Accessibility"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Blind Accessibility",
            "Image-Question pairs"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "dcterms:description": "A re-ordering of the VQA v2 dataset, containing 438K train image/question pairs, designed to elevate the role of image understanding in visual question answering.",
        "dcterms:title": "VQA-CP2",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Visual Question Answering",
            "Image-Question pairs",
            "Multi-modal learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Amir Shahroudy",
            "Jun Liu",
            "Tian-Tsong Ng",
            "Gang Wang"
        ],
        "dcterms:description": "A large-scale dataset for 3D human activity analysis, containing various actions captured in RGB+D format.",
        "dcterms:title": "NTU RGB+D",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Action Recognition",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "3D Human Activity",
            "RGB+D Data",
            "Action Recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    }
]