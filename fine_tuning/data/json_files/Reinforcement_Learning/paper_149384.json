[
    {
        "dcterms:creator": [
            "Y. Goyal",
            "T. Khot",
            "D. Summers-Stay",
            "D. Batra",
            "D. Parikh"
        ],
        "dcterms:description": "The VQA 2.0 dataset is used for training a VQA model capable of answering questions about images in textual format. It includes a variety of image-question pairs to evaluate the model's performance.",
        "dcterms:title": "VQA 2.0 dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Computer Vision"
        ],
        "dcat:keyword": [
            "Image-Question pairs",
            "VQA",
            "Deep Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Visual Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "A. Das",
            "H. Agrawal",
            "L. Zitnick",
            "D. Parikh",
            "D. Batra"
        ],
        "dcterms:description": "The VQA-HAT dataset provides human attention annotations for image-question pairs, allowing for the analysis of how human attention aligns with model attention in visual question answering.",
        "dcterms:title": "VQA-HAT dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Attention Mechanisms"
        ],
        "dcat:keyword": [
            "Human Attention",
            "Attention Maps",
            "VQA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Attention Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "A. Das",
            "H. Agrawal",
            "L. Zitnick",
            "D. Parikh",
            "D. Batra"
        ],
        "dcterms:description": "The Human attention dataset provides annotations of human attention for image-question pairs, which is used to compare human attention with model attention in visual question answering tasks.",
        "dcterms:title": "Human attention dataset",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Question Answering",
            "Attention Mechanisms"
        ],
        "dcat:keyword": [
            "Human Attention",
            "Attention Maps",
            "VQA"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Visual Question Answering",
            "Attention Analysis"
        ]
    }
]