[
    {
        "dcterms:creator": [
            "Daniel M Ziegler",
            "Nisan Stiennon",
            "Jeffrey Wu",
            "Tom B Brown",
            "Alec Radford",
            "Dario Amodei",
            "Paul Christiano",
            "Geoffrey Irving"
        ],
        "dcterms:description": "A dataset used for preference learning, where pairs of generations are collected under the same context, and human preferences are indicated to optimize generating policy.",
        "dcterms:title": "Preference Data",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1909.08593",
        "dcat:theme": [
            "Preference Learning",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Preference dataset",
            "Human feedback",
            "Pairwise comparison"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Jason Wei",
            "Maarten Bosma",
            "Vincent Y Zhao",
            "Kelvin Guu",
            "Adams Wei Yu",
            "Brian Lester",
            "Nan Du",
            "Andrew M Dai",
            "Quoc V Le"
        ],
        "dcterms:description": "A dataset used for pretraining language models, which serves as a foundation for fine-tuning and instruction following.",
        "dcterms:title": "Pretraining Dataset",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2109.01652",
        "dcat:theme": [
            "Language Modeling",
            "Pretraining"
        ],
        "dcat:keyword": [
            "Pretraining dataset",
            "Language models",
            "Fine-tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pretraining"
        ]
    },
    {
        "dcterms:creator": [
            "Hyung Won Chung",
            "Le Hou",
            "Shayne Longpre",
            "Barret Zoph",
            "Yi Tay",
            "William Fedus",
            "Yunxuan Li",
            "Xuezhi Wang",
            "Mostafa Dehghani",
            "Siddhartha Brahma"
        ],
        "dcterms:description": "A dataset used for instruction fine-tuning of language models, aimed at improving their performance on various tasks.",
        "dcterms:title": "SFT Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.11416",
        "dcat:theme": [
            "Instruction Fine-tuning",
            "Language Modeling"
        ],
        "dcat:keyword": [
            "Instruction dataset",
            "Fine-tuning",
            "Language models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction Following"
        ]
    },
    {
        "dcterms:creator": [
            "Rafael Rafailov",
            "Archit Sharma",
            "Eric Mitchell",
            "Stefano Ermon",
            "Christopher D Manning",
            "Chelsea Finn"
        ],
        "dcterms:description": "A dataset used for direct preference optimization, focusing on optimizing generating policy without the need for a reward model.",
        "dcterms:title": "Dref",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.18290",
        "dcat:theme": [
            "Direct Preference Optimization",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Direct preference optimization",
            "Reward model",
            "Policy optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Daniel M Ziegler",
            "Nisan Stiennon",
            "Jeffrey Wu",
            "Tom B Brown",
            "Alec Radford",
            "Dario Amodei",
            "Paul Christiano",
            "Geoffrey Irving"
        ],
        "dcterms:description": "A dataset used for preference learning, similar to the Preference Data, focusing on human preferences for optimizing language models.",
        "dcterms:title": "Dp (Preference Dataset)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1909.08593",
        "dcat:theme": [
            "Preference Learning",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Preference dataset",
            "Human feedback",
            "Pairwise comparison"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Jason Wei",
            "Maarten Bosma",
            "Vincent Y Zhao",
            "Kelvin Guu",
            "Adams Wei Yu",
            "Brian Lester",
            "Nan Du",
            "Andrew M Dai",
            "Quoc V Le"
        ],
        "dcterms:description": "A dataset used for pretraining language models, similar to the Pretraining Dataset, serving as a foundation for fine-tuning.",
        "dcterms:title": "Dpretrain",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2109.01652",
        "dcat:theme": [
            "Language Modeling",
            "Pretraining"
        ],
        "dcat:keyword": [
            "Pretraining dataset",
            "Language models",
            "Fine-tuning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Pretraining"
        ]
    }
]