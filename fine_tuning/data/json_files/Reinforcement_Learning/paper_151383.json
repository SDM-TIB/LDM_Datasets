[
    {
        "dcterms:creator": [
            "Y. Li",
            "D. Choi",
            "J. Chung",
            "N. Kushman",
            "J. Schrittwieser",
            "R. Leblond",
            "T. Eccles",
            "J. Keeling",
            "F. Gimeno",
            "A. Dal Lago"
        ],
        "dcterms:description": "A dataset used for competitive programming tasks, where models generate executable code to solve given problems, evaluated against test cases.",
        "dcterms:title": "CodeContest",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Code Generation",
            "Competitive Programming"
        ],
        "dcat:keyword": [
            "Code generation",
            "Programming tasks",
            "Test cases"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    },
    {
        "dcterms:creator": [
            "J. Dai",
            "X. Pan",
            "R. Sun",
            "J. Ji",
            "X. Xu",
            "M. Liu",
            "Y. Wang",
            "Y. Yang"
        ],
        "dcterms:description": "A dataset containing preference pairs for evaluating the helpfulness and safety of AI assistant responses.",
        "dcterms:title": "SafeRLHF",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2310.12773",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Human feedback",
            "Safety",
            "Helpfulness"
        ],
        "dcat:landingPage": "https://github.com/PKU-Alignment/safe-rlhf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Bai",
            "A. Jones",
            "K. Ndousse",
            "A. Askell",
            "A. Chen",
            "N. DasSarma",
            "D. Drain",
            "S. Fort",
            "D. Ganguli",
            "T. Henighan"
        ],
        "dcterms:description": "A dataset consisting of human preferences on AI assistant responses, used to train models to be helpful and harmless.",
        "dcterms:title": "HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Human feedback",
            "Helpfulness",
            "Harmlessness"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks",
            "S. Basart",
            "S. Kadavath",
            "M. Mazeika",
            "A. Arora",
            "E. Guo",
            "C. Burns",
            "S. Puranik",
            "H. He",
            "D. Song"
        ],
        "dcterms:description": "A benchmark for measuring coding challenge competence, where models generate code based on problem descriptions.",
        "dcterms:title": "APPS",
        "dcterms:issued": "2021",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:2105.09938",
        "dcat:theme": [
            "Code Generation",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Coding challenges",
            "Code generation",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code Generation"
        ]
    }
]