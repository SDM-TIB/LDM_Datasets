To extract datasets from the research paper titled "Speaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models" by Ahmed Hussen Abdelaziz et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experimental results sections** to identify any datasets mentioned. The abstract indicates that the authors trained an acoustic model on "ten thousand hours of audio-only data" and adapted it using "ninety hours of synchronized audio-visual speech." This suggests that there are at least two datasets involved.

Next, I will focus on **section 4 (Experimental Results)**, specifically **subsection 4.1 (Datasets)**, where the authors provide detailed information about the datasets used:

1. **In-house corpus for acoustic model training**: This dataset contains around ten thousand hours of query utterances for a digital assistant. It is not explicitly named, but it is crucial for understanding the training context.

2. **Evaluation dataset**: A separate dataset of 9 hours, consisting of almost seven thousand utterances, is used to evaluate the model. Again, this dataset is not named, but it is important for the evaluation process.

3. **Multimodal dataset for visual speech synthesis**: This dataset contains around ninety hours of multimodal data, with each utterance having synchronized audio, RGB video, and depth streams. The authors mention that the subjects are balanced demographically and that the dataset is divided into training, validation, and testing sets.

Now, I will check the **References section** to find full citations for any datasets mentioned. However, since the in-house corpus and evaluation dataset do not have specific citations, I will note that they are proprietary datasets developed by the authors.

For the multimodal dataset, the authors do not provide a specific citation either, but I will summarize its characteristics as follows:

- **In-house corpus for acoustic model training**: 
  - Description: Contains around ten thousand hours of query utterances for a digital assistant.
  - Citation: Not available (proprietary dataset).

- **Evaluation dataset**: 
  - Description: A separate dataset of 9 hours, almost seven thousand utterances, used for model evaluation.
  - Citation: Not available (proprietary dataset).

- **Multimodal dataset for visual speech synthesis**: 
  - Description: Contains around ninety hours of multimodal data with synchronized audio, RGB video, and depth streams, divided into training (75 hours), validation (11 hours), and testing (4 hours).
  - Citation: Not available (proprietary dataset).

After gathering all this information, I will compile the dataset entries into a structured format that highlights the key details and acknowledges the proprietary nature of the datasets without specific citations.