To extract datasets from the research paper titled "Feedback-Based Tree Search for Reinforcement Learning" by Daniel R. Jiang et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract, introduction, and experimental setup sections** to see if any datasets are mentioned. The abstract provides a brief overview of the research but may not contain specific dataset details. The introduction often contextualizes the research and may reference datasets used in prior work.

Next, I will focus on the **experimental setup section** where the authors describe the implementation details. This section typically includes information about the datasets used for training and testing the models. In this paper, the authors mention training five agents using the game "King of Glory," which suggests that there may be a dataset related to gameplay data.

I will also look for any **tables or figures** that might summarize the datasets or provide additional context about the data used in the experiments. The paper mentions a dataset of approximately 100,000 state/action pairs of human gameplay data, which is crucial information.

Now, I will check the **References section** to find full citations for any datasets mentioned. The paper does not explicitly provide citations for the datasets, but I will note the relevant papers that describe the methodologies or datasets used in the context of the experiments.

1. **King of Glory Dataset**: The authors refer to a dataset of approximately 100,000 state/action pairs of human gameplay data. However, they do not provide a formal citation for this dataset, as it appears to be derived from gameplay rather than a publicly available dataset.

2. **MCTS and RL Techniques**: The paper references several foundational works in reinforcement learning and Monte Carlo Tree Search (MCTS) that may have associated datasets or methodologies. For example, the work by Guo et al. (2014) on using MCTS for Atari games could be relevant, but again, specific datasets are not cited.

In summary, while the paper discusses the use of gameplay data and references other works, it does not provide formal citations for specific datasets. I will document the findings as follows:

- **King of Glory Gameplay Data**: Approximately 100,000 state/action pairs of human gameplay data (no formal citation provided).

- **Guo et al. (2014)**: 
  > Guo, Xiaoxiao, Singh, Satinder, Lee, Honglak, Lewis, Richard L, and Wang, Xiaoshi. *Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning*. In Advances in Neural Information Processing Systems, pp. 3338â€“3346, 2014.

Now, I will compile the dataset entries into a structured format for clarity and future reference.