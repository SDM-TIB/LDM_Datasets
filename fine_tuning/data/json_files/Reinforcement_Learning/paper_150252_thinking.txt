To extract datasets from the research paper titled "Reconstructing Humpty Dumpty: Multi-feature Graph Autoencoder for Open Set Action Recognition" by Dawei Du et al., I will follow a systematic approach.

First, I will read through the **abstract** to identify any datasets mentioned. The abstract indicates that the authors performed experiments on two publicly available action recognition datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will examine the **introduction** section. Here, the authors mention the datasets **HMDB-51** and **UCF-101** as benchmarks for action recognition. This is a strong indication that these datasets are central to their research.

I will then look at the **experiments** section, specifically where the authors describe their methodology and the datasets used. In this section, they confirm that they used **HMDB-51** and **UCF-101** for their experiments, providing additional context about how these datasets were utilized in their research.

Now, I will gather detailed descriptions of each dataset from the **experiments** section:

1. **HMDB-51**: This dataset consists of 6,766 manually annotated videos across 51 action classes, with each class containing at least 101 clips. The videos are sourced from digitized movies and YouTube.

2. **UCF-101**: This dataset contains 13,320 clips across 101 action classes, with clip lengths varying from 1.06 seconds to 71.04 seconds. The videos are collected from user-uploaded content on the Internet.

Next, I will check the **References** section to find the full citations for these datasets. The citations are as follows:

- For **HMDB-51**, the citation is:
  > Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso A. Poggio, and Thomas Serre. *HMDB: A large video database for human motion recognition*. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2556–2563, 2011.

- For **UCF-101**, the citation is:
  > Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. *UCF101: A dataset of 101 human actions classes from videos in the wild*. CoRR, abs/1212.0402, 2012.

Finally, I will compile the dataset entries, ensuring that each dataset is accurately described and properly cited. This will provide a comprehensive overview of the datasets used in the research paper.