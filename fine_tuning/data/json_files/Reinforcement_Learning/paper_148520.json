[
    {
        "dcterms:creator": [
            "Greg Brockman",
            "Vicki Cheung",
            "Ludwig Pettersson",
            "Jonas Schneider",
            "John Schulman",
            "Jie Tang",
            "Wojciech Zaremba"
        ],
        "dcterms:description": "A toolkit for developing and comparing reinforcement learning algorithms, providing a variety of environments and tasks.",
        "dcterms:title": "OpenAI Gym",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1606.01540",
        "dcat:theme": [
            "Reinforcement Learning",
            "Simulation"
        ],
        "dcat:keyword": [
            "Reinforcement learning",
            "Environment",
            "Benchmark"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "E Todorov",
            "T Erez",
            "Y Tassa"
        ],
        "dcterms:description": "A physics engine designed for model-based control, enabling the simulation of complex robotic tasks.",
        "dcterms:title": "Mujoco",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "10.1109/IROS.2012.6386109",
        "dcat:theme": [
            "Physics Simulation",
            "Robotics"
        ],
        "dcat:keyword": [
            "Physics engine",
            "Robotics simulation",
            "Model-based control"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tuomas Haarnoja",
            "Aurick Zhou",
            "Pieter Abbeel",
            "Sergey Levine"
        ],
        "dcterms:description": "An off-policy maximum entropy deep reinforcement learning algorithm that utilizes a stochastic actor.",
        "dcterms:title": "Soft Actor-Critic (SAC)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1801.01290",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Off-policy learning",
            "Maximum entropy",
            "Deep reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Scott Fujimoto",
            "Herke van Hoof",
            "David Meger"
        ],
        "dcterms:description": "An algorithm that addresses function approximation error in actor-critic methods, improving stability and performance.",
        "dcterms:title": "TD3 (Twin Delayed Deep Deterministic policy gradient)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1802.09477",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Actor-Critic",
            "Deterministic policy gradient",
            "Function approximation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tom Schaul",
            "John Quan",
            "Ioannis Antonoglou",
            "David Silver"
        ],
        "dcterms:description": "A method that improves the efficiency of experience replay by prioritizing experiences based on their importance.",
        "dcterms:title": "Prioritized Experience Replay (PER)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1511.05952",
        "dcat:theme": [
            "Reinforcement Learning",
            "Experience Replay"
        ],
        "dcat:keyword": [
            "Experience replay",
            "Prioritization",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Che Wang",
            "Keith Ross"
        ],
        "dcterms:description": "An enhancement to the Soft Actor-Critic algorithm that emphasizes recent experiences while retaining older ones.",
        "dcterms:title": "Emphasizing Recent Experience (ERE)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1906.04009",
        "dcat:theme": [
            "Reinforcement Learning",
            "Experience Replay"
        ],
        "dcat:keyword": [
            "Experience replay",
            "Soft Actor-Critic",
            "Recent experience"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Marcin Andrychowicz",
            "Filip Wolski",
            "Alex Ray",
            "Jonas Schneider",
            "Rachel Fong",
            "Peter Welinder",
            "Bob McGrew",
            "Josh Tobin",
            "OpenAI",
            "Pieter Abbeel",
            "Wojciech Zaremba"
        ],
        "dcterms:description": "A technique that allows agents to learn from past experiences by considering what could have happened instead of what actually happened.",
        "dcterms:title": "Hindsight Experience Replay",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Experience Replay"
        ],
        "dcat:keyword": [
            "Hindsight learning",
            "Experience replay",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Marc Bellemare",
            "Sriram Srinivasan",
            "Georg Ostrovski",
            "Tom Schaul",
            "David Saxton",
            "Remi Munos"
        ],
        "dcterms:description": "A method that combines count-based exploration with intrinsic motivation to enhance exploration in reinforcement learning.",
        "dcterms:title": "Count-Based Exploration",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Exploration"
        ],
        "dcat:keyword": [
            "Count-based exploration",
            "Intrinsic motivation",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]