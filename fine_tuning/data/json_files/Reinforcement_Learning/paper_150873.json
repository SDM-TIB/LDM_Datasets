[
    {
        "dcterms:creator": [
            "R. Taori",
            "I. Gulrajani",
            "T. Zhang",
            "Y. Dubois",
            "X. Li",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "A dataset containing 52,000 instructions covering a range of commands and corresponding demonstrations generated by OpenAIâ€™s text-davinci-003, used for training proxy reward models.",
        "dcterms:title": "Alpaca Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcat:theme": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Instruction-following",
            "Language models",
            "Human feedback"
        ],
        "dcat:landingPage": "https://github.com/tatsu-lab/stanford_alpaca",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following",
            "Reward modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Dubois",
            "X. Li",
            "R. Taori",
            "T. Zhang",
            "I. Gulrajani",
            "J. Ba",
            "C. Guestrin",
            "P. Liang",
            "T. B. Hashimoto"
        ],
        "dcterms:description": "A simulation framework for methods that learn from human feedback, providing splits for different RLHF stages and human preference annotations.",
        "dcterms:title": "AlpacaFarm Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2305.14387",
        "dcat:theme": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Human feedback",
            "Preference learning",
            "Simulation framework"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2305.14387",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference modeling",
            "Reward modeling"
        ]
    }
]