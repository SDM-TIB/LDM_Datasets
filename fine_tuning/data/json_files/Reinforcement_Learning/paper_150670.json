[
    {
        "dcterms:creator": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "dcterms:description": "A curated dataset of harmful prompts used to evaluate GPT-4, consisting of 32 prompts including 16 examples from the GPT-4 technical report and 16 adapted from red-teaming efforts.",
        "dcterms:title": "Curated Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Safety Evaluation",
            "Harmful Content"
        ],
        "dcat:keyword": [
            "Harmful prompts",
            "Safety evaluation",
            "Red-teaming"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "dcterms:description": "A synthetic dataset of harmful prompts generated by GPT-4, consisting of 317 prompts filtered to ensure they elicit restricted behavior.",
        "dcterms:title": "Synthetic Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Safety Evaluation",
            "Harmful Content"
        ],
        "dcat:keyword": [
            "Synthetic prompts",
            "Harmful content",
            "Zero-shot reasoning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Safety Evaluation"
        ]
    }
]