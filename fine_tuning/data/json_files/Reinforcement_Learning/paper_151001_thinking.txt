To extract the datasets from the research paper titled "Modular Blended Attention Network for Video Question Answering" by Mingjie Zhou, I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will start by examining the **abstract** and **introduction** sections. In the abstract, the author mentions conducting experiments on "three commonly used datasets," which indicates that specific datasets will be discussed later in the paper. This prompts me to look for the names of these datasets.

Next, I will navigate to **section 4.1 (Datasets)**, where the author explicitly lists the datasets used in the experiments. Here, I will find detailed descriptions of each dataset, including their characteristics and the number of question-answer pairs they contain. The datasets mentioned are:

1. **TGIF-QA**: This dataset contains 165,000 QA pairs for animated GIFs and categorizes questions into four types, including counting and identifying actions.

2. **MSVD-QA**: This dataset consists of 1,970 video clips and 50,505 question-answer pairs, with questions categorized into five types.

3. **MSRVTT-QA**: This dataset includes 10,000 videos and 243,000 question-answer pairs, with similar question types to MSVD-QA but with more complex video content.

After identifying the datasets, I will check the **References section** to find the full citations for each dataset. The citations are crucial for proper attribution and will be formatted as follows:

- For **TGIF-QA**, the citation is:
  > Y. Jang, Y. Song, C. D. Kim, Y. Yu, Y. Kim, and G. Kim. "Video question answering with spatio-temporal reasoning." *International Journal of Computer Vision*, vol. 127, no. 10, pp. 1385–1412, 2019.

- For **MSVD-QA**, the citation is:
  > D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. "Video question answering via gradually refined attention over appearance and motion." In *Proceedings of the 25th ACM International Conference on Multimedia*, pp. 1645–1653, 2017.

- For **MSRVTT-QA**, the citation is:
  > D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. "Video question answering via gradually refined attention over appearance and motion." In *Proceedings of the 25th ACM International Conference on Multimedia*, pp. 1645–1653, 2017.

Now that I have gathered the dataset names and their respective citations, I will compile this information into a structured format for further processing or review. This ensures that I have accurately captured all necessary details regarding the datasets used in the research paper.