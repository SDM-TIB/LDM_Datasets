[
    {
        "dcterms:creator": [
            "D. Ganguli",
            "L. Lovitt",
            "J. Kernion",
            "A. Askell",
            "Y. Bai",
            "S. Kadavath",
            "B. Mann",
            "E. Perez",
            "N. Schiefer",
            "K. Ndousse",
            "A. Jones",
            "S. Bowman",
            "A. Chen",
            "T. Conerly",
            "N. DasSarma",
            "D. Drain",
            "N. Elhage",
            "S. El-Showk",
            "S. Fort",
            "Z. Hatfield-Dodds",
            "T. Henighan",
            "D. Hernandez",
            "T. Hume",
            "J. Jacobson",
            "S. Johnston",
            "K. Kravec",
            "C. Olsson",
            "S. Ringer",
            "E. Tran-Johnson",
            "D. Amodei",
            "T. Brown",
            "N. Joseph",
            "S. McCandlish",
            "C. Olah",
            "J. Kaplan",
            "J. Clark"
        ],
        "dcterms:description": "The dataset consists of 161k conversations between humans and AI assistants. Each instance comprises a pair of responses generated by a large, albeit undisclosed, language model, accompanied by a preference label indicating the response preferred by humans. The dataset is categorized into two subsets: the helpful subset and the harmless subset.",
        "dcterms:title": "Anthropic/HH-RLHF Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Conversations",
            "AI Assistants",
            "Preference Labels"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning",
            "Reward Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "N. Stiennon",
            "L. Ouyang",
            "J. Wu",
            "D. M. Ziegler",
            "R. Lowe",
            "C. Voss",
            "A. Radford",
            "D. Amodei",
            "P. Christiano"
        ],
        "dcterms:description": "It consists of Reddit posts along with two summaries for each post, with human preferences annotated. The dataset comprises 117k training samples and 13k validation samples.",
        "dcterms:title": "OpenAI/Summary Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Summarization",
            "Human Feedback"
        ],
        "dcat:keyword": [
            "Reddit Posts",
            "Summaries",
            "Human Preferences"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Summarization"
        ]
    },
    {
        "dcterms:creator": [
            "J. Dai",
            "X. Pan",
            "R. Sun",
            "J. Ji",
            "X. Xu",
            "M. Liu",
            "Y. Wang",
            "Y. Yang"
        ],
        "dcterms:description": "A preference dataset comprising 297k conversation comparisons, where each entry is linked to two types of labels: a preference label signifying human preference between two responses and a safety label indicating whether the chosen response adheres to safety standards.",
        "dcterms:title": "PKU/Safety Alignment Dataset",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Safety",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Preference Data",
            "Safety Labels",
            "Human Feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference Learning",
            "Safety Alignment"
        ]
    }
]