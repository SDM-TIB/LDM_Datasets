[
    {
        "dcterms:creator": [
            "Zhilin Yang",
            "Peng Qi",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "William W. Cohen",
            "Ruslan Salakhutdinov",
            "Christopher D. Manning"
        ],
        "dcterms:description": "A dataset for diverse, explainable multi-hop question answering, which includes questions that require reasoning over multiple documents.",
        "dcterms:title": "HOTPOTQA",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Multi-hop reasoning",
            "Question answering",
            "Explainable AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text, designed to evaluate reading comprehension systems.",
        "dcterms:title": "SQUAD",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Machine comprehension",
            "Reading comprehension",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "Adversarial examples derived from the SQUAD dataset to test the robustness of reading comprehension models.",
        "dcterms:title": "Adversarial SQUAD",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Adversarial examples",
            "Robustness testing",
            "Reading comprehension"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Matt Gardner",
            "Yoav Artzi",
            "Victoria Basmov",
            "Jonathan Berant",
            "Ben Bogin",
            "Sihao Chen",
            "Pradeep Dasigi",
            "Dheeru Dua",
            "Yanai Elazar",
            "Ananth Gottumukkala",
            "Nitish Gupta",
            "Hannaneh Hajishirzi",
            "Gabriel Ilharco",
            "Daniel Khashabi",
            "Kevin Lin",
            "Jiangming Liu",
            "Nelson F. Liu",
            "Phoebe Mulcaire",
            "Qiang Ning",
            "Sameer Singh",
            "Noah A. Smith",
            "Sanjay Subramanian",
            "Reut Tsarfaty",
            "Eric Wallace",
            "Ally Zhang",
            "Ben Zhou"
        ],
        "dcterms:description": "A dataset designed to evaluate models' local decision boundaries via contrast sets, which are sets of examples that are similar but differ in specific ways.",
        "dcterms:title": "Contrast Sets",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Contrast sets",
            "Model evaluation",
            "Decision boundaries"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Marco Tulio Ribeiro",
            "Tongshuang Wu",
            "Carlos Guestrin",
            "Sameer Singh"
        ],
        "dcterms:description": "A dataset for behavioral testing of NLP models, focusing on the evaluation of model behavior beyond accuracy.",
        "dcterms:title": "Checklists",
        "dcterms:issued": "2020",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Evaluation",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Behavioral testing",
            "Model evaluation",
            "NLP models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": []
    }
]