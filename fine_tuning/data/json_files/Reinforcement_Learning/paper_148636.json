[
    {
        "dcterms:creator": [
            "M. Sandler",
            "A. Howard",
            "M. Zhu",
            "A. Zhmoginov",
            "L.-C. Chen"
        ],
        "dcterms:description": "MobileNet-V2 is a lightweight deep learning model designed for mobile and edge devices, utilizing inverted residuals and linear bottlenecks to optimize performance and efficiency.",
        "dcterms:title": "MobileNet-V2",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Lightweight model",
            "Mobile deep learning",
            "Convolutional neural network"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "K. He",
            "X. Zhang",
            "S. Ren",
            "J. Sun"
        ],
        "dcterms:description": "ResNet-50 is a deep residual learning framework that enables training of very deep networks by using residual connections to mitigate the vanishing gradient problem.",
        "dcterms:title": "ResNet-50",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Residual learning",
            "Deep neural network",
            "Image recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "M. Tan",
            "B. Chen",
            "R. Pang",
            "V. Vasudevan",
            "M. Sandler",
            "A. Howard",
            "Q. V. Le"
        ],
        "dcterms:description": "MnasNet is a neural architecture search method that optimizes for both accuracy and efficiency, specifically designed for mobile devices.",
        "dcterms:title": "MnasNet",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Neural Architecture Search"
        ],
        "dcat:keyword": [
            "Mobile architecture",
            "Neural architecture search",
            "Efficient model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Classification"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Wu",
            "M. Schuster",
            "Z. Chen",
            "Q. V. Le",
            "M. Norouzi",
            "W. Macherey",
            "M. Krikun",
            "Y. Cao",
            "Q. Gao",
            "K. Macherey"
        ],
        "dcterms:description": "GNMT is Google's Neural Machine Translation system that bridges the gap between human and machine translation, utilizing deep learning techniques.",
        "dcterms:title": "GNMT",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1609.08144",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Translation"
        ],
        "dcat:keyword": [
            "Neural machine translation",
            "Deep learning",
            "Language processing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Machine Translation"
        ]
    },
    {
        "dcterms:creator": [
            "A. Vaswani",
            "N. Shazeer",
            "N. Parmar",
            "J. Uszkoreit",
            "L. Jones",
            "A. N. Gomez",
            "≈Å. Kaiser",
            "I. Polosukhin"
        ],
        "dcterms:description": "The Transformer is a model architecture that relies entirely on self-attention mechanisms, eliminating recurrence and enabling parallelization.",
        "dcterms:title": "Transformer",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Attention mechanism",
            "Sequence modeling",
            "Deep learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "X. He",
            "L. Liao",
            "H. Zhang",
            "L. Nie",
            "X. Hu",
            "T.-S. Chua"
        ],
        "dcterms:description": "NCF is a collaborative filtering model that combines neural networks with matrix factorization techniques for recommendation systems.",
        "dcterms:title": "NCF",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Recommendation Systems",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Collaborative filtering",
            "Neural networks",
            "Recommendation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Recommendation"
        ]
    }
]