[
    {
        "dcterms:creator": [
            "Y. Yuan",
            "A. R. Mahmood"
        ],
        "dcterms:description": "An image-based task that aims to move a UR5 armâ€™s fingertip to a random target designated by a red blob on a monitor. The task requires tens of thousands of images to learn good state representations and control policies.",
        "dcterms:title": "UR5-VisualReacher",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Robotic arm",
            "Vision-based control",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Vision-based control",
            "Robotic manipulation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Yuan",
            "A. R. Mahmood"
        ],
        "dcterms:description": "An image-based task that aims to move Create2 as soon as possible to a target designated by a green piece of paper attached to a wall. The task requires tens of thousands of images to learn good state representations and control policies.",
        "dcterms:title": "Create-Reacher",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Mobile robot",
            "Vision-based control",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Vision-based control",
            "Robotic navigation"
        ]
    },
    {
        "dcterms:creator": [
            "A. R. Mahmood",
            "D. Korenkevych",
            "G. Vasan",
            "W. Ma",
            "J. Bergstra"
        ],
        "dcterms:description": "A computational framework for robotic learning experiments to be reproducible in different locations and under diverse conditions.",
        "dcterms:title": "SenseAct",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Robotics",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Robotic learning",
            "Reproducibility",
            "Framework"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Nair",
            "P. Srinivasan",
            "S. Blackwell",
            "C. Alcicek",
            "R. Fearon",
            "A. De Maria"
        ],
        "dcterms:description": "A distributed learning architecture that focuses on using multiple actors and learners to collect data in parallel and accelerate training in simulation using clusters of CPUs and GPUs.",
        "dcterms:title": "GORILA",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Distributed Learning",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Parallel training",
            "Deep reinforcement learning",
            "Simulation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Abdolmaleki",
            "J. T. Springenberg",
            "Y. Tassa",
            "R. Munos",
            "N. Heess",
            "M. Riedmiller"
        ],
        "dcterms:description": "A method for optimizing policies using maximum a posteriori estimation.",
        "dcterms:title": "Max-APosteriori Policy Optimization (MPO)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "Policy optimization",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Haarnoja",
            "A. Zhou",
            "P. Abbeel",
            "S. Levine"
        ],
        "dcterms:description": "An off-policy maximum entropy deep reinforcement learning algorithm with a stochastic actor.",
        "dcterms:title": "Soft Actor-Critic (SAC)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Maximum entropy",
            "Off-policy learning",
            "Stochastic actor"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Schulman",
            "F. Wolski",
            "P. Dhariwal",
            "A. Radford",
            "O. Klimov"
        ],
        "dcterms:description": "An on-policy policy gradient algorithm that alternates between collecting data via interactions with the environment and optimizing a surrogate objective.",
        "dcterms:title": "Proximal Policy Optimization (PPO)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "On-policy learning",
            "Policy gradient",
            "Surrogate objective"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]