[
    {
        "dcterms:creator": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adri√† Garriga-Alonso"
        ],
        "dcterms:description": "A benchmark designed to evaluate the capabilities of language models across a variety of tasks.",
        "dcterms:title": "Big Bench",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2206.04615",
        "dcat:theme": [
            "Language Model Evaluation"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Models",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Rishi Bommasani",
            "Percy Liang",
            "Tony Lee"
        ],
        "dcterms:description": "A benchmark for evaluating the performance of language models on a variety of natural language understanding tasks.",
        "dcterms:title": "GLUE Benchmark",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Understanding",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Rishi Bommasani",
            "Percy Liang",
            "Tony Lee"
        ],
        "dcterms:description": "An advanced benchmark for evaluating language models on more complex tasks than GLUE.",
        "dcterms:title": "SuperGLUE Benchmark",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Understanding",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "OpenAI"
        ],
        "dcterms:description": "An API for moderating content generated by language models.",
        "dcterms:title": "OpenAI Moderation API",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "https://platform.openai.com/docs/models/overview",
        "dcat:theme": [
            "Content Moderation"
        ],
        "dcat:keyword": [
            "API",
            "Moderation",
            "Language Models"
        ],
        "dcat:landingPage": "https://platform.openai.com/docs/models/overview",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Content Moderation"
        ]
    },
    {
        "dcterms:creator": [
            "Yidong Wang",
            "Zhuohao Yu",
            "Zhengran Zeng",
            "Linyi Yang",
            "Cunxiang Wang",
            "Hao Chen",
            "Chaoya Jiang",
            "Rui Xie",
            "Jindong Wang",
            "Xing Xie"
        ],
        "dcterms:description": "A benchmark for evaluating the instruction tuning optimization of large language models.",
        "dcterms:title": "MMLU",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2306.05087",
        "dcat:theme": [
            "Language Model Evaluation"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Instruction Tuning",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Edward Beeching"
        ],
        "dcterms:description": "A leaderboard for evaluating open large language models.",
        "dcterms:title": "EleutherAI LM Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",
        "dcat:theme": [
            "Language Model Evaluation"
        ],
        "dcat:keyword": [
            "Leaderboard",
            "Evaluation",
            "Language Models"
        ],
        "dcat:landingPage": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "OpenAI Evals Adversarial NLI",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "LIT",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "ParlAI",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text.",
        "dcterms:title": "CoQA",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "10.18653/v1/D16-1203",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Dataset",
            "Machine Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "LAMBADA",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "HellaSwag",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "LogiQA",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "",
        "dcterms:title": "MultiNLI",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "A dataset containing over 100,000 questions for machine comprehension of text.",
        "dcterms:title": "SQUAD",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "10.18653/v1/D16-1203",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Dataset",
            "Machine Comprehension",
            "Question Answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "David Chiang",
            "Peter Cholak"
        ],
        "dcterms:description": "A dataset designed to evaluate the performance of language models on self-attention tasks.",
        "dcterms:title": "Parity Dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2202.12172",
        "dcat:theme": [
            "Language Model Evaluation"
        ],
        "dcat:keyword": [
            "Dataset",
            "Self-Attention",
            "Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Evaluation"
        ]
    }
]