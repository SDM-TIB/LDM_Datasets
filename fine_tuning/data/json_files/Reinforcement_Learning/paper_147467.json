[
    {
        "dcterms:creator": [
            "Tom Schaul",
            "John Quan",
            "Ioannis Antonoglou",
            "David Silver"
        ],
        "dcterms:description": "Experience Replay (ER) is a method that stores an incomplete history of previous agent-environment interactions in a transition buffer, allowing the agent to sample from this buffer during planning to update the value function.",
        "dcterms:title": "Experience Replay (ER)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Sample-based Planning"
        ],
        "dcat:keyword": [
            "Experience Replay",
            "Reinforcement Learning",
            "Transition Buffer"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Function Update",
            "Planning"
        ]
    },
    {
        "dcterms:creator": [
            "M Deisenroth",
            "C E Rasmussen"
        ],
        "dcterms:description": "PILCO is a model-based and data-efficient approach to policy search that utilizes a probabilistic model to optimize policies in continuous state spaces.",
        "dcterms:title": "PILCO",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Search"
        ],
        "dcat:keyword": [
            "Model-based Learning",
            "Data Efficiency",
            "Policy Optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Search"
        ]
    },
    {
        "dcterms:creator": [
            "R.S. Sutton"
        ],
        "dcterms:description": "Dyna is a planning paradigm that interleaves learning and planning by simulating one-step experience to update the action-value function.",
        "dcterms:title": "Dyna",
        "dcterms:issued": "1991",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Planning"
        ],
        "dcat:keyword": [
            "Planning Paradigm",
            "Action-Value Function",
            "Simulation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning",
            "Planning"
        ]
    },
    {
        "dcterms:creator": [
            "Jing Peng",
            "Ronald J Williams"
        ],
        "dcterms:description": "Continuous Gridworld is a continuous variant of a grid-based environment where agents can move in a continuous space and receive rewards based on their position.",
        "dcterms:title": "Continuous Gridworld",
        "dcterms:issued": "1993",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Continuous State Spaces"
        ],
        "dcat:keyword": [
            "Continuous Environment",
            "Gridworld",
            "Reward Structure"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Navigation",
            "Reward Maximization"
        ]
    },
    {
        "dcterms:creator": [
            "A. Strehl",
            "M Littman"
        ],
        "dcterms:description": "River Swim is a continuous state space environment where an agent must swim upstream against a current, receiving rewards based on its position in the river.",
        "dcterms:title": "River Swim",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Exploration"
        ],
        "dcat:keyword": [
            "Continuous State",
            "Reward Structure",
            "Exploration Challenge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration",
            "Reward Maximization"
        ]
    },
    {
        "dcterms:creator": [
            "Dirk Ormoneit",
            "Saunak Sen"
        ],
        "dcterms:description": "Kernel-Based Reinforcement Learning is a method that utilizes kernel functions to generalize learning across states and actions in reinforcement learning tasks.",
        "dcterms:title": "Kernel-Based Reinforcement Learning",
        "dcterms:issued": "2002",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Function Approximation"
        ],
        "dcat:keyword": [
            "Kernel Methods",
            "Generalization",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Learning",
            "Function Approximation"
        ]
    },
    {
        "dcterms:creator": [
            "I J Goodfellow",
            "M Pouget-Abadie",
            "Mehdi Mirza",
            "B Xu",
            "D Warde-Farley",
            "S Ozair",
            "A C Courville",
            "Y Bengio"
        ],
        "dcterms:description": "Generative Adversarial Networks (GANs) are a class of machine learning frameworks where two neural networks contest with each other to generate new data instances.",
        "dcterms:title": "Generative Adversarial Networks",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Generative Models",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Adversarial Learning",
            "Neural Networks",
            "Data Generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Data Generation"
        ]
    },
    {
        "dcterms:creator": [
            "Michael P Holmes",
            "Alexander G Gray",
            "Charles L Isbell"
        ],
        "dcterms:description": "Kernel Density Estimators are non-parametric methods used to estimate the probability density function of a random variable.",
        "dcterms:title": "Kernel Density Estimators",
        "dcterms:issued": "2007",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistical Learning",
            "Density Estimation"
        ],
        "dcat:keyword": [
            "Non-parametric Methods",
            "Density Estimation",
            "Probability Density Function"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Density Estimation"
        ]
    }
]