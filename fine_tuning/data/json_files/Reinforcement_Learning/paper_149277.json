[
    {
        "dcterms:creator": [
            "M. G Bellemare",
            "Y. Naddaf",
            "J. Veness",
            "M. Bowling"
        ],
        "dcterms:description": "The Arcade Learning Environment (ALE) is an evaluation platform for general agents, primarily used for benchmarking reinforcement learning algorithms on Atari games.",
        "dcterms:title": "Arcade Learning Environment (ALE)",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game AI"
        ],
        "dcat:keyword": [
            "Atari games",
            "Benchmarking",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Lukasz Kaiser",
            "Mohammad Babaeizadeh",
            "Piotr Milos",
            "Blazej Osinski",
            "Roy H Campbell",
            "Konrad Czechowski",
            "Dumitru Erhan",
            "Chelsea Finn",
            "Piotr Kozakowski",
            "Sergey Levine"
        ],
        "dcterms:description": "The Atari 100k benchmark is an offshoot of the ALE for evaluating data-efficiency in deep reinforcement learning, where algorithms are evaluated on only 100k steps for each of its games.",
        "dcterms:title": "Atari 100k benchmark",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1903.00374",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game AI"
        ],
        "dcat:keyword": [
            "Data efficiency",
            "Benchmarking",
            "Atari games"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Karl Cobbe",
            "Chris Hesse",
            "Jacob Hilton",
            "John Schulman"
        ],
        "dcterms:description": "Procgen is a benchmark consisting of 16 diverse tasks for evaluating generalization in reinforcement learning.",
        "dcterms:title": "Procgen",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Generalization"
        ],
        "dcat:keyword": [
            "Procedural generation",
            "Benchmarking",
            "Generalization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Yuval Tassa",
            "Yotam Doron",
            "Alistair Muldal",
            "Tom Erez",
            "Yazhe Li",
            "Diego de Las Casas",
            "David Budden",
            "Abbas Abdolmaleki",
            "Josh Merel",
            "Andrew Lefrancq"
        ],
        "dcterms:description": "The DeepMind Control Suite is a set of continuous control tasks for benchmarking reinforcement learning algorithms.",
        "dcterms:title": "DeepMind Control Suite",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1801.00690",
        "dcat:theme": [
            "Reinforcement Learning",
            "Control Tasks"
        ],
        "dcat:keyword": [
            "Continuous control",
            "Benchmarking",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]