[
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Tianle Li",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Eric P. Xing",
            "Joseph E. Gonzalez",
            "Ion Stoica",
            "Hao Zhang"
        ],
        "dcterms:description": "MT-Bench is a benchmark for evaluating large language models (LLMs) in chat settings, focusing on their ability to rank responses based on human-like preferences across multiple languages.",
        "dcterms:title": "MT-Bench",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Language Models",
            "Chat Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Response Ranking",
            "Preference Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Banghua Zhu",
            "Evan Frick",
            "Tianhao Wu",
            "Hanlin Zhu",
            "Karthik Ganesan",
            "Wei-Lin Chiang",
            "Jian Zhang",
            "Jiantao Jiao"
        ],
        "dcterms:description": "Nectar is a preference dataset that samples prompts from various open-source datasets and generates responses using state-of-the-art LLMs, which are then ranked to train models for improved helpfulness and harmlessness.",
        "dcterms:title": "Nectar",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Preference Dataset",
            "Response Generation",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Response Ranking",
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Shuai Bai",
            "Yunfei Chu",
            "Zeyu Cui",
            "Kai Dang",
            "Xiaodong Deng",
            "Yang Fan",
            "Wenbin Ge",
            "Yu Han",
            "Fei Huang",
            "Binyuan Hui",
            "Luo Ji",
            "Mei Li",
            "Junyang Lin",
            "Runji Lin",
            "Dayiheng Liu",
            "Gao Liu",
            "Chengqiang Lu",
            "Keming Lu",
            "Jianxin Ma",
            "Rui Men",
            "Xingzhang Ren",
            "Xuancheng Ren",
            "Chuanqi Tan",
            "Sinan Tan",
            "Jianhong Tu",
            "Peng Wang",
            "Shijie Wang",
            "Wei Wang",
            "Shengguang Wu",
            "Benfeng Xu",
            "Jin Xu",
            "An Yang",
            "Hao Yang",
            "Jian Yang",
            "Shusheng Yang",
            "Yang Yao",
            "Bowen Yu",
            "Hongyi Yuan",
            "Zheng Yuan",
            "Jianwei Zhang",
            "Xingxuan Zhang",
            "Yichang Zhang",
            "Zhenru Zhang",
            "Chang Zhou",
            "Jingren Zhou",
            "Xiaohuan Zhou",
            "Tianhang Zhu"
        ],
        "dcterms:description": "HH-RLHF is a dataset designed for training models to be helpful and harmless by using reinforcement learning from human feedback, focusing on the evaluation of responses to prompts.",
        "dcterms:title": "HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2204.05862",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Reinforcement Learning",
            "Human Feedback",
            "Preference Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Response Evaluation",
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Andreas Köpf",
            "Yannic Kilcher",
            "Dimitri von Rütte",
            "Sotiris Anagnostidis",
            "Zhi Rui Tam",
            "Keith Stevens",
            "Abdullah Barhoum",
            "Duc Nguyen",
            "Oliver Stanley",
            "Richárd Nagyfi"
        ],
        "dcterms:description": "OASST1 is a dataset containing conversations generated by volunteers, aimed at democratizing large language model alignment through diverse conversational prompts and responses.",
        "dcterms:title": "OASST1",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Conversational Dataset",
            "Language Model Alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversation Generation",
            "Response Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Peter Devine"
        ],
        "dcterms:description": "Tagengo is a multilingual chat dataset designed to facilitate the training of language models across various languages, providing a diverse set of prompts and responses.",
        "dcterms:title": "Tagengo",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2405.12612",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Multilingual Dataset",
            "Chat Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Response Generation",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Mitsu is a dataset created by the authors of the paper, focusing on multilingual preference data for training language models.",
        "dcterms:title": "Mitsu",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Tianle Li",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Eric P. Xing",
            "Joseph E. Gonzalez",
            "Ion Stoica",
            "Hao Zhang"
        ],
        "dcterms:description": "LMSYS-Chat-1M is a large-scale dataset of real-world conversations designed to enhance the training of language models in chat applications.",
        "dcterms:title": "LMSYS-Chat-1M",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Conversation Dataset",
            "Language Model Training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Conversation Generation",
            "Response Evaluation"
        ]
    }
]