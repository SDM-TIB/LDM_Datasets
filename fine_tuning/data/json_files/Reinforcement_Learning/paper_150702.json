[
    {
        "dcterms:creator": [
            "E. H. Scao",
            "H. Touvron",
            "G. Penedo"
        ],
        "dcterms:description": "BLOOM is a 176B-parameter open-access multilingual language model designed for various natural language processing tasks.",
        "dcterms:title": "BLOOM",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2211.05100",
        "dcat:theme": [
            "Natural Language Processing",
            "Multilingual Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Multilingual",
            "Open access"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Touvron",
            "M. J. Black"
        ],
        "dcterms:description": "LLaMa-1 is an open and efficient foundation language model that serves as a basis for various NLP tasks.",
        "dcterms:title": "LLaMa-1",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Foundation Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Foundation model",
            "Open access"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "G. Penedo"
        ],
        "dcterms:description": "Falcon is an open large language model that achieves state-of-the-art performance on various benchmarks.",
        "dcterms:title": "Falcon",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Models"
        ],
        "dcat:keyword": [
            "Language model",
            "Open access",
            "State-of-the-art"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language understanding",
            "Text generation"
        ]
    },
    {
        "dcterms:creator": [
            "C. Chen",
            "A. Austin"
        ],
        "dcterms:description": "HumanEval is a benchmark for evaluating large language models trained on code, consisting of programming problems and their solutions.",
        "dcterms:title": "HumanEval",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Programming",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Code evaluation",
            "Programming problems",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation",
            "Programming"
        ]
    },
    {
        "dcterms:creator": [
            "A. Austin"
        ],
        "dcterms:description": "MBPP is a benchmark for evaluating program synthesis with large language models, consisting of programming tasks and their solutions.",
        "dcterms:title": "MBPP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Programming",
            "Code Generation"
        ],
        "dcat:keyword": [
            "Program synthesis",
            "Programming tasks",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Code generation",
            "Programming"
        ]
    },
    {
        "dcterms:creator": [
            "Kwiatkowski et al."
        ],
        "dcterms:description": "Natural Questions is a benchmark for question answering research, providing a dataset of questions and their corresponding answers.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Question answering",
            "Benchmark",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "J. Joshi"
        ],
        "dcterms:description": "TriviaQA is a large-scale distantly supervised challenge dataset for reading comprehension, consisting of trivia questions and their answers.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1705.03551",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Trivia questions",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "R. Rajpurkar"
        ],
        "dcterms:description": "SQuAD is a dataset for reading comprehension that includes questions and answers based on a set of Wikipedia articles.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1806.03822",
        "dcat:theme": [
            "Reading Comprehension",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Reading comprehension",
            "Dataset",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Zhong"
        ],
        "dcterms:description": "AGI Eval is a human-centric benchmark for evaluating foundation models, focusing on various tasks and metrics.",
        "dcterms:title": "AGI Eval",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2304.06364",
        "dcat:theme": [
            "Evaluation",
            "Artificial Intelligence"
        ],
        "dcat:keyword": [
            "Benchmark",
            "Evaluation metrics",
            "Foundation models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks"
        ],
        "dcterms:description": "GSM8K is a benchmark for evaluating mathematical problem solving with a dataset of math problems and their solutions.",
        "dcterms:title": "GSM8K",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2103.03874",
        "dcat:theme": [
            "Mathematics",
            "Problem Solving"
        ],
        "dcat:keyword": [
            "Mathematical reasoning",
            "Benchmark",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical problem solving"
        ]
    },
    {
        "dcterms:creator": [
            "D. Hendrycks"
        ],
        "dcterms:description": "MATH is a benchmark for evaluating mathematical problem solving capabilities of language models.",
        "dcterms:title": "MATH",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Mathematics",
            "Problem Solving"
        ],
        "dcat:keyword": [
            "Mathematical reasoning",
            "Benchmark",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Mathematical problem solving"
        ]
    },
    {
        "dcterms:creator": [
            "J. Dhamala"
        ],
        "dcterms:description": "BOLD is a dataset and metrics for measuring biases in open-ended language generation.",
        "dcterms:title": "BOLD",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Bias Measurement",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Bias",
            "Dataset",
            "Language generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Bias measurement"
        ]
    },
    {
        "dcterms:creator": [
            "T. Hartvigsen"
        ],
        "dcterms:description": "ToxiGen is a large-scale machine-generated dataset for adversarial and implicit hate speech detection.",
        "dcterms:title": "ToxiGen",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Hate Speech Detection",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Hate speech",
            "Dataset",
            "Adversarial"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Hate speech detection"
        ]
    },
    {
        "dcterms:creator": [
            "L. Lin"
        ],
        "dcterms:description": "TruthfulQA is a benchmark for measuring how models mimic human falsehoods.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2109.07958",
        "dcat:theme": [
            "Truthfulness",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Truthfulness",
            "Benchmark",
            "Language model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Truthfulness evaluation"
        ]
    }
]