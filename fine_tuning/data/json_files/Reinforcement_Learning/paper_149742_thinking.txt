To extract datasets from the research paper titled "SELMA: SEmantic Large-scale Multimodal Acquisitions in Variable Weather, Daytime and Viewpoints" by Paolo Testolina et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract** to identify any datasets mentioned. The abstract introduces SELMA as a novel synthetic dataset for semantic segmentation, indicating that it is a key focus of the paper.

Next, I will examine the **introduction** section, where the authors discuss the limitations of existing datasets and the need for a dataset like SELMA. This section may provide context for the dataset's significance but may not list other datasets explicitly.

I will then focus on **section IV (SELMA Dataset Design)**, which is likely to contain detailed information about the SELMA dataset itself. Here, the authors describe the dataset's characteristics, including the number of unique waypoints, the types of sensors used (RGB, depth, semantic cameras, and LiDAR), and the environmental conditions under which the data was collected. 

In this section, I will note the following details about the SELMA dataset:
- It contains over 30,909 unique waypoints acquired from 24 different sensors.
- The dataset includes data collected in 27 different weather and daytime conditions.
- It provides semantic labeling for both camera and LiDAR data into 36 distinct classes.

Next, I will check the **related work** section to see if the authors mention any other datasets that SELMA is compared against. They reference several existing datasets like Cityscapes, Waymo, and KITTI, which are important for context but are not the primary focus of this paper.

I will also look at the **experiments** section to see if the authors validate SELMA against any other datasets. They mention using SELMA for training deep learning models and comparing results with other datasets, which reinforces the importance of SELMA.

Finally, I will consult the **references section** to gather full citations for the datasets mentioned in the paper. The authors reference several datasets, including:
- **Cityscapes**: 
  > Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., & Schiele, B. (2016). The Cityscapes dataset for semantic urban scene understanding. In CVPR (pp. 3213–3223).
  
- **KITTI**: 
  > Geiger, A., Lenz, P., Stiller, C., & Urtasun, R. (2013). Vision meets robotics: The KITTI dataset. IJRR, 32(11), 1231–1237.

- **Waymo**: 
  > Sun, P., Kretzschmar, H., Dotiwalla, A., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., & Caine, B. (2020). Scalability in perception for autonomous driving: Waymo open dataset. In CVPR (pp. 2446–2454).

After gathering all this information, I will compile the dataset entries, ensuring that I include full citations for each dataset mentioned in the paper, particularly focusing on SELMA as the primary dataset of interest.