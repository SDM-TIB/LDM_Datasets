[
    {
        "dcterms:creator": [
            "R. S. Sutton"
        ],
        "dcterms:description": "A simple 10-state MDP where each state has two actions that generate immediate rewards following normal distributions. The MDP is used to illustrate the performance of risk-averse deep Q-learning.",
        "dcterms:title": "10-state MDP",
        "dcterms:issued": "1988",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Markov Decision Processes"
        ],
        "dcat:keyword": [
            "MDP",
            "Risk-averse learning",
            "Deep Q-learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "R. S. Sutton",
            "A. G. Barto"
        ],
        "dcterms:description": "A well-studied 7-state random walk MDP where each non-terminal state has two actions that cause transitions to neighboring states. The rewards are zero except for transitions into the terminal states.",
        "dcterms:title": "7-state random walk MDP",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Markov Decision Processes"
        ],
        "dcat:keyword": [
            "MDP",
            "Random walk",
            "Risk-averse learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    }
]