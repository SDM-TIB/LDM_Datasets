[
    {
        "dcterms:creator": [
            "C. Pike-Burke",
            "S. Grunewalder"
        ],
        "dcterms:description": "Dataset used to study the recovering bandits problem, where the reward depends on the number of rounds elapsed since the last time an arm was pulled.",
        "dcterms:title": "Recovering Bandits",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Recovering rewards",
            "Sequential decision-making",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T. Lattimore",
            "C. Szepesv√°ri"
        ],
        "dcterms:description": "Framework for multi-armed bandit problems, providing a structured approach to decision-making under uncertainty.",
        "dcterms:title": "Multi-Armed Bandit (MAB) Framework",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Multi-Armed Bandits"
        ],
        "dcat:keyword": [
            "Decision-making",
            "Exploration-exploitation",
            "Cumulative rewards"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. Misra",
            "E. M. Schwartz",
            "J. Abernethy"
        ],
        "dcterms:description": "Dataset used for dynamic online pricing experiments, employing multi-armed bandit strategies to optimize pricing decisions.",
        "dcterms:title": "Dynamic Pricing",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Economics",
            "Dynamic Pricing"
        ],
        "dcat:keyword": [
            "Dynamic pricing",
            "Multi-armed bandits",
            "Online experiments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "K. P. Yancey",
            "B. Settles"
        ],
        "dcterms:description": "Dataset for a language-learning application, utilizing multi-armed bandit algorithms to optimize the timing and frequency of notifications.",
        "dcterms:title": "Language-Learning Application",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Education",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Language learning",
            "Notifications",
            "Multi-armed bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "E. Gangan",
            "M. Kudus",
            "E. Ilyushin"
        ],
        "dcterms:description": "Survey dataset of multi-armed bandit algorithms applied to recommendation systems, analyzing their effectiveness and performance.",
        "dcterms:title": "Item Recommendation",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Recommendation Systems",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Item recommendation",
            "Multi-armed bandits",
            "Algorithms survey"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Garivier",
            "E. Moulines"
        ],
        "dcterms:description": "Dataset focusing on non-stationary bandit problems, exploring upper-confidence bound policies.",
        "dcterms:title": "Non-Stationary Bandits",
        "dcterms:issued": "2011",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Non-Stationary Environments"
        ],
        "dcat:keyword": [
            "Non-stationary rewards",
            "Upper-confidence bounds",
            "Bandit problems"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "H. Heidari",
            "M. Kearns",
            "A. Roth"
        ],
        "dcterms:description": "Dataset related to rested bandits, focusing on policy regret bounds for improving and decaying bandits.",
        "dcterms:title": "Rested Bandits",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Improvement"
        ],
        "dcat:keyword": [
            "Rested bandits",
            "Policy regret",
            "Improving bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "O. Besbes",
            "Y. Gur",
            "A. Zeevi"
        ],
        "dcterms:description": "Dataset addressing the stochastic multi-armed bandit problem with non-stationary rewards.",
        "dcterms:title": "Stochastic Multi-Armed Bandit Problem",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Stochastic Processes"
        ],
        "dcat:keyword": [
            "Stochastic bandits",
            "Non-stationary rewards",
            "Multi-armed bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "C. Pike-Burke",
            "S. Grunewalder"
        ],
        "dcterms:description": "Dataset utilizing Gaussian process regression in the context of recovering bandits.",
        "dcterms:title": "Gaussian Process Regression",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Gaussian Processes"
        ],
        "dcat:keyword": [
            "Gaussian processes",
            "Regression",
            "Recovering bandits"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "P. Laforgue",
            "G. Clerici",
            "N. Cesa-Bianchi",
            "R. Gilad-Bachrach"
        ],
        "dcterms:description": "Dataset focusing on combinatorial semi-bandits, analyzing satiation and seasonality effects.",
        "dcterms:title": "Combinatorial Semi-Bandits",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Combinatorial Optimization",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Combinatorial bandits",
            "Satiation",
            "Seasonality"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]