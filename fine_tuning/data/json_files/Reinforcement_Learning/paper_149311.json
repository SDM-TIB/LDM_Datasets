[
    {
        "dcterms:creator": [
            "T. Yu",
            "A. Kumar",
            "R. Rafailov",
            "A. Rajeswaran",
            "S. Levine",
            "C. Finn"
        ],
        "dcterms:description": "COMBO employs an actor-critic scheme where the action value function is learned using both the offline dataset as well as synthetic rollouts.",
        "dcterms:title": "COMBO",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2102.08363",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Based Learning"
        ],
        "dcat:keyword": [
            "Offline Learning",
            "Policy Optimization",
            "Actor-Critic"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2102.08363",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "T. Yu",
            "G. Thomas",
            "L. Yu",
            "S. Ermon",
            "J. Zou",
            "S. Levine",
            "C. Finn",
            "T. Ma"
        ],
        "dcterms:description": "MOPO uses uncertainty quantification as a penalty term and optimizes over the lower bound of the policy evaluation function.",
        "dcterms:title": "MOPO",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2005.13239",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Based Learning"
        ],
        "dcat:keyword": [
            "Offline Learning",
            "Policy Optimization",
            "Uncertainty Quantification"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2005.13239",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "T. Lillicrap",
            "J. Hunt",
            "A. Pritzel",
            "N. Heess",
            "T. Erez",
            "Y. Tassa",
            "D. Silver",
            "D. Wierstra"
        ],
        "dcterms:description": "DDPG is a model-free algorithm that uses deep reinforcement learning for continuous control tasks.",
        "dcterms:title": "DDPG",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1509.02971",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Free Learning"
        ],
        "dcat:keyword": [
            "Continuous Control",
            "Deep Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1509.02971",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Continuous Control"
        ]
    },
    {
        "dcterms:creator": [
            "J. Schulman",
            "S. Levine",
            "P. Moritz",
            "M. I. Jordan",
            "P. Abbeel"
        ],
        "dcterms:description": "TRPO is an algorithm for optimizing policies in reinforcement learning that ensures monotonic improvement.",
        "dcterms:title": "TRPO",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1502.05477v5",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "Policy Optimization",
            "Trust Region"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1502.05477v5",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "T. Haarnoja",
            "A. Zhou",
            "P. Abbeel",
            "S. Levine"
        ],
        "dcterms:description": "SAC is an off-policy actor-critic algorithm that incorporates maximum entropy into the reinforcement learning framework.",
        "dcterms:title": "SAC",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1801.01290v2",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Free Learning"
        ],
        "dcat:keyword": [
            "Maximum Entropy",
            "Off-Policy Learning"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1801.01290v2",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "A. Kumar",
            "A. Zhou",
            "G. Tucker",
            "S. Levine"
        ],
        "dcterms:description": "CQL is a method for offline reinforcement learning that stabilizes Q-learning through bootstrapping error reduction.",
        "dcterms:title": "CQL",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2006.04779",
        "dcat:theme": [
            "Reinforcement Learning",
            "Offline Learning"
        ],
        "dcat:keyword": [
            "Q-Learning",
            "Offline Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2006.04779",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Q-Learning"
        ]
    },
    {
        "dcterms:creator": [
            "R. Kidambi",
            "A. Rajeswaran",
            "P. Netrapalli",
            "T. Joachims"
        ],
        "dcterms:description": "MOReL is a model-based offline reinforcement learning algorithm that learns a model of the environment.",
        "dcterms:title": "MOReL",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/2005.05951",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Based Learning"
        ],
        "dcat:keyword": [
            "Model-Based Learning",
            "Offline Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/2005.05951",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Model Learning"
        ]
    },
    {
        "dcterms:creator": [
            "A. Kumar",
            "A. Fu",
            "G. Tucker",
            "S. Levine"
        ],
        "dcterms:description": "BEAR is a method for stabilizing off-policy Q-learning by reducing bootstrapping error.",
        "dcterms:title": "BEAR",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1906.00949",
        "dcat:theme": [
            "Reinforcement Learning",
            "Offline Learning"
        ],
        "dcat:keyword": [
            "Q-Learning",
            "Bootstrapping"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1906.00949",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Q-Learning"
        ]
    },
    {
        "dcterms:creator": [
            "N. Rhinehart",
            "R. McAllister",
            "S. Levine"
        ],
        "dcterms:description": "DIMs are models that learn to predict future trajectories and use this model for planning and control.",
        "dcterms:title": "DIMs",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1810.06544",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Based Learning"
        ],
        "dcat:keyword": [
            "Imitation Learning",
            "Planning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1810.06544",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Planning"
        ]
    },
    {
        "dcterms:creator": [
            "F. Yu",
            "H. Chen",
            "X. Wang",
            "W. Xian",
            "Y. Chen",
            "F. Liu",
            "V. Madhavan",
            "T. Darrell"
        ],
        "dcterms:description": "BDD100K is a diverse driving dataset designed for heterogeneous multitask learning.",
        "dcterms:title": "BDD100K",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1805.04687",
        "dcat:theme": [
            "Computer Vision",
            "Autonomous Driving"
        ],
        "dcat:keyword": [
            "Driving Dataset",
            "Multitask Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1805.04687",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Object Detection",
            "Segmentation"
        ]
    }
]