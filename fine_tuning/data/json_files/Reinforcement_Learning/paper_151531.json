[
    {
        "dcterms:creator": [
            "T. Liao",
            "H. Yi",
            "Y. Xiu",
            "J. Tang",
            "Y. Huang",
            "J. Thies",
            "M. J. Black"
        ],
        "dcterms:description": "TADA is used for generating 3D avatars from text descriptions, but it lacks shape diversity and exhibits animation artifacts when applying motions for animation.",
        "dcterms:title": "TADA",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-Image"
        ],
        "dcat:keyword": [
            "3D avatars",
            "text description",
            "animation artifacts"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Generation"
        ]
    },
    {
        "dcterms:creator": [
            "M. Zhang",
            "Z. Cai",
            "L. Pan",
            "F. Hong",
            "X. Guo",
            "L. Yang",
            "Z. Liu"
        ],
        "dcterms:description": "MotionDiffuse is leveraged for generating human motion from text descriptions, providing a basis for animating avatars.",
        "dcterms:title": "Motion Diffuse",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Human Motion Generation",
            "Text-to-Motion"
        ],
        "dcat:keyword": [
            "human motion",
            "text-driven generation",
            "diffusion model"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Motion Data",
        "mls:task": [
            "Motion Generation"
        ]
    },
    {
        "dcterms:creator": [
            "L. Zhang",
            "A. Rao",
            "M. Agrawala"
        ],
        "dcterms:description": "ControlNet is utilized to provide conditional control in the text-to-image diffusion models, enhancing the generation process.",
        "dcterms:title": "ControlNet",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-Image Generation",
            "Conditional Control"
        ],
        "dcat:keyword": [
            "conditional control",
            "text-to-image",
            "diffusion models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image Generation"
        ]
    },
    {
        "dcterms:creator": [
            "G. Pavlakos",
            "V. Choutas",
            "N. Ghorbani",
            "T. Bolkart",
            "A. A. Osman",
            "D. Tzionas",
            "M. J. Black"
        ],
        "dcterms:description": "SMPL-X is a model used for representing expressive 3D human bodies, which is essential for avatar generation.",
        "dcterms:title": "SMPL-X",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Human Modeling",
            "Avatar Representation"
        ],
        "dcat:keyword": [
            "3D human model",
            "avatar generation",
            "expressive body capture"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Human Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "A. Radford",
            "J. W. Kim",
            "C. Hallacy",
            "A. Ramesh",
            "G. Goh",
            "S. Agarwal",
            "G. Sastry",
            "A. Askell",
            "P. Mishkin",
            "J. Clark",
            "G. Krueger",
            "I. Sutskever"
        ],
        "dcterms:description": "CLIP is used for learning transferable visual models from natural language supervision, aiding in the alignment of generated avatars with text descriptions.",
        "dcterms:title": "CLIP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Visual Models",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "transferable models",
            "visual understanding",
            "natural language supervision"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Visual Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "B. Poole",
            "A. Jain",
            "J. T. Barron",
            "B. Mildenhall"
        ],
        "dcterms:description": "DreamFusion is utilized for text-to-3D generation using 2D diffusion, contributing to the creation of 4D avatars.",
        "dcterms:title": "DreamFusion",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Text-to-3D Generation",
            "Diffusion Models"
        ],
        "dcat:keyword": [
            "text-to-3D",
            "diffusion model",
            "3D generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "3D Generation"
        ]
    },
    {
        "dcterms:creator": [
            "F. Hong",
            "M. Zhang",
            "L. Pan",
            "Z. Cai",
            "L. Yang",
            "Z. Liu"
        ],
        "dcterms:description": "AvatarCLIP is used for zero-shot text-driven generation and animation of 3D avatars, enhancing the avatar creation process.",
        "dcterms:title": "AvatarCLIP",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "zero-shot generation",
            "3D avatars",
            "text-driven animation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Animation"
        ]
    },
    {
        "dcterms:creator": [
            "X. Liu",
            "X. Zhan",
            "J. Tang",
            "Y. Shan",
            "G. Zeng",
            "D. Lin",
            "X. Liu",
            "Z. Liu"
        ],
        "dcterms:description": "HumanGaussian is utilized for text-driven 3D human generation with Gaussian splatting, contributing to the creation of avatars.",
        "dcterms:title": "HumanGaussian",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Human Generation",
            "Text-to-3D"
        ],
        "dcat:keyword": [
            "text-driven generation",
            "Gaussian splatting",
            "3D human models"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Human Generation"
        ]
    },
    {
        "dcterms:creator": [
            "R. Jiang",
            "C. Wang",
            "J. Zhang",
            "M. Chai",
            "M. He",
            "D. Chen",
            "J. Liao"
        ],
        "dcterms:description": "AvatarCraft is used for transforming text into neural human avatars with parameterized shape and pose control, enhancing avatar generation.",
        "dcterms:title": "AvatarCraft",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Avatar Generation",
            "Text-to-Avatar"
        ],
        "dcat:keyword": [
            "neural avatars",
            "text transformation",
            "shape control"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Creation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Huang",
            "J. Wang",
            "A. Zeng",
            "H. CAO",
            "X. Qi",
            "Y. Shi",
            "Z.-J. Zha",
            "L. Zhang"
        ],
        "dcterms:description": "DreamWaltz is utilized for creating complex 3D animatable avatars, contributing to the generation of 4D avatars.",
        "dcterms:title": "DreamWaltz",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "3D Animation",
            "Avatar Generation"
        ],
        "dcat:keyword": [
            "animatable avatars",
            "3D scenes",
            "text-driven generation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "3D Model",
        "mls:task": [
            "Avatar Animation"
        ]
    }
]