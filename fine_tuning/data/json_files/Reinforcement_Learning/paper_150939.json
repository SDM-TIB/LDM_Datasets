[
    {
        "dcterms:creator": [
            "Yuntao Bai",
            "Saurav Kadavath",
            "Sandipan Kundu",
            "Amanda Askell",
            "Jackson Kernion",
            "Andy Jones",
            "Anna Chen",
            "Anna Goldie",
            "Azalia Mirhoseini",
            "Cameron McKinnon",
            "Carol Chen",
            "Catherine Olson",
            "Christopher Olah",
            "Danny Hernandez",
            "Dawn Drain",
            "Deep Ganguli",
            "Dustin Li",
            "Eli Tran-Johnson",
            "Ethan Perez",
            "Jamie Kerr",
            "Jared Mueller",
            "Jeffrey Ladish",
            "Joshua Landau",
            "Kamal Ndousse",
            "Kamile Lukosuite",
            "Liane Lovitt",
            "Michael Sellitto",
            "Nelson Elhage",
            "Nicholas Schiefer",
            "Noemi Mercado",
            "Nova DasSarma",
            "Robert Lasenby",
            "Robin Larson",
            "Sam Ringer",
            "Scott Johnston",
            "Shauna Kravec",
            "Sheer El Showk",
            "Stanislav Fort",
            "Tamera Lanham",
            "Timothy Telleen-Lawton",
            "Tom Conerly",
            "Tom Henighan",
            "Tristan Hume",
            "Samuel R. Bowman",
            "Zac Hatfield-Dodds",
            "Ben Mann",
            "Dario Amodei",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Tom Brown",
            "Jared Kaplan"
        ],
        "dcterms:description": "A human preference dataset about helpfulness and harmlessness containing approximately 170k dialogues, each with a context and a pair of responses along with an annotated preference label. It includes four subsets: Harmlessbase, Helpfulbase, Helpfulonline, and Helpfulrejection.",
        "dcterms:title": "HH-RLHF",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://github.com/anthropics/hh-rlhf",
        "dcat:theme": [
            "Human Preferences",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Human feedback",
            "Reinforcement learning",
            "Language models",
            "Dialogue dataset"
        ],
        "dcat:landingPage": "https://github.com/anthropics/hh-rlhf",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Preference ranking",
            "Dialogue generation"
        ]
    }
]