[
    {
        "dcterms:creator": [
            "M. Koster",
            "G. Illyes",
            "H. Zeller",
            "L. Sassman"
        ],
        "dcterms:description": "The Robots Exclusion Protocol (REP) is a common standard among websites that regulates the access of autonomous bots, allowing webmasters to state access rules for non-human visitors in a robots.txt file.",
        "dcterms:title": "Robots Exclusion Protocol (REP)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.rfc-editor.org/info/rfc9309",
        "dcat:theme": [
            "Web Standards",
            "Data Protection"
        ],
        "dcat:keyword": [
            "Web crawling",
            "Content control",
            "Robots.txt",
            "Autonomous bots"
        ],
        "dcat:landingPage": "https://www.rfc-editor.org/info/rfc9309",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Web crawling regulation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The TDM Reservation Protocol (TDM Rep) is a web standard that allows publishers to specify their preferences regarding the Text & Data Mining of online resources under their control.",
        "dcterms:title": "TDM Reservation Protocol (TDM Rep)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "https://www.w3.org/2022/tdmrep/",
        "dcat:theme": [
            "Text & Data Mining",
            "Data Sovereignty"
        ],
        "dcat:keyword": [
            "TDM",
            "Data Mining",
            "Web standards",
            "Publisher rights"
        ],
        "dcat:landingPage": "https://www.w3.org/2022/tdmrep/",
        "dcterms:hasVersion": "",
        "dcterms:format": "JSON",
        "mls:task": [
            "Text & Data Mining regulation"
        ]
    },
    {
        "dcterms:creator": [
            "D. Ippolito",
            "Y. W. Yu"
        ],
        "dcterms:description": "DONOTTRAIN is a metadata standard designed to indicate consent for machine learning, allowing content creators to specify whether their data can be used for training AI models.",
        "dcterms:title": "DONOTRAIN Metadata Standard",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Data Consent"
        ],
        "dcat:keyword": [
            "Metadata standard",
            "Consent",
            "Machine Learning",
            "Data usage"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Indicating consent for ML"
        ]
    },
    {
        "dcterms:creator": [
            "M. Dinzinger",
            "M. Granitzer"
        ],
        "dcterms:description": "Common Crawl is a publicly available archive of web pages that can be used for various research purposes, including the analysis of web content and the study of web crawling mechanisms.",
        "dcterms:title": "Common Crawl",
        "dcterms:issued": "2024",
        "dcterms:language": "",
        "dcterms:identifier": "https://commoncrawl.org",
        "dcat:theme": [
            "Web Archiving",
            "Data Analysis"
        ],
        "dcat:keyword": [
            "Web archive",
            "Crawling",
            "Data analysis",
            "Research"
        ],
        "dcat:landingPage": "https://commoncrawl.org",
        "dcterms:hasVersion": "",
        "dcterms:format": "Web Archive",
        "mls:task": [
            "Web content analysis"
        ]
    }
]