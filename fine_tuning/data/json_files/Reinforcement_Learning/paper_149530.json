[
    {
        "dcterms:creator": [
            "W. Zhang",
            "M. Zhu",
            "K. G. Derpanis"
        ],
        "dcterms:description": "A large dataset containing video clips with 13 joints annotated in all frames, including head, shoulders, elbows, wrists, hips, knees, and ankles.",
        "dcterms:title": "Penn Action Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Human Pose Estimation"
        ],
        "dcat:keyword": [
            "Video dataset",
            "Human pose",
            "Joint estimation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Human Pose Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "H. Jhuang",
            "J. Gall",
            "S. Zufﬁ",
            "C. Schmid",
            "M. J. Black"
        ],
        "dcterms:description": "A dataset aimed at understanding action recognition, containing video clips with various actions.",
        "dcterms:title": "sub-JHMDB Dataset",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Computer Vision",
            "Action Recognition"
        ],
        "dcat:keyword": [
            "Action dataset",
            "Video clips",
            "Action recognition"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Video",
        "mls:task": [
            "Action Recognition"
        ]
    },
    {
        "dcterms:creator": [
            "Edward L Ionides"
        ],
        "dcterms:description": "A method for importance sampling that truncates the sampling process to improve efficiency.",
        "dcterms:title": "Truncated Importance Sampling",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Statistics",
            "Sampling Methods"
        ],
        "dcat:keyword": [
            "Importance sampling",
            "Statistical methods"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Anna Harutyunyan",
            "Marc G Bellemare",
            "Tom Stepleton",
            "R´emi Munos"
        ],
        "dcterms:description": "An algorithm that extends Q-learning with off-policy corrections to improve learning efficiency.",
        "dcterms:title": "Q(λ) with Oﬀ-Policy Corrections",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Q-learning",
            "Off-policy learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "R´emi Munos",
            "Tom Stepleton",
            "Anna Harutyunyan",
            "Marc G Bellemare"
        ],
        "dcterms:description": "A method for safe and efficient off-policy reinforcement learning.",
        "dcterms:title": "Non-Markov Retrace",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Off-policy learning",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Jing Peng",
            "Ronald J Williams"
        ],
        "dcterms:description": "An incremental multi-step Q-learning algorithm that improves learning efficiency.",
        "dcterms:title": "Q(λ)",
        "dcterms:issued": "1994",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Q-learning",
            "Multi-step learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Christopher John Cornish Hellaby Watkins"
        ],
        "dcterms:description": "A PhD thesis that introduces the concept of learning from delayed rewards in reinforcement learning.",
        "dcterms:title": "Watkins’ Q(λ)",
        "dcterms:issued": "1989",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Delayed rewards",
            "Q-learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Hamid Reza Maei",
            "Richard S Sutton"
        ],
        "dcterms:description": "A general gradient algorithm for temporal-difference prediction learning with eligibility traces.",
        "dcterms:title": "GQ(λ)",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Gradient algorithm",
            "Temporal-difference learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Ashique Rupam Mahmood",
            "Huizhen Yu",
            "Richard S Sutton"
        ],
        "dcterms:description": "A method for multi-step off-policy learning that does not rely on importance sampling ratios.",
        "dcterms:title": "Multi-step oﬀ-policy learning without importance sampling ratios",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Off-policy learning",
            "Multi-step learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Ziyu Wang",
            "Victor Bapst",
            "Nicolas Heess",
            "Volodymyr Mnih",
            "Remi Munos",
            "Koray Kavukcuoglu",
            "Nando de Freitas"
        ],
        "dcterms:description": "An actor-critic algorithm that improves sample efficiency using experience replay.",
        "dcterms:title": "Sample eﬃcient actor-critic with experience replay",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "dcat:keyword": [
            "Actor-critic",
            "Experience replay"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    }
]