[
    {
        "dcterms:creator": [
            "Marcin Andrychowicz",
            "Filip Wolski",
            "Alex Ray",
            "Jonas Schneider",
            "Rachel Fong",
            "Peter Welinder",
            "Bob McGrew",
            "Josh Tobin",
            "Pieter Abbeel",
            "Wojciech Zaremba"
        ],
        "dcterms:description": "Hindsight Experience Replay (HER) is a technique that allows agents to learn from past experiences by relabeling goals in the episodes they have already experienced, thus enabling more efficient learning in sparse reward settings.",
        "dcterms:title": "Hindsight Experience Replay (HER)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1707.01495",
        "dcat:theme": [
            "Reinforcement Learning",
            "Goal-Conditioned Learning"
        ],
        "dcat:keyword": [
            "Hindsight Experience Replay",
            "Reinforcement Learning",
            "Sparse Rewards"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1707.01495",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Goal Achievement",
            "Policy Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Jonathan Ho",
            "Stefano Ermon"
        ],
        "dcterms:description": "Generative Adversarial Imitation Learning (GAIL) is a framework that combines generative adversarial networks with imitation learning, allowing agents to learn policies by observing expert demonstrations.",
        "dcterms:title": "Goal-conditioned Reinforcement Learning (GAIL)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Imitation Learning",
            "Generative Models"
        ],
        "dcat:keyword": [
            "Generative Adversarial Imitation Learning",
            "Imitation Learning",
            "Goal-Conditioned Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Learning",
            "Goal Achievement"
        ]
    },
    {
        "dcterms:creator": [
            "Soroush Nasiriany"
        ],
        "dcterms:description": "DisCo RL (Distribution-conditioned Reinforcement Learning) is a method that extends goal-conditioned reinforcement learning to learn policies conditioned on distributions, enabling more general-purpose policies.",
        "dcterms:title": "DisCo RL",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "http://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-151.html",
        "dcat:theme": [
            "Reinforcement Learning",
            "Distribution Learning"
        ],
        "dcat:keyword": [
            "Distribution-conditioned Learning",
            "Reinforcement Learning",
            "General-purpose Policies"
        ],
        "dcat:landingPage": "http://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-151.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Learning",
            "Goal Achievement"
        ]
    },
    {
        "dcterms:creator": [
            "Martin Arjovsky",
            "Soumith Chintala",
            "LÃ©on Bottou"
        ],
        "dcterms:description": "Wasserstein GANs (WGAN) introduce a new distance metric for training generative adversarial networks, focusing on the Wasserstein distance to improve stability and convergence.",
        "dcterms:title": "Wasserstein GANs (WGAN)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "http://proceedings.mlr.press/v70/arjovsky17a.html",
        "dcat:theme": [
            "Generative Models",
            "Adversarial Learning"
        ],
        "dcat:keyword": [
            "Wasserstein Distance",
            "Generative Adversarial Networks",
            "Stability"
        ],
        "dcat:landingPage": "http://proceedings.mlr.press/v70/arjovsky17a.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Generative Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Kristian Hartikainen",
            "Xinyang Geng",
            "Tuomas Haarnoja",
            "Sergey Levine"
        ],
        "dcterms:description": "Monte Carlo (MC) distance estimation is a method used to estimate distances in reinforcement learning, particularly for skill discovery in semi-supervised and unsupervised settings.",
        "dcterms:title": "Monte Carlo (MC) distance estimation",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Distance Estimation"
        ],
        "dcat:keyword": [
            "Monte Carlo Methods",
            "Distance Estimation",
            "Skill Discovery"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Skill Discovery",
            "Distance Estimation"
        ]
    },
    {
        "dcterms:creator": [
            "Glen Berseth",
            "Daniel Geng",
            "Coline Devin",
            "Nicholas Rhinehart",
            "Chelsea Finn",
            "Dinesh Jayaraman",
            "Sergey Levine"
        ],
        "dcterms:description": "SMiRL (Surprise Minimizing Reinforcement Learning) focuses on minimizing surprise in reinforcement learning tasks, particularly in unstable environments.",
        "dcterms:title": "SMiRL",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "https://arxiv.org/abs/1912.05510",
        "dcat:theme": [
            "Reinforcement Learning",
            "Surprise Minimization"
        ],
        "dcat:keyword": [
            "Surprise Minimization",
            "Reinforcement Learning",
            "Unstable Environments"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/1912.05510",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Yuri Burda",
            "Harrison Edwards",
            "Amos Storkey",
            "Oleg Klimov"
        ],
        "dcterms:description": "Random Network Distillation (RND) is a method for exploration in reinforcement learning that uses a randomly initialized neural network to provide intrinsic rewards based on prediction errors.",
        "dcterms:title": "RND (Random Network Distillation)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Exploration"
        ],
        "dcat:keyword": [
            "Random Network Distillation",
            "Exploration",
            "Intrinsic Rewards"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Exploration"
        ]
    },
    {
        "dcterms:creator": [
            "Greg Brockman",
            "Vicki Cheung",
            "Ludwig Pettersson",
            "Jonas Schneider",
            "John Schulman",
            "Jie Tang",
            "Wojciech Zaremba"
        ],
        "dcterms:description": "Fetch Robot Tasks are a set of simulated robotics tasks available in OpenAI Gym, designed for evaluating goal-conditioned reinforcement learning algorithms.",
        "dcterms:title": "Fetch Robot Tasks",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1606.01540",
        "dcat:theme": [
            "Robotics",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Fetch Robot",
            "Goal-Conditioned Learning",
            "Robotics Tasks"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1606.01540",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Robotics",
            "Goal Achievement"
        ]
    }
]