To extract datasets from the research paper titled "Learning Visual Affordances with Target-Orientated Deep Q-Network to Grasp Objects by Harnessing Environmental Fixtures" by Hengyue Liang et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, and methodology sections** to identify any datasets mentioned. The abstract discusses a self-supervised learning approach for a grasping task, but it does not explicitly mention any datasets. I will need to look deeper into the paper.

Next, I will examine **section V (Experiment Settings and Results)**, as this section typically contains information about the datasets used for training and testing. Here, the authors mention using the **WISDOM dataset** for training their object instance segmentation network. This is a clear indication that a dataset is involved.

I will also look for any additional datasets mentioned in the **experiments** or **results** sections. The authors describe the training and testing scenes, but they primarily focus on the WISDOM dataset for the object segmentation task.

Now, I will check the **References section** to find the full citation for the WISDOM dataset. The relevant citation is:
> M. Danielczuk, M. Matl, S. Gupta, A. Li, A. Lee, J. Mahler, and K. Goldberg. *Segmenting unknown 3D objects from real depth images using mask R-CNN trained on synthetic data*. In 2019 International Conference on Robotics and Automation (ICRA), pages 7283â€“7290, IEEE, 2019.

Since the paper primarily discusses the WISDOM dataset, I will compile this information into a structured format.

In summary, the dataset extracted from the paper is:
1. **WISDOM Dataset**: Used for training the object instance segmentation network.

I will ensure to include the full citation for the WISDOM dataset in my final output.