[
    {
        "dcterms:creator": [
            "Yuval Tassa",
            "Yotam Doron",
            "Alistair Muldal",
            "Tom Erez",
            "Yazhe Li",
            "Diego de Las Casas",
            "David Budden",
            "Abbas Abdolmaleki",
            "Josh Merel",
            "Andrew Lefrancq"
        ],
        "dcterms:description": "A suite of continuous control tasks designed for reinforcement learning research, providing a diverse set of environments for evaluating algorithms.",
        "dcterms:title": "DeepMind Control Suite",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1801.00690",
        "dcat:theme": [
            "Reinforcement Learning",
            "Control Tasks"
        ],
        "dcat:keyword": [
            "Continuous control",
            "Reinforcement learning",
            "Benchmarking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Gabriel Barth-Maron",
            "Matthew W Hoffman",
            "David Budden",
            "Will Dabney",
            "Dan Horgan",
            "Dhruva Tb",
            "Alistair Muldal",
            "Nicolas Heess",
            "Timothy Lillicrap"
        ],
        "dcterms:description": "A reinforcement learning algorithm that combines distributional learning with off-policy training to improve performance in continuous control tasks.",
        "dcterms:title": "D4PG",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1804.08617",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Gradient Methods"
        ],
        "dcat:keyword": [
            "Distributional learning",
            "Off-policy",
            "Continuous control"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Junmin Zhong",
            "Ruofan Wu",
            "Jennie Si"
        ],
        "dcterms:description": "A method that introduces long N-step surrogate stage rewards to reduce variance in deep reinforcement learning.",
        "dcterms:title": "LNSS (Long N-step Surrogate Stage)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2210.04820",
        "dcat:theme": [
            "Reinforcement Learning",
            "Variance Reduction"
        ],
        "dcat:keyword": [
            "N-step rewards",
            "Variance reduction",
            "Deep reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Tuomas Haarnoja",
            "Aurick Zhou",
            "Kristian Hartikainen",
            "George Tucker",
            "Sehoon Ha",
            "Jie Tan",
            "Vikash Kumar",
            "Henry Zhu",
            "Abhishek Gupta",
            "Pieter Abbeel"
        ],
        "dcterms:description": "An off-policy actor-critic algorithm that incorporates entropy regularization to encourage exploration and improve learning stability.",
        "dcterms:title": "SAC (Soft Actor-Critic)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1812.05905",
        "dcat:theme": [
            "Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "Entropy regularization",
            "Off-policy",
            "Exploration"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Scott Fujimoto",
            "Herke Hoof",
            "David Meger"
        ],
        "dcterms:description": "A deep reinforcement learning algorithm that addresses function approximation error in actor-critic methods by using two Q-functions.",
        "dcterms:title": "TD3 (Twin Delayed Deep Deterministic Policy Gradient)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "Function approximation",
            "Actor-Critic",
            "Continuous control"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Timothy GJ Rudner",
            "Cong Lu",
            "Michael A Osborne",
            "Yarin Gal",
            "Yee Teh"
        ],
        "dcterms:description": "A reinforcement learning method that addresses pathologies in KL-regularized reinforcement learning from expert demonstrations.",
        "dcterms:title": "DDPG (Deep Deterministic Policy Gradient)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Gradient Methods"
        ],
        "dcat:keyword": [
            "Expert demonstrations",
            "Continuous control",
            "Actor-Critic"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "John Schulman",
            "Filip Wolski",
            "Prafulla Dhariwal",
            "Alec Radford",
            "Oleg Klimov"
        ],
        "dcterms:description": "A policy gradient method that optimizes policies by taking small steps in the direction of the policy gradient.",
        "dcterms:title": "PPO (Proximal Policy Optimization)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1707.06347",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Gradient Methods"
        ],
        "dcat:keyword": [
            "Policy optimization",
            "Actor-Critic",
            "Continuous control"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    }
]