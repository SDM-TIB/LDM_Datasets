[
    {
        "dcterms:creator": [
            "M. G. Bellemare",
            "Y. Naddaf",
            "J. Veness",
            "M. Bowling"
        ],
        "dcterms:description": "An evaluation platform for general agents, providing a variety of Atari 2600 games for testing reinforcement learning algorithms.",
        "dcterms:title": "Maze Environment",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game Environment"
        ],
        "dcat:keyword": [
            "Maze",
            "Atari",
            "Reinforcement Learning",
            "Evaluation Platform"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. A. Rusu",
            "S. Flennerhag",
            "D. Rao",
            "R. Pascanu",
            "R. Hadsell"
        ],
        "dcterms:description": "A collection of Atari 2600 games used to probe transfer in deep reinforcement learning without task engineering.",
        "dcterms:title": "Atari 2600 Games",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game Environment"
        ],
        "dcat:keyword": [
            "Atari",
            "Games",
            "Reinforcement Learning",
            "Transfer Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "V. Mnih",
            "K. Kavukcuoglu",
            "D. Silver",
            "A. A. Rusu",
            "J. Veness",
            "M. G. Bellemare",
            "A. Graves",
            "M. Riedmiller",
            "A. K. Fidjeland",
            "G. Ostrovski"
        ],
        "dcterms:description": "A deep reinforcement learning algorithm that combines Q-learning with deep neural networks to achieve human-level control in various environments.",
        "dcterms:title": "DQN (Deep Q-Network)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Deep Q-Network",
            "Reinforcement Learning",
            "Neural Networks"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Hessel",
            "J. Modayil",
            "H. van Hasselt",
            "T. Schaul",
            "G. Ostrovski",
            "W. Dabney",
            "D. Horgan",
            "B. Piot",
            "M. G. Azar",
            "D. Silver"
        ],
        "dcterms:description": "An agent that combines various improvements in deep reinforcement learning, enhancing performance across different tasks.",
        "dcterms:title": "Rainbow Agent",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Rainbow",
            "Deep Reinforcement Learning",
            "Agent"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Barreto",
            "W. Dabney",
            "R. Munos",
            "J. J. Hunt",
            "T. Schaul",
            "H. P. van Hasselt",
            "D. Silver"
        ],
        "dcterms:description": "A method for transfer in reinforcement learning that utilizes successor features to improve learning efficiency across tasks.",
        "dcterms:title": "Successor Features",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Transfer Learning"
        ],
        "dcat:keyword": [
            "Successor Features",
            "Transfer Learning",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Stooke",
            "K. Lee",
            "P. Abbeel",
            "M. Laskin"
        ],
        "dcterms:description": "A method that separates representation learning from reinforcement learning, allowing for improved learning efficiency.",
        "dcterms:title": "Augmented Temporal Contrast (ATC)",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Representation Learning"
        ],
        "dcat:keyword": [
            "Contrastive Learning",
            "Representation Learning",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Jaderberg",
            "V. Mnih",
            "W. M. Czarnecki",
            "T. Schaul",
            "J. Z. Leibo",
            "D. Silver",
            "K. Kavukcuoglu"
        ],
        "dcterms:description": "A framework that incorporates unsupervised auxiliary tasks into reinforcement learning to improve performance.",
        "dcterms:title": "Input Reconstruction (IR)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Auxiliary Tasks"
        ],
        "dcat:keyword": [
            "Input Reconstruction",
            "Auxiliary Tasks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Jaderberg",
            "V. Mnih",
            "W. M. Czarnecki",
            "T. Schaul",
            "J. Z. Leibo",
            "D. Silver",
            "K. Kavukcuoglu"
        ],
        "dcterms:description": "A task that predicts the next state of the agent based on its current state and action, used as an auxiliary task in reinforcement learning.",
        "dcterms:title": "Next Agent State Prediction (NAS)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Auxiliary Tasks"
        ],
        "dcat:keyword": [
            "Next State Prediction",
            "Auxiliary Tasks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Jaderberg",
            "V. Mnih",
            "W. M. Czarnecki",
            "T. Schaul",
            "J. Z. Leibo",
            "D. Silver",
            "K. Kavukcuoglu"
        ],
        "dcterms:description": "An auxiliary task that predicts the immediate reward based on the current state and action in reinforcement learning.",
        "dcterms:title": "Reward Prediction",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Auxiliary Tasks"
        ],
        "dcat:keyword": [
            "Reward Prediction",
            "Auxiliary Tasks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. Jaderberg",
            "V. Mnih",
            "W. M. Czarnecki",
            "T. Schaul",
            "J. Z. Leibo",
            "D. Silver",
            "K. Kavukcuoglu"
        ],
        "dcterms:description": "An auxiliary task that predicts expert-designed targets, such as the coordinates of the agent in the environment.",
        "dcterms:title": "Expert Target Prediction (XY)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Auxiliary Tasks"
        ],
        "dcat:keyword": [
            "Expert Target Prediction",
            "Auxiliary Tasks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "A. Barreto",
            "W. Dabney",
            "R. Munos",
            "J. J. Hunt",
            "T. Schaul",
            "H. P. van Hasselt",
            "D. Silver"
        ],
        "dcterms:description": "An auxiliary task that learns virtual value functions to facilitate learning in transfer tasks.",
        "dcterms:title": "Virtual Value Function Learning (VirtualVF)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Auxiliary Tasks"
        ],
        "dcat:keyword": [
            "Virtual Value Function",
            "Auxiliary Tasks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]