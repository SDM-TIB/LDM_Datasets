[
    {
        "dcterms:creator": [
            "L. Baird"
        ],
        "dcterms:description": "Baird's Counterexample is a 7-state MDP where every state has two actions, one leading to a random transition and the other to a deterministic state. It is used to demonstrate divergence in reinforcement learning with function approximation.",
        "dcterms:title": "Baird's Counterexample",
        "dcterms:issued": "1995",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Function Approximation"
        ],
        "dcat:keyword": [
            "MDP",
            "Divergence",
            "Function Approximation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Stability Testing"
        ]
    },
    {
        "dcterms:creator": [
            "G. Brockman",
            "V. Cheung",
            "L. Pettersson",
            "J. Schneider",
            "J. Schulman",
            "J. Tang",
            "W. Zaremba"
        ],
        "dcterms:description": "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a variety of environments to test and benchmark RL algorithms.",
        "dcterms:title": "OpenAI Gym",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "abs/1606.01540",
        "dcat:theme": [
            "Reinforcement Learning",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Toolkit",
            "Reinforcement Learning",
            "Environments"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Algorithm Development",
            "Performance Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "K. De Asis",
            "B. Bennett",
            "R. S. Sutton"
        ],
        "dcterms:description": "The Checkered Grid World is a 5x5 grid environment where the agent receives rewards based on the color of the square it enters. It is used to evaluate reinforcement learning algorithms in terms of reward prediction.",
        "dcterms:title": "Checkered Grid World",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Environment Simulation"
        ],
        "dcat:keyword": [
            "Grid World",
            "Reward Distribution",
            "Deterministic Movement"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reward Prediction",
            "Policy Evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The Slippery Maze Environment is a maze-like grid world where the agent's actions can be overridden by random actions, simulating a stochastic environment.",
        "dcterms:title": "Slippery Maze Environment",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Maze",
            "Stochastic Environment",
            "Grid World"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "The 19-State Random Walk is a tabular 1-dimensional environment where an agent transitions randomly between states, with terminal states providing rewards.",
        "dcterms:title": "19-State Random Walk",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [],
        "dcat:keyword": [
            "Random Walk",
            "Tabular Environment",
            "Episodic Task"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "V. Mnih",
            "K. Kavukcuoglu",
            "D. Silver",
            "A. A. Rusu",
            "J. Veness",
            "M. G. Bellemare",
            "A. Graves",
            "M. Riedmiller",
            "A. K. Fidjeland",
            "G. Ostrovski",
            "S. Petersen",
            "C. Beattie",
            "A. Sadik",
            "I. Antonoglou",
            "H. King",
            "D. Kumaran",
            "D. Wierstra",
            "S. Legg",
            "D. Hassabis"
        ],
        "dcterms:description": "LunarLander-v2 is a reinforcement learning environment where the agent must land a spacecraft on the lunar surface. It is used to evaluate deep reinforcement learning algorithms.",
        "dcterms:title": "LunarLander-v2",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Control Tasks"
        ],
        "dcat:keyword": [
            "Spacecraft Landing",
            "Deep Reinforcement Learning",
            "Simulation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "v2",
        "dcterms:format": "",
        "mls:task": [
            "Control",
            "Policy Learning"
        ]
    }
]