[
    {
        "dcterms:creator": [
            "Xinyu Zhang",
            "Nandan Thakur",
            "Odunayo Ogundepo",
            "Ehsan Kamalloo",
            "David Alfonso-Hermelo",
            "Xiaoguang Li",
            "Qun Liu",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "dcterms:description": "MIRACL is a multilingual dataset built for the WSDM 2023 Cup challenge, focusing on ad hoc retrieval across 18 different languages, with over 700k high-quality relevance judgments for around 77k queries over Wikipedia.",
        "dcterms:title": "MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "http://miracl.ai/",
        "dcat:theme": [
            "Information Retrieval",
            "Multilingual Processing"
        ],
        "dcat:keyword": [
            "Multilingual dataset",
            "Information retrieval",
            "Ad hoc retrieval",
            "Relevance judgments",
            "Wikipedia"
        ],
        "dcat:landingPage": "http://miracl.ai/",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Ad hoc retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Payal Bajaj",
            "Daniel Campos",
            "Nick Craswell",
            "Li Deng",
            "Jianfeng Gao",
            "Xiaodong Liu",
            "Rangan Majumder",
            "Andrew McNamara",
            "Bhaskar Mitra",
            "Tri Nguyen",
            "Mir Rosenberg",
            "Xia Song",
            "Alina Stoica",
            "Saurabh Tiwary",
            "Tong Wang"
        ],
        "dcterms:description": "MS MARCO is a human-generated machine reading comprehension dataset that has had a transformative impact on the field of information retrieval.",
        "dcterms:title": "MS MARCO",
        "dcterms:issued": "2018",
        "dcterms:language": "English",
        "dcterms:identifier": "arXiv:1611.09268v3",
        "dcat:theme": [
            "Machine Reading Comprehension",
            "Information Retrieval"
        ],
        "dcat:keyword": [
            "Machine reading comprehension",
            "Dataset",
            "Information retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Machine reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "dcterms:description": "SQuAD is a dataset containing over 100,000 questions for machine comprehension of text, designed to evaluate the ability of models to answer questions based on a given context.",
        "dcterms:title": "SQuAD",
        "dcterms:issued": "2016",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Reading Comprehension"
        ],
        "dcat:keyword": [
            "Question answering",
            "Machine comprehension",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "TriviaQA is a large-scale distantly supervised challenge dataset for reading comprehension, designed to evaluate models on their ability to answer trivia questions.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Trivia questions",
            "Reading comprehension",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reading comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "Natural Questions is a benchmark dataset for question answering research, providing a set of questions along with long-form answers from Wikipedia.",
        "dcterms:title": "Natural Questions",
        "dcterms:issued": "2019",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question answering",
            "Dataset",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Shuo Sun",
            "Kevin Duh"
        ],
        "dcterms:description": "CLIR-Matrix is a large collection of bilingual and multilingual datasets for cross-lingual information retrieval, designed to facilitate research in this area.",
        "dcterms:title": "CLIR-Matrix",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-Lingual Information Retrieval"
        ],
        "dcat:keyword": [
            "Cross-lingual retrieval",
            "Bilingual datasets",
            "Multilingual datasets"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Cross-lingual retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Junjie Hu",
            "Sebastian Ruder",
            "Aditya Siddhant",
            "Graham Neubig",
            "Orhan Firat",
            "Melvin Johnson"
        ],
        "dcterms:description": "XTREME is a massively multilingual multi-task benchmark for evaluating cross-lingual generalization across various NLP tasks.",
        "dcterms:title": "XTREME",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-Lingual Generalization"
        ],
        "dcat:keyword": [
            "Multilingual benchmark",
            "Cross-lingual tasks",
            "NLP evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Cross-lingual evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Shayne Longpre",
            "Yi Lu",
            "Joachim Daiber"
        ],
        "dcterms:description": "MKQA is a linguistically diverse benchmark for multilingual open-domain question answering, designed to evaluate models across various languages.",
        "dcterms:title": "MKQA",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multilingual Question Answering"
        ],
        "dcat:keyword": [
            "Open-domain QA",
            "Multilingual dataset",
            "Question answering"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Luiz Henrique Bonifacio",
            "Israel Campiotti",
            "Vitor Jeronymo",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "dcterms:description": "mMARCO is a multilingual version of the MS MARCO passage ranking dataset, designed to evaluate retrieval models across multiple languages.",
        "dcterms:title": "mMARCO",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2108.13897",
        "dcat:theme": [
            "Multilingual Information Retrieval"
        ],
        "dcat:keyword": [
            "Multilingual dataset",
            "Passage ranking",
            "Information retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Passage ranking"
        ]
    },
    {
        "dcterms:creator": [
            "Jonathan H. Clark",
            "Eunsol Choi",
            "Michael Collins",
            "Dan Garrette",
            "Tom Kwiatkowski",
            "Vitaly Nikolaev",
            "Jennimaria Palomaki"
        ],
        "dcterms:description": "TyDi QA is a benchmark for information-seeking question answering in typologically diverse languages, providing a set of questions and answers across multiple languages.",
        "dcterms:title": "TyDi QA",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Information-seeking QA",
            "Diverse languages",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Xinyu Zhang",
            "Xueguang Ma",
            "Peng Shi",
            "Jimmy Lin"
        ],
        "dcterms:description": "Mr. TyDi is a multilingual benchmark for dense retrieval, designed to evaluate retrieval models across multiple languages.",
        "dcterms:title": "Mr. TyDi",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dense Retrieval"
        ],
        "dcat:keyword": [
            "Multilingual benchmark",
            "Dense retrieval",
            "Information retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Dense retrieval"
        ]
    },
    {
        "dcterms:creator": [
            "Akari Asai",
            "Jungo Kasai",
            "Jonathan Clark",
            "Kenton Lee",
            "Eunsol Choi",
            "Hannaneh Hajishirzi"
        ],
        "dcterms:description": "XOR-TyDi is a dataset for cross-lingual open-retrieval question answering, designed to evaluate models on their ability to retrieve answers across languages.",
        "dcterms:title": "XOR-TyDi",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Cross-Lingual Question Answering"
        ],
        "dcat:keyword": [
            "Open-retrieval QA",
            "Cross-lingual",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question answering"
        ]
    }
]