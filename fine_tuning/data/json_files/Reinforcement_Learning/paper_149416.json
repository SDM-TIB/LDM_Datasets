[
    {
        "dcterms:creator": [
            "M. G. Bellemare",
            "Y. Naddaf",
            "J. Veness",
            "M. Bowling"
        ],
        "dcterms:description": "The Atari Games dataset is used as an evaluation platform for general agents in reinforcement learning, providing a variety of games to test algorithms.",
        "dcterms:title": "Atari Games",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Game AI"
        ],
        "dcat:keyword": [
            "Atari",
            "Reinforcement Learning",
            "Game Environment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Game Playing",
            "Policy Evaluation"
        ]
    },
    {
        "dcterms:creator": [],
        "dcterms:description": "Maze environments are designed to test the ability of reinforcement learning algorithms to navigate and learn from complex reward structures.",
        "dcterms:title": "Maze Environments",
        "dcterms:issued": "",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Navigation"
        ],
        "dcat:keyword": [
            "Maze",
            "Reward Structure",
            "Navigation Task"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Navigation",
            "Policy Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "M. G. Bellemare",
            "W. Dabney",
            "R. Munos"
        ],
        "dcterms:description": "C51 is a distributional reinforcement learning algorithm that models the return distribution for each state-action pair, providing a more informative learning signal.",
        "dcterms:title": "C51",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Distributional Learning"
        ],
        "dcat:keyword": [
            "Distributional RL",
            "Return Distribution",
            "Value Function"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "W. Dabney",
            "G. Ostrovski",
            "D. Silver",
            "R. Munos"
        ],
        "dcterms:description": "QR-DQN is an algorithm that uses implicit quantile networks to model the distribution of returns in reinforcement learning, enhancing the learning process.",
        "dcterms:title": "QR-DQN",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Quantile Regression"
        ],
        "dcat:keyword": [
            "Quantile Regression",
            "Distributional RL",
            "Return Distribution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "W. Dabney",
            "M. Rowland",
            "M. Bellemare",
            "R. Munos"
        ],
        "dcterms:description": "IQN is a distributional reinforcement learning algorithm that employs quantile regression to learn the distribution of returns, improving the agent's performance.",
        "dcterms:title": "IQN",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Quantile Regression"
        ],
        "dcat:keyword": [
            "Quantile Regression",
            "Distributional RL",
            "Return Distribution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "T. T. Nguyen",
            "S. Gupta",
            "S. Venkatesh"
        ],
        "dcterms:description": "MMDQN is a distributional reinforcement learning algorithm that utilizes moment matching to improve the learning of return distributions.",
        "dcterms:title": "MMDQN",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2007.12354",
        "dcat:theme": [
            "Reinforcement Learning",
            "Distributional Learning"
        ],
        "dcat:keyword": [
            "Moment Matching",
            "Distributional RL",
            "Return Distribution"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "H. Van Seijen",
            "M. Fatemi",
            "J. Romoff",
            "R. Laroche",
            "T. Barnes",
            "J. Tsang"
        ],
        "dcterms:description": "HRA is a hybrid reward architecture that models separate value functions for different sources of rewards, enhancing the learning process in reinforcement learning.",
        "dcterms:title": "HRA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1706.04208",
        "dcat:theme": [
            "Reinforcement Learning",
            "Hybrid Architecture"
        ],
        "dcat:keyword": [
            "Hybrid Rewards",
            "Value Function",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Lin",
            "D. Yang",
            "L. Zhao",
            "T. Qin",
            "G. Yang",
            "T.-Y. Liu"
        ],
        "dcterms:description": "RD2 is a method for reward decomposition that separates representation learning from reward learning in reinforcement learning.",
        "dcterms:title": "RD2",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Reward Decomposition"
        ],
        "dcat:keyword": [
            "Reward Decomposition",
            "Representation Learning",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "D. Freirich",
            "T. Shimkin",
            "R. Meir",
            "A. Tamar"
        ],
        "dcterms:description": "Bellman GAN is a method that combines distributional reinforcement learning with generative adversarial networks for policy evaluation and exploration.",
        "dcterms:title": "Bellman GAN",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Generative Models"
        ],
        "dcat:keyword": [
            "GAN",
            "Distributional RL",
            "Policy Evaluation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Evaluation",
            "Exploration"
        ]
    },
    {
        "dcterms:creator": [
            "Z. Lin",
            "L. Zhao",
            "D. Yang",
            "T. Qin",
            "G. Yang",
            "T.-Y. Liu"
        ],
        "dcterms:description": "DRDRL is a method for distributional reward decomposition that enhances the learning of multiple reward sources in reinforcement learning.",
        "dcterms:title": "DRDRL",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1911.02166",
        "dcat:theme": [
            "Reinforcement Learning",
            "Reward Decomposition"
        ],
        "dcat:keyword": [
            "Distributional Learning",
            "Reward Decomposition",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Value Estimation",
            "Policy Optimization"
        ]
    }
]