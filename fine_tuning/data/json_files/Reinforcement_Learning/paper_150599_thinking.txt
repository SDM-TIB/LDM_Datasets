To extract datasets from the research paper titled "EdGCon: Auto-assigner of Iconicity Ratings Grounded by Lexical Properties to Aid in Generation of Technical Gestures" by Sameena Hossain et al., I will follow a systematic approach.

First, I will read through the **abstract, introduction, methodology, and evaluation sections** to identify any datasets mentioned. The abstract mentions the collection of 30 ad hoc crowdsourced technical gestures and 31 gestures from the DeafTEC technical corpus, indicating that these are datasets of interest.

Next, I will focus on **section 4 (Evaluation & Results)**, where the authors explicitly describe the datasets used for their experiments:

1. **DeafTEC Data Set**: This dataset consists of 31 technical gestures from the DeafTEC STEM dictionary corpus. The authors assigned iconicity ratings to these gestures based on manual observation. This dataset serves as the "Existing Gesture Set" for their work.

2. **New Technical Gesture Data Set**: This dataset includes 30 ad hoc and crowdsourced technical gestures collected from various internet resources. The authors also assigned iconicity ratings to these gestures based on manual observation.

Now, I will check the **References section** to find the full citations for these datasets:

- For the **DeafTEC Data Set**, the citation is:
  > DeafTEC. 2020. DeafTEC STEM Sign Video Dictionary. https://deaftec.org/stem-dictionary/ Accessed: 2020-09-14.

- For the **New Technical Gesture Data Set**, since it is a collection of gestures from various internet resources, a specific citation may not be available. However, the authors mention that they followed the same process as the DeafTEC dataset for assigning iconicity ratings.

After gathering this information, I will compile the dataset entries, ensuring that I include the full citations for the DeafTEC Data Set and note the nature of the New Technical Gesture Data Set.

Finally, I will prepare the dataset entries in a structured format for further processing or review.