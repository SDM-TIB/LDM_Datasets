[
    {
        "dcterms:creator": [
            "C.J.C.H. Watkins",
            "P. Dayan"
        ],
        "dcterms:description": "A foundational reinforcement learning algorithm that uses a value-based approach to learn the optimal action-value function.",
        "dcterms:title": "Q-learning",
        "dcterms:issued": "1992",
        "dcterms:language": "",
        "dcterms:identifier": "https://link.springer.com/article/10.1023/A:1022676722315",
        "dcat:theme": [
            "Reinforcement Learning",
            "Value-based Learning"
        ],
        "dcat:keyword": [
            "Q-learning",
            "Reinforcement Learning",
            "Value Function"
        ],
        "dcat:landingPage": "https://link.springer.com/article/10.1023/A:1022676722315",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "V. Mnih",
            "K. Kavukcuoglu",
            "D. Silver",
            "A. Graves",
            "I. Antonoglou",
            "D. Wierstra",
            "M. Riedmiller"
        ],
        "dcterms:description": "An advanced version of Q-learning that utilizes deep neural networks to approximate the Q-value function, enabling it to handle high-dimensional state spaces.",
        "dcterms:title": "Deep Q-Network (DQN)",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1312.5602",
        "dcat:theme": [
            "Deep Reinforcement Learning",
            "Value-based Learning"
        ],
        "dcat:keyword": [
            "Deep Q-Network",
            "Neural Networks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1312.5602",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "R.S. Sutton",
            "D. McAllester",
            "S. Singh",
            "Y. Mansour"
        ],
        "dcterms:description": "A policy gradient method that directly optimizes the policy by using the gradient of expected rewards.",
        "dcterms:title": "REINFORCE",
        "dcterms:issued": "2000",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy-based Learning"
        ],
        "dcat:keyword": [
            "Policy Gradient",
            "REINFORCE",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "T.P. Lillicrap",
            "J.J. Hunt",
            "A. Pritzel",
            "N. Heess",
            "T. Erez",
            "Y. Tassa",
            "D. Silver",
            "D. Wierstra"
        ],
        "dcterms:description": "An actor-critic algorithm that uses deep learning to handle continuous action spaces.",
        "dcterms:title": "Deep Deterministic Policy Gradient (DDPG)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Deep Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "DDPG",
            "Actor-Critic",
            "Continuous Control"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Fujimoto",
            "H. Van Hoof",
            "D. Meger"
        ],
        "dcterms:description": "An improvement over DDPG that addresses function approximation error by using two critic networks.",
        "dcterms:title": "Twin Delayed Deep Deterministic (TD3)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Deep Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "TD3",
            "Actor-Critic",
            "Function Approximation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Schulman",
            "F. Wolski",
            "P. Dhariwal",
            "A. Radford",
            "O. Klimov"
        ],
        "dcterms:description": "An on-policy reinforcement learning algorithm that uses a clipped objective function to improve training stability.",
        "dcterms:title": "Proximal Policy Optimization (PPO)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1707.06347",
        "dcat:theme": [
            "Deep Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "PPO",
            "Policy Optimization",
            "Stability"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1707.06347",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]