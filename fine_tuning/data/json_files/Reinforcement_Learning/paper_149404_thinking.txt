To extract datasets from the research paper titled "Single-Modal Entropy based Active Learning for Visual Question Answering" by Dong-Jin Kim et al., I will follow a systematic approach to ensure I capture all relevant information, including full citations for each dataset.

First, I will read through the **abstract, introduction, and experiments sections** to identify any datasets mentioned. The abstract indicates that the authors confirm their findings on various VQA datasets, which suggests that specific datasets will be detailed later in the paper.

Next, I will focus on the **experiments section**. In **section 4.1**, the authors explicitly mention the datasets used for evaluation:

1. **VQA v2 Dataset**: This dataset consists of 1.1 million data points (visual-question-answer triplets) with specific splits for training, validation, and testing. The authors provide details about the number of images, questions, and answers, which indicates its significance in their experiments.

2. **VizWiz VQA Dataset**: This dataset contains 19,425 train image/question pairs. The authors mention that it is a more challenging dataset, which adds context to its use in their experiments.

3. **VQA-CP2 Dataset**: This dataset is a re-ordering of the VQA v2 dataset, containing 438,000 train image/question pairs. The authors discuss its size and relevance to their experiments.

4. **NTU RGB+D Dataset**: Although primarily designed for action recognition, the authors evaluate their method on this dataset to demonstrate the extensibility of their approach. They mention using 37,646 pairs of inputs.

Now, I will check the **References section** to gather full citations for each dataset:

- For the **VQA v2 Dataset**, the citation is:
  > Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., & Parikh, D. (2015). VQA: Visual Question Answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).

- For the **VizWiz VQA Dataset**, the citation is:
  > Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., & Bigham, J. P. (2018). VizWiz Grand Challenge: Answering Visual Questions from Blind People. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

- For the **VQA-CP2 Dataset**, the citation is:
  > Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

- For the **NTU RGB+D Dataset**, the citation is:
  > Shahroudy, A., Liu, J., Ng, T. T., & Wang, G. (2016). NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

After gathering all this information, I will compile the dataset entries into a structured format that includes the dataset names, descriptions, and full citations, ensuring that each dataset is accurately represented and properly cited.