[
    {
        "dcterms:creator": [
            "Y. Bai",
            "A. Jones",
            "K. Ndousse"
        ],
        "dcterms:description": "The HH-RLHF dataset consists of 160k pairwise samples, including 118k helpful and 42k harmless instances, used for training a reward model to evaluate helpfulness and harmlessness in responses.",
        "dcterms:title": "HH-RLHF dataset",
        "dcterms:issued": "2022",
        "dcterms:language": "English",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "dcat:keyword": [
            "Human Feedback",
            "Reinforcement Learning",
            "Dataset",
            "Language Model Training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reward Modeling",
            "Preference Learning"
        ]
    },
    {
        "dcterms:creator": [
            "OpenLMLab"
        ],
        "dcterms:description": "OpenChineseLLaMA is a Chinese large language model developed through incremental pre-training on Chinese datasets, significantly improving understanding and generation abilities in Chinese.",
        "dcterms:title": "OpenChineseLLaMA",
        "dcterms:issued": "2023",
        "dcterms:language": "Chinese",
        "dcterms:identifier": "https://github.com/OpenLMLab/OpenChineseLLaMA",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "Chinese Language Model",
            "Incremental Pre-training",
            "Language Understanding"
        ],
        "dcat:landingPage": "https://github.com/OpenLMLab/OpenChineseLLaMA",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Generation",
            "Language Understanding"
        ]
    },
    {
        "dcterms:creator": [
            "Touvron, H.",
            "T. Lavril",
            "G. Izacard"
        ],
        "dcterms:description": "LLaMA-7B is a foundation language model that serves as a base for various language tasks, providing a robust architecture for language understanding and generation.",
        "dcterms:title": "LLaMA-7B",
        "dcterms:issued": "2023",
        "dcterms:language": "English",
        "dcterms:identifier": "https://arxiv.org/abs/2302.13971",
        "dcat:theme": [
            "Natural Language Processing",
            "Language Model Training"
        ],
        "dcat:keyword": [
            "Foundation Model",
            "Language Understanding",
            "Language Generation"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2302.13971",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling",
            "Text Generation"
        ]
    }
]