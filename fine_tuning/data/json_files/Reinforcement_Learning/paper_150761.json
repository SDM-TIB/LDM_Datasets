[
    {
        "dcterms:creator": [
            "S. Lin",
            "J. Hilton",
            "O. Evans"
        ],
        "dcterms:description": "TruthfulQA is a benchmark dataset used to evaluate the consistency and accuracy of language models in generating answers to questions, particularly focusing on how models mimic human falsehoods.",
        "dcterms:title": "TruthfulQA",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Benchmarking"
        ],
        "dcat:keyword": [
            "Language models",
            "Question answering",
            "Consistency",
            "Accuracy"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "F. Petroni",
            "T. Rockt\"aschel",
            "S. Riedel"
        ],
        "dcterms:description": "LAMA is a probing dataset designed to assess the knowledge contained within language models by evaluating their ability to answer factual questions.",
        "dcterms:title": "LAMA",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Knowledge Evaluation"
        ],
        "dcat:keyword": [
            "Language models",
            "Knowledge base",
            "Probing"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Knowledge Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Elazar",
            "N. Kassner",
            "S. Ravfogel"
        ],
        "dcterms:description": "ParaRel is a dataset aimed at measuring and improving the consistency of pretrained language models by analyzing their predictions for masked tokens.",
        "dcterms:title": "ParaRel",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Evaluation"
        ],
        "dcat:keyword": [
            "Consistency",
            "Language models",
            "Masked language modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Y. Zhang",
            "J. Baldridge",
            "L. He"
        ],
        "dcterms:description": "PAWS is a dataset that generates paraphrase adversaries through word scrambling to evaluate the robustness of models in recognizing paraphrases.",
        "dcterms:title": "PAWS",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Paraphrase Detection"
        ],
        "dcat:keyword": [
            "Paraphrase",
            "Adversarial examples",
            "Natural language understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Paraphrase Detection"
        ]
    },
    {
        "dcterms:creator": [
            "A. Williams",
            "N. Nangia",
            "S. R. Bowman"
        ],
        "dcterms:description": "MNLI is a broad-coverage challenge corpus designed for evaluating sentence understanding through inference tasks.",
        "dcterms:title": "MNLI",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Inference"
        ],
        "dcat:keyword": [
            "Natural language inference",
            "Sentence understanding"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Natural Language Inference"
        ]
    },
    {
        "dcterms:creator": [
            "P. He",
            "X. Liu",
            "J. Gao",
            "W. Chen"
        ],
        "dcterms:description": "DeBERTa is a language model that enhances BERT with disentangled attention mechanisms to improve its performance on various NLP tasks.",
        "dcterms:title": "DeBERTa",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Development"
        ],
        "dcat:keyword": [
            "Language model",
            "Disentangled attention"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    },
    {
        "dcterms:creator": [
            "P. He",
            "J. Gao",
            "W. Chen"
        ],
        "dcterms:description": "DeBERTaV3 is an improved version of DeBERTa that utilizes ELECTRA-style pre-training with gradient-disentangled embedding sharing.",
        "dcterms:title": "DeBERTaV3",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Model Development"
        ],
        "dcat:keyword": [
            "Language model",
            "ELECTRA",
            "Pre-training"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Language Modeling"
        ]
    }
]