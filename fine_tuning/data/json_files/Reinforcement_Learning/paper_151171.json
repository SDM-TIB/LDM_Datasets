[
    {
        "dcterms:creator": [
            "Fabio Petroni",
            "Aleksandra Piktus",
            "Angela Fan",
            "Patrick Lewis",
            "Majid Yazdani",
            "Nicola De Cao",
            "James Thorne",
            "Yacine Jernite",
            "Vladimir Karpukhin",
            "Jean Maillard",
            "Vassilis Plachouras",
            "Tim Rockt√§schel",
            "Sebastian Riedel"
        ],
        "dcterms:description": "KILT is a benchmark for knowledge intensive language tasks, providing a diverse set of datasets for evaluating various NLP tasks.",
        "dcterms:title": "KILT",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Knowledge Intensive Language Tasks"
        ],
        "dcat:keyword": [
            "Benchmark",
            "NLP",
            "Knowledge Retrieval"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "Tom Kwiatkowski",
            "Jennimaria Palomaki",
            "Olivia Redfield",
            "Michael Collins",
            "Ankur Parikh",
            "Chris Alberti",
            "Danielle Epstein",
            "Illia Polosukhin",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Llion Jones",
            "Matthew Kelcey",
            "Ming-Wei Chang",
            "Andrew M. Dai",
            "Jakob Uszkoreit",
            "Quoc Le",
            "Slav Petrov"
        ],
        "dcterms:description": "Natural Questions is a benchmark dataset for question answering research, containing real user questions and corresponding Wikipedia passages.",
        "dcterms:title": "Natural Questions (NQ)",
        "dcterms:issued": "2019",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Question Answering"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Benchmark",
            "Wikipedia"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "dcterms:description": "TriviaQA is a large scale distantly supervised challenge dataset for reading comprehension, designed to test the ability of models to answer trivia questions.",
        "dcterms:title": "TriviaQA",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reading Comprehension"
        ],
        "dcat:keyword": [
            "Trivia Questions",
            "Reading Comprehension",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reading Comprehension"
        ]
    },
    {
        "dcterms:creator": [
            "Zhilin Yang",
            "Peng Qi",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "William Cohen",
            "Ruslan Salakhutdinov",
            "Christopher D. Manning"
        ],
        "dcterms:description": "HotpotQA is a dataset for diverse, explainable multi-hop question answering, requiring models to reason over multiple documents.",
        "dcterms:title": "HotpotQA",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Multi-hop Question Answering"
        ],
        "dcat:keyword": [
            "Question Answering",
            "Multi-hop Reasoning",
            "Dataset"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Question Answering"
        ]
    },
    {
        "dcterms:creator": [
            "Andreas Vlachos",
            "Christos Christodoulopoulos",
            "James Thorne"
        ],
        "dcterms:description": "FEVER is a large-scale dataset for fact extraction and verification, designed to evaluate models on their ability to verify factual claims.",
        "dcterms:title": "FEVER",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Fact Verification"
        ],
        "dcat:keyword": [
            "Fact Verification",
            "Dataset",
            "NLP"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Fact Verification"
        ]
    },
    {
        "dcterms:creator": [
            "Emily Dinan",
            "Stephen Roller",
            "Kurt Shuster",
            "Angela Fan",
            "Michael Auli",
            "Jason Weston"
        ],
        "dcterms:description": "Wizard of Wikipedia is a dataset for training knowledge-powered conversational agents, focusing on dialogue generation.",
        "dcterms:title": "Wizard of Wikipedia (WoW)",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Dialogue Generation"
        ],
        "dcat:keyword": [
            "Conversational Agents",
            "Dialogue",
            "Knowledge"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Dialogue Generation"
        ]
    }
]