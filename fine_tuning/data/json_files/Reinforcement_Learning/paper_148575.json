[
    {
        "dcterms:creator": [
            "John Schulman",
            "Sergey Levine",
            "Kevin M. O. H. Lee",
            "Michael Jordan",
            "Emma Pierson"
        ],
        "dcterms:description": "Trust Region Policy Optimization (TRPO) is a policy gradient method that optimizes policies while ensuring that the new policy is not too different from the old policy, using constraints based on Kullback-Leibler divergence.",
        "dcterms:title": "Trust Region Policy Optimization (TRPO)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "Policy Gradient",
            "Kullback-Leibler Divergence",
            "Optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "Volodymyr Mnih",
            "Nicolas Heess",
            "Alexandra Graves",
            "Julian Schmitt",
            "Ivo Danihelka",
            "David Silver",
            "Nando de Freitas"
        ],
        "dcterms:description": "Asynchronous Advantage Actor-Critic (A3C) is a reinforcement learning algorithm that uses multiple agents to explore the environment in parallel and updates a shared model, optimizing the policy and value function simultaneously.",
        "dcterms:title": "Asynchronous Advantage Actor-Critic (A3C)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "Asynchronous Learning",
            "Policy Optimization",
            "Value Function"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization",
            "Value Function Approximation"
        ]
    },
    {
        "dcterms:creator": [
            "Jonathan Ho",
            "Stefano Ermon"
        ],
        "dcterms:description": "Generative Adversarial Imitation Learning (GAIL) is a framework that allows an agent to learn a policy by imitating expert demonstrations without needing to recover the reward function, using adversarial training techniques.",
        "dcterms:title": "Generative Adversarial Imitation Learning (GAIL)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Imitation Learning",
            "Adversarial Learning"
        ],
        "dcat:keyword": [
            "Generative Adversarial Networks",
            "Imitation Learning",
            "Policy Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Imitation Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Brian D Ziebart",
            "J Andrew Bagnell",
            "Anind K Dey"
        ],
        "dcterms:description": "Maximum causal entropy inverse reinforcement learning aims to recover a policy that not only performs well but also captures diverse behaviors by maximizing the causal entropy of the recovered policy.",
        "dcterms:title": "Maximum causal entropy IRL",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Inverse Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Causal Entropy",
            "Policy Recovery",
            "Behavior Modeling"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Inverse Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "Sergey Levine",
            "Vladlen Koltun"
        ],
        "dcterms:description": "Guided Policy Search (GPS) is a method for training complex policies by imitating a teacher policy while optimizing a specified cost function, using alternating gradient steps on primal and dual variables.",
        "dcterms:title": "Guided Policy Search (GPS)",
        "dcterms:issued": "2013",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "Policy Imitation",
            "Gradient Descent",
            "High-Dimensional Policies"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "Jan Peters",
            "Katharina Mulling",
            "Yasemin Altun"
        ],
        "dcterms:description": "Relative entropy policy search is a method that formulates policy optimization as a relative entropy minimization problem, allowing for efficient policy updates.",
        "dcterms:title": "Relative entropy policy search",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "Relative Entropy",
            "Policy Optimization",
            "Efficient Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "Lin Xiao"
        ],
        "dcterms:description": "Dual averaging for distributed optimization is a method that provides convergence analysis and network scaling for optimization problems in a distributed setting.",
        "dcterms:title": "Dual averaging for distributed optimization",
        "dcterms:issued": "2010",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Optimization",
            "Distributed Learning"
        ],
        "dcat:keyword": [
            "Distributed Optimization",
            "Convergence Analysis",
            "Stochastic Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "Joshua Achiam",
            "Nicolas Heess",
            "A. Tamar",
            "J. Abbeel"
        ],
        "dcterms:description": "Constrained Policy Optimization (CPO) is a method that incorporates safety constraints into policy optimization, ensuring that the expected performance remains within specified limits.",
        "dcterms:title": "Constrained Policy Optimization (CPO)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Policy Optimization"
        ],
        "dcat:keyword": [
            "Safety Constraints",
            "Policy Optimization",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Policy Optimization"
        ]
    }
]