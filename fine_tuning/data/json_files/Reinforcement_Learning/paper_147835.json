[
    {
        "dcterms:creator": [
            "Michael Behrisch",
            "Daniel Krajzewicz",
            "Melanie Weber"
        ],
        "dcterms:description": "SUMO is a traffic simulation environment used for modeling urban mobility scenarios, allowing for the simulation of various traffic conditions and vehicle behaviors.",
        "dcterms:title": "SUMO (Simulation of Urban Mobility)",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1007/978-3-662-45079-6",
        "dcat:theme": [
            "Traffic Simulation",
            "Urban Mobility"
        ],
        "dcat:keyword": [
            "Traffic simulation",
            "Urban mobility",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "https://doi.org/10.1007/978-3-662-45079-6",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Traffic simulation",
            "Reinforcement learning"
        ]
    },
    {
        "dcterms:creator": [
            "Ahmad El Sallab",
            "Mohammed Abdou",
            "Etienne Perot",
            "Senthil Yogamani"
        ],
        "dcterms:description": "TORCS is an open racing car simulator that provides a platform for testing and developing autonomous driving algorithms.",
        "dcterms:title": "TORCS (The Open Racing Car Simulator)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1612.04340",
        "dcat:theme": [
            "Racing Simulation",
            "Autonomous Driving"
        ],
        "dcat:keyword": [
            "Racing simulator",
            "Autonomous driving",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1612.04340",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Lane keeping assist",
            "Autonomous driving"
        ]
    },
    {
        "dcterms:creator": [
            "Carlos Guestrin",
            "Daphne Koller",
            "Ronald Parr",
            "Shobha Venkataraman"
        ],
        "dcterms:description": "Factored MDPs are used to represent large state spaces in a more manageable way by exploiting internal structures, allowing for efficient solution algorithms.",
        "dcterms:title": "Factored MDPs",
        "dcterms:issued": "2003",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.1613/jair.1000",
        "dcat:theme": [
            "Reinforcement Learning",
            "Decision Processes"
        ],
        "dcat:keyword": [
            "Factored MDP",
            "Reinforcement learning",
            "Decision making"
        ],
        "dcat:landingPage": "https://doi.org/10.1613/jair.1000",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Decision making",
            "Reinforcement learning"
        ]
    },
    {
        "dcterms:creator": [
            "Max Jaderberg",
            "Volodymyr Mnih",
            "Wojciech Marian Czarnecki",
            "Tom Schaul",
            "Joel Z. Leibo",
            "David Silver",
            "Koray Kavukcuoglu"
        ],
        "dcterms:description": "Auxiliary POMDPs are used to train factored Q functions, providing additional features that improve data efficiency in reinforcement learning tasks.",
        "dcterms:title": "Auxiliary POMDPs",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "http://arxiv.org/abs/1611.05397",
        "dcat:theme": [
            "Reinforcement Learning",
            "Auxiliary Tasks"
        ],
        "dcat:keyword": [
            "POMDP",
            "Reinforcement learning",
            "Auxiliary tasks"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1611.05397",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement learning",
            "Task training"
        ]
    }
]