[
    {
        "dcterms:creator": [
            "Broderick Arneson",
            "Ryan Hayward",
            "Philip Henderson"
        ],
        "dcterms:description": "A database of starting positions generated from 10,000 games played by a noisy version of a strong hex playing program based on alpha-beta search called Wolve.",
        "dcterms:title": "Position Database",
        "dcterms:issued": "2008",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Artificial Intelligence"
        ],
        "dcat:keyword": [
            "Hex",
            "Game positions",
            "Alpha-beta search"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Game Position Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Vadim V Anshelevich"
        ],
        "dcterms:description": "A heuristic based on electrical resistance used to evaluate hex game positions, providing action values for moves.",
        "dcterms:title": "Hex Game Positions (from the heuristic based on electrical resistance)",
        "dcterms:issued": "2000",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Artificial Intelligence"
        ],
        "dcat:keyword": [
            "Hex",
            "Heuristic evaluation",
            "Electrical resistance"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Game Position Evaluation"
        ]
    },
    {
        "dcterms:creator": [
            "Shih-Chieh Huang",
            "Broderick Arneson",
            "Ryan B. Hayward",
            "Martin Mller",
            "Jakub Pawlewicz"
        ],
        "dcterms:description": "A pattern-based Monte Carlo Tree Search (MCTS) player for the game of Hex.",
        "dcterms:title": "MoHex",
        "dcterms:issued": "2014",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Game Theory",
            "Artificial Intelligence"
        ],
        "dcat:keyword": [
            "Hex",
            "Monte Carlo Tree Search",
            "Game AI"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "2.0",
        "dcterms:format": "",
        "mls:task": [
            "Game Playing"
        ]
    },
    {
        "dcterms:creator": [
            "Volodymyr Mnih",
            "Koray Kavukcuoglu",
            "David Silver",
            "Andrei A. Rusu",
            "Joel Veness",
            "Marc G. Bellemare",
            "Alex Graves",
            "Martin Riedmiller",
            "Andreas K. Fidjeland",
            "Georg Ostrovski",
            "Stig Petersen",
            "Charles Beattie",
            "Amir Sadik",
            "Ioannis Antonoglou",
            "Helen King",
            "Dharshan Kumaran",
            "Daan Wierstra",
            "Shane Legg",
            "Demis Hassabis"
        ],
        "dcterms:description": "A reinforcement learning algorithm that learns a mapping from states to action values using deep neural networks and experience replay.",
        "dcterms:title": "Deep Q-learning with Experience Replay",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Machine Learning",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Deep Q-learning",
            "Reinforcement Learning",
            "Experience Replay"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    }
]