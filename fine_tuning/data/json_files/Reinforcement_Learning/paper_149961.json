[
    {
        "dcterms:creator": [
            "Debbie S. Ma",
            "Joshua Correll",
            "Bernd Wittenbrink"
        ],
        "dcterms:description": "The Chicago Face Database (CFD) contains self-classified choices from its definition of race and ethnicity categories, consisting of Latina, Asian, Black, and White and self-classified sex categories of Female and Male. It was collected with consent for use in research.",
        "dcterms:title": "Chicago Face Database (CFD)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "https://doi.org/10.3758/s13428-014-0532-5",
        "dcat:theme": [
            "Facial Recognition",
            "Bias in AI"
        ],
        "dcat:keyword": [
            "Face dataset",
            "Race",
            "Gender",
            "Stereotypes"
        ],
        "dcat:landingPage": "https://doi.org/10.3758/s13428-014-0532-5",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image",
        "mls:task": [
            "Facial Recognition",
            "Bias Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Christoph Schuhmann",
            "Richard Vencu",
            "Romain Beaumont",
            "Robert Kaczmarczyk",
            "Clayton Mullis",
            "Aarush Katta",
            "Theo Coombes",
            "Jenia Jitsev",
            "Aran Komatsuzaki"
        ],
        "dcterms:description": "LAION-400M is an open dataset of CLIP-filtered 400 million image-text pairs, which contains troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content.",
        "dcterms:title": "LAION-400M",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:2111.02114",
        "dcat:theme": [
            "Image-Text Pairs",
            "Bias in AI"
        ],
        "dcat:keyword": [
            "Image dataset",
            "Text dataset",
            "Bias",
            "Stereotypes"
        ],
        "dcat:landingPage": "https://arxiv.org/abs/2111.02114",
        "dcterms:hasVersion": "",
        "dcterms:format": "Image-Text Pair",
        "mls:task": [
            "Image-Text Matching",
            "Bias Analysis"
        ]
    },
    {
        "dcterms:creator": [
            "Alec Radford",
            "Jong Wook Kim",
            "Chris Hallacy",
            "Aditya Ramesh",
            "Gabriel Goh",
            "Sandhini Agarwal",
            "Girish Sastry",
            "Amanda Askell",
            "Pamela Mishkin",
            "Jack Clark",
            "Gretchen Krueger",
            "Ilya Sutskever"
        ],
        "dcterms:description": "OpenAI CLIP is a neural network that matches images to captions by training on toxic internet data, with expected harmful outcomes. It is known to contain bias and is not recommended for untested and unconstrained deployment.",
        "dcterms:title": "OpenAI CLIP",
        "dcterms:issued": "2021",
        "dcterms:language": "",
        "dcterms:identifier": "https://proceedings.mlr.press/v139/radford21a.html",
        "dcat:theme": [
            "Image-Text Matching",
            "Bias in AI"
        ],
        "dcat:keyword": [
            "Neural Network",
            "Image Matching",
            "Bias",
            "Stereotypes"
        ],
        "dcat:landingPage": "https://proceedings.mlr.press/v139/radford21a.html",
        "dcterms:hasVersion": "",
        "dcterms:format": "Model",
        "mls:task": [
            "Image-Text Matching",
            "Bias Analysis"
        ]
    }
]