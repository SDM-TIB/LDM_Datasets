To extract datasets from the research paper titled "End-to-end Learning of Action Detection from Frame Glimpses in Videos" by Serena Yeung et al., I will follow a systematic approach to identify and document the datasets mentioned in the paper.

First, I will read through the **abstract, introduction, and experiments sections** to locate any references to datasets. The abstract mentions that the model achieves state-of-the-art results on the **THUMOS’14** and **ActivityNet** datasets, indicating that these are the primary datasets used in the research.

Next, I will examine the **experiments section** (specifically section 4) to gather more details about these datasets. In section 4.2, the authors describe the **THUMOS’14 dataset**, which consists of 20 classes of sports actions. They provide performance metrics and comparisons with other methods, confirming its use in their experiments. Similarly, in section 4.3, they discuss the **ActivityNet dataset**, which contains a large number of temporal annotations for various activities.

Now, I will look at the **References section** to find the full citations for these datasets:

1. For the **THUMOS’14 dataset**, the citation is:
   > Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar. *THUMOS challenge: Action recognition with a large number of classes*. http://crcv.ucf.edu/THUMOS14/, 2014.

2. For the **ActivityNet dataset**, the citation is:
   > F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles. *Activitynet: A large-scale video benchmark for human activity understanding*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961–970, 2015.

Having identified the datasets and their citations, I will now compile this information into a structured format for further use. This includes ensuring that each dataset is clearly described along with its citation, which is crucial for proper attribution in any subsequent analysis or application.