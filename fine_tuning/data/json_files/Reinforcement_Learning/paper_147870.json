[
    {
        "dcterms:creator": [
            "R. S. Sutton",
            "A. G. Barto"
        ],
        "dcterms:description": "The Mountain Car Environment is a classic reinforcement learning problem where an underpowered car must reach the top of a hill. The environment allows for faster training times and larger sample sizes.",
        "dcterms:title": "Mountain Car Environment",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Control Systems"
        ],
        "dcat:keyword": [
            "Mountain Car",
            "Reinforcement Learning",
            "Environment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Control",
            "Optimization"
        ]
    },
    {
        "dcterms:creator": [
            "R. Munos",
            "T. Stepleton",
            "A. Harutyunyan",
            "M. G. Bellemare"
        ],
        "dcterms:description": "Retrace(λ) is a multi-step off-policy reinforcement learning algorithm that uses eligibility traces to improve learning efficiency.",
        "dcterms:title": "Retrace(λ)",
        "dcterms:issued": "2016",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1606.02647",
        "dcat:theme": [
            "Reinforcement Learning",
            "Off-Policy Learning"
        ],
        "dcat:keyword": [
            "Retrace",
            "Off-Policy",
            "Multi-Step Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1606.02647",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "V. Mnih",
            "K. Kavukcuoglu",
            "D. Silver",
            "A. A. Rusu",
            "J. Veness",
            "M. G. Bellemare",
            "D. Hassabis"
        ],
        "dcterms:description": "DQN (Deep Q-Network) is an architecture that combines Q-learning with deep neural networks to approximate the action-value function.",
        "dcterms:title": "DQN Architecture",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "http://dx.doi.org/10.1038/nature14236",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Deep Q-Network",
            "Neural Networks",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "http://dx.doi.org/10.1038/nature14236",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Horgan",
            "J. Quan",
            "D. Budden",
            "G. Barth-Maron",
            "M. Hessel",
            "H. van Hasselt",
            "D. Silver"
        ],
        "dcterms:description": "Ape-X is a distributed reinforcement learning architecture that utilizes prioritized experience replay to improve sample efficiency.",
        "dcterms:title": "Ape-X",
        "dcterms:issued": "2018",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1803.00933",
        "dcat:theme": [
            "Reinforcement Learning",
            "Distributed Learning"
        ],
        "dcat:keyword": [
            "Ape-X",
            "Distributed Learning",
            "Experience Replay"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1803.00933",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "M. Hessel",
            "J. Modayil",
            "H. van Hasselt",
            "T. Schaul",
            "G. Ostrovski",
            "W. Dabney",
            "D. Silver"
        ],
        "dcterms:description": "Rainbow is a deep reinforcement learning algorithm that combines several improvements to the DQN architecture.",
        "dcterms:title": "Rainbow",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1710.02298",
        "dcat:theme": [
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "dcat:keyword": [
            "Rainbow",
            "DQN",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1710.02298",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "D. Precup",
            "R. S. Sutton",
            "S. Singh"
        ],
        "dcterms:description": "Tree Backup is an off-policy reinforcement learning algorithm that uses eligibility traces to evaluate policies.",
        "dcterms:title": "Tree Backup",
        "dcterms:issued": "2000",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Off-Policy Learning"
        ],
        "dcat:keyword": [
            "Tree Backup",
            "Eligibility Traces",
            "Off-Policy Learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "C. J. C. H. Watkins"
        ],
        "dcterms:description": "Q-learning is a model-free reinforcement learning algorithm that learns the value of actions in states.",
        "dcterms:title": "Q-learning",
        "dcterms:issued": "1989",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Model-Free Learning"
        ],
        "dcat:keyword": [
            "Q-learning",
            "Reinforcement Learning",
            "Value Function"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "G. A. Rummery"
        ],
        "dcterms:description": "Sarsa is an on-policy reinforcement learning algorithm that updates the action-value function based on the action taken.",
        "dcterms:title": "Sarsa",
        "dcterms:issued": "1995",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "On-Policy Learning"
        ],
        "dcat:keyword": [
            "Sarsa",
            "Reinforcement Learning",
            "On-Policy"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    },
    {
        "dcterms:creator": [
            "K. D. Asis",
            "J. F. Hernandez-Garcia",
            "G. Z. Holland",
            "R. S. Sutton"
        ],
        "dcterms:description": "Q(σ) is a multi-step reinforcement learning algorithm that combines elements of Sarsa and Tree Backup.",
        "dcterms:title": "Q(σ)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "arXiv:1703.01327",
        "dcat:theme": [
            "Reinforcement Learning",
            "Multi-Step Learning"
        ],
        "dcat:keyword": [
            "Q(σ)",
            "Multi-Step Learning",
            "Reinforcement Learning"
        ],
        "dcat:landingPage": "http://arxiv.org/abs/1703.01327",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": [
            "Reinforcement Learning"
        ]
    }
]