[
    {
        "dcterms:creator": [
            "R. Bellman"
        ],
        "dcterms:description": "A general framework to model sequential decision-making systems, where an agent chooses actions based on a policy to maximize long-term rewards.",
        "dcterms:title": "Markov Decision Processes (MDP)",
        "dcterms:issued": "1954",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Decision Making"
        ],
        "dcat:keyword": [
            "Sequential decision-making",
            "Reward maximization",
            "Dynamic programming"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. M. Kakade"
        ],
        "dcterms:description": "A policy gradient method that uses natural gradient descent, which is a smoother approximation of policy iteration.",
        "dcterms:title": "Natural Policy Gradient (NPG)",
        "dcterms:issued": "2001",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Policy gradient",
            "Natural gradient descent",
            "Smoother approximation"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "M. G. Azar",
            "V. GÃ³mez",
            "H. J. Kappen"
        ],
        "dcterms:description": "A variant of policy iteration that characterizes its convergence bounds.",
        "dcterms:title": "Dynamic Policy Programming",
        "dcterms:issued": "2012",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Dynamic Programming"
        ],
        "dcat:keyword": [
            "Policy iteration",
            "Convergence bounds"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "S. Bhatnagar",
            "R. S. Sutton",
            "M. Ghavamzadeh",
            "M. Lee"
        ],
        "dcterms:description": "Natural actor-critic algorithms that combine the benefits of actor-critic methods with natural gradient techniques.",
        "dcterms:title": "Natural Actor-Critic",
        "dcterms:issued": "2009",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Actor-Critic Methods"
        ],
        "dcat:keyword": [
            "Actor-critic",
            "Natural gradient"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Schulman",
            "S. Levine",
            "P. Abbeel",
            "M. Jordan",
            "P. Moritz"
        ],
        "dcterms:description": "An optimization algorithm for policy gradient methods that uses trust regions to ensure stable updates.",
        "dcterms:title": "Trust Region Policy Optimization (TRPO)",
        "dcterms:issued": "2015",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Trust region",
            "Policy optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Schulman",
            "F. Wolski",
            "P. Dhariwal",
            "A. Radford",
            "O. Klimov"
        ],
        "dcterms:description": "An algorithm that improves upon TRPO by using a simpler and more efficient approach to policy optimization.",
        "dcterms:title": "Proximal Policy Optimization (PPO)",
        "dcterms:issued": "2017",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Optimization"
        ],
        "dcat:keyword": [
            "Proximal optimization",
            "Policy optimization"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    },
    {
        "dcterms:creator": [
            "J. Zhang",
            "A. Koppel",
            "A. S. Bedi",
            "C. Szepesvari",
            "M. Wang"
        ],
        "dcterms:description": "A method for reinforcement learning that incorporates variational principles into policy gradient methods.",
        "dcterms:title": "Variational Policy Gradient Method",
        "dcterms:issued": "2020",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Reinforcement Learning",
            "Variational Methods"
        ],
        "dcat:keyword": [
            "Variational methods",
            "Policy gradient"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "",
        "mls:task": []
    }
]