To extract datasets from the research paper titled "On Quantitative Evaluations of Counterfactuals" by Frederik Hvilshøj et al., I will follow a systematic approach to identify and document the datasets mentioned in the text.

First, I will read through the **abstract, introduction, and methodology sections** to see if any datasets are explicitly mentioned. The abstract discusses the evaluation of visual counterfactual examples but does not specify any datasets. Therefore, I will proceed to the introduction.

In the **introduction**, the authors mention the need for understanding machine learning models and refer to counterfactual examples but do not name any datasets. I will continue to the **experiments section**, where datasets are more likely to be detailed.

In **section 4 (Experiments)**, the authors describe their experimental setup and mention two datasets:

1. **FakeMNIST**: This dataset is described as an artificial dataset generated from MNIST images, where the digits are shuffled and assigned new random labels. The authors explain how the dataset is structured and its purpose in evaluating counterfactual methods.

2. **CelebA-HQ**: This dataset consists of high-quality images of faces, each associated with multiple binary class labels. The authors use this dataset to evaluate the performance of their counterfactual methods in a more complex setting.

Next, I will check the **References section** to find the full citations for these datasets. The citations are crucial for proper documentation:

- For **FakeMNIST**, the citation is:
  > Frederik Hvilshøj, Alexandros Iosiﬁdis, and Ira Assent. *ECINN: Efficient Counterfactuals from Invertible Neural Networks*. CoRR, abs/2103.13701, 2021. URL https://arxiv.org/abs/2103.13701.

- For **CelebA-HQ**, the citation is:
  > Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. *Progressive Growing of GANs for Improved Quality, Stability, and Variation*. In ICLR, 2018.

Now that I have identified the datasets and their citations, I will compile this information into a structured format for further use.

In summary, I have extracted the following datasets from the paper:

1. **FakeMNIST**: An artificial dataset generated from MNIST images.
   - Citation: Frederik Hvilshøj, Alexandros Iosiﬁdis, and Ira Assent. *ECINN: Efficient Counterfactuals from Invertible Neural Networks*. CoRR, abs/2103.13701, 2021. URL https://arxiv.org/abs/2103.13701.

2. **CelebA-HQ**: A dataset of high-quality face images with multiple binary class labels.
   - Citation: Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. *Progressive Growing of GANs for Improved Quality, Stability, and Variation*. In ICLR, 2018.

This structured approach ensures that I have accurately captured the necessary details about the datasets used in the research paper.