[
    {
        "dcterms:creator": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "dcterms:description": "A dataset consisting of instruction-following prompts with responses sampled from multiple foundation models, including GPT-4, GPT-3.5, and OPT-IML, where responses are ranked by GPT-4.",
        "dcterms:title": "GPT-4-LLM",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Instruction tuning",
            "Language model",
            "Human feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following",
            "Response ranking"
        ]
    },
    {
        "dcterms:creator": [
            "Zhewei Yao",
            "Reza Yazdani Aminabadi",
            "Olatunji Ruwase",
            "Samyam Rajbhandari",
            "Xiaoxia Wu",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Minjia Zhang",
            "Conglong Li",
            "Connor Holmes",
            "Zhongzhu Zhou",
            "Michael Wyatt",
            "Molly Smith",
            "Lev Kurilenko",
            "Heyang Qin",
            "Masahiro Tanaka",
            "Shuai Che",
            "Shuaiwen Leon Song",
            "Yuxiong He"
        ],
        "dcterms:description": "A collection of various open-source ChatBot or Assistant style datasets, including 'Dahoas/rm-static', 'Dahoas/full-hh-rlhf', 'Dahoas/synthetic-instruct-gptj-pairwise', and 'yitingxie/rlhf-reward-datasets', used for RLHF training.",
        "dcterms:title": "Open-Source Assistant Datasets",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "ChatBot",
            "Assistant",
            "Reinforcement Learning from Human Feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Chatbot training",
            "Response generation"
        ]
    },
    {
        "dcterms:creator": [
            "Nisan Stiennon",
            "Long Ouyang",
            "Jeff Wu",
            "Daniel M. Ziegler",
            "Ryan Lowe",
            "Chelsea Voss",
            "Alec Radford",
            "Dario Amodei",
            "Paul Christiano"
        ],
        "dcterms:description": "A dataset used for training models to summarize text based on human feedback, specifically modified to contain one preference completion pair per prompt.",
        "dcterms:title": "Learning to Summarize (Reddit TL;DR dataset)",
        "dcterms:issued": "2022",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Text Summarization"
        ],
        "dcat:keyword": [
            "Summarization",
            "Human feedback",
            "Preference learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Text summarization"
        ]
    },
    {
        "dcterms:creator": [
            "Nathan Lambert",
            "Lewis Tunstall",
            "Nazneen Rajani",
            "Tristan Thrush"
        ],
        "dcterms:description": "A dataset consisting of questions from StackExchange with multiple completions ranked by user votes, used to train models for question answering.",
        "dcterms:title": "StackExchange",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "dcat:keyword": [
            "StackExchange",
            "Question answering",
            "User ranking"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Question answering"
        ]
    },
    {
        "dcterms:creator": [
            "Zhewei Yao",
            "Reza Yazdani Aminabadi",
            "Olatunji Ruwase",
            "Samyam Rajbhandari",
            "Xiaoxia Wu",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Minjia Zhang",
            "Conglong Li",
            "Connor Holmes",
            "Zhongzhu Zhou",
            "Michael Wyatt",
            "Molly Smith",
            "Lev Kurilenko",
            "Heyang Qin",
            "Masahiro Tanaka",
            "Shuai Che",
            "Shuaiwen Leon Song",
            "Yuxiong He"
        ],
        "dcterms:description": "A dataset used for training reward models in reinforcement learning from human feedback, containing static responses.",
        "dcterms:title": "Dahoas/rm-static",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reward model",
            "Reinforcement learning",
            "Human feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reward modeling"
        ]
    },
    {
        "dcterms:creator": [
            "Zhewei Yao",
            "Reza Yazdani Aminabadi",
            "Olatunji Ruwase",
            "Samyam Rajbhandari",
            "Xiaoxia Wu",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Minjia Zhang",
            "Conglong Li",
            "Connor Holmes",
            "Zhongzhu Zhou",
            "Michael Wyatt",
            "Molly Smith",
            "Lev Kurilenko",
            "Heyang Qin",
            "Masahiro Tanaka",
            "Shuai Che",
            "Shuaiwen Leon Song",
            "Yuxiong He"
        ],
        "dcterms:description": "A dataset containing full human feedback reinforcement learning data, used for training models to align with human preferences.",
        "dcterms:title": "Dahoas/full-hh-rlhf",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Human feedback",
            "Reinforcement learning",
            "Alignment"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Model alignment"
        ]
    },
    {
        "dcterms:creator": [
            "Zhewei Yao",
            "Reza Yazdani Aminabadi",
            "Olatunji Ruwase",
            "Samyam Rajbhandari",
            "Xiaoxia Wu",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Minjia Zhang",
            "Conglong Li",
            "Connor Holmes",
            "Zhongzhu Zhou",
            "Michael Wyatt",
            "Molly Smith",
            "Lev Kurilenko",
            "Heyang Qin",
            "Masahiro Tanaka",
            "Shuai Che",
            "Shuaiwen Leon Song",
            "Yuxiong He"
        ],
        "dcterms:description": "A dataset containing synthetic instruction pairs for training models in reinforcement learning from human feedback.",
        "dcterms:title": "Dahoas/synthetic-instruct-gptj-pairwise",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Synthetic data",
            "Instruction pairs",
            "Reinforcement learning"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Instruction following"
        ]
    },
    {
        "dcterms:creator": [
            "Zhewei Yao",
            "Reza Yazdani Aminabadi",
            "Olatunji Ruwase",
            "Samyam Rajbhandari",
            "Xiaoxia Wu",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Minjia Zhang",
            "Conglong Li",
            "Connor Holmes",
            "Zhongzhu Zhou",
            "Michael Wyatt",
            "Molly Smith",
            "Lev Kurilenko",
            "Heyang Qin",
            "Masahiro Tanaka",
            "Shuai Che",
            "Shuaiwen Leon Song",
            "Yuxiong He"
        ],
        "dcterms:description": "A dataset containing reward datasets for reinforcement learning from human feedback.",
        "dcterms:title": "yitingxie/rlhf-reward-datasets",
        "dcterms:issued": "2023",
        "dcterms:language": "",
        "dcterms:identifier": "",
        "dcat:theme": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "dcat:keyword": [
            "Reward datasets",
            "Reinforcement learning",
            "Human feedback"
        ],
        "dcat:landingPage": "",
        "dcterms:hasVersion": "",
        "dcterms:format": "Text",
        "mls:task": [
            "Reward modeling"
        ]
    }
]